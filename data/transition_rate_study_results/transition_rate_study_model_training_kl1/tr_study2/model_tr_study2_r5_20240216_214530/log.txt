Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r5', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2717387453

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.518112511343032		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.006642411375556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.762377461359293 | validation: 5.031443467607502]
	TIME [epoch: 48 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.415309116450765		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.940261899331548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.177785507891158 | validation: 4.525557680366999]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.170571292950031		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.01333435886954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.091952825909786 | validation: 2.2832286980370413]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1360182508918597		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.272190189229134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.204104220060497 | validation: 1.5128956269418958]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8454071739952493		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8242248434317307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.83481600871349 | validation: 1.379873397607817]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9629845525428151		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6061273265208402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7845559395318276 | validation: 1.3794547563096389]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6414079536879291		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4893326023338618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5653702780108953 | validation: 1.6914069693792935]
	TIME [epoch: 8.74 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6236506674369147		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4374224267013684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.530536547069141 | validation: 2.1698595861452956]
	TIME [epoch: 8.7 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.539379640143939		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3605812900150591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.449980465079499 | validation: 1.1552921101358335]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2005319089226918		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4013666693313724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3009492891270322 | validation: 1.7464602940156921]
	TIME [epoch: 8.7 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1582492486969604		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0174107771296428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0878300129133018 | validation: 1.1143808098486156]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0330491253058427		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0577821033206611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.045415614313252 | validation: 1.3483420772845442]
	TIME [epoch: 8.71 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7958844235219146		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2104089234633906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0031466734926526 | validation: 1.186920545944407]
	TIME [epoch: 8.69 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9575530532859642		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7029960743418557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8302745638139098 | validation: 0.6992265728248956]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9085336971649116		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6917823890330637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8001580430989875 | validation: 1.6018718780910526]
	TIME [epoch: 8.7 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8306788976588508		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7254559939561902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7780674458075205 | validation: 1.0726175691321196]
	TIME [epoch: 8.71 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8025352432101815		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6283651702394473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7154502067248143 | validation: 0.5262039306630053]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5909923423019482		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7342545347530831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6626234385275156 | validation: 0.4927065143763814]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7530796368912742		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6381178530458185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6955987449685465 | validation: 0.5329499698616162]
	TIME [epoch: 8.7 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5939748588036828		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5686293336380706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5813020962208768 | validation: 0.4466142435466786]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9688416058427978		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6836210698777768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8262313378602872 | validation: 0.8463652793594472]
	TIME [epoch: 8.73 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5287640952379568		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5399482431764293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5343561692071931 | validation: 0.5340397576879374]
	TIME [epoch: 8.72 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.666973005180828		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5683865937738288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6176797994773284 | validation: 0.5561239361313708]
	TIME [epoch: 8.69 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6677258256932707		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5985720680108748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6331489468520728 | validation: 0.5148104099897758]
	TIME [epoch: 8.72 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6346536236370165		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5876199110468369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6111367673419266 | validation: 0.5171344015653082]
	TIME [epoch: 8.72 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5680074239848454		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5126623626211535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5403348933029993 | validation: 1.0106540102963248]
	TIME [epoch: 8.7 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.636621270313038		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6421337801253929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6393775252192155 | validation: 0.5112710742112143]
	TIME [epoch: 8.7 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.57023978078238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5599972018345714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5651184913084757 | validation: 1.3124189758064122]
	TIME [epoch: 8.71 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6913268241470217		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5903937353165162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6408602797317691 | validation: 0.8881545189009674]
	TIME [epoch: 8.72 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5350665992203807		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6326203843837315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5838434918020561 | validation: 0.573866116511603]
	TIME [epoch: 8.7 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5601133210529549		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5543516112605539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5572324661567543 | validation: 0.6321425343566504]
	TIME [epoch: 8.7 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4982039415667945		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5157268972897466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5069654194282707 | validation: 0.5039603163675984]
	TIME [epoch: 8.71 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6201066837830866		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5032343974133743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5616705405982305 | validation: 0.7391949076356253]
	TIME [epoch: 8.73 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5024176433475398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.566786343273106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.534601993310323 | validation: 0.4533935714041448]
	TIME [epoch: 8.71 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5653769747748366		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49649196825014696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5309344715124918 | validation: 1.0924334594632659]
	TIME [epoch: 8.71 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5809907127991637		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5769174724251664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.578954092612165 | validation: 0.4301470663855199]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6037739792367851		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5068897543258004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5553318667812926 | validation: 0.4753604169800727]
	TIME [epoch: 8.68 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5303688712227179		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5723262506016514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5513475609121847 | validation: 0.8248489298223636]
	TIME [epoch: 8.68 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5955669810703136		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5767164576535209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5861417193619172 | validation: 0.4731757428243829]
	TIME [epoch: 8.65 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.460875516713194		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5340076281047249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4974415724089595 | validation: 0.5127638165997895]
	TIME [epoch: 8.65 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5734788151774471		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46175658233969263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5176176987585699 | validation: 0.8236754291349964]
	TIME [epoch: 8.65 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7316173860416003		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5540015972208423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6428094916312215 | validation: 0.6413241226814821]
	TIME [epoch: 8.67 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6545653559063934		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6099691446746449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6322672502905191 | validation: 0.4058628314519846]
	TIME [epoch: 8.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5150903416766266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6470546771625394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.581072509419583 | validation: 0.4609134012727783]
	TIME [epoch: 8.68 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5654795729575864		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5941645167293619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5798220448434741 | validation: 0.9281443645155112]
	TIME [epoch: 8.66 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5020866113629731		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7471730075076227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6246298094352979 | validation: 0.4515495763325764]
	TIME [epoch: 8.67 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5770164984581069		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46057707135817844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5187967849081427 | validation: 0.9872292492005224]
	TIME [epoch: 8.68 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4507900874121982		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5159980317813507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48339405959677445 | validation: 0.639870143726271]
	TIME [epoch: 8.66 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45010163073934606		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4532112371787947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4516564339590704 | validation: 0.6760396538679678]
	TIME [epoch: 8.67 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49898955231772046		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.686423160809998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5927063565638593 | validation: 1.4547354033965385]
	TIME [epoch: 8.65 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6073598826030148		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4776952785972455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.54252758060013 | validation: 0.35679699257637976]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44931469800018886		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45950199254203516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45440834527111207 | validation: 0.34858784625823025]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6569421510382651		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4182184201110065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5375802855746358 | validation: 0.5769562400654207]
	TIME [epoch: 8.65 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4709086968300282		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6618010246399038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5663548607349661 | validation: 0.4308084420028846]
	TIME [epoch: 8.67 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5224018708158804		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5762576771589873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.549329773987434 | validation: 0.5632394118891904]
	TIME [epoch: 8.68 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4932320575429644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4199018021888368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4565669298659006 | validation: 0.421330513203352]
	TIME [epoch: 8.68 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47372839579785475		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5847950730387866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5292617344183207 | validation: 1.1453389818107989]
	TIME [epoch: 8.67 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5315236381021391		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5041230226281181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5178233303651286 | validation: 0.6138258648405301]
	TIME [epoch: 8.66 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.541739649922833		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47837639895766176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5100580244402474 | validation: 0.4702013110355625]
	TIME [epoch: 8.67 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.408112437664507		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4840098477838316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4460611427241692 | validation: 0.406066524086439]
	TIME [epoch: 8.68 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42295275502006796		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42765003881833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4253013969191989 | validation: 0.39521900149009737]
	TIME [epoch: 8.67 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4255308635711502		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4880485760788441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45678971982499716 | validation: 0.38810876751431567]
	TIME [epoch: 8.68 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46929578301154995		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4470164791884293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45815613109998965 | validation: 0.4265916595910124]
	TIME [epoch: 8.66 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4494248247819502		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.40915421053352913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4292895176577397 | validation: 0.36392040863282865]
	TIME [epoch: 8.71 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4183953705223967		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47229116900769397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44534326976504535 | validation: 0.40566402318349404]
	TIME [epoch: 8.66 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3789453882015299		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4421569676785082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41055117794001905 | validation: 0.4082557003630047]
	TIME [epoch: 8.69 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4332237556298663		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3914835260421808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4123536408360236 | validation: 0.516556375770129]
	TIME [epoch: 8.67 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4481097880281286		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5016559613340734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4748828746811011 | validation: 0.35526240929062025]
	TIME [epoch: 8.68 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3805902031634042		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42122485100582246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4009075270846133 | validation: 0.3734491268111805]
	TIME [epoch: 8.68 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4507969539581323		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4133892983882192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4320931261731757 | validation: 0.6917858749692724]
	TIME [epoch: 8.67 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44738169359479113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4412787084128105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4443302010038009 | validation: 0.5633690141004796]
	TIME [epoch: 8.66 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3851442130216288		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3978355535576524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3914898832896405 | validation: 0.5039082508713006]
	TIME [epoch: 8.66 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4273220814475371		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38642974943305297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.406875915440295 | validation: 0.33165476729976756]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4097121812986278		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3760079905567104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3928600859276691 | validation: 1.0781569067642665]
	TIME [epoch: 8.67 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40574402500438544		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4149225750591462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4103333000317658 | validation: 0.5085226363243464]
	TIME [epoch: 8.68 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4871701936280604		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3551388475037837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4211545205659221 | validation: 0.34307402332084713]
	TIME [epoch: 8.66 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34920773927940985		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36444582169056455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3568267804849872 | validation: 1.0110642959226377]
	TIME [epoch: 8.69 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4451377255939561		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48337463205615866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4642561788250573 | validation: 0.35715366745308963]
	TIME [epoch: 8.67 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3520027656042058		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36894270263079043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36047273411749814 | validation: 0.3108220168717908]
	TIME [epoch: 8.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4161345877482141		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4440727275278077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4301036576380109 | validation: 0.4217822382416898]
	TIME [epoch: 8.68 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35198347649615397		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3384794490005538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3452314627483539 | validation: 0.3514633594268397]
	TIME [epoch: 8.67 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40179928439678914		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38600635880707934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39390282160193424 | validation: 0.7799978953508763]
	TIME [epoch: 8.69 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.595303798371503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3897558171981941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49252980778484856 | validation: 0.3907852217351724]
	TIME [epoch: 8.67 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40224158475987937		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.36035958749353025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3813005861267048 | validation: 0.437405621340609]
	TIME [epoch: 8.67 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34321185433642254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3580790694952583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3506454619158404 | validation: 0.3397294221792803]
	TIME [epoch: 8.67 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33315841520771877		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38595095126012174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35955468323392026 | validation: 0.7740640220407191]
	TIME [epoch: 8.69 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.457624854048687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3616160743575644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40962046420312576 | validation: 0.3352129593616132]
	TIME [epoch: 8.67 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38563636534607026		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4162029714502256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40091966839814785 | validation: 0.4985016328192979]
	TIME [epoch: 8.67 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37366235386721314		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3564213984445014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3650418761558572 | validation: 0.313115724418216]
	TIME [epoch: 8.67 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37063420521656143		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31626361922558244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34344891222107193 | validation: 0.34874827164494454]
	TIME [epoch: 8.69 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3636845712924027		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2960392748327832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32986192306259304 | validation: 0.4063724156301626]
	TIME [epoch: 8.68 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3490971033452467		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28639834668971825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31774772501748244 | validation: 0.39362212725158763]
	TIME [epoch: 8.66 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42795962602948023		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.37919816826219255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40357889714583645 | validation: 0.3964471188572673]
	TIME [epoch: 8.66 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36145239303643445		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3135471559904178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33749977451342617 | validation: 0.42978763493188815]
	TIME [epoch: 8.68 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36458166699680367		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3343917862988746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34948672664783914 | validation: 0.48270920565399594]
	TIME [epoch: 8.68 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34181985197908726		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2951987376049173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31850929479200235 | validation: 0.34924706592512933]
	TIME [epoch: 8.67 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31325718442422873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.357659839192742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33545851180848535 | validation: 0.27719090232651067]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2942893166581509		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27313823905377654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28371377785596363 | validation: 0.23549925236712474]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3290110889415243		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31876650043944676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3238887946904855 | validation: 0.2744685735423835]
	TIME [epoch: 8.72 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3271952236380703		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3608691766459512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34403220014201075 | validation: 0.26307285965635363]
	TIME [epoch: 8.68 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28675303319675116		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3291116878107954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30793236050377326 | validation: 0.48100672329653466]
	TIME [epoch: 8.67 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35915057686860186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30772828040142397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3334394286350129 | validation: 0.525980422223912]
	TIME [epoch: 8.67 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30395346250087873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.31903454193977143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3114940022203251 | validation: 0.347286947882776]
	TIME [epoch: 8.67 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3519508615151171		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2671721840864241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30956152280077065 | validation: 0.2429568310113148]
	TIME [epoch: 8.69 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4149834549326645		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3617826483458712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3883830516392678 | validation: 0.4782773673873954]
	TIME [epoch: 8.68 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3208307913937023		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3149612955038096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3178960434487559 | validation: 0.5138152288493099]
	TIME [epoch: 8.68 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49492713751567574		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3712492121031664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.433088174809421 | validation: 0.5081769568250926]
	TIME [epoch: 8.68 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31742723618594293		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28243574530907434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29993149074750863 | validation: 0.5242171094724364]
	TIME [epoch: 8.69 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3579481106801482		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24871072959276752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3033294201364579 | validation: 0.23853789733693215]
	TIME [epoch: 8.67 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27161638464269233		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3344483140879383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30303234936531526 | validation: 0.21779399881976946]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34664419104627653		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3013173211200879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32398075608318216 | validation: 0.406258463366828]
	TIME [epoch: 8.68 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25323834559529096		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3021455194959298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2776919325456104 | validation: 0.2179676665327232]
	TIME [epoch: 8.7 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2414704008620569		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2504933890737877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2459818949679223 | validation: 0.2369910082790108]
	TIME [epoch: 8.69 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3281378413361601		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.344712780252835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3364253107944975 | validation: 0.2670599168549036]
	TIME [epoch: 8.67 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26060224665201837		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2464702833433551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2535362649976867 | validation: 0.3996903847236224]
	TIME [epoch: 8.66 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34402522009335534		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27045164351792755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3072384318056414 | validation: 0.39752906345157946]
	TIME [epoch: 8.68 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28239205708783327		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2828255339816247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28260879553472906 | validation: 0.30412106669098027]
	TIME [epoch: 8.68 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32011572041278885		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.28502496655890164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3025703434858452 | validation: 0.3195941193925913]
	TIME [epoch: 8.66 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23850244798783357		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25008439727970694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24429342263377016 | validation: 0.22789714381342718]
	TIME [epoch: 8.68 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25050260416992903		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2234703632394503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2369864837046897 | validation: 0.45480295935681814]
	TIME [epoch: 8.68 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2518007177629017		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24291858722109164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24735965249199668 | validation: 0.18526313316403176]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2808422883800687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22165614727188854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2512492178259786 | validation: 0.2853934403329603]
	TIME [epoch: 8.68 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27904611996571893		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21409644737564554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24657128367068215 | validation: 0.12890568200222302]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2582095425404351		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24144788445594428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24982871349818972 | validation: 0.17410801687317076]
	TIME [epoch: 8.65 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46437803988075954		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.397743465837414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4310607528590868 | validation: 0.14678810425453914]
	TIME [epoch: 8.69 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3306875408170338		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2315975829510979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28114256188406583 | validation: 0.12296470692354967]
	TIME [epoch: 8.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18389057116547644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24657584886861814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21523321001704726 | validation: 0.25255560953408746]
	TIME [epoch: 8.69 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20271564832604563		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23797241499430127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2203440316601734 | validation: 0.13533159416338647]
	TIME [epoch: 8.68 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2017426670409836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1941454924925931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19794407976678832 | validation: 0.08771602096296309]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16161877007198283		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25008388891653033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20585132949425655 | validation: 0.15842894913169964]
	TIME [epoch: 8.71 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15265801055933798		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.29070783274585843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22168292165259823 | validation: 0.2038766583960414]
	TIME [epoch: 8.68 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19136071908176727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2554917401511439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22342622961645558 | validation: 0.08672988738207948]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19365386991396377		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19617196781174523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19491291886285453 | validation: 0.2027414574027248]
	TIME [epoch: 8.7 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20605519769042696		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.278504774522523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24227998610647497 | validation: 0.34138641058485314]
	TIME [epoch: 8.74 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24872887900184612		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21146907227478334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23009897563831472 | validation: 0.14157494564895345]
	TIME [epoch: 8.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2310300174494369		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20518239414993852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2181062057996877 | validation: 0.1402210778826397]
	TIME [epoch: 8.72 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2940193818467921		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.207543438840114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2507814103434531 | validation: 0.268167993955792]
	TIME [epoch: 8.7 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22686856238789285		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19354085140315747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21020470689552515 | validation: 0.1502109141648748]
	TIME [epoch: 8.72 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22171803527917683		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22375882556419344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22273843042168512 | validation: 0.16639023462012606]
	TIME [epoch: 8.71 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18798863013382067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16504805693749652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1765183435356586 | validation: 0.25357688960465413]
	TIME [epoch: 8.72 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20260487869071397		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18041135833318678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1915081185119504 | validation: 0.12673223011280052]
	TIME [epoch: 8.71 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16690832848001763		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16115492698289047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16403162773145402 | validation: 0.09872529581277127]
	TIME [epoch: 8.72 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16727047377168264		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1888160922267629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17804328299922279 | validation: 0.22606114559901397]
	TIME [epoch: 8.71 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20273208603070852		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.246748370296328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2247402281635183 | validation: 0.19460463353212418]
	TIME [epoch: 8.69 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19070370428632658		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15779338823829234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1742485462623094 | validation: 0.271770236240655]
	TIME [epoch: 8.71 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16493117753438216		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21952938007935474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19223027880686844 | validation: 0.142803904303594]
	TIME [epoch: 8.71 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14697918342485228		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16891329848822365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15794624095653795 | validation: 0.10952891109687828]
	TIME [epoch: 8.99 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24175833128331387		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18682191452699565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21429012290515476 | validation: 0.13279383353351773]
	TIME [epoch: 8.71 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16377620399018455		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15968765550544722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16173192974781586 | validation: 0.12800876352869783]
	TIME [epoch: 8.7 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1752683770779532		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1381985361828844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15673345663041882 | validation: 0.16802966015288492]
	TIME [epoch: 8.69 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1843081400460521		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1497539214770056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16703103076152884 | validation: 0.1454345561070997]
	TIME [epoch: 8.72 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15339704927654313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1395774292205245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14648723924853382 | validation: 0.17129117417352022]
	TIME [epoch: 8.71 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12706104563092965		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12043714341043318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12374909452068146 | validation: 0.18066415872036823]
	TIME [epoch: 8.69 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19556107086485214		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15358475180469333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17457291133477276 | validation: 0.13442723379333996]
	TIME [epoch: 8.7 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22160229792333644		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3143768879086261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2679895929159813 | validation: 0.1479624144501897]
	TIME [epoch: 8.72 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16736443311552443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17004406295164706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16870424803358577 | validation: 0.12607781172685362]
	TIME [epoch: 8.72 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17333755285886743		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27456032519907175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2239489390289696 | validation: 0.0986870868837766]
	TIME [epoch: 8.71 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20796088432611265		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14967434731754933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.178817615821831 | validation: 0.18335225103222524]
	TIME [epoch: 8.71 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15125812163833632		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1535679475872886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15241303461281241 | validation: 0.08633638204838637]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15846985016499474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21670409345112995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18758697180806233 | validation: 1.2026780899497016]
	TIME [epoch: 8.74 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35131723990542907		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21094416190698312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2811307009062061 | validation: 0.1708150800661]
	TIME [epoch: 8.74 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20038081243698064		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18515612206819673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19276846725258873 | validation: 0.24588546732491018]
	TIME [epoch: 8.7 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16892255101877302		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11541370931322051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14216813016599678 | validation: 0.2185229610698485]
	TIME [epoch: 8.71 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15831642392843717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1252877567159247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.141802090322181 | validation: 0.18653348422724578]
	TIME [epoch: 8.71 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16604400935343336		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14591597650769852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15597999293056597 | validation: 0.09870890478800964]
	TIME [epoch: 8.72 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13359569792290726		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18866343299555466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16112956545923096 | validation: 0.09739747182053485]
	TIME [epoch: 8.71 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15533302519059355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2028420777255288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1790875514580612 | validation: 0.11681214119577098]
	TIME [epoch: 8.71 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21297326229777108		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15180831803278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18239079016527554 | validation: 0.08861839913778935]
	TIME [epoch: 8.73 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12868810152354618		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1355897947932077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13213894815837693 | validation: 0.07750499819398815]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14827253805733215		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14789397682010672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14808325743871945 | validation: 0.061123250161649684]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17717339714379854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18293999685298437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18005669699839147 | validation: 0.22460099277618595]
	TIME [epoch: 8.72 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20178884646939763		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12376790487323748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16277837567131753 | validation: 0.034318092913525784]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2780055128531611		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11957339494671389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1987894538999375 | validation: 0.16663254855199894]
	TIME [epoch: 8.71 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15590324187108326		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1606843809124238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15829381139175352 | validation: 0.13092210008304053]
	TIME [epoch: 8.76 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12854690075003392		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19578359352882455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16216524713942926 | validation: 0.2368689285799081]
	TIME [epoch: 8.75 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17753299172567344		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20158501248616362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1895590021059185 | validation: 0.0746946594270021]
	TIME [epoch: 8.73 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22944157761895528		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18586155713599029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20765156737747276 | validation: 0.18958213299156362]
	TIME [epoch: 8.74 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1923374082458995		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24573929822369797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21903835323479875 | validation: 0.19033500535092024]
	TIME [epoch: 8.74 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2198944390961271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15133355101871254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18561399505741982 | validation: 0.11702932017536999]
	TIME [epoch: 8.74 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13509754048924222		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11808836159758762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12659295104341492 | validation: 0.151906704979874]
	TIME [epoch: 8.73 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12189223032410532		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18279600102873803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15234411567642164 | validation: 0.1355567613330695]
	TIME [epoch: 8.71 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1514399494459112		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15088469375889774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15116232160240445 | validation: 0.2692293188237286]
	TIME [epoch: 8.73 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1492068675724015		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.129269211229446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13923803940092377 | validation: 0.0809417886024232]
	TIME [epoch: 8.72 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13281877871047204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16817300979210673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15049589425128937 | validation: 0.15658551740195592]
	TIME [epoch: 8.74 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12086435324501561		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1356335764397757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12824896484239565 | validation: 0.14051838509865988]
	TIME [epoch: 8.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23937224415802955		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16632350571245133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20284787493524048 | validation: 0.4135452677297594]
	TIME [epoch: 8.73 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19763107461611068		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1515573706548335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17459422263547214 | validation: 0.1884029645124974]
	TIME [epoch: 8.76 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1150662600677014		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1167649953559988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11591562771185011 | validation: 0.22324740956519368]
	TIME [epoch: 8.74 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13971011345531736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16620274215587078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15295642780559407 | validation: 0.07190800744200165]
	TIME [epoch: 8.73 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13094137617059595		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1574447419417236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14419305905615978 | validation: 0.09597490952067615]
	TIME [epoch: 8.73 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1205254332500341		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16146354667035712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1409944899601956 | validation: 0.08556627665422875]
	TIME [epoch: 8.72 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16891845273109515		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11888261494468925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1439005338378922 | validation: 0.1484190610096891]
	TIME [epoch: 8.72 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10877271212325815		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16670848190759308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1377405970154256 | validation: 0.10125311571366798]
	TIME [epoch: 8.71 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10468062774039569		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12165660918381219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11316861846210391 | validation: 0.1117032850950465]
	TIME [epoch: 8.75 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1497946813068357		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10974642219863076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12977055175273322 | validation: 0.17756201620109247]
	TIME [epoch: 8.71 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13233687837480945		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17777492928991212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15505590383236076 | validation: 0.16844461256072293]
	TIME [epoch: 8.76 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11770327709285869		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10177564524473932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10973946116879903 | validation: 0.1963489581061465]
	TIME [epoch: 8.72 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09362293592160909		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.104574474825515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09909870537356204 | validation: 0.13253391532639436]
	TIME [epoch: 8.68 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11873132245773929		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14334684783619606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13103908514696766 | validation: 0.10043166250023736]
	TIME [epoch: 8.71 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15252954601906282		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08641391745812929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11947173173859602 | validation: 0.0933093130345827]
	TIME [epoch: 8.71 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11489202314759747		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09856722012623882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10672962163691815 | validation: 0.16277304146562932]
	TIME [epoch: 8.75 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2099291221168545		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09152390237692205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1507265122468882 | validation: 0.12049647365951695]
	TIME [epoch: 8.71 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11608748043467292		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16163250190518824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13885999116993059 | validation: 0.3177479712636724]
	TIME [epoch: 8.73 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1646320912582269		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17428404439432976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1694580678262783 | validation: 0.180304058354136]
	TIME [epoch: 8.73 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16865076091022158		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10168641601159764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13516858846090962 | validation: 0.09973962881852952]
	TIME [epoch: 8.73 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1025489727976874		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13223321972369936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1173910962606934 | validation: 0.10015591753775274]
	TIME [epoch: 8.73 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12301452719236206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14234603565136522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13268028142186367 | validation: 0.3822544171143174]
	TIME [epoch: 8.72 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1972278626195923		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11148270610831597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15435528436395413 | validation: 0.09889777500462255]
	TIME [epoch: 8.71 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12743005737705043		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12199153371796656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1247107955475085 | validation: 0.18408717968608465]
	TIME [epoch: 8.74 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16065386647780944		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14202109666631907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15133748157206428 | validation: 0.3442449078120541]
	TIME [epoch: 8.72 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1422909327281638		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2119862115117121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17713857211993794 | validation: 0.1291339928867813]
	TIME [epoch: 8.73 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2209372911049406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1777605696397321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1993489303723363 | validation: 0.0645996829570151]
	TIME [epoch: 8.72 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10484974946249073		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1868639540390504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14585685175077057 | validation: 0.1874859168083717]
	TIME [epoch: 8.74 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17692783606130164		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15516828037967642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16604805822048901 | validation: 0.2606744452318327]
	TIME [epoch: 8.74 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3189096068851051		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12826823722168376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22358892205339442 | validation: 0.14359365584531736]
	TIME [epoch: 8.71 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.161475817991948		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1580597656838191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15976779183788353 | validation: 0.170391611091706]
	TIME [epoch: 8.71 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11213795888506395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09678124807346322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1044596034792636 | validation: 0.1010681954641697]
	TIME [epoch: 8.71 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15953818425414687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.086237761136465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12288797269530594 | validation: 0.1994004670472299]
	TIME [epoch: 8.75 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20117886957542783		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10244712732581981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15181299845062385 | validation: 0.10798838172976348]
	TIME [epoch: 8.72 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08596678938334701		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21462685340846335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15029682139590517 | validation: 0.2382638666657308]
	TIME [epoch: 8.72 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14519042424406697		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2822596569509829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21372504059752492 | validation: 0.22213493924935218]
	TIME [epoch: 8.71 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16839390652449712		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14963233951606858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1590131230202829 | validation: 0.20119755134310327]
	TIME [epoch: 8.75 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15417102517801465		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12908649463846644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1416287599082406 | validation: 0.09270287508615402]
	TIME [epoch: 8.7 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11518459898852054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09476652502128097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10497556200490074 | validation: 0.059625406065337505]
	TIME [epoch: 8.7 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16264416665137207		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18458019277218748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17361217971177978 | validation: 0.10404887696007914]
	TIME [epoch: 8.71 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12353600858701345		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1749989851413834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1492674968641984 | validation: 0.12391063185558741]
	TIME [epoch: 8.74 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10168820670833982		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14719013763897384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12443917217365685 | validation: 0.16393092189747044]
	TIME [epoch: 8.72 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13215161129577968		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.154522610094656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14333711069521787 | validation: 0.12291629210036814]
	TIME [epoch: 8.74 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13917444807256274		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1804441071456585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1598092776091106 | validation: 0.16338882518634845]
	TIME [epoch: 8.72 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1641954936471206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1376977545018114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15094662407446602 | validation: 0.11059071280983322]
	TIME [epoch: 8.74 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1590434927110544		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15456835572160416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15680592421632925 | validation: 0.14697466107211146]
	TIME [epoch: 8.75 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11394740751237172		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09004977819478102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10199859285357633 | validation: 0.4555297161207599]
	TIME [epoch: 8.74 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17394452920177125		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15710472806752857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1655246286346499 | validation: 0.16132375882282401]
	TIME [epoch: 8.71 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16272808917742157		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.23039812672480964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1965631079511156 | validation: 0.19303182707078348]
	TIME [epoch: 8.72 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1800633705464309		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2015779106331625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19082064058979673 | validation: 0.192832505770964]
	TIME [epoch: 8.74 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2195579354780685		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14993400003921303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18474596775864077 | validation: 0.11780003140390066]
	TIME [epoch: 8.72 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1454244482147727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12869871349479084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13706158085478176 | validation: 0.09924992400556029]
	TIME [epoch: 8.71 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1366922968013226		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10010395614813147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11839812647472703 | validation: 0.07794496873189245]
	TIME [epoch: 8.75 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09443121453361629		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11856275774899105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10649698614130369 | validation: 0.1896104527511109]
	TIME [epoch: 8.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17285555240477082		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12268419566223579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14776987403350333 | validation: 0.1969360964007832]
	TIME [epoch: 8.72 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21567690839914744		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1805314745941064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19810419149662692 | validation: 0.11186076235059823]
	TIME [epoch: 8.69 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11156547709208994		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16426441899437455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13791494804323226 | validation: 0.18606002261534826]
	TIME [epoch: 8.71 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14548421449579912		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1559393647225394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15071178960916926 | validation: 0.19294162666560674]
	TIME [epoch: 8.7 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3097781133009522		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4841348230829192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39695646819193575 | validation: 0.2623514469127274]
	TIME [epoch: 8.72 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3508297530115391		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2846728798731239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31775131644233157 | validation: 0.12433140054787288]
	TIME [epoch: 8.73 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13883727070740878		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10780782031159111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12332254550949995 | validation: 0.05986677448306329]
	TIME [epoch: 8.7 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13690605333559416		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10815894068835297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12253249701197355 | validation: 0.18039773958153266]
	TIME [epoch: 8.7 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09606177476881869		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13270814664523703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11438496070702786 | validation: 0.42647115363532406]
	TIME [epoch: 8.72 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1959158641610682		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2629412280663145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2294285461136914 | validation: 0.2955825143164061]
	TIME [epoch: 8.72 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2188942864061246		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.120646432587031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16977035949657782 | validation: 0.14280548396222054]
	TIME [epoch: 8.72 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15920100732025583		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15061980049074913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1549104039055025 | validation: 0.13227290661533858]
	TIME [epoch: 8.72 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16876551224108347		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10594456288509817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1373550375630908 | validation: 0.13249250478520314]
	TIME [epoch: 8.74 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09818062390459428		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15036984774672513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12427523582565972 | validation: 0.2533574273616171]
	TIME [epoch: 8.95 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18710526171165912		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16513351100612866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17611938635889388 | validation: 0.09784417040611101]
	TIME [epoch: 8.71 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09047110378471555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12566773264283362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10806941821377461 | validation: 0.09517610444685674]
	TIME [epoch: 8.69 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.101939547931392		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21574303575913628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15884129184526413 | validation: 0.18247497977525406]
	TIME [epoch: 8.72 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1319576747118833		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08143288824102966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10669528147645646 | validation: 0.08354374929088909]
	TIME [epoch: 8.73 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09737309505167643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08252189287890024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08994749396528835 | validation: 0.07695933046208228]
	TIME [epoch: 8.74 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11063307475826267		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13598277551503352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1233079251366481 | validation: 0.17285110830313627]
	TIME [epoch: 8.74 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1484789674757418		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16463947471519588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15655922109546885 | validation: 0.22338673508269258]
	TIME [epoch: 8.72 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2365624974003079		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1216446275298058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17910356246505688 | validation: 0.22988156546769348]
	TIME [epoch: 8.72 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1125327417454681		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1649379449871003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13873534336628418 | validation: 0.19223308786550578]
	TIME [epoch: 8.73 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11249008673166153		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12886839807522774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12067924240344463 | validation: 0.11592965245533385]
	TIME [epoch: 8.72 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11493599985375427		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13116828844933878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12305214415154656 | validation: 0.1586157743808146]
	TIME [epoch: 8.73 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13056261987372314		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0994387167821176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11500066832792037 | validation: 0.20086305259698484]
	TIME [epoch: 8.72 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11500789802317266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1145142781132378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11476108806820522 | validation: 0.13311842464269852]
	TIME [epoch: 8.75 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16688446650808159		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10349474088649173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13518960369728664 | validation: 0.11969467982655418]
	TIME [epoch: 8.72 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0823147085530174		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14370011792503729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11300741323902737 | validation: 0.49125140207473184]
	TIME [epoch: 8.71 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1221305938044602		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11185139243448732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11699099311947374 | validation: 0.13822485746499522]
	TIME [epoch: 8.69 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12450205935470313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10571433121007243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11510819528238778 | validation: 0.12810841557031777]
	TIME [epoch: 8.73 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13084847340542666		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11828555863216872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12456701601879772 | validation: 0.11988424335737174]
	TIME [epoch: 8.69 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0965851590469852		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12437905578374356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11048210741536439 | validation: 0.09114465539496833]
	TIME [epoch: 8.71 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17239102896856617		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1224241462977816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14740758763317388 | validation: 0.14239062207587422]
	TIME [epoch: 8.7 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13740992749529163		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08919707978601146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11330350364065153 | validation: 0.08897918442744444]
	TIME [epoch: 8.73 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15414636520037836		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11333960105587866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13374298312812852 | validation: 0.15424793116374502]
	TIME [epoch: 8.72 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0957248743934862		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1864191538589342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1410720141262102 | validation: 0.09280651732326828]
	TIME [epoch: 8.71 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09978484523755336		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09316047216750056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09647265870252696 | validation: 0.2547597129122364]
	TIME [epoch: 8.7 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12545527118650984		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10386063284665066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11465795201658027 | validation: 0.08092136485233724]
	TIME [epoch: 8.74 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10025443754058432		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11110611977986681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10568027866022559 | validation: 0.2942450661800247]
	TIME [epoch: 8.77 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08898477151558024		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10945829102635533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09922153127096779 | validation: 0.06416441897219632]
	TIME [epoch: 8.73 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07204370192863437		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15980738936328764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.115925545645961 | validation: 0.20188344436048533]
	TIME [epoch: 8.72 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11226114331020813		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14010976185785795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12618545258403308 | validation: 0.1856594554141365]
	TIME [epoch: 8.71 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09790339645260158		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11262707799481615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10526523722370887 | validation: 0.11355607283949462]
	TIME [epoch: 8.74 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13609516435534566		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11155871194675435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12382693815105002 | validation: 0.26914563884604914]
	TIME [epoch: 8.73 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11654817258252877		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09957264248818333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10806040753535603 | validation: 0.13547205083470445]
	TIME [epoch: 8.75 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1383511431678851		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12175552490659329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1300533340372392 | validation: 0.09502989484019728]
	TIME [epoch: 8.72 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11556211102652325		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11221351122750489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1138878111270141 | validation: 0.07228962506487221]
	TIME [epoch: 8.74 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17143245396442014		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08412174465226674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12777709930834344 | validation: 0.04819519206575499]
	TIME [epoch: 8.75 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15505576847616936		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12232769013977451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1386917293079719 | validation: 0.2143907090000388]
	TIME [epoch: 8.71 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13419562196233706		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1949113465292046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1645534842457708 | validation: 0.19923598181810379]
	TIME [epoch: 8.7 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12391091482477368		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10212927551762607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11302009517119986 | validation: 0.07976456805851341]
	TIME [epoch: 8.71 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11334957541611479		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1685397160326192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.140944645724367 | validation: 0.1477669200921056]
	TIME [epoch: 8.75 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1375242042850861		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10748258977366625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12250339702937618 | validation: 0.11893250941436119]
	TIME [epoch: 8.73 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13280018236786706		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.38126434194331865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2570322621555929 | validation: 0.2530777906750778]
	TIME [epoch: 8.72 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15206828613395576		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.106716763564963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12939252484945937 | validation: 0.06259690627131426]
	TIME [epoch: 8.72 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12093880232503562		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21529872985033985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16811876608768772 | validation: 0.1695083296429326]
	TIME [epoch: 8.75 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14417172735624362		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20004911077042903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1721104190633363 | validation: 0.0988238158825593]
	TIME [epoch: 8.72 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11225764400349245		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16257884781590423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13741824590969837 | validation: 0.1662669237829618]
	TIME [epoch: 8.73 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10056564678493954		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24080489491289883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1706852708489192 | validation: 0.30594905729489863]
	TIME [epoch: 8.72 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1602195478708247		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17137317861445817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16579636324264146 | validation: 0.19437561924454538]
	TIME [epoch: 8.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15000746899414305		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19486133708962508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17243440304188404 | validation: 0.1284837034630683]
	TIME [epoch: 8.73 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15004854371581494		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09721465247026315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12363159809303906 | validation: 0.0659002724425232]
	TIME [epoch: 8.7 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11353413956988372		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1479807485336034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13075744405174355 | validation: 0.4701114923201827]
	TIME [epoch: 8.72 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2622278236303499		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12996652441349776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19609717402192386 | validation: 0.11892615838626594]
	TIME [epoch: 8.7 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07354688641693344		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10899000088515746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09126844365104546 | validation: 0.10694362008895385]
	TIME [epoch: 8.74 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10401845946706853		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13798815786624843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1210033086666585 | validation: 0.12683042916810358]
	TIME [epoch: 8.71 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10529482098795444		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11921589661199905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11225535879997675 | validation: 0.14790715599790516]
	TIME [epoch: 8.71 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2090335667569935		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17770618822331768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19336987749015555 | validation: 0.11539984808890819]
	TIME [epoch: 8.72 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08423749152578687		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10901237482855071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09662493317716878 | validation: 0.07442663648163278]
	TIME [epoch: 8.73 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1292561813459442		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09518579723952053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11222098929273236 | validation: 0.08378592937089334]
	TIME [epoch: 8.72 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17117200048201173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14002162131241144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1555968108972116 | validation: 0.17729272690600462]
	TIME [epoch: 8.73 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17539634838631546		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1510376985050837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16321702344569952 | validation: 0.18071274818533178]
	TIME [epoch: 8.72 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10166269747014398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10981587310022842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10573928528518617 | validation: 0.16919139132356803]
	TIME [epoch: 8.73 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09028709661160757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14152108877783504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11590409269472131 | validation: 0.1816852388012426]
	TIME [epoch: 8.73 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3467881203037994		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30426340541756564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3255257628606825 | validation: 0.270824791522671]
	TIME [epoch: 8.72 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22374021841023928		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.22494551766538623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22434286803781278 | validation: 0.24046945899743355]
	TIME [epoch: 8.71 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20886200166232854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09985840700315848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1543602043327435 | validation: 0.07022964503107296]
	TIME [epoch: 8.7 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08392406358003449		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14999197317436244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11695801837719848 | validation: 0.06964748608208612]
	TIME [epoch: 8.73 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11530591388580051		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11182036103253852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1135631374591695 | validation: 0.07203951437536281]
	TIME [epoch: 8.71 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1544741171489252		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12877680089386173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14162545902139348 | validation: 0.16606698305169118]
	TIME [epoch: 8.71 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13849795833461107		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07785081154979837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10817438494220473 | validation: 0.06329369831792869]
	TIME [epoch: 8.74 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12399053253573149		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09979140480223614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1118909686689838 | validation: 0.13331157923613152]
	TIME [epoch: 8.7 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08724010975077656		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07475113259273714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08099562117175686 | validation: 0.11516704476801956]
	TIME [epoch: 8.7 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11267920598928508		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1192003126107245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1159397593000048 | validation: 0.072639966945267]
	TIME [epoch: 8.71 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10967248977010884		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11707748695681541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11337498836346212 | validation: 0.06064065590146456]
	TIME [epoch: 8.69 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07641185582622126		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08260158716067682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07950672149344903 | validation: 0.09830416371560263]
	TIME [epoch: 8.71 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11630758483336892		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13814345182504606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1272255183292075 | validation: 0.06632850915787991]
	TIME [epoch: 8.72 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14018767756806066		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10698882497867424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12358825127336741 | validation: 0.08539506469117042]
	TIME [epoch: 8.69 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08742403901498533		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15067795037315218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11905099469406874 | validation: 0.28596348481825395]
	TIME [epoch: 8.7 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16373894453333335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07811249241490327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1209257184741183 | validation: 0.04879935992849377]
	TIME [epoch: 8.68 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1379307680162126		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09235983879221103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11514530340421178 | validation: 0.053560733780951726]
	TIME [epoch: 8.73 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0784439486279655		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08060540070784253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.079524674667904 | validation: 0.13817439291903372]
	TIME [epoch: 8.71 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11532878631821837		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1307941333762302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12306145984722427 | validation: 0.15281116072922635]
	TIME [epoch: 8.69 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13604834947271865		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11059839585194878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12332337266233373 | validation: 0.09448552315358746]
	TIME [epoch: 8.71 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13427190259963967		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11292371312446867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12359780786205417 | validation: 0.157086399266833]
	TIME [epoch: 8.69 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11820114220278202		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09491206065407405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10655660142842802 | validation: 0.16022951564841167]
	TIME [epoch: 8.72 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08586883043853998		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10133130863264643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0936000695355932 | validation: 0.07332693817075973]
	TIME [epoch: 8.7 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08423038258853881		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10356028019855315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09389533139354597 | validation: 0.13528614712625323]
	TIME [epoch: 8.69 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08881430241570094		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11205562984621556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10043496613095823 | validation: 0.31758737180566377]
	TIME [epoch: 8.69 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13466692918786718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09005619444717361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1123615618175204 | validation: 0.09575789736189524]
	TIME [epoch: 8.72 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12889222107417753		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0957112517523686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11230173641327308 | validation: 0.09528977873610434]
	TIME [epoch: 8.69 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11797131480912833		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16856541069655223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14326836275284027 | validation: 0.18023328579210052]
	TIME [epoch: 8.7 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12068750501481998		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09071337405320098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1057004395340105 | validation: 0.15956984745704977]
	TIME [epoch: 8.69 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.149631916666376		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1244189572415173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13702543695394667 | validation: 0.08071099906981702]
	TIME [epoch: 8.73 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1022167016231558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0784577552168815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09033722842001865 | validation: 0.11069617267774395]
	TIME [epoch: 8.7 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09805820684209558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13479392011315855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11642606347762707 | validation: 0.10275827603157969]
	TIME [epoch: 8.71 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13461708224100755		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10179464665985874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11820586445043313 | validation: 0.10756100526206094]
	TIME [epoch: 8.71 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11433986614644351		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07342878918812737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09388432766728544 | validation: 0.09166810189629357]
	TIME [epoch: 8.75 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08508156652700127		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10551439140123905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09529797896412015 | validation: 0.14933766574120302]
	TIME [epoch: 8.73 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09979667194164231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14854862466447472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12417264830305852 | validation: 0.11737977197844904]
	TIME [epoch: 8.73 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09624947134537506		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10261773250168185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09943360192352846 | validation: 0.046577195425728975]
	TIME [epoch: 8.73 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10836862065781436		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08194388561311458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09515625313546446 | validation: 0.06218435904009775]
	TIME [epoch: 8.73 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11374715728833062		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09564646132754459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10469680930793761 | validation: 0.06264223797430765]
	TIME [epoch: 8.75 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08020781636761268		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07642985836704379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07831883736732823 | validation: 0.0907942160449903]
	TIME [epoch: 8.72 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08809201913774449		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08778974448932002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08794088181353227 | validation: 0.09860604206744031]
	TIME [epoch: 8.72 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11509056347627555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10895952639750497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11202504493689025 | validation: 0.09355913321956104]
	TIME [epoch: 8.69 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1172972609608089		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1630907433265724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14019400214369063 | validation: 0.1518500383321437]
	TIME [epoch: 8.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10522129132740984		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12676385700037385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11599257416389186 | validation: 0.19012589812946404]
	TIME [epoch: 8.73 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11145264949495048		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08601826029446442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09873545489470745 | validation: 0.18900089080234783]
	TIME [epoch: 8.72 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11327017073814746		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11788651500536038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1155783428717539 | validation: 0.08139694707316061]
	TIME [epoch: 8.71 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08027436919767575		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11822939096964538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09925188008366057 | validation: 0.1908391177763755]
	TIME [epoch: 8.75 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07947367100287892		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08168849227204075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08058108163745983 | validation: 0.12086114997504303]
	TIME [epoch: 8.74 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0974467288924252		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10711756854804744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1022821487202363 | validation: 0.13886332119612604]
	TIME [epoch: 8.73 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11309298090823557		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0717560914555201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09242453618187785 | validation: 0.2570443366074213]
	TIME [epoch: 8.72 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10617728033378597		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07178094834166861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08897911433772727 | validation: 0.035258237350659034]
	TIME [epoch: 8.72 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10738625714235057		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15643995420398954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13191310567317005 | validation: 0.1055262078650332]
	TIME [epoch: 8.77 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1512038192535854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15040862705047991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15080622315203268 | validation: 0.16509039200848405]
	TIME [epoch: 8.72 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17685537430208226		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2526949458909375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21477516009650985 | validation: 0.2841940320294031]
	TIME [epoch: 8.72 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14847413031450532		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24025018068632042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1943621555004129 | validation: 0.07375265942746276]
	TIME [epoch: 8.7 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14464678659092603		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12098151249334757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13281414954213683 | validation: 0.23426234892172337]
	TIME [epoch: 8.73 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12050419596222942		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07236853010367719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09643636303295328 | validation: 0.04611850310224817]
	TIME [epoch: 8.73 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07540698096394688		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.17948643683227178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12744670889810933 | validation: 0.11227940624508284]
	TIME [epoch: 8.72 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11028803150972155		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07991593596025441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09510198373498797 | validation: 0.07039192229865121]
	TIME [epoch: 8.72 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09452622757312008		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10753501720174267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10103062238743137 | validation: 0.07520590684492814]
	TIME [epoch: 8.73 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09746194083671339		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08293874326226987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09020034204949162 | validation: 0.04507889761660813]
	TIME [epoch: 8.74 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0972805113426238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12420071750897566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11074061442579972 | validation: 0.2583006806595599]
	TIME [epoch: 8.72 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15784420814574546		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09103582697203644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12444001755889093 | validation: 0.10145474010650775]
	TIME [epoch: 8.71 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10711990997977842		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09870448743497222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10291219870737534 | validation: 0.07725175269632398]
	TIME [epoch: 8.71 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17085261476414107		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24494856100279572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20790058788346838 | validation: 0.06417065404002154]
	TIME [epoch: 8.74 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11520074181060973		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12048490585752832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11784282383406904 | validation: 0.14762772374547506]
	TIME [epoch: 8.73 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10987582922575309		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1380615904024945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1239687098141238 | validation: 0.09487958891333334]
	TIME [epoch: 8.73 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10060006797112626		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06188767010422591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08124386903767608 | validation: 0.10995255856064247]
	TIME [epoch: 8.75 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07973938185448678		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09234021616325899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08603979900887289 | validation: 0.16433002832470942]
	TIME [epoch: 8.75 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12322244661182291		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08953910390203729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10638077525693009 | validation: 0.27105176855601754]
	TIME [epoch: 8.75 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14309333096497728		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12365076178262072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13337204637379899 | validation: 0.06421322409495808]
	TIME [epoch: 8.73 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08123348926797791		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12174635636607531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10148992281702662 | validation: 0.06966949585882264]
	TIME [epoch: 8.71 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11156959505276838		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1136081395762589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11258886731451367 | validation: 0.07448698231428914]
	TIME [epoch: 8.73 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09493655435364472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08364077144781885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08928866290073179 | validation: 0.13502410390191222]
	TIME [epoch: 8.75 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11883066334919508		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10152297606036861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11017681970478184 | validation: 0.16893850009869896]
	TIME [epoch: 8.71 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15659359241440513		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20034960860257703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17847160050849104 | validation: 0.04705844895021601]
	TIME [epoch: 8.72 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06909497555365716		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11414417483010686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09161957519188202 | validation: 0.14827326993449014]
	TIME [epoch: 8.73 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10415994865479059		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0636537773974581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08390686302612435 | validation: 0.08808380840864626]
	TIME [epoch: 8.73 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16843906502983752		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.25796457841094445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21320182172039098 | validation: 0.27926222304138715]
	TIME [epoch: 8.73 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15754086270505124		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11176065039629128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1346507565506712 | validation: 0.08367129179109839]
	TIME [epoch: 8.7 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12080579506243198		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1857879694820337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15329688227223282 | validation: 0.12977912876756037]
	TIME [epoch: 8.7 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1024644667699099		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08156340126193036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09201393401592013 | validation: 0.12673532294673537]
	TIME [epoch: 8.71 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09913412833384355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1029563479857252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10104523815978436 | validation: 0.12858295220015423]
	TIME [epoch: 8.72 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10192072344206322		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10544778949077409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10368425646641866 | validation: 0.1627339009421917]
	TIME [epoch: 8.7 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1692217185112593		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0904151877121443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1298184531117018 | validation: 0.09388121920043974]
	TIME [epoch: 8.72 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10478810418222743		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13469541745065003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11974176081643875 | validation: 0.06662735764841476]
	TIME [epoch: 8.71 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07510755438441113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10883754137893002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197254788167056 | validation: 0.16554827299338418]
	TIME [epoch: 8.77 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13545382242440646		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.21109813035012742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17327597638726694 | validation: 0.3905597848577019]
	TIME [epoch: 8.72 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2572235091772893		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.30333876306973717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28028113612351324 | validation: 0.5077358278551047]
	TIME [epoch: 8.69 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25507751731074435		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13696220628377423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1960198617972593 | validation: 0.15030390094952167]
	TIME [epoch: 8.71 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13189776056263824		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20061850055632044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16625813055947936 | validation: 0.17765853842984147]
	TIME [epoch: 8.74 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12347792860023969		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14736882716437028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.135423377882305 | validation: 0.25535649368230573]
	TIME [epoch: 8.74 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11458491401795376		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07267755612486393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09363123507140886 | validation: 0.125358486747842]
	TIME [epoch: 8.72 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08613855435852763		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08385485563726947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08499670499789856 | validation: 0.1102771775830892]
	TIME [epoch: 8.72 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11685804177887352		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11151252390616795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11418528284252072 | validation: 0.16543580380646006]
	TIME [epoch: 8.74 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09387435016896988		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09376036111397124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09381735564147056 | validation: 0.08713529823631432]
	TIME [epoch: 8.74 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09267392484032473		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12259597592405944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10763495038219209 | validation: 0.19107988782377136]
	TIME [epoch: 8.71 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16734983011341012		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13498902402988908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15116942707164963 | validation: 0.05985774991417388]
	TIME [epoch: 8.72 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11469702901111187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.27007382037739003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19238542469425096 | validation: 0.5129748100732123]
	TIME [epoch: 8.72 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31818334337221466		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15454701138328772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23636517737775117 | validation: 0.3159019549115632]
	TIME [epoch: 8.76 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11642785170753736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.098666921535343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10754738662144017 | validation: 0.12006331426737175]
	TIME [epoch: 8.71 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09068407859948267		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09067907130713024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09068157495330645 | validation: 0.0933549569896288]
	TIME [epoch: 8.73 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07683744880767672		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08048773765227132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07866259322997401 | validation: 0.050618103737526246]
	TIME [epoch: 8.73 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06304531574984222		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08149615269170105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07227073422077165 | validation: 0.12619209416909685]
	TIME [epoch: 8.77 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07646029267080337		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12299751251531037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09972890259305689 | validation: 0.11208976062662909]
	TIME [epoch: 8.72 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08867043319496787		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11305240170750275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10086141745123531 | validation: 0.083196891439432]
	TIME [epoch: 8.73 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07873385381686356		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12038243732102294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09955814556894325 | validation: 0.19102606370132658]
	TIME [epoch: 8.75 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13885780869989112		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.05399956638847601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09642868754418356 | validation: 0.08310338186932513]
	TIME [epoch: 8.76 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09947759845305933		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06526988130451254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08237373987878592 | validation: 0.05952286247726478]
	TIME [epoch: 8.76 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08154204795699163		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09094385063815473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08624294929757319 | validation: 0.08660564833918989]
	TIME [epoch: 8.71 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08844911030590512		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12498346138444287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10671628584517401 | validation: 0.08837441587352732]
	TIME [epoch: 8.73 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08325749590393375		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10468831966224115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09397290778308744 | validation: 0.17542948895852128]
	TIME [epoch: 8.72 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12750238573614478		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10958000677358068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11854119625486273 | validation: 0.4236601177409536]
	TIME [epoch: 8.74 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13390154823062564		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11504701209810586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12447428016436576 | validation: 0.0750870774255403]
	TIME [epoch: 8.72 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07852953137310606		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18177359786503233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1301515646190692 | validation: 0.38048212197748565]
	TIME [epoch: 8.71 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41623494739545386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12836507362958408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27230001051251895 | validation: 0.08516383956731913]
	TIME [epoch: 8.72 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09481721203006514		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09869329995785833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09675525599396176 | validation: 0.11697518179456895]
	TIME [epoch: 8.75 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09849971073781424		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.14335960370652512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12092965722216967 | validation: 0.14560269830938474]
	TIME [epoch: 8.75 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1434197705056925		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09777282004679859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12059629527624553 | validation: 0.09979865772991321]
	TIME [epoch: 8.72 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11018343578811558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10962189192170424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1099026638549099 | validation: 0.2700238388750049]
	TIME [epoch: 8.72 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11031790988153693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.20807011265325506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.159194011267396 | validation: 0.3697354766961336]
	TIME [epoch: 8.76 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1407631869629961		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09541785867512531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1180905228190607 | validation: 0.03904806437169993]
	TIME [epoch: 8.74 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07207418569826408		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09502135791693064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08354777180759736 | validation: 0.10527447529869934]
	TIME [epoch: 8.7 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1439184548780147		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09983279009989454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1218756224889546 | validation: 0.1570697212165796]
	TIME [epoch: 8.72 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1146078766510166		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11174931724772566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11317859694937113 | validation: 0.12478781293750213]
	TIME [epoch: 8.72 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13344498476845462		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06956918434505785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10150708455675624 | validation: 0.09804186833029604]
	TIME [epoch: 8.73 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16254127918907957		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08503031209708109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12378579564308037 | validation: 0.14720459493988514]
	TIME [epoch: 8.72 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09765856421135381		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10514088270363893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10139972345749637 | validation: 0.07932024423728327]
	TIME [epoch: 8.74 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11054598223680738		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09062406482710941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10058502353195842 | validation: 0.10482487582671139]
	TIME [epoch: 8.73 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08156486508346793		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08973188048646823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08564837278496808 | validation: 0.154017298129474]
	TIME [epoch: 8.74 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22329202157645453		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16813869010873736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19571535584259594 | validation: 0.23898510646607257]
	TIME [epoch: 8.74 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1608461978685787		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09807422529566319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12946021158212093 | validation: 0.11338009285445116]
	TIME [epoch: 8.71 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11282016929568636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09429785755004393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10355901342286514 | validation: 0.07715826837550274]
	TIME [epoch: 8.73 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08816580193463638		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0888096435906459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08848772276264114 | validation: 0.24771530910360887]
	TIME [epoch: 8.73 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12076234772356671		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06380634190800986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09228434481578829 | validation: 0.0759706907201945]
	TIME [epoch: 8.73 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12087004149944373		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10701275529570849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11394139839757614 | validation: 0.11813324049590387]
	TIME [epoch: 8.73 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12707519216951413		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11730989254013782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12219254235482598 | validation: 0.0952409244969769]
	TIME [epoch: 8.71 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10343516879376742		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09918606559979483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1013106171967811 | validation: 0.08495390735280027]
	TIME [epoch: 8.72 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10248923054110917		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10638795377420471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10443859215765697 | validation: 0.07571459046337921]
	TIME [epoch: 8.74 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1107457440986874		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10426556517249172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10750565463558956 | validation: 0.08240013087311161]
	TIME [epoch: 8.72 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08191822158536224		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.260369376635862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1711437991106121 | validation: 0.11222669080575648]
	TIME [epoch: 8.71 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1951522780194795		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.08607114586539096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14061171194243524 | validation: 0.10123551611057109]
	TIME [epoch: 8.7 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05520390540716286		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09043416922744699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07281903731730491 | validation: 0.15767434652692264]
	TIME [epoch: 8.73 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0818894286380061		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09741984577934057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08965463720867334 | validation: 0.09059233131822429]
	TIME [epoch: 8.73 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11781565502617404		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.3194453764849665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2186305157555703 | validation: 0.1573471489628226]
	TIME [epoch: 8.73 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36084948310016207		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.24554923595847522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30319935952931865 | validation: 0.2297954238582925]
	TIME [epoch: 8.71 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20821455616011147		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07209302048962754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14015378832486952 | validation: 0.09235223911338061]
	TIME [epoch: 8.73 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0772161311912676		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.09594070276197272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08657841697662015 | validation: 0.12097188116890908]
	TIME [epoch: 8.76 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12592155972013144		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18046686376266588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1531942117413987 | validation: 0.1515445307050251]
	TIME [epoch: 8.72 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2702124662856242		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.18470564694362318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22745905661462368 | validation: 0.06836362482213859]
	TIME [epoch: 8.74 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1715287930555634		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.15845341508425786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16499110406991063 | validation: 0.14675572648037205]
	TIME [epoch: 8.72 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1652530752422082		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.138020725662656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1516369004524321 | validation: 0.11309694756903667]
	TIME [epoch: 8.72 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08072648352555999		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11215657080454762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09644152716505382 | validation: 0.18913640293806633]
	TIME [epoch: 8.71 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09228561635257433		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.16857563483589028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1304306255942323 | validation: 0.05840194765374368]
	TIME [epoch: 8.72 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12674661150459593		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.06689978782874742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09682319966667167 | validation: 0.14808881366888813]
	TIME [epoch: 8.7 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11285285792500639		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13766764407024143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1252602509976239 | validation: 0.06171790460654979]
	TIME [epoch: 8.74 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12049851190928315		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.19617762579376033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15833806885152177 | validation: 0.31182512440205995]
	TIME [epoch: 8.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27849770787588013		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13304882268967455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20577326528277734 | validation: 0.14707256360489807]
	TIME [epoch: 8.73 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42971538395406855		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.280403218822607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3550593013883378 | validation: 0.24861294870045333]
	TIME [epoch: 8.69 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17653593199260809		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2306047405164943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20357033625455118 | validation: 0.34423625574099503]
	TIME [epoch: 8.72 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2377235069733618		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1667694489650557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20224647796920872 | validation: 0.15030248449558]
	TIME [epoch: 8.74 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1692965326684119		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12400757797974957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14665205532408074 | validation: 0.213874888846383]
	TIME [epoch: 8.72 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14833609360272942		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11281478133913074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13057543747093006 | validation: 0.09371019885670032]
	TIME [epoch: 8.73 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0882877328580111		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1087967755708353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0985422542144232 | validation: 0.17793754769283152]
	TIME [epoch: 8.74 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1104917266382301		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.2504028774516457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18044730204493792 | validation: 0.11238355151343478]
	TIME [epoch: 8.74 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11952123482577073		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10125681240091913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1103890236133449 | validation: 0.11834137866005526]
	TIME [epoch: 8.72 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08223625187888506		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1123740720546172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09730516196675112 | validation: 0.05282296944063608]
	TIME [epoch: 8.71 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06137253629219808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11502271996142581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08819762812681195 | validation: 0.11954792251551757]
	TIME [epoch: 8.73 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12462682949982304		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11902410227719466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12182546588850887 | validation: 0.13953636103784595]
	TIME [epoch: 8.72 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08011754388793638		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10085731319688811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09048742854241223 | validation: 0.050968388980697776]
	TIME [epoch: 8.75 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07602431875169062		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.13083155731033494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10342793803101277 | validation: 0.048761878232146214]
	TIME [epoch: 8.73 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06193820619499529		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07205806362725231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0669981349111238 | validation: 0.1496615388404679]
	TIME [epoch: 8.74 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08267033048826346		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1261080113873499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10438917093780667 | validation: 0.09125239213510722]
	TIME [epoch: 8.72 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1197343994644335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0956074834450382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10767094145473584 | validation: 0.12661068207646664]
	TIME [epoch: 8.76 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09822369197661208		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0805093281369369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0893665100567745 | validation: 0.05402691958597541]
	TIME [epoch: 8.71 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0693602865683217		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.10877150751647077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08906589704239623 | validation: 0.14675277836900283]
	TIME [epoch: 8.73 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08844430295057276		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.12378186995651823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10611308645354549 | validation: 0.056721683962523334]
	TIME [epoch: 8.72 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057077803517649385		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0723623942259362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06472009887179278 | validation: 0.05320098898514488]
	TIME [epoch: 8.73 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0969199839520704		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.1599160252727549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1284180046124126 | validation: 0.12701316843053775]
	TIME [epoch: 8.72 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16093534201296766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11596071669676185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13844802935486475 | validation: 0.1325638056554842]
	TIME [epoch: 8.7 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11643267575086043		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11005992723125371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11324630149105705 | validation: 0.08912243798570937]
	TIME [epoch: 8.7 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07673870747658931		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.0639916375481784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07036517251238386 | validation: 0.07365142050648424]
	TIME [epoch: 8.72 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09618429604866313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11492109981716019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10555269793291167 | validation: 0.14539936639440612]
	TIME [epoch: 8.75 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10810245737843512		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.07524196974431749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09167221356137631 | validation: 0.030366658474702886]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06179645937828937		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11167617489461582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08673631713645259 | validation: 0.04689459589337989]
	TIME [epoch: 8.73 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07062264393790287		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.11498368005003876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0928031619939708 | validation: 0.03909076399808906]
	TIME [epoch: 8.73 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09988093662459492		[learning rate: 0.0099862]
		[batch 20/20] avg loss: 0.12313905282446998		[learning rate: 0.0099709]
	Learning Rate: 0.00997088
	LOSS [training: 0.11150999472453245 | validation: 0.15498788190765725]
	TIME [epoch: 8.78 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0767007839407888		[learning rate: 0.0099556]
		[batch 20/20] avg loss: 0.05245935257525619		[learning rate: 0.0099403]
	Learning Rate: 0.00994031
	LOSS [training: 0.0645800682580225 | validation: 0.06376348323400774]
	TIME [epoch: 8.73 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07696209300724378		[learning rate: 0.0099251]
		[batch 20/20] avg loss: 0.1148555944159582		[learning rate: 0.0099098]
	Learning Rate: 0.00990984
	LOSS [training: 0.095908843711601 | validation: 0.11289619306115782]
	TIME [epoch: 8.73 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05925269731888916		[learning rate: 0.0098946]
		[batch 20/20] avg loss: 0.14035090697939706		[learning rate: 0.0098795]
	Learning Rate: 0.00987946
	LOSS [training: 0.09980180214914311 | validation: 0.07709976498588665]
	TIME [epoch: 8.73 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04250137055515455		[learning rate: 0.0098643]
		[batch 20/20] avg loss: 0.1311222338122225		[learning rate: 0.0098492]
	Learning Rate: 0.00984918
	LOSS [training: 0.08681180218368852 | validation: 0.06719781328812546]
	TIME [epoch: 8.75 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06642805479196932		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 0.0774598394071435		[learning rate: 0.009819]
	Learning Rate: 0.00981899
	LOSS [training: 0.07194394709955643 | validation: 0.05991881928860332]
	TIME [epoch: 8.74 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07380852126080668		[learning rate: 0.0098039]
		[batch 20/20] avg loss: 0.12408318070921709		[learning rate: 0.0097889]
	Learning Rate: 0.00978889
	LOSS [training: 0.09894585098501188 | validation: 0.11399374475694377]
	TIME [epoch: 8.72 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07746533578060996		[learning rate: 0.0097739]
		[batch 20/20] avg loss: 0.061767346470290876		[learning rate: 0.0097589]
	Learning Rate: 0.00975888
	LOSS [training: 0.06961634112545043 | validation: 0.0654125800017856]
	TIME [epoch: 8.73 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08954663199890024		[learning rate: 0.0097439]
		[batch 20/20] avg loss: 0.08983677262324836		[learning rate: 0.009729]
	Learning Rate: 0.00972897
	LOSS [training: 0.08969170231107429 | validation: 0.045216120214286434]
	TIME [epoch: 8.7 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09867746763299115		[learning rate: 0.009714]
		[batch 20/20] avg loss: 0.09606250287888543		[learning rate: 0.0096991]
	Learning Rate: 0.00969914
	LOSS [training: 0.09736998525593829 | validation: 0.11974807672096738]
	TIME [epoch: 8.73 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08893526983260455		[learning rate: 0.0096843]
		[batch 20/20] avg loss: 0.07901044154668993		[learning rate: 0.0096694]
	Learning Rate: 0.00966941
	LOSS [training: 0.08397285568964724 | validation: 0.14763629121822572]
	TIME [epoch: 8.77 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09197245778304346		[learning rate: 0.0096546]
		[batch 20/20] avg loss: 0.13755545595544344		[learning rate: 0.0096398]
	Learning Rate: 0.00963977
	LOSS [training: 0.11476395686924346 | validation: 0.05243270902437386]
	TIME [epoch: 8.75 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05795158189918141		[learning rate: 0.009625]
		[batch 20/20] avg loss: 0.08677161533220709		[learning rate: 0.0096102]
	Learning Rate: 0.00961022
	LOSS [training: 0.07236159861569424 | validation: 0.061173585691078676]
	TIME [epoch: 8.73 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07535788049638403		[learning rate: 0.0095955]
		[batch 20/20] avg loss: 0.07400657206907099		[learning rate: 0.0095808]
	Learning Rate: 0.00958076
	LOSS [training: 0.0746822262827275 | validation: 0.03988731273020403]
	TIME [epoch: 8.96 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10700545212709975		[learning rate: 0.0095661]
		[batch 20/20] avg loss: 0.10840547784325179		[learning rate: 0.0095514]
	Learning Rate: 0.00955139
	LOSS [training: 0.10770546498517579 | validation: 0.07349697624472742]
	TIME [epoch: 8.7 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10711229649615535		[learning rate: 0.0095367]
		[batch 20/20] avg loss: 0.1426089127330999		[learning rate: 0.0095221]
	Learning Rate: 0.00952211
	LOSS [training: 0.1248606046146276 | validation: 0.15033380315852574]
	TIME [epoch: 8.71 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19194524392042173		[learning rate: 0.0095075]
		[batch 20/20] avg loss: 0.12145493024514087		[learning rate: 0.0094929]
	Learning Rate: 0.00949292
	LOSS [training: 0.15670008708278127 | validation: 0.13977107632149505]
	TIME [epoch: 8.69 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07820824829722926		[learning rate: 0.0094784]
		[batch 20/20] avg loss: 0.0888566936344437		[learning rate: 0.0094638]
	Learning Rate: 0.00946382
	LOSS [training: 0.0835324709658365 | validation: 0.16711413829898164]
	TIME [epoch: 8.71 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1281312578419263		[learning rate: 0.0094493]
		[batch 20/20] avg loss: 0.16998135442275558		[learning rate: 0.0094348]
	Learning Rate: 0.00943481
	LOSS [training: 0.14905630613234092 | validation: 0.0743752001166471]
	TIME [epoch: 8.69 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09934807477240575		[learning rate: 0.0094203]
		[batch 20/20] avg loss: 0.12312042739909967		[learning rate: 0.0094059]
	Learning Rate: 0.00940589
	LOSS [training: 0.11123425108575272 | validation: 0.07092815191965918]
	TIME [epoch: 8.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1062069739079913		[learning rate: 0.0093915]
		[batch 20/20] avg loss: 0.07976836902362684		[learning rate: 0.0093771]
	Learning Rate: 0.00937706
	LOSS [training: 0.09298767146580907 | validation: 0.11066077706287587]
	TIME [epoch: 8.71 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08515180359215016		[learning rate: 0.0093627]
		[batch 20/20] avg loss: 0.16326877859661332		[learning rate: 0.0093483]
	Learning Rate: 0.00934831
	LOSS [training: 0.12421029109438175 | validation: 0.09259017759334663]
	TIME [epoch: 8.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09124226414723753		[learning rate: 0.009334]
		[batch 20/20] avg loss: 0.15207178045014974		[learning rate: 0.0093197]
	Learning Rate: 0.00931966
	LOSS [training: 0.12165702229869364 | validation: 0.27573141779252053]
	TIME [epoch: 8.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09358573720428194		[learning rate: 0.0093054]
		[batch 20/20] avg loss: 0.08733589988367066		[learning rate: 0.0092911]
	Learning Rate: 0.00929109
	LOSS [training: 0.09046081854397629 | validation: 0.06424233568455584]
	TIME [epoch: 8.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.066416070686891		[learning rate: 0.0092768]
		[batch 20/20] avg loss: 0.10216345717471323		[learning rate: 0.0092626]
	Learning Rate: 0.00926261
	LOSS [training: 0.08428976393080212 | validation: 0.1025432413452132]
	TIME [epoch: 8.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058635120342212944		[learning rate: 0.0092484]
		[batch 20/20] avg loss: 0.08294815000116015		[learning rate: 0.0092342]
	Learning Rate: 0.00923422
	LOSS [training: 0.07079163517168655 | validation: 0.0705543213277809]
	TIME [epoch: 8.71 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0795344272242807		[learning rate: 0.0092201]
		[batch 20/20] avg loss: 0.09525555785751741		[learning rate: 0.0092059]
	Learning Rate: 0.00920591
	LOSS [training: 0.08739499254089904 | validation: 0.031953889153903244]
	TIME [epoch: 8.68 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07658859938670057		[learning rate: 0.0091918]
		[batch 20/20] avg loss: 0.06636354674496467		[learning rate: 0.0091777]
	Learning Rate: 0.00917769
	LOSS [training: 0.07147607306583262 | validation: 0.09006597493159965]
	TIME [epoch: 8.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06335445475638284		[learning rate: 0.0091636]
		[batch 20/20] avg loss: 0.1107988208090072		[learning rate: 0.0091496]
	Learning Rate: 0.00914956
	LOSS [training: 0.08707663778269502 | validation: 0.1640751466537246]
	TIME [epoch: 8.69 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12602987995536477		[learning rate: 0.0091355]
		[batch 20/20] avg loss: 0.07932724591476825		[learning rate: 0.0091215]
	Learning Rate: 0.00912151
	LOSS [training: 0.1026785629350665 | validation: 0.059000514491078404]
	TIME [epoch: 8.72 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10099408075828911		[learning rate: 0.0091075]
		[batch 20/20] avg loss: 0.06373450744081766		[learning rate: 0.0090935]
	Learning Rate: 0.00909355
	LOSS [training: 0.08236429409955338 | validation: 0.035462927743595204]
	TIME [epoch: 8.69 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04482582034628079		[learning rate: 0.0090796]
		[batch 20/20] avg loss: 0.06675159023435466		[learning rate: 0.0090657]
	Learning Rate: 0.00906567
	LOSS [training: 0.05578870529031774 | validation: 0.08189037930402646]
	TIME [epoch: 8.69 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0803071643779912		[learning rate: 0.0090518]
		[batch 20/20] avg loss: 0.10776895692433149		[learning rate: 0.0090379]
	Learning Rate: 0.00903788
	LOSS [training: 0.09403806065116134 | validation: 0.07241886528058378]
	TIME [epoch: 8.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056029831815309385		[learning rate: 0.009024]
		[batch 20/20] avg loss: 0.06443366507251687		[learning rate: 0.0090102]
	Learning Rate: 0.00901018
	LOSS [training: 0.06023174844391312 | validation: 0.07227711346796062]
	TIME [epoch: 8.68 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08121215765880463		[learning rate: 0.0089964]
		[batch 20/20] avg loss: 0.08244284775020527		[learning rate: 0.0089826]
	Learning Rate: 0.00898256
	LOSS [training: 0.08182750270450495 | validation: 0.06612505434090243]
	TIME [epoch: 8.69 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07347242603014804		[learning rate: 0.0089688]
		[batch 20/20] avg loss: 0.061621524040594414		[learning rate: 0.008955]
	Learning Rate: 0.00895502
	LOSS [training: 0.06754697503537124 | validation: 0.0407183963140624]
	TIME [epoch: 8.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053905608226369826		[learning rate: 0.0089413]
		[batch 20/20] avg loss: 0.09022588900301619		[learning rate: 0.0089276]
	Learning Rate: 0.00892757
	LOSS [training: 0.07206574861469303 | validation: 0.07468368204068339]
	TIME [epoch: 8.67 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06663923821191023		[learning rate: 0.0089139]
		[batch 20/20] avg loss: 0.09317140790417766		[learning rate: 0.0089002]
	Learning Rate: 0.0089002
	LOSS [training: 0.07990532305804394 | validation: 0.11086678419368251]
	TIME [epoch: 8.69 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06719400516489057		[learning rate: 0.0088866]
		[batch 20/20] avg loss: 0.07415826929472509		[learning rate: 0.0088729]
	Learning Rate: 0.00887292
	LOSS [training: 0.07067613722980784 | validation: 0.045732578719561996]
	TIME [epoch: 8.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07073847736124478		[learning rate: 0.0088593]
		[batch 20/20] avg loss: 0.06954039735825578		[learning rate: 0.0088457]
	Learning Rate: 0.00884572
	LOSS [training: 0.07013943735975028 | validation: 0.037216055644363824]
	TIME [epoch: 8.72 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06177017963855345		[learning rate: 0.0088322]
		[batch 20/20] avg loss: 0.06002763990967573		[learning rate: 0.0088186]
	Learning Rate: 0.00881861
	LOSS [training: 0.06089890977411459 | validation: 0.0984288866253193]
	TIME [epoch: 8.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06530441305141346		[learning rate: 0.0088051]
		[batch 20/20] avg loss: 0.075112710963718		[learning rate: 0.0087916]
	Learning Rate: 0.00879157
	LOSS [training: 0.07020856200756573 | validation: 0.0842419437678543]
	TIME [epoch: 8.69 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.075408099978334		[learning rate: 0.0087781]
		[batch 20/20] avg loss: 0.10509274279836522		[learning rate: 0.0087646]
	Learning Rate: 0.00876462
	LOSS [training: 0.0902504213883496 | validation: 0.053569666064089604]
	TIME [epoch: 8.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09184981850233873		[learning rate: 0.0087512]
		[batch 20/20] avg loss: 0.06778139204299041		[learning rate: 0.0087378]
	Learning Rate: 0.00873776
	LOSS [training: 0.07981560527266457 | validation: 0.060545208896566914]
	TIME [epoch: 8.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0793298610475297		[learning rate: 0.0087244]
		[batch 20/20] avg loss: 0.11228035929714113		[learning rate: 0.008711]
	Learning Rate: 0.00871097
	LOSS [training: 0.09580511017233542 | validation: 0.033485147942019844]
	TIME [epoch: 8.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07902009686296517		[learning rate: 0.0086976]
		[batch 20/20] avg loss: 0.12508082271127127		[learning rate: 0.0086843]
	Learning Rate: 0.00868427
	LOSS [training: 0.10205045978711821 | validation: 0.06824572714567798]
	TIME [epoch: 10.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07282078770917402		[learning rate: 0.0086709]
		[batch 20/20] avg loss: 0.09002063918436028		[learning rate: 0.0086576]
	Learning Rate: 0.00865765
	LOSS [training: 0.08142071344676714 | validation: 0.06819966788733883]
	TIME [epoch: 8.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06043457023299629		[learning rate: 0.0086444]
		[batch 20/20] avg loss: 0.08624876681342963		[learning rate: 0.0086311]
	Learning Rate: 0.00863111
	LOSS [training: 0.07334166852321296 | validation: 0.08155769315993194]
	TIME [epoch: 8.68 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05919619425446343		[learning rate: 0.0086179]
		[batch 20/20] avg loss: 0.0600096304378883		[learning rate: 0.0086047]
	Learning Rate: 0.00860465
	LOSS [training: 0.05960291234617586 | validation: 0.13232252675008951]
	TIME [epoch: 8.69 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07420449023154976		[learning rate: 0.0085915]
		[batch 20/20] avg loss: 0.06384161049944945		[learning rate: 0.0085783]
	Learning Rate: 0.00857828
	LOSS [training: 0.06902305036549962 | validation: 0.11471627754659403]
	TIME [epoch: 8.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07237198980179341		[learning rate: 0.0085651]
		[batch 20/20] avg loss: 0.06120515652370847		[learning rate: 0.008552]
	Learning Rate: 0.00855198
	LOSS [training: 0.06678857316275093 | validation: 0.030070100419263484]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06559906484744739		[learning rate: 0.0085389]
		[batch 20/20] avg loss: 0.06860246704582873		[learning rate: 0.0085258]
	Learning Rate: 0.00852576
	LOSS [training: 0.06710076594663808 | validation: 0.03402364888965422]
	TIME [epoch: 8.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046866639492665715		[learning rate: 0.0085127]
		[batch 20/20] avg loss: 0.07790600211999213		[learning rate: 0.0084996]
	Learning Rate: 0.00849963
	LOSS [training: 0.06238632080632893 | validation: 0.03543497639856195]
	TIME [epoch: 8.67 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0890501693817269		[learning rate: 0.0084866]
		[batch 20/20] avg loss: 0.108000607377621		[learning rate: 0.0084736]
	Learning Rate: 0.00847358
	LOSS [training: 0.09852538837967395 | validation: 0.29912951677019317]
	TIME [epoch: 8.71 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19835326394514446		[learning rate: 0.0084606]
		[batch 20/20] avg loss: 0.09367124406437355		[learning rate: 0.0084476]
	Learning Rate: 0.0084476
	LOSS [training: 0.146012254004759 | validation: 0.13836467489090584]
	TIME [epoch: 8.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05145443718477133		[learning rate: 0.0084346]
		[batch 20/20] avg loss: 0.05947999087137733		[learning rate: 0.0084217]
	Learning Rate: 0.0084217
	LOSS [training: 0.05546721402807433 | validation: 0.10267090924449308]
	TIME [epoch: 8.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07550449614969704		[learning rate: 0.0084088]
		[batch 20/20] avg loss: 0.1407097924876314		[learning rate: 0.0083959]
	Learning Rate: 0.00839589
	LOSS [training: 0.10810714431866422 | validation: 0.15180219029443917]
	TIME [epoch: 8.67 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07257291677389137		[learning rate: 0.008383]
		[batch 20/20] avg loss: 0.09149880300843657		[learning rate: 0.0083702]
	Learning Rate: 0.00837015
	LOSS [training: 0.08203585989116396 | validation: 0.08603239094656029]
	TIME [epoch: 8.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12911025474666024		[learning rate: 0.0083573]
		[batch 20/20] avg loss: 0.12020336033932806		[learning rate: 0.0083445]
	Learning Rate: 0.00834449
	LOSS [training: 0.12465680754299413 | validation: 0.29552714766341276]
	TIME [epoch: 8.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11293712110153013		[learning rate: 0.0083317]
		[batch 20/20] avg loss: 0.044396404324562266		[learning rate: 0.0083189]
	Learning Rate: 0.00831891
	LOSS [training: 0.07866676271304622 | validation: 0.06271527889185276]
	TIME [epoch: 8.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08415068243289886		[learning rate: 0.0083062]
		[batch 20/20] avg loss: 0.068551805638206		[learning rate: 0.0082934]
	Learning Rate: 0.00829341
	LOSS [training: 0.07635124403555242 | validation: 0.18956945758782293]
	TIME [epoch: 8.67 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07297237124086187		[learning rate: 0.0082807]
		[batch 20/20] avg loss: 0.07890566576787625		[learning rate: 0.008268]
	Learning Rate: 0.00826799
	LOSS [training: 0.07593901850436904 | validation: 0.08160433541552031]
	TIME [epoch: 8.66 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07355557612585542		[learning rate: 0.0082553]
		[batch 20/20] avg loss: 0.07662623361015004		[learning rate: 0.0082426]
	Learning Rate: 0.00824265
	LOSS [training: 0.07509090486800273 | validation: 0.10592442533638352]
	TIME [epoch: 8.67 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10290382162415157		[learning rate: 0.00823]
		[batch 20/20] avg loss: 0.1322766441725709		[learning rate: 0.0082174]
	Learning Rate: 0.00821738
	LOSS [training: 0.11759023289836121 | validation: 0.10425452983074643]
	TIME [epoch: 8.69 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08814603083155648		[learning rate: 0.0082048]
		[batch 20/20] avg loss: 0.04539868961875502		[learning rate: 0.0081922]
	Learning Rate: 0.00819219
	LOSS [training: 0.06677236022515573 | validation: 0.07659859248145048]
	TIME [epoch: 8.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051789880496854665		[learning rate: 0.0081796]
		[batch 20/20] avg loss: 0.10587581635002923		[learning rate: 0.0081671]
	Learning Rate: 0.00816708
	LOSS [training: 0.07883284842344196 | validation: 0.06087954025515692]
	TIME [epoch: 8.68 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07615059511636818		[learning rate: 0.0081545]
		[batch 20/20] avg loss: 0.08049420151132658		[learning rate: 0.008142]
	Learning Rate: 0.00814204
	LOSS [training: 0.07832239831384738 | validation: 0.03873326249924848]
	TIME [epoch: 8.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06991216237099615		[learning rate: 0.0081296]
		[batch 20/20] avg loss: 0.06949061391628399		[learning rate: 0.0081171]
	Learning Rate: 0.00811708
	LOSS [training: 0.06970138814364009 | validation: 0.06076933330534752]
	TIME [epoch: 8.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07661789125369264		[learning rate: 0.0081046]
		[batch 20/20] avg loss: 0.0469482099093629		[learning rate: 0.0080922]
	Learning Rate: 0.0080922
	LOSS [training: 0.06178305058152776 | validation: 0.08952660960499]
	TIME [epoch: 8.71 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0597346200402095		[learning rate: 0.0080798]
		[batch 20/20] avg loss: 0.04051453132610831		[learning rate: 0.0080674]
	Learning Rate: 0.00806739
	LOSS [training: 0.05012457568315891 | validation: 0.09028565071891309]
	TIME [epoch: 8.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059171634545761		[learning rate: 0.008055]
		[batch 20/20] avg loss: 0.04044871106482294		[learning rate: 0.0080427]
	Learning Rate: 0.00804267
	LOSS [training: 0.04981017280529197 | validation: 0.03867527752937294]
	TIME [epoch: 8.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054639697614356275		[learning rate: 0.0080303]
		[batch 20/20] avg loss: 0.12513551990216437		[learning rate: 0.008018]
	Learning Rate: 0.00801801
	LOSS [training: 0.08988760875826032 | validation: 0.13997198604246328]
	TIME [epoch: 8.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08204357027495143		[learning rate: 0.0080057]
		[batch 20/20] avg loss: 0.06451297461402737		[learning rate: 0.0079934]
	Learning Rate: 0.00799343
	LOSS [training: 0.0732782724444894 | validation: 0.09639959761395524]
	TIME [epoch: 8.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06155220634436154		[learning rate: 0.0079812]
		[batch 20/20] avg loss: 0.08446315390641061		[learning rate: 0.0079689]
	Learning Rate: 0.00796893
	LOSS [training: 0.07300768012538607 | validation: 0.05698411103169161]
	TIME [epoch: 8.71 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07146817491734683		[learning rate: 0.0079567]
		[batch 20/20] avg loss: 0.05691643923376035		[learning rate: 0.0079445]
	Learning Rate: 0.0079445
	LOSS [training: 0.0641923070755536 | validation: 0.0752330797916699]
	TIME [epoch: 8.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05933699952059669		[learning rate: 0.0079323]
		[batch 20/20] avg loss: 0.057201708597012436		[learning rate: 0.0079201]
	Learning Rate: 0.00792015
	LOSS [training: 0.058269354058804554 | validation: 0.0610989803738486]
	TIME [epoch: 8.69 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07595170710931329		[learning rate: 0.007908]
		[batch 20/20] avg loss: 0.06576520269856784		[learning rate: 0.0078959]
	Learning Rate: 0.00789587
	LOSS [training: 0.07085845490394055 | validation: 0.5239045920923272]
	TIME [epoch: 8.69 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10916392243354148		[learning rate: 0.0078838]
		[batch 20/20] avg loss: 0.053793404816066745		[learning rate: 0.0078717]
	Learning Rate: 0.00787167
	LOSS [training: 0.08147866362480412 | validation: 0.03595793722917809]
	TIME [epoch: 8.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05153875778192304		[learning rate: 0.0078596]
		[batch 20/20] avg loss: 0.05376324671351869		[learning rate: 0.0078475]
	Learning Rate: 0.00784754
	LOSS [training: 0.05265100224772087 | validation: 0.04749422374361409]
	TIME [epoch: 8.71 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0675196913678215		[learning rate: 0.0078355]
		[batch 20/20] avg loss: 0.08592052019971598		[learning rate: 0.0078235]
	Learning Rate: 0.00782348
	LOSS [training: 0.07672010578376874 | validation: 0.05291644306538874]
	TIME [epoch: 8.69 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0572396097671064		[learning rate: 0.0078115]
		[batch 20/20] avg loss: 0.04849670440466604		[learning rate: 0.0077995]
	Learning Rate: 0.0077995
	LOSS [training: 0.052868157085886214 | validation: 0.08133336046589482]
	TIME [epoch: 8.69 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047494014972051385		[learning rate: 0.0077875]
		[batch 20/20] avg loss: 0.11152364191178646		[learning rate: 0.0077756]
	Learning Rate: 0.00777559
	LOSS [training: 0.07950882844191892 | validation: 0.07749925396321922]
	TIME [epoch: 8.68 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08917073672015964		[learning rate: 0.0077637]
		[batch 20/20] avg loss: 0.058735785266469755		[learning rate: 0.0077518]
	Learning Rate: 0.00775175
	LOSS [training: 0.0739532609933147 | validation: 0.11231246439133222]
	TIME [epoch: 8.69 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06894749132948552		[learning rate: 0.0077399]
		[batch 20/20] avg loss: 0.06767703314955895		[learning rate: 0.007728]
	Learning Rate: 0.00772799
	LOSS [training: 0.06831226223952223 | validation: 0.062053001197055216]
	TIME [epoch: 8.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0471406214515305		[learning rate: 0.0077161]
		[batch 20/20] avg loss: 0.07982518333524695		[learning rate: 0.0077043]
	Learning Rate: 0.0077043
	LOSS [training: 0.06348290239338875 | validation: 0.12312011550556273]
	TIME [epoch: 8.68 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07336595879982369		[learning rate: 0.0076925]
		[batch 20/20] avg loss: 0.060289768055454204		[learning rate: 0.0076807]
	Learning Rate: 0.00768069
	LOSS [training: 0.06682786342763895 | validation: 0.05500056141795573]
	TIME [epoch: 8.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0764359670866043		[learning rate: 0.0076689]
		[batch 20/20] avg loss: 0.12287375806228089		[learning rate: 0.0076571]
	Learning Rate: 0.00765714
	LOSS [training: 0.0996548625744426 | validation: 0.06156906978934315]
	TIME [epoch: 8.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09724186662695902		[learning rate: 0.0076454]
		[batch 20/20] avg loss: 0.09080864984325607		[learning rate: 0.0076337]
	Learning Rate: 0.00763367
	LOSS [training: 0.09402525823510753 | validation: 0.04389510779323712]
	TIME [epoch: 8.72 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04960936952384184		[learning rate: 0.007622]
		[batch 20/20] avg loss: 0.057792709407300316		[learning rate: 0.0076103]
	Learning Rate: 0.00761027
	LOSS [training: 0.05370103946557109 | validation: 0.05377696561313222]
	TIME [epoch: 8.71 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05650172518340496		[learning rate: 0.0075986]
		[batch 20/20] avg loss: 0.04751627700596962		[learning rate: 0.0075869]
	Learning Rate: 0.00758694
	LOSS [training: 0.05200900109468731 | validation: 0.05135968107355496]
	TIME [epoch: 8.69 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04754651901704563		[learning rate: 0.0075753]
		[batch 20/20] avg loss: 0.08940154632409868		[learning rate: 0.0075637]
	Learning Rate: 0.00756368
	LOSS [training: 0.06847403267057216 | validation: 0.1254554065853215]
	TIME [epoch: 8.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09766246333346136		[learning rate: 0.0075521]
		[batch 20/20] avg loss: 0.13069004211927537		[learning rate: 0.0075405]
	Learning Rate: 0.0075405
	LOSS [training: 0.11417625272636836 | validation: 0.1324291136416015]
	TIME [epoch: 8.69 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10573373983229502		[learning rate: 0.0075289]
		[batch 20/20] avg loss: 0.10493566841533761		[learning rate: 0.0075174]
	Learning Rate: 0.00751738
	LOSS [training: 0.10533470412381632 | validation: 0.04291336606501424]
	TIME [epoch: 8.71 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06540886688826672		[learning rate: 0.0075059]
		[batch 20/20] avg loss: 0.03966162155398606		[learning rate: 0.0074943]
	Learning Rate: 0.00749434
	LOSS [training: 0.0525352442211264 | validation: 0.06177731915691001]
	TIME [epoch: 8.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07387996002537701		[learning rate: 0.0074828]
		[batch 20/20] avg loss: 0.05177647792214966		[learning rate: 0.0074714]
	Learning Rate: 0.00747137
	LOSS [training: 0.06282821897376334 | validation: 0.036723401439598645]
	TIME [epoch: 8.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04823755680698587		[learning rate: 0.0074599]
		[batch 20/20] avg loss: 0.1654058054444316		[learning rate: 0.0074485]
	Learning Rate: 0.00744846
	LOSS [training: 0.10682168112570874 | validation: 0.11787923109812079]
	TIME [epoch: 8.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0745090839364033		[learning rate: 0.007437]
		[batch 20/20] avg loss: 0.08487133507738197		[learning rate: 0.0074256]
	Learning Rate: 0.00742563
	LOSS [training: 0.07969020950689265 | validation: 0.058666944732397384]
	TIME [epoch: 8.69 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07649622862075074		[learning rate: 0.0074142]
		[batch 20/20] avg loss: 0.0540191440741394		[learning rate: 0.0074029]
	Learning Rate: 0.00740287
	LOSS [training: 0.06525768634744508 | validation: 0.05139215071013211]
	TIME [epoch: 8.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04320830203800961		[learning rate: 0.0073915]
		[batch 20/20] avg loss: 0.04481861098774857		[learning rate: 0.0073802]
	Learning Rate: 0.00738017
	LOSS [training: 0.04401345651287909 | validation: 0.058046016079288255]
	TIME [epoch: 8.7 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06245202608239988		[learning rate: 0.0073689]
		[batch 20/20] avg loss: 0.03972938833596672		[learning rate: 0.0073576]
	Learning Rate: 0.00735755
	LOSS [training: 0.051090707209183295 | validation: 0.045241022807585594]
	TIME [epoch: 8.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08411526394864907		[learning rate: 0.0073463]
		[batch 20/20] avg loss: 0.11304489672045874		[learning rate: 0.007335]
	Learning Rate: 0.007335
	LOSS [training: 0.09858008033455391 | validation: 0.07758244598625326]
	TIME [epoch: 8.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047042778071775294		[learning rate: 0.0073237]
		[batch 20/20] avg loss: 0.05840572605127755		[learning rate: 0.0073125]
	Learning Rate: 0.00731251
	LOSS [training: 0.05272425206152642 | validation: 0.037995588552220426]
	TIME [epoch: 8.69 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04925296550507011		[learning rate: 0.0073013]
		[batch 20/20] avg loss: 0.09231404475728186		[learning rate: 0.0072901]
	Learning Rate: 0.0072901
	LOSS [training: 0.07078350513117597 | validation: 0.10873684728898572]
	TIME [epoch: 8.71 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0694194810065957		[learning rate: 0.0072789]
		[batch 20/20] avg loss: 0.061959757231807966		[learning rate: 0.0072678]
	Learning Rate: 0.00726775
	LOSS [training: 0.06568961911920185 | validation: 0.04117722628797339]
	TIME [epoch: 8.67 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05031982308292003		[learning rate: 0.0072566]
		[batch 20/20] avg loss: 0.05837446629728207		[learning rate: 0.0072455]
	Learning Rate: 0.00724547
	LOSS [training: 0.054347144690101046 | validation: 0.11165605971547031]
	TIME [epoch: 8.66 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05516659539099612		[learning rate: 0.0072344]
		[batch 20/20] avg loss: 0.06486784352856094		[learning rate: 0.0072233]
	Learning Rate: 0.00722326
	LOSS [training: 0.06001721945977854 | validation: 0.09028782524275361]
	TIME [epoch: 8.68 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05334176791949665		[learning rate: 0.0072122]
		[batch 20/20] avg loss: 0.05800516906624927		[learning rate: 0.0072011]
	Learning Rate: 0.00720112
	LOSS [training: 0.05567346849287297 | validation: 0.08721031466959442]
	TIME [epoch: 8.69 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05817379036304138		[learning rate: 0.0071901]
		[batch 20/20] avg loss: 0.04112399188159488		[learning rate: 0.007179]
	Learning Rate: 0.00717904
	LOSS [training: 0.04964889112231813 | validation: 0.1482471908690228]
	TIME [epoch: 8.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05560426054083535		[learning rate: 0.007168]
		[batch 20/20] avg loss: 0.09123502413971105		[learning rate: 0.007157]
	Learning Rate: 0.00715704
	LOSS [training: 0.07341964234027318 | validation: 0.049139561882227116]
	TIME [epoch: 8.69 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07522528734941228		[learning rate: 0.0071461]
		[batch 20/20] avg loss: 0.05324118318318213		[learning rate: 0.0071351]
	Learning Rate: 0.0071351
	LOSS [training: 0.06423323526629722 | validation: 0.13590907382369144]
	TIME [epoch: 8.69 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09357298772713869		[learning rate: 0.0071242]
		[batch 20/20] avg loss: 0.04641808145022121		[learning rate: 0.0071132]
	Learning Rate: 0.00711323
	LOSS [training: 0.06999553458867995 | validation: 0.04163487943354504]
	TIME [epoch: 8.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07092538943648202		[learning rate: 0.0071023]
		[batch 20/20] avg loss: 0.07080705903892749		[learning rate: 0.0070914]
	Learning Rate: 0.00709142
	LOSS [training: 0.07086622423770475 | validation: 0.10222293033500357]
	TIME [epoch: 8.71 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09466497014024085		[learning rate: 0.0070805]
		[batch 20/20] avg loss: 0.0748585332780756		[learning rate: 0.0070697]
	Learning Rate: 0.00706968
	LOSS [training: 0.0847617517091582 | validation: 0.16209856162943556]
	TIME [epoch: 8.69 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10044920588609978		[learning rate: 0.0070588]
		[batch 20/20] avg loss: 0.08364104918884295		[learning rate: 0.007048]
	Learning Rate: 0.00704801
	LOSS [training: 0.09204512753747136 | validation: 0.045832184448832025]
	TIME [epoch: 8.69 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06037450703384597		[learning rate: 0.0070372]
		[batch 20/20] avg loss: 0.06977055118620978		[learning rate: 0.0070264]
	Learning Rate: 0.00702641
	LOSS [training: 0.06507252911002788 | validation: 0.0808509684051157]
	TIME [epoch: 8.69 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08438396690921379		[learning rate: 0.0070156]
		[batch 20/20] avg loss: 0.0774758379587421		[learning rate: 0.0070049]
	Learning Rate: 0.00700487
	LOSS [training: 0.08092990243397795 | validation: 0.04149010135873602]
	TIME [epoch: 8.69 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05874361491758453		[learning rate: 0.0069941]
		[batch 20/20] avg loss: 0.05529217498848721		[learning rate: 0.0069834]
	Learning Rate: 0.0069834
	LOSS [training: 0.05701789495303587 | validation: 0.2130071881820842]
	TIME [epoch: 8.71 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07443980490317288		[learning rate: 0.0069727]
		[batch 20/20] avg loss: 0.07619681793052328		[learning rate: 0.006962]
	Learning Rate: 0.00696199
	LOSS [training: 0.0753183114168481 | validation: 0.10495962090412093]
	TIME [epoch: 8.69 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07832136181180761		[learning rate: 0.0069513]
		[batch 20/20] avg loss: 0.08488001954044079		[learning rate: 0.0069406]
	Learning Rate: 0.00694065
	LOSS [training: 0.08160069067612422 | validation: 0.10470297994927787]
	TIME [epoch: 8.68 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0909352377457038		[learning rate: 0.00693]
		[batch 20/20] avg loss: 0.054272845464447085		[learning rate: 0.0069194]
	Learning Rate: 0.00691937
	LOSS [training: 0.07260404160507546 | validation: 0.02935551464333708]
	TIME [epoch: 8.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06799504543093601		[learning rate: 0.0069088]
		[batch 20/20] avg loss: 0.06798196892163819		[learning rate: 0.0068982]
	Learning Rate: 0.00689816
	LOSS [training: 0.0679885071762871 | validation: 0.045390098891315414]
	TIME [epoch: 8.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06806088305189578		[learning rate: 0.0068876]
		[batch 20/20] avg loss: 0.05421156752430732		[learning rate: 0.006877]
	Learning Rate: 0.00687702
	LOSS [training: 0.061136225288101545 | validation: 0.1443651223163964]
	TIME [epoch: 8.72 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07201355893850336		[learning rate: 0.0068665]
		[batch 20/20] avg loss: 0.06651236188437522		[learning rate: 0.0068559]
	Learning Rate: 0.00685593
	LOSS [training: 0.06926296041143931 | validation: 0.15091596418078523]
	TIME [epoch: 8.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0709003291361414		[learning rate: 0.0068454]
		[batch 20/20] avg loss: 0.0835756649906318		[learning rate: 0.0068349]
	Learning Rate: 0.00683492
	LOSS [training: 0.07723799706338659 | validation: 0.08951925836323527]
	TIME [epoch: 8.71 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0968868528013212		[learning rate: 0.0068244]
		[batch 20/20] avg loss: 0.05877451467528531		[learning rate: 0.006814]
	Learning Rate: 0.00681397
	LOSS [training: 0.07783068373830325 | validation: 0.034736222616708]
	TIME [epoch: 8.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04699746583405554		[learning rate: 0.0068035]
		[batch 20/20] avg loss: 0.06217810846559964		[learning rate: 0.0067931]
	Learning Rate: 0.00679308
	LOSS [training: 0.054587787149827594 | validation: 0.14861227626334844]
	TIME [epoch: 8.68 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07343379777090961		[learning rate: 0.0067827]
		[batch 20/20] avg loss: 0.06810607301106496		[learning rate: 0.0067723]
	Learning Rate: 0.00677225
	LOSS [training: 0.07076993539098732 | validation: 0.024192617169621945]
	TIME [epoch: 8.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04481074887979299		[learning rate: 0.0067619]
		[batch 20/20] avg loss: 0.06469510795671482		[learning rate: 0.0067515]
	Learning Rate: 0.0067515
	LOSS [training: 0.05475292841825391 | validation: 0.04117622884732795]
	TIME [epoch: 8.72 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04891475901702498		[learning rate: 0.0067411]
		[batch 20/20] avg loss: 0.04893044548499022		[learning rate: 0.0067308]
	Learning Rate: 0.0067308
	LOSS [training: 0.04892260225100759 | validation: 0.05497176757799345]
	TIME [epoch: 8.71 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0453776475633038		[learning rate: 0.0067205]
		[batch 20/20] avg loss: 0.05449905407866037		[learning rate: 0.0067102]
	Learning Rate: 0.00671017
	LOSS [training: 0.04993835082098208 | validation: 0.05104366112057758]
	TIME [epoch: 8.71 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055995788909449305		[learning rate: 0.0066999]
		[batch 20/20] avg loss: 0.07308071866623803		[learning rate: 0.0066896]
	Learning Rate: 0.0066896
	LOSS [training: 0.0645382537878437 | validation: 0.05658227925751966]
	TIME [epoch: 8.7 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040137437770905664		[learning rate: 0.0066793]
		[batch 20/20] avg loss: 0.07673181322965916		[learning rate: 0.0066691]
	Learning Rate: 0.00666909
	LOSS [training: 0.058434625500282424 | validation: 0.09319258669755495]
	TIME [epoch: 8.73 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06337946503788947		[learning rate: 0.0066589]
		[batch 20/20] avg loss: 0.10382405454817642		[learning rate: 0.0066486]
	Learning Rate: 0.00664865
	LOSS [training: 0.08360175979303293 | validation: 0.13220683311324705]
	TIME [epoch: 8.71 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12730341686094265		[learning rate: 0.0066384]
		[batch 20/20] avg loss: 0.11688307325632315		[learning rate: 0.0066283]
	Learning Rate: 0.00662827
	LOSS [training: 0.12209324505863292 | validation: 0.0811757128286046]
	TIME [epoch: 8.74 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0685769649019637		[learning rate: 0.0066181]
		[batch 20/20] avg loss: 0.065007078131928		[learning rate: 0.0066079]
	Learning Rate: 0.00660795
	LOSS [training: 0.06679202151694587 | validation: 0.08105008473191165]
	TIME [epoch: 8.72 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06760307219665426		[learning rate: 0.0065978]
		[batch 20/20] avg loss: 0.06253593981629584		[learning rate: 0.0065877]
	Learning Rate: 0.00658769
	LOSS [training: 0.06506950600647506 | validation: 0.04347973196287726]
	TIME [epoch: 8.74 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05299955832643014		[learning rate: 0.0065776]
		[batch 20/20] avg loss: 0.03850591906771057		[learning rate: 0.0065675]
	Learning Rate: 0.0065675
	LOSS [training: 0.04575273869707036 | validation: 0.04078892638243879]
	TIME [epoch: 8.76 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06358137806974148		[learning rate: 0.0065574]
		[batch 20/20] avg loss: 0.0703275737508885		[learning rate: 0.0065474]
	Learning Rate: 0.00654737
	LOSS [training: 0.066954475910315 | validation: 0.1586076413591726]
	TIME [epoch: 8.74 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04008735309747975		[learning rate: 0.0065373]
		[batch 20/20] avg loss: 0.05278715577238342		[learning rate: 0.0065273]
	Learning Rate: 0.0065273
	LOSS [training: 0.04643725443493159 | validation: 0.038339771062573656]
	TIME [epoch: 8.74 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05765507731328715		[learning rate: 0.0065173]
		[batch 20/20] avg loss: 0.054198250334537824		[learning rate: 0.0065073]
	Learning Rate: 0.00650729
	LOSS [training: 0.05592666382391248 | validation: 0.09154386056719689]
	TIME [epoch: 8.73 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06450307118646054		[learning rate: 0.0064973]
		[batch 20/20] avg loss: 0.05300597992681057		[learning rate: 0.0064873]
	Learning Rate: 0.00648734
	LOSS [training: 0.05875452555663555 | validation: 0.05624197748974247]
	TIME [epoch: 8.73 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09775024611922914		[learning rate: 0.0064774]
		[batch 20/20] avg loss: 0.08949393847175612		[learning rate: 0.0064675]
	Learning Rate: 0.00646745
	LOSS [training: 0.09362209229549262 | validation: 0.044566637021391395]
	TIME [epoch: 8.75 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08785543704096821		[learning rate: 0.0064575]
		[batch 20/20] avg loss: 0.04272609329998337		[learning rate: 0.0064476]
	Learning Rate: 0.00644763
	LOSS [training: 0.06529076517047579 | validation: 0.030702725202766946]
	TIME [epoch: 8.71 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0688125920854255		[learning rate: 0.0064377]
		[batch 20/20] avg loss: 0.06684333354593659		[learning rate: 0.0064279]
	Learning Rate: 0.00642786
	LOSS [training: 0.06782796281568106 | validation: 0.027309040732693003]
	TIME [epoch: 8.71 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037734858005067665		[learning rate: 0.006418]
		[batch 20/20] avg loss: 0.051440802882366445		[learning rate: 0.0064082]
	Learning Rate: 0.00640816
	LOSS [training: 0.044587830443717044 | validation: 0.04851411904003391]
	TIME [epoch: 8.71 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05943998655151968		[learning rate: 0.0063983]
		[batch 20/20] avg loss: 0.04413804765242797		[learning rate: 0.0063885]
	Learning Rate: 0.00638852
	LOSS [training: 0.05178901710197383 | validation: 0.046665276134374045]
	TIME [epoch: 8.71 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04912283227271615		[learning rate: 0.0063787]
		[batch 20/20] avg loss: 0.03554399601395318		[learning rate: 0.0063689]
	Learning Rate: 0.00636893
	LOSS [training: 0.04233341414333468 | validation: 0.04396448489408614]
	TIME [epoch: 8.74 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0448282070249172		[learning rate: 0.0063592]
		[batch 20/20] avg loss: 0.1033020313513326		[learning rate: 0.0063494]
	Learning Rate: 0.00634941
	LOSS [training: 0.0740651191881249 | validation: 0.07410622101364747]
	TIME [epoch: 8.73 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04863311904771504		[learning rate: 0.0063397]
		[batch 20/20] avg loss: 0.0477219911402148		[learning rate: 0.0063299]
	Learning Rate: 0.00632995
	LOSS [training: 0.048177555093964916 | validation: 0.04683526330625528]
	TIME [epoch: 8.73 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06494146939901796		[learning rate: 0.0063202]
		[batch 20/20] avg loss: 0.05478200333964215		[learning rate: 0.0063105]
	Learning Rate: 0.00631054
	LOSS [training: 0.05986173636933005 | validation: 0.07029820059717926]
	TIME [epoch: 8.73 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03824655663765947		[learning rate: 0.0063009]
		[batch 20/20] avg loss: 0.045397775621658296		[learning rate: 0.0062912]
	Learning Rate: 0.0062912
	LOSS [training: 0.04182216612965888 | validation: 0.057419833871012134]
	TIME [epoch: 8.74 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05325991858972928		[learning rate: 0.0062815]
		[batch 20/20] avg loss: 0.06373354303771442		[learning rate: 0.0062719]
	Learning Rate: 0.00627191
	LOSS [training: 0.05849673081372185 | validation: 0.11276559347702127]
	TIME [epoch: 8.72 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07200055731716655		[learning rate: 0.0062623]
		[batch 20/20] avg loss: 0.06288827170237628		[learning rate: 0.0062527]
	Learning Rate: 0.00625269
	LOSS [training: 0.06744441450977141 | validation: 0.06575251490964926]
	TIME [epoch: 8.71 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07052450963815468		[learning rate: 0.0062431]
		[batch 20/20] avg loss: 0.09423048563084356		[learning rate: 0.0062335]
	Learning Rate: 0.00623352
	LOSS [training: 0.08237749763449911 | validation: 0.09357065825065783]
	TIME [epoch: 8.72 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04906137804291961		[learning rate: 0.006224]
		[batch 20/20] avg loss: 0.047582471067532404		[learning rate: 0.0062144]
	Learning Rate: 0.00621441
	LOSS [training: 0.04832192455522601 | validation: 0.053211614009675445]
	TIME [epoch: 8.73 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04448976449178221		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.03561535736212386		[learning rate: 0.0061954]
	Learning Rate: 0.00619536
	LOSS [training: 0.040052560926953036 | validation: 0.07011324473449601]
	TIME [epoch: 8.75 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04959066671501148		[learning rate: 0.0061859]
		[batch 20/20] avg loss: 0.04795060365141253		[learning rate: 0.0061764]
	Learning Rate: 0.00617637
	LOSS [training: 0.048770635183212 | validation: 0.05359290602822958]
	TIME [epoch: 8.73 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04997472608579243		[learning rate: 0.0061669]
		[batch 20/20] avg loss: 0.0436674875810031		[learning rate: 0.0061574]
	Learning Rate: 0.00615744
	LOSS [training: 0.046821106833397755 | validation: 0.02749456846683868]
	TIME [epoch: 8.74 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0436318599086986		[learning rate: 0.006148]
		[batch 20/20] avg loss: 0.04602145552508676		[learning rate: 0.0061386]
	Learning Rate: 0.00613856
	LOSS [training: 0.04482665771689268 | validation: 0.044398109344626485]
	TIME [epoch: 8.72 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05398450689390575		[learning rate: 0.0061291]
		[batch 20/20] avg loss: 0.03336007185909771		[learning rate: 0.0061197]
	Learning Rate: 0.00611974
	LOSS [training: 0.043672289376501715 | validation: 0.010670262158706038]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04369423017796751		[learning rate: 0.0061104]
		[batch 20/20] avg loss: 0.033692628463615606		[learning rate: 0.006101]
	Learning Rate: 0.00610099
	LOSS [training: 0.038693429320791554 | validation: 0.025783560054189726]
	TIME [epoch: 8.72 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0394972931897899		[learning rate: 0.0060916]
		[batch 20/20] avg loss: 0.06043009377531248		[learning rate: 0.0060823]
	Learning Rate: 0.00608228
	LOSS [training: 0.04996369348255118 | validation: 0.0742021674942072]
	TIME [epoch: 8.7 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05321842607998686		[learning rate: 0.006073]
		[batch 20/20] avg loss: 0.047291504465890505		[learning rate: 0.0060636]
	Learning Rate: 0.00606364
	LOSS [training: 0.05025496527293869 | validation: 0.07083817460119322]
	TIME [epoch: 8.71 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05214799150697008		[learning rate: 0.0060543]
		[batch 20/20] avg loss: 0.06186571758440286		[learning rate: 0.0060451]
	Learning Rate: 0.00604505
	LOSS [training: 0.05700685454568647 | validation: 0.04546451376172328]
	TIME [epoch: 8.7 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05249146515021693		[learning rate: 0.0060358]
		[batch 20/20] avg loss: 0.08162522061842445		[learning rate: 0.0060265]
	Learning Rate: 0.00602652
	LOSS [training: 0.06705834288432069 | validation: 0.0930684346641858]
	TIME [epoch: 8.69 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07083346429895852		[learning rate: 0.0060173]
		[batch 20/20] avg loss: 0.06610413326709161		[learning rate: 0.006008]
	Learning Rate: 0.00600805
	LOSS [training: 0.06846879878302507 | validation: 0.05245566682393853]
	TIME [epoch: 8.74 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038378137904531665		[learning rate: 0.0059988]
		[batch 20/20] avg loss: 0.07650566766310873		[learning rate: 0.0059896]
	Learning Rate: 0.00598963
	LOSS [training: 0.05744190278382022 | validation: 0.08389592986308744]
	TIME [epoch: 8.71 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05127907068334786		[learning rate: 0.0059804]
		[batch 20/20] avg loss: 0.054309029941014574		[learning rate: 0.0059713]
	Learning Rate: 0.00597127
	LOSS [training: 0.05279405031218122 | validation: 0.03837017278343655]
	TIME [epoch: 8.71 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03284158537713929		[learning rate: 0.0059621]
		[batch 20/20] avg loss: 0.031263593544492305		[learning rate: 0.005953]
	Learning Rate: 0.00595297
	LOSS [training: 0.0320525894608158 | validation: 0.024750898970227767]
	TIME [epoch: 8.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037224123041492994		[learning rate: 0.0059438]
		[batch 20/20] avg loss: 0.04876975686124146		[learning rate: 0.0059347]
	Learning Rate: 0.00593472
	LOSS [training: 0.04299693995136722 | validation: 0.06128018750090049]
	TIME [epoch: 8.72 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05599596471101653		[learning rate: 0.0059256]
		[batch 20/20] avg loss: 0.033706934362231225		[learning rate: 0.0059165]
	Learning Rate: 0.00591652
	LOSS [training: 0.044851449536623886 | validation: 0.08340372558916788]
	TIME [epoch: 8.75 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04603958626093353		[learning rate: 0.0059074]
		[batch 20/20] avg loss: 0.06925131857043379		[learning rate: 0.0058984]
	Learning Rate: 0.00589839
	LOSS [training: 0.057645452415683654 | validation: 0.04085280980316968]
	TIME [epoch: 8.71 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06813978608664786		[learning rate: 0.0058893]
		[batch 20/20] avg loss: 0.08957930676750653		[learning rate: 0.0058803]
	Learning Rate: 0.00588031
	LOSS [training: 0.0788595464270772 | validation: 0.0703132952087829]
	TIME [epoch: 8.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046244706095967716		[learning rate: 0.0058713]
		[batch 20/20] avg loss: 0.04011228449726008		[learning rate: 0.0058623]
	Learning Rate: 0.00586228
	LOSS [training: 0.0431784952966139 | validation: 0.05986115603677816]
	TIME [epoch: 8.7 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028733712114804834		[learning rate: 0.0058533]
		[batch 20/20] avg loss: 0.03682657350830774		[learning rate: 0.0058443]
	Learning Rate: 0.00584431
	LOSS [training: 0.032780142811556286 | validation: 0.03729315878076796]
	TIME [epoch: 8.73 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05012187753557603		[learning rate: 0.0058353]
		[batch 20/20] avg loss: 0.06673480827182163		[learning rate: 0.0058264]
	Learning Rate: 0.0058264
	LOSS [training: 0.058428342903698825 | validation: 0.048316531004759314]
	TIME [epoch: 8.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05310119489383042		[learning rate: 0.0058175]
		[batch 20/20] avg loss: 0.03781842333301601		[learning rate: 0.0058085]
	Learning Rate: 0.00580854
	LOSS [training: 0.0454598091134232 | validation: 0.11621065018477736]
	TIME [epoch: 8.71 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04326714443156086		[learning rate: 0.0057996]
		[batch 20/20] avg loss: 0.03814682046611439		[learning rate: 0.0057907]
	Learning Rate: 0.00579073
	LOSS [training: 0.04070698244883762 | validation: 0.040829817204769386]
	TIME [epoch: 8.71 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03177822632191799		[learning rate: 0.0057818]
		[batch 20/20] avg loss: 0.04824976588362685		[learning rate: 0.005773]
	Learning Rate: 0.00577298
	LOSS [training: 0.040013996102772426 | validation: 0.06143251925858276]
	TIME [epoch: 8.72 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052245594805544616		[learning rate: 0.0057641]
		[batch 20/20] avg loss: 0.046885461331956084		[learning rate: 0.0057553]
	Learning Rate: 0.00575528
	LOSS [training: 0.04956552806875034 | validation: 0.06715272678160944]
	TIME [epoch: 8.74 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02877664829067486		[learning rate: 0.0057465]
		[batch 20/20] avg loss: 0.05912622090935622		[learning rate: 0.0057376]
	Learning Rate: 0.00573764
	LOSS [training: 0.043951434600015535 | validation: 0.16854954841663433]
	TIME [epoch: 8.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0746425791375426		[learning rate: 0.0057288]
		[batch 20/20] avg loss: 0.06147551961775315		[learning rate: 0.0057201]
	Learning Rate: 0.00572005
	LOSS [training: 0.06805904937764787 | validation: 0.06168940161131667]
	TIME [epoch: 8.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05332621181163418		[learning rate: 0.0057113]
		[batch 20/20] avg loss: 0.0523433749588071		[learning rate: 0.0057025]
	Learning Rate: 0.00570252
	LOSS [training: 0.05283479338522064 | validation: 0.03940339053725921]
	TIME [epoch: 8.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05825975688972633		[learning rate: 0.0056938]
		[batch 20/20] avg loss: 0.04874058445893602		[learning rate: 0.005685]
	Learning Rate: 0.00568504
	LOSS [training: 0.053500170674331174 | validation: 0.045854796919852656]
	TIME [epoch: 8.73 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03993289039955898		[learning rate: 0.0056763]
		[batch 20/20] avg loss: 0.03981300756143883		[learning rate: 0.0056676]
	Learning Rate: 0.00566761
	LOSS [training: 0.0398729489804989 | validation: 0.07259476175750097]
	TIME [epoch: 8.74 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052833810307040416		[learning rate: 0.0056589]
		[batch 20/20] avg loss: 0.03493539850289542		[learning rate: 0.0056502]
	Learning Rate: 0.00565024
	LOSS [training: 0.043884604404967914 | validation: 0.0720305195990794]
	TIME [epoch: 8.72 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08954260579855562		[learning rate: 0.0056416]
		[batch 20/20] avg loss: 0.056965719616531685		[learning rate: 0.0056329]
	Learning Rate: 0.00563292
	LOSS [training: 0.07325416270754365 | validation: 0.07031080415392814]
	TIME [epoch: 8.69 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0675452525482422		[learning rate: 0.0056243]
		[batch 20/20] avg loss: 0.043926701133185496		[learning rate: 0.0056156]
	Learning Rate: 0.00561565
	LOSS [training: 0.05573597684071384 | validation: 0.07240999511418797]
	TIME [epoch: 8.71 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03134301384369052		[learning rate: 0.005607]
		[batch 20/20] avg loss: 0.036872671795156395		[learning rate: 0.0055984]
	Learning Rate: 0.00559843
	LOSS [training: 0.03410784281942346 | validation: 0.03467762924678773]
	TIME [epoch: 8.72 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03929217293163131		[learning rate: 0.0055898]
		[batch 20/20] avg loss: 0.06615962305050213		[learning rate: 0.0055813]
	Learning Rate: 0.00558127
	LOSS [training: 0.05272589799106673 | validation: 0.09438887314870134]
	TIME [epoch: 8.74 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06614649476910266		[learning rate: 0.0055727]
		[batch 20/20] avg loss: 0.042706811426752186		[learning rate: 0.0055642]
	Learning Rate: 0.00556416
	LOSS [training: 0.05442665309792742 | validation: 0.035870315785621665]
	TIME [epoch: 8.74 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03859979134750473		[learning rate: 0.0055556]
		[batch 20/20] avg loss: 0.05998123497402001		[learning rate: 0.0055471]
	Learning Rate: 0.00554711
	LOSS [training: 0.04929051316076237 | validation: 0.0861886689111171]
	TIME [epoch: 8.71 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04918917893738571		[learning rate: 0.0055386]
		[batch 20/20] avg loss: 0.047492821878683186		[learning rate: 0.0055301]
	Learning Rate: 0.0055301
	LOSS [training: 0.048341000408034455 | validation: 0.052651226670460044]
	TIME [epoch: 8.72 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03348122884464463		[learning rate: 0.0055216]
		[batch 20/20] avg loss: 0.030481049281528692		[learning rate: 0.0055132]
	Learning Rate: 0.00551315
	LOSS [training: 0.03198113906308666 | validation: 0.022893593024518336]
	TIME [epoch: 8.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05152215357250883		[learning rate: 0.0055047]
		[batch 20/20] avg loss: 0.043403044056497725		[learning rate: 0.0054963]
	Learning Rate: 0.00549625
	LOSS [training: 0.04746259881450328 | validation: 0.052284618747358985]
	TIME [epoch: 8.73 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057292511091557606		[learning rate: 0.0054878]
		[batch 20/20] avg loss: 0.037795120178961854		[learning rate: 0.0054794]
	Learning Rate: 0.0054794
	LOSS [training: 0.04754381563525972 | validation: 0.022981671854974225]
	TIME [epoch: 8.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03753714254253508		[learning rate: 0.005471]
		[batch 20/20] avg loss: 0.05595695943031285		[learning rate: 0.0054626]
	Learning Rate: 0.00546261
	LOSS [training: 0.04674705098642397 | validation: 0.07376302766005913]
	TIME [epoch: 8.71 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034297010252824056		[learning rate: 0.0054542]
		[batch 20/20] avg loss: 0.03624307092322672		[learning rate: 0.0054459]
	Learning Rate: 0.00544586
	LOSS [training: 0.03527004058802539 | validation: 0.03797188534610508]
	TIME [epoch: 8.72 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031279828047989504		[learning rate: 0.0054375]
		[batch 20/20] avg loss: 0.041496292731616494		[learning rate: 0.0054292]
	Learning Rate: 0.00542917
	LOSS [training: 0.036388060389802995 | validation: 0.043348074330227554]
	TIME [epoch: 8.72 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046212183585609114		[learning rate: 0.0054208]
		[batch 20/20] avg loss: 0.02571212469717915		[learning rate: 0.0054125]
	Learning Rate: 0.00541253
	LOSS [training: 0.035962154141394136 | validation: 0.04765126456620129]
	TIME [epoch: 8.74 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047847206579635426		[learning rate: 0.0054042]
		[batch 20/20] avg loss: 0.036324503007358025		[learning rate: 0.0053959]
	Learning Rate: 0.00539593
	LOSS [training: 0.04208585479349673 | validation: 0.032491640484299054]
	TIME [epoch: 8.71 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04913960649432404		[learning rate: 0.0053877]
		[batch 20/20] avg loss: 0.042530884699577845		[learning rate: 0.0053794]
	Learning Rate: 0.00537939
	LOSS [training: 0.045835245596950944 | validation: 0.03891859636138398]
	TIME [epoch: 8.73 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02883921023005263		[learning rate: 0.0053711]
		[batch 20/20] avg loss: 0.0738013605121585		[learning rate: 0.0053629]
	Learning Rate: 0.0053629
	LOSS [training: 0.05132028537110557 | validation: 0.018883309193459003]
	TIME [epoch: 8.73 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027138674695243347		[learning rate: 0.0053547]
		[batch 20/20] avg loss: 0.08529014857370841		[learning rate: 0.0053465]
	Learning Rate: 0.00534646
	LOSS [training: 0.056214411634475894 | validation: 0.043860949715540984]
	TIME [epoch: 8.75 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03562680760027881		[learning rate: 0.0053383]
		[batch 20/20] avg loss: 0.07470984714977036		[learning rate: 0.0053301]
	Learning Rate: 0.00533008
	LOSS [training: 0.055168327375024584 | validation: 0.02329182922965377]
	TIME [epoch: 8.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047666711481599386		[learning rate: 0.0053219]
		[batch 20/20] avg loss: 0.04149905955691284		[learning rate: 0.0053137]
	Learning Rate: 0.00531374
	LOSS [training: 0.04458288551925611 | validation: 0.05871156655531526]
	TIME [epoch: 8.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03203686314676238		[learning rate: 0.0053056]
		[batch 20/20] avg loss: 0.03819470235052256		[learning rate: 0.0052974]
	Learning Rate: 0.00529745
	LOSS [training: 0.035115782748642466 | validation: 0.030050871967822294]
	TIME [epoch: 8.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027647629694661096		[learning rate: 0.0052893]
		[batch 20/20] avg loss: 0.06315668472773565		[learning rate: 0.0052812]
	Learning Rate: 0.00528121
	LOSS [training: 0.04540215721119838 | validation: 0.03723935177218309]
	TIME [epoch: 8.72 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043267195145737423		[learning rate: 0.0052731]
		[batch 20/20] avg loss: 0.04394305870980807		[learning rate: 0.005265]
	Learning Rate: 0.00526502
	LOSS [training: 0.04360512692777274 | validation: 0.04224169195940587]
	TIME [epoch: 8.75 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030550008265005386		[learning rate: 0.0052569]
		[batch 20/20] avg loss: 0.05852792002798592		[learning rate: 0.0052489]
	Learning Rate: 0.00524888
	LOSS [training: 0.04453896414649565 | validation: 0.07257868418972771]
	TIME [epoch: 8.74 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04962731053638784		[learning rate: 0.0052408]
		[batch 20/20] avg loss: 0.047058182096749286		[learning rate: 0.0052328]
	Learning Rate: 0.00523279
	LOSS [training: 0.048342746316568555 | validation: 0.06493119541503792]
	TIME [epoch: 8.72 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0438131966559209		[learning rate: 0.0052248]
		[batch 20/20] avg loss: 0.037142063169260664		[learning rate: 0.0052167]
	Learning Rate: 0.00521675
	LOSS [training: 0.04047762991259078 | validation: 0.059147607992912826]
	TIME [epoch: 8.71 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05446379226120598		[learning rate: 0.0052087]
		[batch 20/20] avg loss: 0.05705601544199403		[learning rate: 0.0052008]
	Learning Rate: 0.00520076
	LOSS [training: 0.0557599038516 | validation: 0.06584569112396277]
	TIME [epoch: 8.71 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05803244766105142		[learning rate: 0.0051928]
		[batch 20/20] avg loss: 0.034471906534399625		[learning rate: 0.0051848]
	Learning Rate: 0.00518482
	LOSS [training: 0.04625217709772553 | validation: 0.037886311845561946]
	TIME [epoch: 8.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027611880202426255		[learning rate: 0.0051769]
		[batch 20/20] avg loss: 0.05351018255752051		[learning rate: 0.0051689]
	Learning Rate: 0.00516892
	LOSS [training: 0.040561031379973383 | validation: 0.05719631159831974]
	TIME [epoch: 8.71 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06252622858601306		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.0513636755732025		[learning rate: 0.0051531]
	Learning Rate: 0.00515308
	LOSS [training: 0.05694495207960778 | validation: 0.03949903053457267]
	TIME [epoch: 8.71 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04006447747237674		[learning rate: 0.0051452]
		[batch 20/20] avg loss: 0.06944632261503024		[learning rate: 0.0051373]
	Learning Rate: 0.00513728
	LOSS [training: 0.054755400043703495 | validation: 0.05255011562548175]
	TIME [epoch: 8.72 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045146676462549884		[learning rate: 0.0051294]
		[batch 20/20] avg loss: 0.04465917458083918		[learning rate: 0.0051215]
	Learning Rate: 0.00512153
	LOSS [training: 0.04490292552169453 | validation: 0.06140509778900018]
	TIME [epoch: 8.73 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040025529746448484		[learning rate: 0.0051137]
		[batch 20/20] avg loss: 0.05611427368312416		[learning rate: 0.0051058]
	Learning Rate: 0.00510583
	LOSS [training: 0.048069901714786326 | validation: 0.005360074475184215]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0501330835711693		[learning rate: 0.005098]
		[batch 20/20] avg loss: 0.058025592886776854		[learning rate: 0.0050902]
	Learning Rate: 0.00509018
	LOSS [training: 0.054079338228973074 | validation: 0.025581046687153376]
	TIME [epoch: 8.72 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032521556709687455		[learning rate: 0.0050824]
		[batch 20/20] avg loss: 0.049626345240728544		[learning rate: 0.0050746]
	Learning Rate: 0.00507458
	LOSS [training: 0.04107395097520801 | validation: 0.04836638222446593]
	TIME [epoch: 8.73 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03916917432508054		[learning rate: 0.0050668]
		[batch 20/20] avg loss: 0.04644503302621092		[learning rate: 0.005059]
	Learning Rate: 0.00505902
	LOSS [training: 0.04280710367564573 | validation: 0.0629625808859009]
	TIME [epoch: 8.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04103613775451177		[learning rate: 0.0050513]
		[batch 20/20] avg loss: 0.053599828210483355		[learning rate: 0.0050435]
	Learning Rate: 0.00504352
	LOSS [training: 0.047317982982497564 | validation: 0.04877236537505815]
	TIME [epoch: 8.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03008335232928841		[learning rate: 0.0050358]
		[batch 20/20] avg loss: 0.06633123130070692		[learning rate: 0.0050281]
	Learning Rate: 0.00502805
	LOSS [training: 0.048207291814997666 | validation: 0.10680676613381702]
	TIME [epoch: 8.75 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03194584027028301		[learning rate: 0.0050203]
		[batch 20/20] avg loss: 0.04782044685784764		[learning rate: 0.0050126]
	Learning Rate: 0.00501264
	LOSS [training: 0.03988314356406532 | validation: 0.03585509478079389]
	TIME [epoch: 8.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033340784660021924		[learning rate: 0.005005]
		[batch 20/20] avg loss: 0.022763281217033977		[learning rate: 0.0049973]
	Learning Rate: 0.00499728
	LOSS [training: 0.028052032938527947 | validation: 0.027951014984815253]
	TIME [epoch: 8.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028874653498738113		[learning rate: 0.0049896]
		[batch 20/20] avg loss: 0.035768892133567834		[learning rate: 0.004982]
	Learning Rate: 0.00498196
	LOSS [training: 0.03232177281615297 | validation: 0.01760427465652874]
	TIME [epoch: 8.74 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04935584850501005		[learning rate: 0.0049743]
		[batch 20/20] avg loss: 0.049666140077904264		[learning rate: 0.0049667]
	Learning Rate: 0.00496669
	LOSS [training: 0.04951099429145716 | validation: 0.03398890997423784]
	TIME [epoch: 8.73 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027389940663801255		[learning rate: 0.0049591]
		[batch 20/20] avg loss: 0.05308101879275769		[learning rate: 0.0049515]
	Learning Rate: 0.00495146
	LOSS [training: 0.04023547972827947 | validation: 0.08076119909371583]
	TIME [epoch: 8.76 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04348578518821987		[learning rate: 0.0049439]
		[batch 20/20] avg loss: 0.05807192852801333		[learning rate: 0.0049363]
	Learning Rate: 0.00493628
	LOSS [training: 0.0507788568581166 | validation: 0.03706181623701362]
	TIME [epoch: 8.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025951963980148825		[learning rate: 0.0049287]
		[batch 20/20] avg loss: 0.042559662920335986		[learning rate: 0.0049211]
	Learning Rate: 0.00492115
	LOSS [training: 0.0342558134502424 | validation: 0.049574167885494846]
	TIME [epoch: 8.74 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039506338834987255		[learning rate: 0.0049136]
		[batch 20/20] avg loss: 0.04213843951994933		[learning rate: 0.0049061]
	Learning Rate: 0.00490607
	LOSS [training: 0.0408223891774683 | validation: 0.036567781221179034]
	TIME [epoch: 8.73 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05942312528230888		[learning rate: 0.0048985]
		[batch 20/20] avg loss: 0.0420580207538086		[learning rate: 0.004891]
	Learning Rate: 0.00489103
	LOSS [training: 0.050740573018058746 | validation: 0.04549768488502684]
	TIME [epoch: 8.73 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046580813584205516		[learning rate: 0.0048835]
		[batch 20/20] avg loss: 0.038229199969560854		[learning rate: 0.004876]
	Learning Rate: 0.00487603
	LOSS [training: 0.042405006776883185 | validation: 0.036725477178534804]
	TIME [epoch: 8.75 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051344152171004034		[learning rate: 0.0048686]
		[batch 20/20] avg loss: 0.0445939769437584		[learning rate: 0.0048611]
	Learning Rate: 0.00486109
	LOSS [training: 0.047969064557381216 | validation: 0.02358239359867305]
	TIME [epoch: 8.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03320866211933142		[learning rate: 0.0048536]
		[batch 20/20] avg loss: 0.049423460307231416		[learning rate: 0.0048462]
	Learning Rate: 0.00484618
	LOSS [training: 0.041316061213281415 | validation: 0.042215898180348585]
	TIME [epoch: 8.73 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035245587409749465		[learning rate: 0.0048388]
		[batch 20/20] avg loss: 0.03707100456758533		[learning rate: 0.0048313]
	Learning Rate: 0.00483133
	LOSS [training: 0.036158295988667405 | validation: 0.04261759924283874]
	TIME [epoch: 8.72 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05105249804588656		[learning rate: 0.0048239]
		[batch 20/20] avg loss: 0.05941907389293512		[learning rate: 0.0048165]
	Learning Rate: 0.00481652
	LOSS [training: 0.05523578596941085 | validation: 0.02857159923070651]
	TIME [epoch: 8.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05170967269627312		[learning rate: 0.0048091]
		[batch 20/20] avg loss: 0.02114025915764119		[learning rate: 0.0048018]
	Learning Rate: 0.00480176
	LOSS [training: 0.03642496592695716 | validation: 0.02813411953923729]
	TIME [epoch: 8.75 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027027981434729852		[learning rate: 0.0047944]
		[batch 20/20] avg loss: 0.04370669877427301		[learning rate: 0.004787]
	Learning Rate: 0.00478704
	LOSS [training: 0.03536734010450144 | validation: 0.039543336986104106]
	TIME [epoch: 8.69 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03242656093307626		[learning rate: 0.0047797]
		[batch 20/20] avg loss: 0.03826178330171543		[learning rate: 0.0047724]
	Learning Rate: 0.00477236
	LOSS [training: 0.035344172117395836 | validation: 0.02294011270473569]
	TIME [epoch: 8.71 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029974681909113603		[learning rate: 0.004765]
		[batch 20/20] avg loss: 0.05126864247476328		[learning rate: 0.0047577]
	Learning Rate: 0.00475773
	LOSS [training: 0.04062166219193845 | validation: 0.11573554767306843]
	TIME [epoch: 8.71 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059082671657434346		[learning rate: 0.0047504]
		[batch 20/20] avg loss: 0.024535803394933428		[learning rate: 0.0047431]
	Learning Rate: 0.00474315
	LOSS [training: 0.04180923752618389 | validation: 0.02002532279487967]
	TIME [epoch: 8.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039484244772412636		[learning rate: 0.0047359]
		[batch 20/20] avg loss: 0.07077649878249032		[learning rate: 0.0047286]
	Learning Rate: 0.00472861
	LOSS [training: 0.05513037177745147 | validation: 0.0649878271761741]
	TIME [epoch: 8.74 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047441154852651934		[learning rate: 0.0047214]
		[batch 20/20] avg loss: 0.06918697212257952		[learning rate: 0.0047141]
	Learning Rate: 0.00471411
	LOSS [training: 0.05831406348761573 | validation: 0.022461183961494723]
	TIME [epoch: 8.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05671980685659548		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.04846969319525704		[learning rate: 0.0046997]
	Learning Rate: 0.00469966
	LOSS [training: 0.05259475002592625 | validation: 0.07437279946135247]
	TIME [epoch: 8.74 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03702747898763074		[learning rate: 0.0046925]
		[batch 20/20] avg loss: 0.04372579688984999		[learning rate: 0.0046853]
	Learning Rate: 0.00468526
	LOSS [training: 0.04037663793874036 | validation: 0.06557217336157918]
	TIME [epoch: 8.74 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05638756530665014		[learning rate: 0.0046781]
		[batch 20/20] avg loss: 0.03819794557930718		[learning rate: 0.0046709]
	Learning Rate: 0.00467089
	LOSS [training: 0.04729275544297867 | validation: 0.0379588074802992]
	TIME [epoch: 8.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039168662686904616		[learning rate: 0.0046637]
		[batch 20/20] avg loss: 0.02814292670775642		[learning rate: 0.0046566]
	Learning Rate: 0.00465658
	LOSS [training: 0.033655794697330516 | validation: 0.04263029911850896]
	TIME [epoch: 8.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04330155691994213		[learning rate: 0.0046494]
		[batch 20/20] avg loss: 0.03423495216429668		[learning rate: 0.0046423]
	Learning Rate: 0.0046423
	LOSS [training: 0.0387682545421194 | validation: 0.012112263231885158]
	TIME [epoch: 8.73 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027716090011403138		[learning rate: 0.0046352]
		[batch 20/20] avg loss: 0.038073411553831205		[learning rate: 0.0046281]
	Learning Rate: 0.00462807
	LOSS [training: 0.032894750782617166 | validation: 0.02024853046706722]
	TIME [epoch: 8.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04959492199098948		[learning rate: 0.004621]
		[batch 20/20] avg loss: 0.03311873900212373		[learning rate: 0.0046139]
	Learning Rate: 0.00461388
	LOSS [training: 0.041356830496556606 | validation: 0.047304360201888704]
	TIME [epoch: 8.72 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027972331500251153		[learning rate: 0.0046068]
		[batch 20/20] avg loss: 0.030012071248760867		[learning rate: 0.0045997]
	Learning Rate: 0.00459974
	LOSS [training: 0.02899220137450601 | validation: 0.041157398407491406]
	TIME [epoch: 8.75 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03421201984324709		[learning rate: 0.0045927]
		[batch 20/20] avg loss: 0.0223408623949683		[learning rate: 0.0045856]
	Learning Rate: 0.00458564
	LOSS [training: 0.0282764411191077 | validation: 0.015263821061964049]
	TIME [epoch: 8.8 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024401333584258666		[learning rate: 0.0045786]
		[batch 20/20] avg loss: 0.04081829066508631		[learning rate: 0.0045716]
	Learning Rate: 0.00457158
	LOSS [training: 0.032609812124672496 | validation: 0.03896893528685332]
	TIME [epoch: 8.71 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06509940864565993		[learning rate: 0.0045646]
		[batch 20/20] avg loss: 0.03126253483909267		[learning rate: 0.0045576]
	Learning Rate: 0.00455757
	LOSS [training: 0.048180971742376295 | validation: 0.020064344739930094]
	TIME [epoch: 8.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032204601496432984		[learning rate: 0.0045506]
		[batch 20/20] avg loss: 0.03009897003290518		[learning rate: 0.0045436]
	Learning Rate: 0.0045436
	LOSS [training: 0.031151785764669083 | validation: 0.049107509143299935]
	TIME [epoch: 8.71 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03754478404085394		[learning rate: 0.0045366]
		[batch 20/20] avg loss: 0.031715513487256035		[learning rate: 0.0045297]
	Learning Rate: 0.00452967
	LOSS [training: 0.03463014876405499 | validation: 0.06656870091481301]
	TIME [epoch: 8.75 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031228182286063205		[learning rate: 0.0045227]
		[batch 20/20] avg loss: 0.031340917270377536		[learning rate: 0.0045158]
	Learning Rate: 0.00451579
	LOSS [training: 0.031284549778220365 | validation: 0.028047755522136154]
	TIME [epoch: 8.73 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031613670503702875		[learning rate: 0.0045089]
		[batch 20/20] avg loss: 0.03285703068531383		[learning rate: 0.0045019]
	Learning Rate: 0.00450194
	LOSS [training: 0.03223535059450835 | validation: 0.022619268640302503]
	TIME [epoch: 8.71 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07897258161779962		[learning rate: 0.004495]
		[batch 20/20] avg loss: 0.03250132481925007		[learning rate: 0.0044881]
	Learning Rate: 0.00448814
	LOSS [training: 0.05573695321852484 | validation: 0.05229781774120651]
	TIME [epoch: 8.73 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04652620618642939		[learning rate: 0.0044813]
		[batch 20/20] avg loss: 0.06503521541204178		[learning rate: 0.0044744]
	Learning Rate: 0.00447438
	LOSS [training: 0.05578071079923559 | validation: 0.04693180383326858]
	TIME [epoch: 8.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05107741733984807		[learning rate: 0.0044675]
		[batch 20/20] avg loss: 0.02583679906130181		[learning rate: 0.0044607]
	Learning Rate: 0.00446067
	LOSS [training: 0.03845710820057494 | validation: 0.03865903573207899]
	TIME [epoch: 8.73 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03139723412883304		[learning rate: 0.0044538]
		[batch 20/20] avg loss: 0.036614525438176435		[learning rate: 0.004447]
	Learning Rate: 0.00444699
	LOSS [training: 0.03400587978350474 | validation: 0.04002312814232245]
	TIME [epoch: 8.73 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04339355905580552		[learning rate: 0.0044402]
		[batch 20/20] avg loss: 0.018690504756992014		[learning rate: 0.0044334]
	Learning Rate: 0.00443336
	LOSS [training: 0.03104203190639876 | validation: 0.0285017855595784]
	TIME [epoch: 8.71 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024297568217118832		[learning rate: 0.0044266]
		[batch 20/20] avg loss: 0.03298167784148551		[learning rate: 0.0044198]
	Learning Rate: 0.00441977
	LOSS [training: 0.028639623029302164 | validation: 0.011632945580970791]
	TIME [epoch: 8.72 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031635640418244335		[learning rate: 0.004413]
		[batch 20/20] avg loss: 0.04025394925729585		[learning rate: 0.0044062]
	Learning Rate: 0.00440622
	LOSS [training: 0.0359447948377701 | validation: 0.037219631041886456]
	TIME [epoch: 8.74 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036765106592404354		[learning rate: 0.0043995]
		[batch 20/20] avg loss: 0.02736567948446513		[learning rate: 0.0043927]
	Learning Rate: 0.00439272
	LOSS [training: 0.032065393038434746 | validation: 0.045901366538290994]
	TIME [epoch: 8.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031985712438555705		[learning rate: 0.004386]
		[batch 20/20] avg loss: 0.028381117619558065		[learning rate: 0.0043793]
	Learning Rate: 0.00437925
	LOSS [training: 0.030183415029056887 | validation: 0.01655616792013677]
	TIME [epoch: 8.71 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034903590487817736		[learning rate: 0.0043725]
		[batch 20/20] avg loss: 0.05069589429014817		[learning rate: 0.0043658]
	Learning Rate: 0.00436583
	LOSS [training: 0.042799742388982945 | validation: 0.06908245257799059]
	TIME [epoch: 8.69 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06023231255206777		[learning rate: 0.0043591]
		[batch 20/20] avg loss: 0.038900530514635545		[learning rate: 0.0043524]
	Learning Rate: 0.00435245
	LOSS [training: 0.049566421533351654 | validation: 0.02702641279596342]
	TIME [epoch: 8.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06025195644100736		[learning rate: 0.0043458]
		[batch 20/20] avg loss: 0.07400941505993018		[learning rate: 0.0043391]
	Learning Rate: 0.0043391
	LOSS [training: 0.06713068575046877 | validation: 0.04222787850041837]
	TIME [epoch: 8.74 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04608202207099491		[learning rate: 0.0043324]
		[batch 20/20] avg loss: 0.12935824198057336		[learning rate: 0.0043258]
	Learning Rate: 0.0043258
	LOSS [training: 0.08772013202578414 | validation: 0.06087741985757887]
	TIME [epoch: 8.73 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09016847237830293		[learning rate: 0.0043192]
		[batch 20/20] avg loss: 0.05660724515480272		[learning rate: 0.0043125]
	Learning Rate: 0.00431254
	LOSS [training: 0.07338785876655282 | validation: 0.03449973254389544]
	TIME [epoch: 8.71 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043887535068110495		[learning rate: 0.0043059]
		[batch 20/20] avg loss: 0.04933855540139622		[learning rate: 0.0042993]
	Learning Rate: 0.00429932
	LOSS [training: 0.046613045234753354 | validation: 0.07296520880508256]
	TIME [epoch: 8.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07958504797561308		[learning rate: 0.0042927]
		[batch 20/20] avg loss: 0.049929689757843294		[learning rate: 0.0042861]
	Learning Rate: 0.00428614
	LOSS [training: 0.0647573688667282 | validation: 0.053750297802393224]
	TIME [epoch: 8.73 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044943542197243655		[learning rate: 0.0042796]
		[batch 20/20] avg loss: 0.03494656009567246		[learning rate: 0.004273]
	Learning Rate: 0.004273
	LOSS [training: 0.039945051146458056 | validation: 0.04333638007205437]
	TIME [epoch: 8.74 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04667370018281811		[learning rate: 0.0042664]
		[batch 20/20] avg loss: 0.034617778981388944		[learning rate: 0.0042599]
	Learning Rate: 0.00425991
	LOSS [training: 0.040645739582103525 | validation: 0.04572369639677562]
	TIME [epoch: 8.71 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04705340075872407		[learning rate: 0.0042534]
		[batch 20/20] avg loss: 0.05322827132721025		[learning rate: 0.0042468]
	Learning Rate: 0.00424685
	LOSS [training: 0.050140836042967166 | validation: 0.031172998352908952]
	TIME [epoch: 8.71 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0695966447386142		[learning rate: 0.0042403]
		[batch 20/20] avg loss: 0.05998935476971186		[learning rate: 0.0042338]
	Learning Rate: 0.00423383
	LOSS [training: 0.06479299975416301 | validation: 0.049839857201784604]
	TIME [epoch: 8.71 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07342017940529047		[learning rate: 0.0042273]
		[batch 20/20] avg loss: 0.06740665124625941		[learning rate: 0.0042209]
	Learning Rate: 0.00422085
	LOSS [training: 0.07041341532577494 | validation: 0.0878188441986149]
	TIME [epoch: 8.7 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05057100673177338		[learning rate: 0.0042144]
		[batch 20/20] avg loss: 0.04014093176762682		[learning rate: 0.0042079]
	Learning Rate: 0.00420791
	LOSS [training: 0.045355969249700104 | validation: 0.04727726778422701]
	TIME [epoch: 8.73 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03751862643617296		[learning rate: 0.0042015]
		[batch 20/20] avg loss: 0.04091266820237403		[learning rate: 0.004195]
	Learning Rate: 0.00419501
	LOSS [training: 0.03921564731927349 | validation: 0.031233963638989747]
	TIME [epoch: 8.7 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02553459539742482		[learning rate: 0.0041886]
		[batch 20/20] avg loss: 0.038797842825598214		[learning rate: 0.0041822]
	Learning Rate: 0.00418215
	LOSS [training: 0.03216621911151152 | validation: 0.09027609204347613]
	TIME [epoch: 8.7 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03694193443885595		[learning rate: 0.0041757]
		[batch 20/20] avg loss: 0.05144572249533534		[learning rate: 0.0041693]
	Learning Rate: 0.00416933
	LOSS [training: 0.044193828467095644 | validation: 0.035356656046699235]
	TIME [epoch: 8.71 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0383309120056832		[learning rate: 0.0041629]
		[batch 20/20] avg loss: 0.03802315391323362		[learning rate: 0.0041566]
	Learning Rate: 0.00415655
	LOSS [training: 0.038177032959458415 | validation: 0.027053662709088922]
	TIME [epoch: 8.74 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027466090448895343		[learning rate: 0.0041502]
		[batch 20/20] avg loss: 0.027186416648723316		[learning rate: 0.0041438]
	Learning Rate: 0.00414381
	LOSS [training: 0.027326253548809333 | validation: 0.02150223603392222]
	TIME [epoch: 8.71 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04667149376375259		[learning rate: 0.0041375]
		[batch 20/20] avg loss: 0.02781728572545656		[learning rate: 0.0041311]
	Learning Rate: 0.00413111
	LOSS [training: 0.037244389744604575 | validation: 0.040008660804135884]
	TIME [epoch: 8.71 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061883770600564825		[learning rate: 0.0041248]
		[batch 20/20] avg loss: 0.1354371741666371		[learning rate: 0.0041184]
	Learning Rate: 0.00411845
	LOSS [training: 0.09866047238360096 | validation: 0.03883003172714035]
	TIME [epoch: 8.71 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04575683392807392		[learning rate: 0.0041121]
		[batch 20/20] avg loss: 0.04774814855812799		[learning rate: 0.0041058]
	Learning Rate: 0.00410582
	LOSS [training: 0.04675249124310097 | validation: 0.020960987349170512]
	TIME [epoch: 8.71 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04614314602928095		[learning rate: 0.0040995]
		[batch 20/20] avg loss: 0.05049291159521423		[learning rate: 0.0040932]
	Learning Rate: 0.00409323
	LOSS [training: 0.0483180288122476 | validation: 0.03428660694096381]
	TIME [epoch: 8.73 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044202659316282845		[learning rate: 0.004087]
		[batch 20/20] avg loss: 0.030916740653279207		[learning rate: 0.0040807]
	Learning Rate: 0.00408069
	LOSS [training: 0.03755969998478103 | validation: 0.026937370781352377]
	TIME [epoch: 8.72 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03615807840207796		[learning rate: 0.0040744]
		[batch 20/20] avg loss: 0.02825046228183531		[learning rate: 0.0040682]
	Learning Rate: 0.00406818
	LOSS [training: 0.03220427034195664 | validation: 0.037135682768095125]
	TIME [epoch: 8.71 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023751202827348834		[learning rate: 0.0040619]
		[batch 20/20] avg loss: 0.020219486405676994		[learning rate: 0.0040557]
	Learning Rate: 0.00405571
	LOSS [training: 0.021985344616512914 | validation: 0.017645339971567475]
	TIME [epoch: 8.71 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030198539074417587		[learning rate: 0.0040495]
		[batch 20/20] avg loss: 0.03959407497778039		[learning rate: 0.0040433]
	Learning Rate: 0.00404328
	LOSS [training: 0.034896307026098985 | validation: 0.02965299543431941]
	TIME [epoch: 8.71 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03316645323081971		[learning rate: 0.0040371]
		[batch 20/20] avg loss: 0.040968010120491366		[learning rate: 0.0040309]
	Learning Rate: 0.00403088
	LOSS [training: 0.03706723167565553 | validation: 0.015896836837813885]
	TIME [epoch: 8.71 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04693358973955703		[learning rate: 0.0040247]
		[batch 20/20] avg loss: 0.04302639173423231		[learning rate: 0.0040185]
	Learning Rate: 0.00401852
	LOSS [training: 0.04497999073689468 | validation: 0.03209400836930116]
	TIME [epoch: 8.68 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03289531884457543		[learning rate: 0.0040124]
		[batch 20/20] avg loss: 0.029662352204363373		[learning rate: 0.0040062]
	Learning Rate: 0.00400621
	LOSS [training: 0.0312788355244694 | validation: 0.0288934681355733]
	TIME [epoch: 8.71 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05209994645744241		[learning rate: 0.0040001]
		[batch 20/20] avg loss: 0.030676509805235765		[learning rate: 0.0039939]
	Learning Rate: 0.00399393
	LOSS [training: 0.04138822813133909 | validation: 0.024751148886073913]
	TIME [epoch: 8.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026861220264900322		[learning rate: 0.0039878]
		[batch 20/20] avg loss: 0.02945137649044291		[learning rate: 0.0039817]
	Learning Rate: 0.00398168
	LOSS [training: 0.02815629837767162 | validation: 0.02396602359648812]
	TIME [epoch: 8.7 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027753793336604732		[learning rate: 0.0039756]
		[batch 20/20] avg loss: 0.023870392632166608		[learning rate: 0.0039695]
	Learning Rate: 0.00396948
	LOSS [training: 0.025812092984385665 | validation: 0.04808859834289656]
	TIME [epoch: 8.73 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019812443308415893		[learning rate: 0.0039634]
		[batch 20/20] avg loss: 0.029925391947350766		[learning rate: 0.0039573]
	Learning Rate: 0.00395731
	LOSS [training: 0.024868917627883326 | validation: 0.03486801380929851]
	TIME [epoch: 8.73 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019788431563308444		[learning rate: 0.0039512]
		[batch 20/20] avg loss: 0.01865575524022537		[learning rate: 0.0039452]
	Learning Rate: 0.00394518
	LOSS [training: 0.0192220934017669 | validation: 0.01926745599871048]
	TIME [epoch: 8.71 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015043376584833049		[learning rate: 0.0039391]
		[batch 20/20] avg loss: 0.03224954805181043		[learning rate: 0.0039331]
	Learning Rate: 0.00393308
	LOSS [training: 0.023646462318321737 | validation: 0.025494336738335982]
	TIME [epoch: 8.7 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03672240196152118		[learning rate: 0.0039271]
		[batch 20/20] avg loss: 0.04494027463637977		[learning rate: 0.003921]
	Learning Rate: 0.00392103
	LOSS [training: 0.04083133829895048 | validation: 0.04848000748240954]
	TIME [epoch: 8.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021572034040471096		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.048362535178599675		[learning rate: 0.003909]
	Learning Rate: 0.00390901
	LOSS [training: 0.03496728460953538 | validation: 0.04746206166047945]
	TIME [epoch: 8.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037820352431668945		[learning rate: 0.003903]
		[batch 20/20] avg loss: 0.026813992302284255		[learning rate: 0.003897]
	Learning Rate: 0.00389703
	LOSS [training: 0.03231717236697661 | validation: 0.021679491857146065]
	TIME [epoch: 8.69 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04427259522521668		[learning rate: 0.003891]
		[batch 20/20] avg loss: 0.022229758403604225		[learning rate: 0.0038851]
	Learning Rate: 0.00388508
	LOSS [training: 0.03325117681441044 | validation: 0.01241264559988037]
	TIME [epoch: 8.7 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030751967243063748		[learning rate: 0.0038791]
		[batch 20/20] avg loss: 0.021102168697501266		[learning rate: 0.0038732]
	Learning Rate: 0.00387317
	LOSS [training: 0.02592706797028251 | validation: 0.027207781863128278]
	TIME [epoch: 8.68 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031527438384867226		[learning rate: 0.0038672]
		[batch 20/20] avg loss: 0.03234138973544583		[learning rate: 0.0038613]
	Learning Rate: 0.0038613
	LOSS [training: 0.03193441406015653 | validation: 0.03322879463990056]
	TIME [epoch: 8.73 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030786252442279328		[learning rate: 0.0038554]
		[batch 20/20] avg loss: 0.04319471321720062		[learning rate: 0.0038495]
	Learning Rate: 0.00384946
	LOSS [training: 0.036990482829739976 | validation: 0.039838305697596596]
	TIME [epoch: 8.71 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023203249397222757		[learning rate: 0.0038436]
		[batch 20/20] avg loss: 0.04882860447939143		[learning rate: 0.0038377]
	Learning Rate: 0.00383766
	LOSS [training: 0.03601592693830709 | validation: 0.05928190128643261]
	TIME [epoch: 8.73 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036844473699841264		[learning rate: 0.0038318]
		[batch 20/20] avg loss: 0.021231051224476476		[learning rate: 0.0038259]
	Learning Rate: 0.0038259
	LOSS [training: 0.029037762462158872 | validation: 0.029536037537319284]
	TIME [epoch: 8.72 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01714722119307661		[learning rate: 0.00382]
		[batch 20/20] avg loss: 0.04833063813566466		[learning rate: 0.0038142]
	Learning Rate: 0.00381417
	LOSS [training: 0.03273892966437063 | validation: 0.039574805016912726]
	TIME [epoch: 8.7 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03184882711411664		[learning rate: 0.0038083]
		[batch 20/20] avg loss: 0.03194870945858684		[learning rate: 0.0038025]
	Learning Rate: 0.00380248
	LOSS [training: 0.03189876828635175 | validation: 0.025221764451536653]
	TIME [epoch: 8.74 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030226148848198797		[learning rate: 0.0037966]
		[batch 20/20] avg loss: 0.030789415211981787		[learning rate: 0.0037908]
	Learning Rate: 0.00379082
	LOSS [training: 0.030507782030090295 | validation: 0.022651318168618467]
	TIME [epoch: 8.72 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023591677747217522		[learning rate: 0.003785]
		[batch 20/20] avg loss: 0.02338722532503711		[learning rate: 0.0037792]
	Learning Rate: 0.0037792
	LOSS [training: 0.02348945153612732 | validation: 0.02171366037088716]
	TIME [epoch: 8.72 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028309341563365835		[learning rate: 0.0037734]
		[batch 20/20] avg loss: 0.019197943338868505		[learning rate: 0.0037676]
	Learning Rate: 0.00376762
	LOSS [training: 0.023753642451117177 | validation: 0.020004513463139383]
	TIME [epoch: 8.72 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026847983464037967		[learning rate: 0.0037618]
		[batch 20/20] avg loss: 0.027753626622553658		[learning rate: 0.0037561]
	Learning Rate: 0.00375607
	LOSS [training: 0.027300805043295816 | validation: 0.015488615731322864]
	TIME [epoch: 8.71 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03014800015375175		[learning rate: 0.0037503]
		[batch 20/20] avg loss: 0.037272350926283185		[learning rate: 0.0037446]
	Learning Rate: 0.00374455
	LOSS [training: 0.03371017554001746 | validation: 0.02318960177537882]
	TIME [epoch: 8.75 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036733569950506396		[learning rate: 0.0037388]
		[batch 20/20] avg loss: 0.05051347165293792		[learning rate: 0.0037331]
	Learning Rate: 0.00373307
	LOSS [training: 0.04362352080172216 | validation: 0.05767180479110359]
	TIME [epoch: 8.71 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03648579059939078		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.03919794397332192		[learning rate: 0.0037216]
	Learning Rate: 0.00372163
	LOSS [training: 0.03784186728635634 | validation: 0.04776720028931015]
	TIME [epoch: 8.71 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04549647624904999		[learning rate: 0.0037159]
		[batch 20/20] avg loss: 0.052455799059845		[learning rate: 0.0037102]
	Learning Rate: 0.00371022
	LOSS [training: 0.0489761376544475 | validation: 0.04361663074258506]
	TIME [epoch: 8.69 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04495856586439001		[learning rate: 0.0037045]
		[batch 20/20] avg loss: 0.04379060198937657		[learning rate: 0.0036988]
	Learning Rate: 0.00369885
	LOSS [training: 0.0443745839268833 | validation: 0.03434257560311712]
	TIME [epoch: 8.69 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04057124395386284		[learning rate: 0.0036932]
		[batch 20/20] avg loss: 0.048148981593616816		[learning rate: 0.0036875]
	Learning Rate: 0.00368751
	LOSS [training: 0.04436011277373983 | validation: 0.044048183540287424]
	TIME [epoch: 8.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031646090484563724		[learning rate: 0.0036819]
		[batch 20/20] avg loss: 0.0478728884613996		[learning rate: 0.0036762]
	Learning Rate: 0.00367621
	LOSS [training: 0.03975948947298166 | validation: 0.04243332191656714]
	TIME [epoch: 8.7 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03403625685882667		[learning rate: 0.0036706]
		[batch 20/20] avg loss: 0.06381441898614197		[learning rate: 0.0036649]
	Learning Rate: 0.00366494
	LOSS [training: 0.04892533792248431 | validation: 0.0714043374968333]
	TIME [epoch: 8.7 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06757661317325217		[learning rate: 0.0036593]
		[batch 20/20] avg loss: 0.04617536592996338		[learning rate: 0.0036537]
	Learning Rate: 0.0036537
	LOSS [training: 0.056875989551607775 | validation: 0.03458188678588355]
	TIME [epoch: 8.69 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038796131487333324		[learning rate: 0.0036481]
		[batch 20/20] avg loss: 0.020256836884664422		[learning rate: 0.0036425]
	Learning Rate: 0.0036425
	LOSS [training: 0.02952648418599888 | validation: 0.018133003298694383]
	TIME [epoch: 8.72 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03573660115359291		[learning rate: 0.0036369]
		[batch 20/20] avg loss: 0.04405429841176396		[learning rate: 0.0036313]
	Learning Rate: 0.00363134
	LOSS [training: 0.039895449782678435 | validation: 0.04290429061175232]
	TIME [epoch: 8.72 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022683492141303253		[learning rate: 0.0036258]
		[batch 20/20] avg loss: 0.022388699617961732		[learning rate: 0.0036202]
	Learning Rate: 0.00362021
	LOSS [training: 0.022536095879632487 | validation: 0.03755992916467185]
	TIME [epoch: 8.71 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028927569243916512		[learning rate: 0.0036147]
		[batch 20/20] avg loss: 0.0348999134945573		[learning rate: 0.0036091]
	Learning Rate: 0.00360911
	LOSS [training: 0.0319137413692369 | validation: 0.021475101805681653]
	TIME [epoch: 8.71 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025282793528695058		[learning rate: 0.0036036]
		[batch 20/20] avg loss: 0.029809476773772704		[learning rate: 0.003598]
	Learning Rate: 0.00359805
	LOSS [training: 0.027546135151233886 | validation: 0.04200549252552235]
	TIME [epoch: 8.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030683259248056604		[learning rate: 0.0035925]
		[batch 20/20] avg loss: 0.01897054790581566		[learning rate: 0.003587]
	Learning Rate: 0.00358702
	LOSS [training: 0.024826903576936128 | validation: 0.036730528275196586]
	TIME [epoch: 8.73 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02985518887382347		[learning rate: 0.0035815]
		[batch 20/20] avg loss: 0.03319460525872108		[learning rate: 0.003576]
	Learning Rate: 0.00357602
	LOSS [training: 0.03152489706627228 | validation: 0.04169956594168629]
	TIME [epoch: 8.71 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0431716903350635		[learning rate: 0.0035705]
		[batch 20/20] avg loss: 0.02680995257638698		[learning rate: 0.0035651]
	Learning Rate: 0.00356506
	LOSS [training: 0.034990821455725234 | validation: 0.043225897358634156]
	TIME [epoch: 8.69 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0356301270368818		[learning rate: 0.0035596]
		[batch 20/20] avg loss: 0.02075863967761752		[learning rate: 0.0035541]
	Learning Rate: 0.00355413
	LOSS [training: 0.028194383357249657 | validation: 0.057414155930997]
	TIME [epoch: 8.7 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04635587225234817		[learning rate: 0.0035487]
		[batch 20/20] avg loss: 0.023653982130292545		[learning rate: 0.0035432]
	Learning Rate: 0.00354324
	LOSS [training: 0.035004927191320354 | validation: 0.024582743930413882]
	TIME [epoch: 8.7 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029504114997492354		[learning rate: 0.0035378]
		[batch 20/20] avg loss: 0.027583956175720393		[learning rate: 0.0035324]
	Learning Rate: 0.00353237
	LOSS [training: 0.028544035586606377 | validation: 0.026603676739214863]
	TIME [epoch: 8.73 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016990624600388762		[learning rate: 0.003527]
		[batch 20/20] avg loss: 0.0549890200806916		[learning rate: 0.0035215]
	Learning Rate: 0.00352155
	LOSS [training: 0.03598982234054018 | validation: 0.10054387407486087]
	TIME [epoch: 8.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03386664140666618		[learning rate: 0.0035161]
		[batch 20/20] avg loss: 0.033434004973651676		[learning rate: 0.0035108]
	Learning Rate: 0.00351075
	LOSS [training: 0.033650323190158926 | validation: 0.03555306118594039]
	TIME [epoch: 8.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02542848262971189		[learning rate: 0.0035054]
		[batch 20/20] avg loss: 0.04191552825214085		[learning rate: 0.0035]
	Learning Rate: 0.00349999
	LOSS [training: 0.03367200544092637 | validation: 0.04734435148602485]
	TIME [epoch: 8.69 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01565161303573303		[learning rate: 0.0034946]
		[batch 20/20] avg loss: 0.019159332813622493		[learning rate: 0.0034893]
	Learning Rate: 0.00348926
	LOSS [training: 0.01740547292467776 | validation: 0.06094584182893011]
	TIME [epoch: 8.69 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03230751356776252		[learning rate: 0.0034839]
		[batch 20/20] avg loss: 0.026419343414320774		[learning rate: 0.0034786]
	Learning Rate: 0.00347856
	LOSS [training: 0.02936342849104164 | validation: 0.02645584232877369]
	TIME [epoch: 8.73 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031706274497847436		[learning rate: 0.0034732]
		[batch 20/20] avg loss: 0.018617860222426202		[learning rate: 0.0034679]
	Learning Rate: 0.0034679
	LOSS [training: 0.02516206736013682 | validation: 0.013257407001227944]
	TIME [epoch: 9.15 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032489567326197656		[learning rate: 0.0034626]
		[batch 20/20] avg loss: 0.019974614362427014		[learning rate: 0.0034573]
	Learning Rate: 0.00345727
	LOSS [training: 0.02623209084431234 | validation: 0.02244265171122365]
	TIME [epoch: 8.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017884787679875204		[learning rate: 0.003452]
		[batch 20/20] avg loss: 0.025142459809354978		[learning rate: 0.0034467]
	Learning Rate: 0.00344667
	LOSS [training: 0.02151362374461509 | validation: 0.028526719376538542]
	TIME [epoch: 8.69 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03786376268780002		[learning rate: 0.0034414]
		[batch 20/20] avg loss: 0.02080919163584651		[learning rate: 0.0034361]
	Learning Rate: 0.00343611
	LOSS [training: 0.029336477161823266 | validation: 0.023384448776807027]
	TIME [epoch: 8.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03033312970671089		[learning rate: 0.0034308]
		[batch 20/20] avg loss: 0.031085366419392912		[learning rate: 0.0034256]
	Learning Rate: 0.00342557
	LOSS [training: 0.0307092480630519 | validation: 0.04129506736067552]
	TIME [epoch: 8.73 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02394740579334919		[learning rate: 0.0034203]
		[batch 20/20] avg loss: 0.029611057179797645		[learning rate: 0.0034151]
	Learning Rate: 0.00341507
	LOSS [training: 0.026779231486573424 | validation: 0.030165705198908658]
	TIME [epoch: 8.67 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029679624679386857		[learning rate: 0.0034098]
		[batch 20/20] avg loss: 0.02357134934224979		[learning rate: 0.0034046]
	Learning Rate: 0.0034046
	LOSS [training: 0.02662548701081832 | validation: 0.009800400248168084]
	TIME [epoch: 8.71 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027900542528701232		[learning rate: 0.0033994]
		[batch 20/20] avg loss: 0.012994385950868825		[learning rate: 0.0033942]
	Learning Rate: 0.00339417
	LOSS [training: 0.02044746423978503 | validation: 0.03312293592303315]
	TIME [epoch: 8.7 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029822671679664438		[learning rate: 0.003389]
		[batch 20/20] avg loss: 0.016268637805220677		[learning rate: 0.0033838]
	Learning Rate: 0.00338376
	LOSS [training: 0.023045654742442558 | validation: 0.0663930157730701]
	TIME [epoch: 8.74 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04018765648735317		[learning rate: 0.0033786]
		[batch 20/20] avg loss: 0.028472951975643597		[learning rate: 0.0033734]
	Learning Rate: 0.00337339
	LOSS [training: 0.03433030423149838 | validation: 0.0239058078326169]
	TIME [epoch: 8.73 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016426000428039256		[learning rate: 0.0033682]
		[batch 20/20] avg loss: 0.0209961549743135		[learning rate: 0.0033631]
	Learning Rate: 0.00336305
	LOSS [training: 0.01871107770117638 | validation: 0.017660650539558665]
	TIME [epoch: 8.71 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02537697551749536		[learning rate: 0.0033579]
		[batch 20/20] avg loss: 0.026268170777221815		[learning rate: 0.0033527]
	Learning Rate: 0.00335274
	LOSS [training: 0.025822573147358592 | validation: 0.03371156707490344]
	TIME [epoch: 8.71 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02092440138951221		[learning rate: 0.0033476]
		[batch 20/20] avg loss: 0.018821414321870795		[learning rate: 0.0033425]
	Learning Rate: 0.00334246
	LOSS [training: 0.019872907855691502 | validation: 0.031617909755764596]
	TIME [epoch: 8.71 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031012842813474207		[learning rate: 0.0033373]
		[batch 20/20] avg loss: 0.022573926503634593		[learning rate: 0.0033322]
	Learning Rate: 0.00333222
	LOSS [training: 0.026793384658554402 | validation: 0.011137135177456136]
	TIME [epoch: 8.74 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019994952623639532		[learning rate: 0.0033271]
		[batch 20/20] avg loss: 0.02031347062334472		[learning rate: 0.003322]
	Learning Rate: 0.003322
	LOSS [training: 0.02015421162349213 | validation: 0.053082718362436096]
	TIME [epoch: 8.73 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019614158302070413		[learning rate: 0.0033169]
		[batch 20/20] avg loss: 0.02650440818460869		[learning rate: 0.0033118]
	Learning Rate: 0.00331182
	LOSS [training: 0.023059283243339553 | validation: 0.02531948067644267]
	TIME [epoch: 8.72 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025842791849680895		[learning rate: 0.0033067]
		[batch 20/20] avg loss: 0.015116274693402635		[learning rate: 0.0033017]
	Learning Rate: 0.00330167
	LOSS [training: 0.020479533271541762 | validation: 0.06093471592531176]
	TIME [epoch: 8.72 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05272098897538342		[learning rate: 0.0032966]
		[batch 20/20] avg loss: 0.03233223423076004		[learning rate: 0.0032915]
	Learning Rate: 0.00329155
	LOSS [training: 0.04252661160307173 | validation: 0.021741182788049034]
	TIME [epoch: 8.72 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03994745099373907		[learning rate: 0.0032865]
		[batch 20/20] avg loss: 0.03383227352134442		[learning rate: 0.0032815]
	Learning Rate: 0.00328146
	LOSS [training: 0.03688986225754175 | validation: 0.05238021773353957]
	TIME [epoch: 8.74 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0495108482172742		[learning rate: 0.0032764]
		[batch 20/20] avg loss: 0.02142418348529534		[learning rate: 0.0032714]
	Learning Rate: 0.0032714
	LOSS [training: 0.03546751585128478 | validation: 0.02408840392804891]
	TIME [epoch: 8.71 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0224628067842019		[learning rate: 0.0032664]
		[batch 20/20] avg loss: 0.016482661763746943		[learning rate: 0.0032614]
	Learning Rate: 0.00326137
	LOSS [training: 0.01947273427397442 | validation: 0.048546026682720726]
	TIME [epoch: 8.71 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012876986633590886		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.02103944639594842		[learning rate: 0.0032514]
	Learning Rate: 0.00325137
	LOSS [training: 0.01695821651476965 | validation: 0.026671187315328755]
	TIME [epoch: 8.71 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026968117896616565		[learning rate: 0.0032464]
		[batch 20/20] avg loss: 0.01750953807149378		[learning rate: 0.0032414]
	Learning Rate: 0.00324141
	LOSS [training: 0.022238827984055177 | validation: 0.031194714358152565]
	TIME [epoch: 8.72 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03544093436257578		[learning rate: 0.0032364]
		[batch 20/20] avg loss: 0.02837453812148713		[learning rate: 0.0032315]
	Learning Rate: 0.00323147
	LOSS [training: 0.031907736242031456 | validation: 0.029322564394829907]
	TIME [epoch: 8.72 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024374042951190156		[learning rate: 0.0032265]
		[batch 20/20] avg loss: 0.015345099690275135		[learning rate: 0.0032216]
	Learning Rate: 0.00322156
	LOSS [training: 0.019859571320732648 | validation: 0.004617374806785322]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02476373042192367		[learning rate: 0.0032166]
		[batch 20/20] avg loss: 0.027347791658771776		[learning rate: 0.0032117]
	Learning Rate: 0.00321169
	LOSS [training: 0.026055761040347725 | validation: 0.03497627316092516]
	TIME [epoch: 8.71 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027665963837215268		[learning rate: 0.0032068]
		[batch 20/20] avg loss: 0.02703610728562006		[learning rate: 0.0032018]
	Learning Rate: 0.00320184
	LOSS [training: 0.02735103556141767 | validation: 0.03708220859664026]
	TIME [epoch: 8.72 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02152623805661561		[learning rate: 0.0031969]
		[batch 20/20] avg loss: 0.008448903125644823		[learning rate: 0.003192]
	Learning Rate: 0.00319203
	LOSS [training: 0.01498757059113022 | validation: 0.021276493585591104]
	TIME [epoch: 8.75 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019481582519575918		[learning rate: 0.0031871]
		[batch 20/20] avg loss: 0.014360590530753836		[learning rate: 0.0031822]
	Learning Rate: 0.00318224
	LOSS [training: 0.01692108652516488 | validation: 0.01849243410570742]
	TIME [epoch: 8.74 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020787984571320913		[learning rate: 0.0031774]
		[batch 20/20] avg loss: 0.021003397523872668		[learning rate: 0.0031725]
	Learning Rate: 0.00317249
	LOSS [training: 0.02089569104759679 | validation: 0.02120835650790407]
	TIME [epoch: 8.73 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018656674728153407		[learning rate: 0.0031676]
		[batch 20/20] avg loss: 0.017909902658270492		[learning rate: 0.0031628]
	Learning Rate: 0.00316276
	LOSS [training: 0.018283288693211948 | validation: 0.015851841309029502]
	TIME [epoch: 8.72 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02851852232861295		[learning rate: 0.0031579]
		[batch 20/20] avg loss: 0.02109936842514116		[learning rate: 0.0031531]
	Learning Rate: 0.00315307
	LOSS [training: 0.024808945376877053 | validation: 0.023846933609999576]
	TIME [epoch: 8.72 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009982674062779542		[learning rate: 0.0031482]
		[batch 20/20] avg loss: 0.021935576407773928		[learning rate: 0.0031434]
	Learning Rate: 0.0031434
	LOSS [training: 0.015959125235276737 | validation: 0.03128590094232007]
	TIME [epoch: 8.72 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03636231363681971		[learning rate: 0.0031386]
		[batch 20/20] avg loss: 0.019656151471943904		[learning rate: 0.0031338]
	Learning Rate: 0.00313377
	LOSS [training: 0.028009232554381806 | validation: 0.03335267093459604]
	TIME [epoch: 8.74 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026049422286757933		[learning rate: 0.003129]
		[batch 20/20] avg loss: 0.020273935834930555		[learning rate: 0.0031242]
	Learning Rate: 0.00312416
	LOSS [training: 0.02316167906084425 | validation: 0.02328491079086753]
	TIME [epoch: 8.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017235479727164257		[learning rate: 0.0031194]
		[batch 20/20] avg loss: 0.01554336016526515		[learning rate: 0.0031146]
	Learning Rate: 0.00311458
	LOSS [training: 0.0163894199462147 | validation: 0.0038017149364284928]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014523729188511506		[learning rate: 0.0031098]
		[batch 20/20] avg loss: 0.025502273343450992		[learning rate: 0.003105]
	Learning Rate: 0.00310504
	LOSS [training: 0.02001300126598125 | validation: 0.034521616083727204]
	TIME [epoch: 8.74 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029415438937559914		[learning rate: 0.0031003]
		[batch 20/20] avg loss: 0.01612706864015924		[learning rate: 0.0030955]
	Learning Rate: 0.00309552
	LOSS [training: 0.022771253788859573 | validation: 0.03499512075787881]
	TIME [epoch: 8.75 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02315114234433395		[learning rate: 0.0030908]
		[batch 20/20] avg loss: 0.018895504089274495		[learning rate: 0.003086]
	Learning Rate: 0.00308603
	LOSS [training: 0.02102332321680422 | validation: 0.030317516088855804]
	TIME [epoch: 8.74 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012638117676101582		[learning rate: 0.0030813]
		[batch 20/20] avg loss: 0.02141774194914701		[learning rate: 0.0030766]
	Learning Rate: 0.00307657
	LOSS [training: 0.017027929812624295 | validation: 0.013521490800545833]
	TIME [epoch: 8.72 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02581247965248406		[learning rate: 0.0030718]
		[batch 20/20] avg loss: 0.032544821280805145		[learning rate: 0.0030671]
	Learning Rate: 0.00306714
	LOSS [training: 0.029178650466644602 | validation: 0.022119259565406293]
	TIME [epoch: 8.72 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026679576154560847		[learning rate: 0.0030624]
		[batch 20/20] avg loss: 0.034785841840095005		[learning rate: 0.0030577]
	Learning Rate: 0.00305774
	LOSS [training: 0.03073270899732792 | validation: 0.018769129327135158]
	TIME [epoch: 8.74 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063139531650052635		[learning rate: 0.003053]
		[batch 20/20] avg loss: 0.032301382298517124		[learning rate: 0.0030484]
	Learning Rate: 0.00304836
	LOSS [training: 0.01930766773176119 | validation: 0.023997001756038513]
	TIME [epoch: 8.74 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010696465780060557		[learning rate: 0.0030437]
		[batch 20/20] avg loss: 0.017728537067856424		[learning rate: 0.003039]
	Learning Rate: 0.00303902
	LOSS [training: 0.014212501423958492 | validation: 0.04068697383893151]
	TIME [epoch: 8.75 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017729908776214703		[learning rate: 0.0030344]
		[batch 20/20] avg loss: 0.031271494159265334		[learning rate: 0.0030297]
	Learning Rate: 0.0030297
	LOSS [training: 0.024500701467740017 | validation: 0.03632158108240791]
	TIME [epoch: 8.74 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01958403237394748		[learning rate: 0.0030251]
		[batch 20/20] avg loss: 0.020449375503268		[learning rate: 0.0030204]
	Learning Rate: 0.00302042
	LOSS [training: 0.02001670393860774 | validation: 0.01603636037021332]
	TIME [epoch: 8.74 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009210720673071212		[learning rate: 0.0030158]
		[batch 20/20] avg loss: 0.004473956420901074		[learning rate: 0.0030112]
	Learning Rate: 0.00301116
	LOSS [training: 0.006842338546986145 | validation: 0.030213593621870802]
	TIME [epoch: 8.73 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04136180246839781		[learning rate: 0.0030065]
		[batch 20/20] avg loss: 0.02042722461105485		[learning rate: 0.0030019]
	Learning Rate: 0.00300193
	LOSS [training: 0.030894513539726325 | validation: 0.029632918515954506]
	TIME [epoch: 8.74 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030602166106022444		[learning rate: 0.0029973]
		[batch 20/20] avg loss: 0.08532732874478165		[learning rate: 0.0029927]
	Learning Rate: 0.00299272
	LOSS [training: 0.05796474742540203 | validation: 0.08000556786097594]
	TIME [epoch: 8.73 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036789181202328845		[learning rate: 0.0029881]
		[batch 20/20] avg loss: 0.023094705209287816		[learning rate: 0.0029835]
	Learning Rate: 0.00298355
	LOSS [training: 0.029941943205808334 | validation: 0.040013820092923286]
	TIME [epoch: 8.73 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035695977258958324		[learning rate: 0.002979]
		[batch 20/20] avg loss: 0.017154285052217413		[learning rate: 0.0029744]
	Learning Rate: 0.0029744
	LOSS [training: 0.026425131155587867 | validation: 0.027918261806504967]
	TIME [epoch: 8.76 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016468837388634618		[learning rate: 0.0029698]
		[batch 20/20] avg loss: 0.024303882956086018		[learning rate: 0.0029653]
	Learning Rate: 0.00296529
	LOSS [training: 0.020386360172360316 | validation: 0.01375107280771526]
	TIME [epoch: 8.74 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03671720718100941		[learning rate: 0.0029607]
		[batch 20/20] avg loss: 0.044756507526059856		[learning rate: 0.0029562]
	Learning Rate: 0.0029562
	LOSS [training: 0.04073685735353463 | validation: 0.01939536633655804]
	TIME [epoch: 8.78 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03042116114225137		[learning rate: 0.0029517]
		[batch 20/20] avg loss: 0.02386833019973574		[learning rate: 0.0029471]
	Learning Rate: 0.00294713
	LOSS [training: 0.027144745670993547 | validation: 0.020603064638414097]
	TIME [epoch: 8.75 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029250876615780574		[learning rate: 0.0029426]
		[batch 20/20] avg loss: 0.04355956424466083		[learning rate: 0.0029381]
	Learning Rate: 0.0029381
	LOSS [training: 0.03640522043022071 | validation: 0.026909286873694482]
	TIME [epoch: 8.75 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022163737453719433		[learning rate: 0.0029336]
		[batch 20/20] avg loss: 0.029312287390175513		[learning rate: 0.0029291]
	Learning Rate: 0.00292909
	LOSS [training: 0.02573801242194747 | validation: 0.034649862080576646]
	TIME [epoch: 8.75 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0316330125895664		[learning rate: 0.0029246]
		[batch 20/20] avg loss: 0.025113573831049164		[learning rate: 0.0029201]
	Learning Rate: 0.00292011
	LOSS [training: 0.028373293210307782 | validation: 0.01582014922072243]
	TIME [epoch: 8.75 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01844903407796732		[learning rate: 0.0029156]
		[batch 20/20] avg loss: 0.02794966536135286		[learning rate: 0.0029112]
	Learning Rate: 0.00291116
	LOSS [training: 0.023199349719660093 | validation: 0.026019332591907456]
	TIME [epoch: 8.78 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027512966876543598		[learning rate: 0.0029067]
		[batch 20/20] avg loss: 0.036795117914142104		[learning rate: 0.0029022]
	Learning Rate: 0.00290224
	LOSS [training: 0.032154042395342856 | validation: 0.0396203155023376]
	TIME [epoch: 8.75 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03521202793576074		[learning rate: 0.0028978]
		[batch 20/20] avg loss: 0.04709889070305008		[learning rate: 0.0028933]
	Learning Rate: 0.00289334
	LOSS [training: 0.04115545931940541 | validation: 0.024732068656355785]
	TIME [epoch: 8.74 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028180925589172297		[learning rate: 0.0028889]
		[batch 20/20] avg loss: 0.04102241373969341		[learning rate: 0.0028845]
	Learning Rate: 0.00288447
	LOSS [training: 0.03460166966443286 | validation: 0.017245800942587937]
	TIME [epoch: 8.75 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022297999992212197		[learning rate: 0.00288]
		[batch 20/20] avg loss: 0.04671376899269917		[learning rate: 0.0028756]
	Learning Rate: 0.00287563
	LOSS [training: 0.03450588449245569 | validation: 0.01747821544092022]
	TIME [epoch: 8.73 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01368609754105686		[learning rate: 0.0028712]
		[batch 20/20] avg loss: 0.015590928401554719		[learning rate: 0.0028668]
	Learning Rate: 0.00286682
	LOSS [training: 0.014638512971305789 | validation: 0.027937536074254938]
	TIME [epoch: 8.74 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021480492284307463		[learning rate: 0.0028624]
		[batch 20/20] avg loss: 0.01498425992316483		[learning rate: 0.002858]
	Learning Rate: 0.00285803
	LOSS [training: 0.018232376103736146 | validation: 0.020891808275873017]
	TIME [epoch: 8.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013808560466979595		[learning rate: 0.0028536]
		[batch 20/20] avg loss: 0.018746077331054503		[learning rate: 0.0028493]
	Learning Rate: 0.00284927
	LOSS [training: 0.01627731889901705 | validation: 0.02230887662576214]
	TIME [epoch: 8.74 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02393766782180821		[learning rate: 0.0028449]
		[batch 20/20] avg loss: 0.018032406689247257		[learning rate: 0.0028405]
	Learning Rate: 0.00284053
	LOSS [training: 0.020985037255527736 | validation: 0.020466094375459508]
	TIME [epoch: 8.73 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04024216712970121		[learning rate: 0.0028362]
		[batch 20/20] avg loss: 0.021772149139625566		[learning rate: 0.0028318]
	Learning Rate: 0.00283183
	LOSS [training: 0.0310071581346634 | validation: 0.026317984280875457]
	TIME [epoch: 8.75 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013448708405272273		[learning rate: 0.0028275]
		[batch 20/20] avg loss: 0.01679514520362748		[learning rate: 0.0028231]
	Learning Rate: 0.00282315
	LOSS [training: 0.015121926804449876 | validation: 0.014571531810531302]
	TIME [epoch: 8.74 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016189665892864247		[learning rate: 0.0028188]
		[batch 20/20] avg loss: 0.030573160752843942		[learning rate: 0.0028145]
	Learning Rate: 0.00281449
	LOSS [training: 0.02338141332285409 | validation: 0.015155466544287873]
	TIME [epoch: 8.72 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028697573955548727		[learning rate: 0.0028102]
		[batch 20/20] avg loss: 0.018691148033585698		[learning rate: 0.0028059]
	Learning Rate: 0.00280586
	LOSS [training: 0.02369436099456721 | validation: 0.00906715058308223]
	TIME [epoch: 8.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02765082662839279		[learning rate: 0.0028016]
		[batch 20/20] avg loss: 0.030891535101727798		[learning rate: 0.0027973]
	Learning Rate: 0.00279726
	LOSS [training: 0.0292711808650603 | validation: 0.02483399173949463]
	TIME [epoch: 8.72 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019947473267184015		[learning rate: 0.002793]
		[batch 20/20] avg loss: 0.012367246674737185		[learning rate: 0.0027887]
	Learning Rate: 0.00278869
	LOSS [training: 0.016157359970960596 | validation: 0.020382166588465284]
	TIME [epoch: 8.75 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021105957369007083		[learning rate: 0.0027844]
		[batch 20/20] avg loss: 0.020318032259479597		[learning rate: 0.0027801]
	Learning Rate: 0.00278014
	LOSS [training: 0.02071199481424334 | validation: 0.032108547454438335]
	TIME [epoch: 8.73 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015419502982513475		[learning rate: 0.0027759]
		[batch 20/20] avg loss: 0.014945145633875879		[learning rate: 0.0027716]
	Learning Rate: 0.00277162
	LOSS [training: 0.015182324308194675 | validation: 0.03559257828089484]
	TIME [epoch: 8.75 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02102230804690151		[learning rate: 0.0027674]
		[batch 20/20] avg loss: 0.00921588967933085		[learning rate: 0.0027631]
	Learning Rate: 0.00276312
	LOSS [training: 0.015119098863116182 | validation: 0.01281887787991871]
	TIME [epoch: 8.71 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028713096047609337		[learning rate: 0.0027589]
		[batch 20/20] avg loss: 0.018413178935689993		[learning rate: 0.0027547]
	Learning Rate: 0.00275465
	LOSS [training: 0.023563137491649667 | validation: 0.02770527946746116]
	TIME [epoch: 8.73 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01374657628440255		[learning rate: 0.0027504]
		[batch 20/20] avg loss: 0.028537411448238053		[learning rate: 0.0027462]
	Learning Rate: 0.00274621
	LOSS [training: 0.021141993866320305 | validation: 0.04501946549511253]
	TIME [epoch: 8.76 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031384870874798014		[learning rate: 0.002742]
		[batch 20/20] avg loss: 0.024003464944590002		[learning rate: 0.0027378]
	Learning Rate: 0.00273779
	LOSS [training: 0.027694167909694013 | validation: 0.018792326312420383]
	TIME [epoch: 8.74 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01365072306833521		[learning rate: 0.0027336]
		[batch 20/20] avg loss: 0.03923859163296269		[learning rate: 0.0027294]
	Learning Rate: 0.0027294
	LOSS [training: 0.026444657350648954 | validation: 0.035662170796902384]
	TIME [epoch: 8.74 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029747428890131666		[learning rate: 0.0027252]
		[batch 20/20] avg loss: 0.046087027442202756		[learning rate: 0.002721]
	Learning Rate: 0.00272103
	LOSS [training: 0.03791722816616721 | validation: 0.024564943521768516]
	TIME [epoch: 8.75 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02732271422714449		[learning rate: 0.0027169]
		[batch 20/20] avg loss: 0.013610561469056917		[learning rate: 0.0027127]
	Learning Rate: 0.00271269
	LOSS [training: 0.020466637848100704 | validation: 0.02271566987593085]
	TIME [epoch: 8.74 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010592012855011518		[learning rate: 0.0027085]
		[batch 20/20] avg loss: 0.009518398715148603		[learning rate: 0.0027044]
	Learning Rate: 0.00270437
	LOSS [training: 0.01005520578508006 | validation: 0.03806674204034175]
	TIME [epoch: 8.77 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015458737795942262		[learning rate: 0.0027002]
		[batch 20/20] avg loss: 0.01866413149894013		[learning rate: 0.0026961]
	Learning Rate: 0.00269608
	LOSS [training: 0.017061434647441195 | validation: 0.014570656509483169]
	TIME [epoch: 8.74 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017649122156398512		[learning rate: 0.0026919]
		[batch 20/20] avg loss: 0.028031448135897218		[learning rate: 0.0026878]
	Learning Rate: 0.00268782
	LOSS [training: 0.022840285146147863 | validation: 0.035676767806370815]
	TIME [epoch: 8.75 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01943355156266683		[learning rate: 0.0026837]
		[batch 20/20] avg loss: 0.022917892882454723		[learning rate: 0.0026796]
	Learning Rate: 0.00267958
	LOSS [training: 0.021175722222560776 | validation: 0.015723473508012443]
	TIME [epoch: 8.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018946285835726925		[learning rate: 0.0026755]
		[batch 20/20] avg loss: 0.023022187744668056		[learning rate: 0.0026714]
	Learning Rate: 0.00267137
	LOSS [training: 0.020984236790197487 | validation: 0.019845600062853977]
	TIME [epoch: 8.75 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025260539497994013		[learning rate: 0.0026673]
		[batch 20/20] avg loss: 0.015744595637973473		[learning rate: 0.0026632]
	Learning Rate: 0.00266318
	LOSS [training: 0.02050256756798374 | validation: 0.01801228452236399]
	TIME [epoch: 8.77 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006026422984938169		[learning rate: 0.0026591]
		[batch 20/20] avg loss: 0.021598523676408823		[learning rate: 0.002655]
	Learning Rate: 0.00265501
	LOSS [training: 0.01381247333067349 | validation: 0.05237513743568675]
	TIME [epoch: 8.73 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027144300506797937		[learning rate: 0.0026509]
		[batch 20/20] avg loss: 0.043235822786955806		[learning rate: 0.0026469]
	Learning Rate: 0.00264687
	LOSS [training: 0.035190061646876866 | validation: 0.05267283519530813]
	TIME [epoch: 8.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024880647217536533		[learning rate: 0.0026428]
		[batch 20/20] avg loss: 0.022766945799332174		[learning rate: 0.0026388]
	Learning Rate: 0.00263876
	LOSS [training: 0.02382379650843436 | validation: 0.02703564578270716]
	TIME [epoch: 8.75 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01605032626636529		[learning rate: 0.0026347]
		[batch 20/20] avg loss: 0.012761656054726966		[learning rate: 0.0026307]
	Learning Rate: 0.00263067
	LOSS [training: 0.014405991160546134 | validation: 0.008508850766246029]
	TIME [epoch: 8.76 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009644852537335168		[learning rate: 0.0026266]
		[batch 20/20] avg loss: 0.04024011365313569		[learning rate: 0.0026226]
	Learning Rate: 0.00262261
	LOSS [training: 0.02494248309523543 | validation: 0.011979804863065217]
	TIME [epoch: 8.74 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012890708305318438		[learning rate: 0.0026186]
		[batch 20/20] avg loss: 0.012714823661965672		[learning rate: 0.0026146]
	Learning Rate: 0.00261457
	LOSS [training: 0.012802765983642056 | validation: 0.010488264976700178]
	TIME [epoch: 8.74 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011277888550349495		[learning rate: 0.0026106]
		[batch 20/20] avg loss: 0.012973834159505749		[learning rate: 0.0026066]
	Learning Rate: 0.00260655
	LOSS [training: 0.01212586135492762 | validation: 0.013200885544120945]
	TIME [epoch: 8.74 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021119593361650573		[learning rate: 0.0026026]
		[batch 20/20] avg loss: 0.034563082920186915		[learning rate: 0.0025986]
	Learning Rate: 0.00259856
	LOSS [training: 0.027841338140918742 | validation: 0.02939687544917209]
	TIME [epoch: 8.72 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02940692666276563		[learning rate: 0.0025946]
		[batch 20/20] avg loss: 0.02678180856398014		[learning rate: 0.0025906]
	Learning Rate: 0.0025906
	LOSS [training: 0.02809436761337289 | validation: 0.023313618813164978]
	TIME [epoch: 8.75 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018305418514423712		[learning rate: 0.0025866]
		[batch 20/20] avg loss: 0.010310859599457827		[learning rate: 0.0025827]
	Learning Rate: 0.00258266
	LOSS [training: 0.014308139056940769 | validation: 0.016315150199375267]
	TIME [epoch: 8.74 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02109904684459212		[learning rate: 0.0025787]
		[batch 20/20] avg loss: 0.012439181458898137		[learning rate: 0.0025747]
	Learning Rate: 0.00257474
	LOSS [training: 0.01676911415174513 | validation: 0.0381713358490498]
	TIME [epoch: 8.73 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03095803562639418		[learning rate: 0.0025708]
		[batch 20/20] avg loss: 0.01747395705215296		[learning rate: 0.0025668]
	Learning Rate: 0.00256685
	LOSS [training: 0.02421599633927357 | validation: 0.03553106634562341]
	TIME [epoch: 8.74 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018167872267197658		[learning rate: 0.0025629]
		[batch 20/20] avg loss: 0.013397094702471917		[learning rate: 0.002559]
	Learning Rate: 0.00255898
	LOSS [training: 0.01578248348483479 | validation: 0.018502221708917153]
	TIME [epoch: 8.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017115902222311043		[learning rate: 0.0025551]
		[batch 20/20] avg loss: 0.015259073476908119		[learning rate: 0.0025511]
	Learning Rate: 0.00255113
	LOSS [training: 0.016187487849609584 | validation: 0.0324801419284734]
	TIME [epoch: 8.76 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007753448785813187		[learning rate: 0.0025472]
		[batch 20/20] avg loss: 0.014023070989014442		[learning rate: 0.0025433]
	Learning Rate: 0.00254331
	LOSS [training: 0.010888259887413817 | validation: 0.01771731399345229]
	TIME [epoch: 8.72 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03157367786088146		[learning rate: 0.0025394]
		[batch 20/20] avg loss: 0.01662562709736839		[learning rate: 0.0025355]
	Learning Rate: 0.00253552
	LOSS [training: 0.024099652479124926 | validation: 0.018356975304546993]
	TIME [epoch: 8.72 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013616935960383773		[learning rate: 0.0025316]
		[batch 20/20] avg loss: 0.011553042944482205		[learning rate: 0.0025277]
	Learning Rate: 0.00252774
	LOSS [training: 0.01258498945243299 | validation: 0.0185186098561307]
	TIME [epoch: 8.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012705841667761397		[learning rate: 0.0025239]
		[batch 20/20] avg loss: 0.011576554682682505		[learning rate: 0.00252]
	Learning Rate: 0.00252
	LOSS [training: 0.01214119817522195 | validation: 0.02663173031361636]
	TIME [epoch: 8.73 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019152979495519598		[learning rate: 0.0025161]
		[batch 20/20] avg loss: 0.015448232667589434		[learning rate: 0.0025123]
	Learning Rate: 0.00251227
	LOSS [training: 0.017300606081554516 | validation: 0.012974977388324583]
	TIME [epoch: 8.75 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020864149067518867		[learning rate: 0.0025084]
		[batch 20/20] avg loss: 0.016226577842035555		[learning rate: 0.0025046]
	Learning Rate: 0.00250457
	LOSS [training: 0.018545363454777204 | validation: 0.011965628189314698]
	TIME [epoch: 8.73 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021311141317874933		[learning rate: 0.0025007]
		[batch 20/20] avg loss: 0.028859255847872623		[learning rate: 0.0024969]
	Learning Rate: 0.00249689
	LOSS [training: 0.025085198582873773 | validation: 0.02257381662737771]
	TIME [epoch: 8.73 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013379678425796782		[learning rate: 0.0024931]
		[batch 20/20] avg loss: 0.015793119674609203		[learning rate: 0.0024892]
	Learning Rate: 0.00248924
	LOSS [training: 0.014586399050202994 | validation: 0.0128215488389193]
	TIME [epoch: 8.74 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01913428819256324		[learning rate: 0.0024854]
		[batch 20/20] avg loss: 0.02218082609046562		[learning rate: 0.0024816]
	Learning Rate: 0.00248161
	LOSS [training: 0.020657557141514425 | validation: 0.07916221435128827]
	TIME [epoch: 8.76 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037228889583464334		[learning rate: 0.0024778]
		[batch 20/20] avg loss: 0.014960486440378826		[learning rate: 0.002474]
	Learning Rate: 0.002474
	LOSS [training: 0.02609468801192158 | validation: 0.01429786794384763]
	TIME [epoch: 8.74 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04785322113110391		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.03493309533049299		[learning rate: 0.0024664]
	Learning Rate: 0.00246642
	LOSS [training: 0.04139315823079845 | validation: 0.023215339020284456]
	TIME [epoch: 8.74 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03719593840637973		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.017653313033774844		[learning rate: 0.0024589]
	Learning Rate: 0.00245886
	LOSS [training: 0.027424625720077284 | validation: 0.021736044879147067]
	TIME [epoch: 8.75 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033393957912011704		[learning rate: 0.0024551]
		[batch 20/20] avg loss: 0.032404099283306884		[learning rate: 0.0024513]
	Learning Rate: 0.00245132
	LOSS [training: 0.0328990285976593 | validation: 0.03226322879616012]
	TIME [epoch: 8.73 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01801482061272085		[learning rate: 0.0024476]
		[batch 20/20] avg loss: 0.030170358437720212		[learning rate: 0.0024438]
	Learning Rate: 0.00244381
	LOSS [training: 0.024092589525220535 | validation: 0.05264904503043781]
	TIME [epoch: 8.75 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037910355048208506		[learning rate: 0.0024401]
		[batch 20/20] avg loss: 0.018367897665956708		[learning rate: 0.0024363]
	Learning Rate: 0.00243631
	LOSS [training: 0.028139126357082607 | validation: 0.029965437055419493]
	TIME [epoch: 8.77 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01189078983064867		[learning rate: 0.0024326]
		[batch 20/20] avg loss: 0.020058105780045368		[learning rate: 0.0024288]
	Learning Rate: 0.00242885
	LOSS [training: 0.015974447805347017 | validation: 0.00040689670150997225]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_961.pth
	Model improved!!!
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01105035655442267		[learning rate: 0.0024251]
		[batch 20/20] avg loss: 0.007430886610888467		[learning rate: 0.0024214]
	Learning Rate: 0.0024214
	LOSS [training: 0.009240621582655565 | validation: 0.017073233963394635]
	TIME [epoch: 8.73 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01296188183314249		[learning rate: 0.0024177]
		[batch 20/20] avg loss: 0.00596074213694075		[learning rate: 0.002414]
	Learning Rate: 0.00241398
	LOSS [training: 0.00946131198504162 | validation: 0.020368722607518645]
	TIME [epoch: 8.73 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026774199392173948		[learning rate: 0.0024103]
		[batch 20/20] avg loss: 0.015422134018846534		[learning rate: 0.0024066]
	Learning Rate: 0.00240658
	LOSS [training: 0.021098166705510242 | validation: 0.023420587585398233]
	TIME [epoch: 8.75 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016812388428787413		[learning rate: 0.0024029]
		[batch 20/20] avg loss: 0.011092564380511394		[learning rate: 0.0023992]
	Learning Rate: 0.0023992
	LOSS [training: 0.0139524764046494 | validation: 0.021862431198663723]
	TIME [epoch: 8.75 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021807543463329908		[learning rate: 0.0023955]
		[batch 20/20] avg loss: 0.017455574401274022		[learning rate: 0.0023918]
	Learning Rate: 0.00239185
	LOSS [training: 0.019631558932301965 | validation: 0.015338342945270124]
	TIME [epoch: 8.76 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018175841326924496		[learning rate: 0.0023882]
		[batch 20/20] avg loss: 0.03925084683967457		[learning rate: 0.0023845]
	Learning Rate: 0.00238451
	LOSS [training: 0.02871334408329953 | validation: 0.07635862146152575]
	TIME [epoch: 8.73 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0224203587023413		[learning rate: 0.0023809]
		[batch 20/20] avg loss: 0.015491198182681808		[learning rate: 0.0023772]
	Learning Rate: 0.00237721
	LOSS [training: 0.018955778442511555 | validation: 0.010338636098887344]
	TIME [epoch: 8.73 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010320526762418661		[learning rate: 0.0023736]
		[batch 20/20] avg loss: 0.011887653174506688		[learning rate: 0.0023699]
	Learning Rate: 0.00236992
	LOSS [training: 0.011104089968462672 | validation: 0.033778760820035225]
	TIME [epoch: 8.76 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02009423260654686		[learning rate: 0.0023663]
		[batch 20/20] avg loss: 0.020301565273014775		[learning rate: 0.0023627]
	Learning Rate: 0.00236265
	LOSS [training: 0.020197898939780815 | validation: 0.025029234523118477]
	TIME [epoch: 8.75 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013638419306627661		[learning rate: 0.002359]
		[batch 20/20] avg loss: 0.011486832506348929		[learning rate: 0.0023554]
	Learning Rate: 0.00235541
	LOSS [training: 0.012562625906488295 | validation: 0.02242642686255494]
	TIME [epoch: 8.75 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02671599212591156		[learning rate: 0.0023518]
		[batch 20/20] avg loss: 0.013959394496763955		[learning rate: 0.0023482]
	Learning Rate: 0.00234819
	LOSS [training: 0.020337693311337758 | validation: 0.02870930550087392]
	TIME [epoch: 8.72 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015747186153834556		[learning rate: 0.0023446]
		[batch 20/20] avg loss: 0.015817938748713074		[learning rate: 0.002341]
	Learning Rate: 0.00234099
	LOSS [training: 0.015782562451273813 | validation: 0.008934911603280417]
	TIME [epoch: 8.72 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010926099889336092		[learning rate: 0.0023374]
		[batch 20/20] avg loss: 0.01915818250248908		[learning rate: 0.0023338]
	Learning Rate: 0.00233382
	LOSS [training: 0.015042141195912583 | validation: 0.03732635721274164]
	TIME [epoch: 8.74 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019382191265941453		[learning rate: 0.0023302]
		[batch 20/20] avg loss: 0.012994887622961474		[learning rate: 0.0023267]
	Learning Rate: 0.00232666
	LOSS [training: 0.016188539444451467 | validation: 0.01850495697932626]
	TIME [epoch: 8.72 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014778142435719444		[learning rate: 0.0023231]
		[batch 20/20] avg loss: 0.022103664840660557		[learning rate: 0.0023195]
	Learning Rate: 0.00231953
	LOSS [training: 0.018440903638189995 | validation: 0.041702513057745104]
	TIME [epoch: 8.72 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0215527034017508		[learning rate: 0.002316]
		[batch 20/20] avg loss: 0.018384134319935783		[learning rate: 0.0023124]
	Learning Rate: 0.00231242
	LOSS [training: 0.019968418860843287 | validation: 0.008332070842950944]
	TIME [epoch: 8.72 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021159984139456453		[learning rate: 0.0023089]
		[batch 20/20] avg loss: 0.007329119653715568		[learning rate: 0.0023053]
	Learning Rate: 0.00230533
	LOSS [training: 0.014244551896586011 | validation: 0.017897207196408894]
	TIME [epoch: 8.74 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0132212756384747		[learning rate: 0.0023018]
		[batch 20/20] avg loss: 0.016884151958189177		[learning rate: 0.0022983]
	Learning Rate: 0.00229826
	LOSS [training: 0.015052713798331941 | validation: 0.02756837951392882]
	TIME [epoch: 8.77 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010014683015444725		[learning rate: 0.0022947]
		[batch 20/20] avg loss: 0.017666835519411388		[learning rate: 0.0022912]
	Learning Rate: 0.00229122
	LOSS [training: 0.013840759267428055 | validation: 0.017680685920452292]
	TIME [epoch: 8.73 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006785349476582475		[learning rate: 0.0022877]
		[batch 20/20] avg loss: 0.01097350787022952		[learning rate: 0.0022842]
	Learning Rate: 0.0022842
	LOSS [training: 0.008879428673405998 | validation: 0.007292791805230389]
	TIME [epoch: 8.74 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010559482881938583		[learning rate: 0.0022807]
		[batch 20/20] avg loss: 0.013395186842707347		[learning rate: 0.0022772]
	Learning Rate: 0.00227719
	LOSS [training: 0.011977334862322964 | validation: 0.023946694254318703]
	TIME [epoch: 8.74 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017695669862655798		[learning rate: 0.0022737]
		[batch 20/20] avg loss: 0.01220982686654077		[learning rate: 0.0022702]
	Learning Rate: 0.00227021
	LOSS [training: 0.014952748364598284 | validation: 0.015385087509095115]
	TIME [epoch: 8.75 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018614155384468884		[learning rate: 0.0022667]
		[batch 20/20] avg loss: 0.015577973466023645		[learning rate: 0.0022633]
	Learning Rate: 0.00226325
	LOSS [training: 0.017096064425246266 | validation: 0.010800588238348237]
	TIME [epoch: 8.74 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015088711937393228		[learning rate: 0.0022598]
		[batch 20/20] avg loss: 0.010903781184271485		[learning rate: 0.0022563]
	Learning Rate: 0.00225632
	LOSS [training: 0.012996246560832356 | validation: 0.008307351697338101]
	TIME [epoch: 8.73 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006074681569440693		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.029637326934256848		[learning rate: 0.0022494]
	Learning Rate: 0.0022494
	LOSS [training: 0.01785600425184877 | validation: 0.01878497288747113]
	TIME [epoch: 8.72 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018327096663881708		[learning rate: 0.0022459]
		[batch 20/20] avg loss: 0.013758914526818766		[learning rate: 0.0022425]
	Learning Rate: 0.0022425
	LOSS [training: 0.01604300559535024 | validation: 0.00880807283279171]
	TIME [epoch: 8.73 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013586721831975582		[learning rate: 0.0022391]
		[batch 20/20] avg loss: 0.003990078506497954		[learning rate: 0.0022356]
	Learning Rate: 0.00223563
	LOSS [training: 0.008788400169236767 | validation: 0.000258996299041001]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_988.pth
	Model improved!!!
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010306028799262118		[learning rate: 0.0022322]
		[batch 20/20] avg loss: 0.028346821066002954		[learning rate: 0.0022288]
	Learning Rate: 0.00222878
	LOSS [training: 0.019326424932632535 | validation: 0.024805409369404702]
	TIME [epoch: 8.72 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013827069810639558		[learning rate: 0.0022254]
		[batch 20/20] avg loss: 0.008031630517238602		[learning rate: 0.0022219]
	Learning Rate: 0.00222194
	LOSS [training: 0.010929350163939082 | validation: 0.007263178724674553]
	TIME [epoch: 8.73 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012006182138600389		[learning rate: 0.0022185]
		[batch 20/20] avg loss: 0.03070233016906685		[learning rate: 0.0022151]
	Learning Rate: 0.00221513
	LOSS [training: 0.021354256153833618 | validation: 0.05347153245293537]
	TIME [epoch: 8.74 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027711129197540284		[learning rate: 0.0022117]
		[batch 20/20] avg loss: 0.019661476261153485		[learning rate: 0.0022083]
	Learning Rate: 0.00220834
	LOSS [training: 0.023686302729346888 | validation: 0.023661819460992044]
	TIME [epoch: 8.74 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005302888016192719		[learning rate: 0.002205]
		[batch 20/20] avg loss: 0.020947583318883425		[learning rate: 0.0022016]
	Learning Rate: 0.00220157
	LOSS [training: 0.013125235667538074 | validation: 0.0027030400004151977]
	TIME [epoch: 8.75 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010410279046298631		[learning rate: 0.0021982]
		[batch 20/20] avg loss: 0.007526760515870532		[learning rate: 0.0021948]
	Learning Rate: 0.00219483
	LOSS [training: 0.008968519781084582 | validation: 0.006207621533279542]
	TIME [epoch: 8.72 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016655639042807313		[learning rate: 0.0021915]
		[batch 20/20] avg loss: 0.0033556177441813537		[learning rate: 0.0021881]
	Learning Rate: 0.0021881
	LOSS [training: 0.010005628393494337 | validation: 0.00951517745148197]
	TIME [epoch: 8.75 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011050861080181553		[learning rate: 0.0021847]
		[batch 20/20] avg loss: 0.005437766513274699		[learning rate: 0.0021814]
	Learning Rate: 0.00218139
	LOSS [training: 0.008244313796728126 | validation: 0.010006288932237204]
	TIME [epoch: 8.73 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022147779289283808		[learning rate: 0.002178]
		[batch 20/20] avg loss: 0.010507410591039473		[learning rate: 0.0021747]
	Learning Rate: 0.0021747
	LOSS [training: 0.01632759494016164 | validation: 0.010037474271758474]
	TIME [epoch: 8.72 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015875967022540294		[learning rate: 0.0021714]
		[batch 20/20] avg loss: 0.011986690229103715		[learning rate: 0.002168]
	Learning Rate: 0.00216804
	LOSS [training: 0.013931328625822003 | validation: 0.03057378645640269]
	TIME [epoch: 8.75 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006707512571140747		[learning rate: 0.0021647]
		[batch 20/20] avg loss: 0.021516274907304702		[learning rate: 0.0021614]
	Learning Rate: 0.00216139
	LOSS [training: 0.014111893739222722 | validation: 0.02528372408238278]
	TIME [epoch: 8.75 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019767870767447786		[learning rate: 0.0021581]
		[batch 20/20] avg loss: 0.00845136948590153		[learning rate: 0.0021548]
	Learning Rate: 0.00215477
	LOSS [training: 0.014109620126674655 | validation: 0.012520272452770117]
	TIME [epoch: 8.74 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00910162909383949		[learning rate: 0.0021515]
		[batch 20/20] avg loss: 0.0161462934450045		[learning rate: 0.0021482]
	Learning Rate: 0.00214816
	LOSS [training: 0.012623961269421993 | validation: 0.022751251625914046]
	TIME [epoch: 8.71 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010974095047410924		[learning rate: 0.0021449]
		[batch 20/20] avg loss: 0.010630311547616351		[learning rate: 0.0021416]
	Learning Rate: 0.00214158
	LOSS [training: 0.010802203297513639 | validation: 0.017900912593473872]
	TIME [epoch: 8.72 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004502944686088828		[learning rate: 0.0021383]
		[batch 20/20] avg loss: 0.007960712036530034		[learning rate: 0.002135]
	Learning Rate: 0.00213501
	LOSS [training: 0.00623182836130943 | validation: 0.005682067275579564]
	TIME [epoch: 8.75 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005448021590369034		[learning rate: 0.0021317]
		[batch 20/20] avg loss: 0.030947160163492072		[learning rate: 0.0021285]
	Learning Rate: 0.00212847
	LOSS [training: 0.018197590876930552 | validation: 0.019503511162353077]
	TIME [epoch: 8.73 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009359045415214384		[learning rate: 0.0021252]
		[batch 20/20] avg loss: 0.008681655178867497		[learning rate: 0.0021219]
	Learning Rate: 0.00212194
	LOSS [training: 0.009020350297040942 | validation: 0.003038252593882506]
	TIME [epoch: 8.74 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01304824810010236		[learning rate: 0.0021187]
		[batch 20/20] avg loss: 0.018800494425573765		[learning rate: 0.0021154]
	Learning Rate: 0.00211544
	LOSS [training: 0.015924371262838063 | validation: 0.008707440685060113]
	TIME [epoch: 8.72 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007705694120348637		[learning rate: 0.0021122]
		[batch 20/20] avg loss: 0.002908710539987081		[learning rate: 0.002109]
	Learning Rate: 0.00210895
	LOSS [training: 0.005307202330167858 | validation: 0.005829258802399023]
	TIME [epoch: 8.73 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0051596587549701456		[learning rate: 0.0021057]
		[batch 20/20] avg loss: 0.010267974982674413		[learning rate: 0.0021025]
	Learning Rate: 0.00210249
	LOSS [training: 0.007713816868822279 | validation: 0.021756168331054034]
	TIME [epoch: 8.75 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01129279088410298		[learning rate: 0.0020993]
		[batch 20/20] avg loss: 0.010569545469636062		[learning rate: 0.002096]
	Learning Rate: 0.00209604
	LOSS [training: 0.010931168176869519 | validation: 0.01732461392645398]
	TIME [epoch: 8.74 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017401870781343854		[learning rate: 0.0020928]
		[batch 20/20] avg loss: 0.019612507754047347		[learning rate: 0.0020896]
	Learning Rate: 0.00208962
	LOSS [training: 0.0185071892676956 | validation: 0.025397916210847256]
	TIME [epoch: 8.73 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014442010309539368		[learning rate: 0.0020864]
		[batch 20/20] avg loss: 0.021851361128041476		[learning rate: 0.0020832]
	Learning Rate: 0.00208321
	LOSS [training: 0.01814668571879042 | validation: 0.03620018368876644]
	TIME [epoch: 8.73 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009830930221603485		[learning rate: 0.00208]
		[batch 20/20] avg loss: 0.008216967006083516		[learning rate: 0.0020768]
	Learning Rate: 0.00207683
	LOSS [training: 0.0090239486138435 | validation: 0.013644515921687674]
	TIME [epoch: 8.74 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012732019405329375		[learning rate: 0.0020736]
		[batch 20/20] avg loss: 0.01264569492343683		[learning rate: 0.0020705]
	Learning Rate: 0.00207046
	LOSS [training: 0.012688857164383102 | validation: 0.014104467357013687]
	TIME [epoch: 8.74 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014844557804944133		[learning rate: 0.0020673]
		[batch 20/20] avg loss: 0.012604631973899699		[learning rate: 0.0020641]
	Learning Rate: 0.00206411
	LOSS [training: 0.013724594889421918 | validation: 0.01599259151107888]
	TIME [epoch: 8.72 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01208256326951298		[learning rate: 0.0020609]
		[batch 20/20] avg loss: 0.014076055348923958		[learning rate: 0.0020578]
	Learning Rate: 0.00205778
	LOSS [training: 0.013079309309218468 | validation: 0.02336599260090193]
	TIME [epoch: 8.73 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012946783888106134		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.016872633050573762		[learning rate: 0.0020515]
	Learning Rate: 0.00205148
	LOSS [training: 0.01490970846933995 | validation: 0.018204991915955742]
	TIME [epoch: 8.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01164690423392246		[learning rate: 0.0020483]
		[batch 20/20] avg loss: 0.008838158388734385		[learning rate: 0.0020452]
	Learning Rate: 0.00204519
	LOSS [training: 0.01024253131132842 | validation: 0.011626436351419554]
	TIME [epoch: 8.73 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011425031074881572		[learning rate: 0.0020421]
		[batch 20/20] avg loss: 0.008413124860009585		[learning rate: 0.0020389]
	Learning Rate: 0.00203892
	LOSS [training: 0.00991907796744558 | validation: 0.010694614651601379]
	TIME [epoch: 8.72 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009568637598772318		[learning rate: 0.0020358]
		[batch 20/20] avg loss: 0.011636937950636077		[learning rate: 0.0020327]
	Learning Rate: 0.00203267
	LOSS [training: 0.010602787774704196 | validation: 0.009565304204260794]
	TIME [epoch: 8.76 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007759297098265274		[learning rate: 0.0020296]
		[batch 20/20] avg loss: 0.013261778369235777		[learning rate: 0.0020264]
	Learning Rate: 0.00202644
	LOSS [training: 0.010510537733750526 | validation: 0.021537652916866115]
	TIME [epoch: 8.74 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018285103681229896		[learning rate: 0.0020233]
		[batch 20/20] avg loss: 0.010246526991635339		[learning rate: 0.0020202]
	Learning Rate: 0.00202023
	LOSS [training: 0.014265815336432616 | validation: 0.014795548140187293]
	TIME [epoch: 8.74 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012887375952045522		[learning rate: 0.0020171]
		[batch 20/20] avg loss: 0.01649882683770911		[learning rate: 0.002014]
	Learning Rate: 0.00201403
	LOSS [training: 0.014693101394877317 | validation: 0.045001383858010696]
	TIME [epoch: 8.73 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019314922338085128		[learning rate: 0.0020109]
		[batch 20/20] avg loss: 0.02925144466407865		[learning rate: 0.0020079]
	Learning Rate: 0.00200786
	LOSS [training: 0.024283183501081888 | validation: 0.020055348982509387]
	TIME [epoch: 8.73 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017574224667345956		[learning rate: 0.0020048]
		[batch 20/20] avg loss: 0.011068680562710877		[learning rate: 0.0020017]
	Learning Rate: 0.0020017
	LOSS [training: 0.014321452615028412 | validation: 0.021276171457381393]
	TIME [epoch: 8.72 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008892580047274227		[learning rate: 0.0019986]
		[batch 20/20] avg loss: 0.008420171088183787		[learning rate: 0.0019956]
	Learning Rate: 0.00199557
	LOSS [training: 0.008656375567729007 | validation: 0.006149271673003935]
	TIME [epoch: 8.74 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008456510694194		[learning rate: 0.0019925]
		[batch 20/20] avg loss: 0.015568259150631824		[learning rate: 0.0019895]
	Learning Rate: 0.00198945
	LOSS [training: 0.012012384922412913 | validation: 0.01705665618612245]
	TIME [epoch: 8.75 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012354777649347937		[learning rate: 0.0019864]
		[batch 20/20] avg loss: 0.016055926289478835		[learning rate: 0.0019834]
	Learning Rate: 0.00198335
	LOSS [training: 0.014205351969413384 | validation: 0.02468473095225182]
	TIME [epoch: 8.71 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023802025627448525		[learning rate: 0.0019803]
		[batch 20/20] avg loss: 0.01618422316939295		[learning rate: 0.0019773]
	Learning Rate: 0.00197727
	LOSS [training: 0.019993124398420735 | validation: 0.0060771577780134875]
	TIME [epoch: 8.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01071407713480281		[learning rate: 0.0019742]
		[batch 20/20] avg loss: 0.007989825429164713		[learning rate: 0.0019712]
	Learning Rate: 0.00197121
	LOSS [training: 0.009351951281983762 | validation: 0.017414799976956667]
	TIME [epoch: 8.71 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02163414141916358		[learning rate: 0.0019682]
		[batch 20/20] avg loss: 0.018207532943702216		[learning rate: 0.0019652]
	Learning Rate: 0.00196517
	LOSS [training: 0.019920837181432904 | validation: 0.008572886364214148]
	TIME [epoch: 8.75 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014958075866485763		[learning rate: 0.0019622]
		[batch 20/20] avg loss: 0.00248451949735795		[learning rate: 0.0019591]
	Learning Rate: 0.00195915
	LOSS [training: 0.008721297681921855 | validation: 0.005597851686239296]
	TIME [epoch: 8.72 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012176609755988246		[learning rate: 0.0019561]
		[batch 20/20] avg loss: 0.009081581766715594		[learning rate: 0.0019531]
	Learning Rate: 0.00195314
	LOSS [training: 0.010629095761351919 | validation: 0.01947446891258935]
	TIME [epoch: 8.75 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012669508961915807		[learning rate: 0.0019501]
		[batch 20/20] avg loss: 0.018086371228525824		[learning rate: 0.0019472]
	Learning Rate: 0.00194715
	LOSS [training: 0.015377940095220815 | validation: 0.02925669801724059]
	TIME [epoch: 8.72 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021604909359412203		[learning rate: 0.0019442]
		[batch 20/20] avg loss: 0.01947677688841206		[learning rate: 0.0019412]
	Learning Rate: 0.00194118
	LOSS [training: 0.020540843123912134 | validation: 0.023638327804656006]
	TIME [epoch: 8.71 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010383804556706804		[learning rate: 0.0019382]
		[batch 20/20] avg loss: 0.009875703845905312		[learning rate: 0.0019352]
	Learning Rate: 0.00193523
	LOSS [training: 0.010129754201306058 | validation: 0.008275738334545913]
	TIME [epoch: 8.72 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013425826892708534		[learning rate: 0.0019323]
		[batch 20/20] avg loss: 0.009040470539634157		[learning rate: 0.0019293]
	Learning Rate: 0.0019293
	LOSS [training: 0.011233148716171346 | validation: 0.006111699782674472]
	TIME [epoch: 8.74 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008232521011375327		[learning rate: 0.0019263]
		[batch 20/20] avg loss: 0.024057768901068446		[learning rate: 0.0019234]
	Learning Rate: 0.00192339
	LOSS [training: 0.016145144956221887 | validation: 0.023392266016682546]
	TIME [epoch: 8.71 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02462057146848249		[learning rate: 0.0019204]
		[batch 20/20] avg loss: 0.005118354680348878		[learning rate: 0.0019175]
	Learning Rate: 0.00191749
	LOSS [training: 0.014869463074415685 | validation: 0.00928144723445273]
	TIME [epoch: 8.72 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009863614080550072		[learning rate: 0.0019145]
		[batch 20/20] avg loss: 0.013094681850174289		[learning rate: 0.0019116]
	Learning Rate: 0.00191161
	LOSS [training: 0.011479147965362183 | validation: 0.017489004229263534]
	TIME [epoch: 8.72 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010297943734315777		[learning rate: 0.0019087]
		[batch 20/20] avg loss: 0.004944206849480497		[learning rate: 0.0019058]
	Learning Rate: 0.00190575
	LOSS [training: 0.007621075291898138 | validation: 0.015212145963656717]
	TIME [epoch: 8.72 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007724114373525851		[learning rate: 0.0019028]
		[batch 20/20] avg loss: 0.003955454944191958		[learning rate: 0.0018999]
	Learning Rate: 0.00189991
	LOSS [training: 0.005839784658858904 | validation: 0.013047346900309266]
	TIME [epoch: 8.72 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006008516048580149		[learning rate: 0.001897]
		[batch 20/20] avg loss: 0.010263475182354726		[learning rate: 0.0018941]
	Learning Rate: 0.00189409
	LOSS [training: 0.008135995615467437 | validation: 0.015280671997610233]
	TIME [epoch: 8.71 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004767806657499425		[learning rate: 0.0018912]
		[batch 20/20] avg loss: 0.010335969434594278		[learning rate: 0.0018883]
	Learning Rate: 0.00188828
	LOSS [training: 0.007551888046046851 | validation: 0.006131577372947823]
	TIME [epoch: 8.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011855608895346577		[learning rate: 0.0018854]
		[batch 20/20] avg loss: 0.01080705364035253		[learning rate: 0.0018825]
	Learning Rate: 0.00188249
	LOSS [training: 0.011331331267849553 | validation: -0.0007408646528256031]
	TIME [epoch: 8.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_1044.pth
	Model improved!!!
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009854997489081477		[learning rate: 0.0018796]
		[batch 20/20] avg loss: 0.010270580790141495		[learning rate: 0.0018767]
	Learning Rate: 0.00187672
	LOSS [training: 0.010062789139611486 | validation: 0.03044890425523321]
	TIME [epoch: 8.72 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02325999123733538		[learning rate: 0.0018738]
		[batch 20/20] avg loss: 0.009815310750445347		[learning rate: 0.001871]
	Learning Rate: 0.00187097
	LOSS [training: 0.016537650993890363 | validation: 0.012388105837043556]
	TIME [epoch: 8.76 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004283882603655698		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.00561977530666009		[learning rate: 0.0018652]
	Learning Rate: 0.00186523
	LOSS [training: 0.004951828955157894 | validation: 0.008500142466233106]
	TIME [epoch: 8.74 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009013904490087807		[learning rate: 0.0018624]
		[batch 20/20] avg loss: 0.010802010990873202		[learning rate: 0.0018595]
	Learning Rate: 0.00185952
	LOSS [training: 0.009907957740480505 | validation: 0.028390705963925143]
	TIME [epoch: 8.74 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019865615438282212		[learning rate: 0.0018567]
		[batch 20/20] avg loss: 0.012224555856575437		[learning rate: 0.0018538]
	Learning Rate: 0.00185382
	LOSS [training: 0.016045085647428823 | validation: 0.020229639565735702]
	TIME [epoch: 8.73 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0049819324426249745		[learning rate: 0.001851]
		[batch 20/20] avg loss: 0.011373611460258462		[learning rate: 0.0018481]
	Learning Rate: 0.00184813
	LOSS [training: 0.00817777195144172 | validation: 0.011438751914779163]
	TIME [epoch: 8.73 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018125027462731196		[learning rate: 0.0018453]
		[batch 20/20] avg loss: 0.01660042816508868		[learning rate: 0.0018425]
	Learning Rate: 0.00184247
	LOSS [training: 0.017362727813909937 | validation: 0.016918255854364093]
	TIME [epoch: 8.77 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015528882196185275		[learning rate: 0.0018396]
		[batch 20/20] avg loss: 0.015495938421108427		[learning rate: 0.0018368]
	Learning Rate: 0.00183682
	LOSS [training: 0.015512410308646848 | validation: 0.018489063604693788]
	TIME [epoch: 8.72 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011727553219699805		[learning rate: 0.001834]
		[batch 20/20] avg loss: 0.012099245714872284		[learning rate: 0.0018312]
	Learning Rate: 0.00183119
	LOSS [training: 0.011913399467286047 | validation: 0.032353066893337425]
	TIME [epoch: 8.73 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01615904065631829		[learning rate: 0.0018284]
		[batch 20/20] avg loss: 0.015443778829020038		[learning rate: 0.0018256]
	Learning Rate: 0.00182558
	LOSS [training: 0.01580140974266916 | validation: 0.004938803599227045]
	TIME [epoch: 8.76 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025900314058390807		[learning rate: 0.0018228]
		[batch 20/20] avg loss: 0.013189860288436763		[learning rate: 0.00182]
	Learning Rate: 0.00181998
	LOSS [training: 0.007889945847137921 | validation: 0.012802678947753339]
	TIME [epoch: 8.74 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012809570609440773		[learning rate: 0.0018172]
		[batch 20/20] avg loss: 0.01469886374042488		[learning rate: 0.0018144]
	Learning Rate: 0.0018144
	LOSS [training: 0.013754217174932826 | validation: 0.015048601999291956]
	TIME [epoch: 8.81 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011925951492816226		[learning rate: 0.0018116]
		[batch 20/20] avg loss: 0.01711734508494523		[learning rate: 0.0018088]
	Learning Rate: 0.00180884
	LOSS [training: 0.014521648288880726 | validation: 0.01256039053105712]
	TIME [epoch: 8.72 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012364151339520148		[learning rate: 0.0018061]
		[batch 20/20] avg loss: 0.00936470797106567		[learning rate: 0.0018033]
	Learning Rate: 0.00180329
	LOSS [training: 0.01086442965529291 | validation: 0.007542434923229955]
	TIME [epoch: 8.72 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00649769834031314		[learning rate: 0.0018005]
		[batch 20/20] avg loss: 0.013787243903155494		[learning rate: 0.0017978]
	Learning Rate: 0.00179777
	LOSS [training: 0.010142471121734315 | validation: 0.0027700365040271126]
	TIME [epoch: 8.74 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01116724226552576		[learning rate: 0.001795]
		[batch 20/20] avg loss: 0.0037766798647250526		[learning rate: 0.0017923]
	Learning Rate: 0.00179226
	LOSS [training: 0.007471961065125407 | validation: 0.006678436441855258]
	TIME [epoch: 8.72 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01477753016659555		[learning rate: 0.0017895]
		[batch 20/20] avg loss: 0.010621341660376352		[learning rate: 0.0017868]
	Learning Rate: 0.00178676
	LOSS [training: 0.012699435913485952 | validation: 0.014160325269237155]
	TIME [epoch: 8.74 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015932459339178324		[learning rate: 0.001784]
		[batch 20/20] avg loss: 0.01064984407920193		[learning rate: 0.0017813]
	Learning Rate: 0.00178128
	LOSS [training: 0.013291151709190121 | validation: 0.010208050679983378]
	TIME [epoch: 8.71 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00352205503883765		[learning rate: 0.0017786]
		[batch 20/20] avg loss: 0.01801799083374965		[learning rate: 0.0017758]
	Learning Rate: 0.00177582
	LOSS [training: 0.01077002293629365 | validation: 0.008598250011820395]
	TIME [epoch: 8.72 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013284580044050148		[learning rate: 0.0017731]
		[batch 20/20] avg loss: 0.0030972417062497615		[learning rate: 0.0017704]
	Learning Rate: 0.00177038
	LOSS [training: 0.008190910875149957 | validation: 0.011708239891109373]
	TIME [epoch: 8.72 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011706477270897398		[learning rate: 0.0017677]
		[batch 20/20] avg loss: 0.011330500579506195		[learning rate: 0.001765]
	Learning Rate: 0.00176495
	LOSS [training: 0.011518488925201796 | validation: 0.004102247678203599]
	TIME [epoch: 8.75 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01308300579655446		[learning rate: 0.0017622]
		[batch 20/20] avg loss: 0.01323776369104603		[learning rate: 0.0017595]
	Learning Rate: 0.00175954
	LOSS [training: 0.013160384743800246 | validation: 0.0032982761124004606]
	TIME [epoch: 8.73 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01024563001609757		[learning rate: 0.0017568]
		[batch 20/20] avg loss: 0.011216949487607646		[learning rate: 0.0017541]
	Learning Rate: 0.00175415
	LOSS [training: 0.010731289751852604 | validation: 0.008989034530409931]
	TIME [epoch: 8.73 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01087550477561772		[learning rate: 0.0017515]
		[batch 20/20] avg loss: 0.01123411895970455		[learning rate: 0.0017488]
	Learning Rate: 0.00174877
	LOSS [training: 0.011054811867661133 | validation: 0.007059755799792547]
	TIME [epoch: 8.73 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00641224242226913		[learning rate: 0.0017461]
		[batch 20/20] avg loss: 0.006628636863374992		[learning rate: 0.0017434]
	Learning Rate: 0.00174341
	LOSS [training: 0.006520439642822061 | validation: 0.01536006899051448]
	TIME [epoch: 8.74 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025246773943269673		[learning rate: 0.0017407]
		[batch 20/20] avg loss: 0.007037420681948731		[learning rate: 0.0017381]
	Learning Rate: 0.00173807
	LOSS [training: 0.0047810490381378495 | validation: 0.00210301521627603]
	TIME [epoch: 8.75 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008211949980456686		[learning rate: 0.0017354]
		[batch 20/20] avg loss: 0.011629180864940455		[learning rate: 0.0017327]
	Learning Rate: 0.00173274
	LOSS [training: 0.00992056542269857 | validation: 0.00890771413351689]
	TIME [epoch: 8.73 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012330760182717091		[learning rate: 0.0017301]
		[batch 20/20] avg loss: 0.013139237888099559		[learning rate: 0.0017274]
	Learning Rate: 0.00172743
	LOSS [training: 0.012734999035408326 | validation: 0.023750148836080107]
	TIME [epoch: 8.74 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013271796422837318		[learning rate: 0.0017248]
		[batch 20/20] avg loss: 0.01505539698366434		[learning rate: 0.0017221]
	Learning Rate: 0.00172213
	LOSS [training: 0.01416359670325083 | validation: 0.007300002180018832]
	TIME [epoch: 8.73 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001076058896081178		[learning rate: 0.0017195]
		[batch 20/20] avg loss: 0.006545410060656084		[learning rate: 0.0017169]
	Learning Rate: 0.00171685
	LOSS [training: 0.0038107344783686312 | validation: 0.017007391090462294]
	TIME [epoch: 8.75 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0120663797300774		[learning rate: 0.0017142]
		[batch 20/20] avg loss: 0.013967439437698487		[learning rate: 0.0017116]
	Learning Rate: 0.00171159
	LOSS [training: 0.013016909583887945 | validation: 0.025997047790787715]
	TIME [epoch: 8.76 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006737562374545642		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.012606649918529361		[learning rate: 0.0017063]
	Learning Rate: 0.00170634
	LOSS [training: 0.0096721061465375 | validation: 0.020159276816481417]
	TIME [epoch: 8.73 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006218641675377659		[learning rate: 0.0017037]
		[batch 20/20] avg loss: 0.010988946340549048		[learning rate: 0.0017011]
	Learning Rate: 0.00170111
	LOSS [training: 0.008603794007963354 | validation: 0.018654889556392145]
	TIME [epoch: 8.73 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013683756666617714		[learning rate: 0.0016985]
		[batch 20/20] avg loss: 0.010819140593511224		[learning rate: 0.0016959]
	Learning Rate: 0.0016959
	LOSS [training: 0.01225144863006447 | validation: 0.014145647823294485]
	TIME [epoch: 8.73 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010602102059489624		[learning rate: 0.0016933]
		[batch 20/20] avg loss: 0.01762649192752507		[learning rate: 0.0016907]
	Learning Rate: 0.0016907
	LOSS [training: 0.014114296993507348 | validation: 0.032059235940734815]
	TIME [epoch: 8.74 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01450280772938482		[learning rate: 0.0016881]
		[batch 20/20] avg loss: 0.012813645280678923		[learning rate: 0.0016855]
	Learning Rate: 0.00168552
	LOSS [training: 0.01365822650503187 | validation: 0.014902047699153412]
	TIME [epoch: 8.77 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005590628166978203		[learning rate: 0.0016829]
		[batch 20/20] avg loss: 0.007953982324334136		[learning rate: 0.0016804]
	Learning Rate: 0.00168035
	LOSS [training: 0.00677230524565617 | validation: 0.010535764469970654]
	TIME [epoch: 8.75 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018407672944095992		[learning rate: 0.0016778]
		[batch 20/20] avg loss: 0.007851786553838646		[learning rate: 0.0016752]
	Learning Rate: 0.0016752
	LOSS [training: 0.01312972974896732 | validation: 0.008853560882170302]
	TIME [epoch: 8.73 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009359541702588042		[learning rate: 0.0016726]
		[batch 20/20] avg loss: 0.008711085470157701		[learning rate: 0.0016701]
	Learning Rate: 0.00167006
	LOSS [training: 0.009035313586372874 | validation: 0.022507064076029863]
	TIME [epoch: 8.72 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017876547122639648		[learning rate: 0.0016675]
		[batch 20/20] avg loss: 0.011169232122403509		[learning rate: 0.0016649]
	Learning Rate: 0.00166495
	LOSS [training: 0.014522889622521577 | validation: 0.007520160857522916]
	TIME [epoch: 8.74 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005776155701342855		[learning rate: 0.0016624]
		[batch 20/20] avg loss: 0.0014522183351673532		[learning rate: 0.0016598]
	Learning Rate: 0.00165984
	LOSS [training: 0.0036141870182551036 | validation: 0.007561961010266769]
	TIME [epoch: 8.72 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014253036763640236		[learning rate: 0.0016573]
		[batch 20/20] avg loss: 0.002759655176739947		[learning rate: 0.0016548]
	Learning Rate: 0.00165475
	LOSS [training: 0.008506345970190092 | validation: 0.01128112144793892]
	TIME [epoch: 8.73 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011434568875968507		[learning rate: 0.0016522]
		[batch 20/20] avg loss: -0.0008863542102877875		[learning rate: 0.0016497]
	Learning Rate: 0.00164968
	LOSS [training: 0.0052741073328403595 | validation: 0.0030914500885136876]
	TIME [epoch: 8.73 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035735131635285305		[learning rate: 0.0016472]
		[batch 20/20] avg loss: 0.013102494595102423		[learning rate: 0.0016446]
	Learning Rate: 0.00164462
	LOSS [training: 0.004764490715786945 | validation: 0.00787389467644085]
	TIME [epoch: 8.74 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010136642085275862		[learning rate: 0.0016421]
		[batch 20/20] avg loss: 0.00553668378309585		[learning rate: 0.0016396]
	Learning Rate: 0.00163958
	LOSS [training: 0.007836662934185857 | validation: 0.00980925210197535]
	TIME [epoch: 8.76 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01489918401801903		[learning rate: 0.0016371]
		[batch 20/20] avg loss: 0.014295129147028626		[learning rate: 0.0016346]
	Learning Rate: 0.00163456
	LOSS [training: 0.01459715658252383 | validation: 0.004312399370947768]
	TIME [epoch: 8.75 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012161594874050802		[learning rate: 0.001632]
		[batch 20/20] avg loss: 0.004583365731909564		[learning rate: 0.0016295]
	Learning Rate: 0.00162955
	LOSS [training: 0.008372480302980184 | validation: 0.008740710275896605]
	TIME [epoch: 8.74 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024076569453395702		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.01987156126197963		[learning rate: 0.0016246]
	Learning Rate: 0.00162455
	LOSS [training: 0.021974065357687665 | validation: 0.021264230826508378]
	TIME [epoch: 8.75 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0076086629176056325		[learning rate: 0.0016221]
		[batch 20/20] avg loss: 0.016778549089592608		[learning rate: 0.0016196]
	Learning Rate: 0.00161957
	LOSS [training: 0.012193606003599118 | validation: 0.018834159722448895]
	TIME [epoch: 8.73 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009058119658759972		[learning rate: 0.0016171]
		[batch 20/20] avg loss: 0.009878267971395242		[learning rate: 0.0016146]
	Learning Rate: 0.00161461
	LOSS [training: 0.009468193815077606 | validation: 0.03595240519211301]
	TIME [epoch: 8.77 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007422020799831591		[learning rate: 0.0016121]
		[batch 20/20] avg loss: 0.01055019024673786		[learning rate: 0.0016097]
	Learning Rate: 0.00160966
	LOSS [training: 0.008986105523284726 | validation: 0.009118895814260154]
	TIME [epoch: 8.74 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057018924080176605		[learning rate: 0.0016072]
		[batch 20/20] avg loss: 0.010669506936488148		[learning rate: 0.0016047]
	Learning Rate: 0.00160472
	LOSS [training: 0.008185699672252902 | validation: 0.004497874636361228]
	TIME [epoch: 8.72 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002449589729625977		[learning rate: 0.0016023]
		[batch 20/20] avg loss: 0.011543022712396685		[learning rate: 0.0015998]
	Learning Rate: 0.0015998
	LOSS [training: 0.006996306221011332 | validation: 0.012308786368768518]
	TIME [epoch: 8.73 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004361950065751422		[learning rate: 0.0015973]
		[batch 20/20] avg loss: 0.00722129332959289		[learning rate: 0.0015949]
	Learning Rate: 0.0015949
	LOSS [training: 0.0057916216976721556 | validation: 0.0038991884508191455]
	TIME [epoch: 8.72 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016762509661827754		[learning rate: 0.0015925]
		[batch 20/20] avg loss: 0.013387600174041076		[learning rate: 0.00159]
	Learning Rate: 0.00159001
	LOSS [training: 0.015075054917934419 | validation: 0.002785182930998649]
	TIME [epoch: 8.73 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009416665944253836		[learning rate: 0.0015876]
		[batch 20/20] avg loss: 0.01421586489333639		[learning rate: 0.0015851]
	Learning Rate: 0.00158514
	LOSS [training: 0.011816265418795113 | validation: 0.016409734983389584]
	TIME [epoch: 8.73 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009767145244493002		[learning rate: 0.0015827]
		[batch 20/20] avg loss: 0.007922909755495516		[learning rate: 0.0015803]
	Learning Rate: 0.00158028
	LOSS [training: 0.008845027499994261 | validation: 0.01384888645377779]
	TIME [epoch: 8.73 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007061190618437975		[learning rate: 0.0015779]
		[batch 20/20] avg loss: 0.013614483864688146		[learning rate: 0.0015754]
	Learning Rate: 0.00157543
	LOSS [training: 0.01033783724156306 | validation: 0.017580683977214392]
	TIME [epoch: 8.74 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009406950716510747		[learning rate: 0.001573]
		[batch 20/20] avg loss: 0.007152872778166232		[learning rate: 0.0015706]
	Learning Rate: 0.0015706
	LOSS [training: 0.00827991174733849 | validation: 0.02473628492636917]
	TIME [epoch: 8.75 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009069430255643279		[learning rate: 0.0015682]
		[batch 20/20] avg loss: 0.008923876774596676		[learning rate: 0.0015658]
	Learning Rate: 0.00156579
	LOSS [training: 0.00899665351511998 | validation: 0.007047909828390478]
	TIME [epoch: 8.75 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030783676119838918		[learning rate: 0.0015634]
		[batch 20/20] avg loss: 0.008351886327424783		[learning rate: 0.001561]
	Learning Rate: 0.00156099
	LOSS [training: 0.005715126969704337 | validation: 0.00964376333344027]
	TIME [epoch: 8.73 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00790841441331052		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.005827832760700663		[learning rate: 0.0015562]
	Learning Rate: 0.0015562
	LOSS [training: 0.006868123587005591 | validation: 0.0039028981536512563]
	TIME [epoch: 8.72 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004384056990353689		[learning rate: 0.0015538]
		[batch 20/20] avg loss: 0.0017312774066619522		[learning rate: 0.0015514]
	Learning Rate: 0.00155143
	LOSS [training: 0.00305766719850782 | validation: 0.010802323626878843]
	TIME [epoch: 8.75 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006203231621857794		[learning rate: 0.0015491]
		[batch 20/20] avg loss: 0.010371781738570108		[learning rate: 0.0015467]
	Learning Rate: 0.00154668
	LOSS [training: 0.005496052450377944 | validation: 0.011121738903192306]
	TIME [epoch: 8.76 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00676504669713546		[learning rate: 0.0015443]
		[batch 20/20] avg loss: 0.014394250372282457		[learning rate: 0.0015419]
	Learning Rate: 0.00154194
	LOSS [training: 0.010579648534708958 | validation: 0.026124944032200934]
	TIME [epoch: 8.76 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010355342781209913		[learning rate: 0.0015396]
		[batch 20/20] avg loss: 0.003216814180962631		[learning rate: 0.0015372]
	Learning Rate: 0.00153721
	LOSS [training: 0.006786078481086273 | validation: 0.016050665037571805]
	TIME [epoch: 8.77 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007744825103208133		[learning rate: 0.0015349]
		[batch 20/20] avg loss: 0.00961113392085199		[learning rate: 0.0015325]
	Learning Rate: 0.0015325
	LOSS [training: 0.008677979512030061 | validation: 0.017025223113825514]
	TIME [epoch: 8.74 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005233045128978604		[learning rate: 0.0015301]
		[batch 20/20] avg loss: 0.003224700681389618		[learning rate: 0.0015278]
	Learning Rate: 0.0015278
	LOSS [training: 0.0042288729051841115 | validation: 0.006070608017138069]
	TIME [epoch: 8.76 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003049811802633156		[learning rate: 0.0015255]
		[batch 20/20] avg loss: 0.0026366295983459476		[learning rate: 0.0015231]
	Learning Rate: 0.00152312
	LOSS [training: 0.0028432207004895514 | validation: 0.0014794975726435497]
	TIME [epoch: 8.77 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004436062219456977		[learning rate: 0.0015208]
		[batch 20/20] avg loss: -0.0016403483275714181		[learning rate: 0.0015184]
	Learning Rate: 0.00151845
	LOSS [training: 0.0013978569459427797 | validation: 0.006788665381988444]
	TIME [epoch: 8.77 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0113603768966071		[learning rate: 0.0015161]
		[batch 20/20] avg loss: 0.007351946952735759		[learning rate: 0.0015138]
	Learning Rate: 0.00151379
	LOSS [training: 0.009356161924671428 | validation: 0.019889320560837906]
	TIME [epoch: 8.75 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0069275704850647836		[learning rate: 0.0015115]
		[batch 20/20] avg loss: 0.005863316212169488		[learning rate: 0.0015092]
	Learning Rate: 0.00150915
	LOSS [training: 0.006395443348617136 | validation: 0.010501174660892123]
	TIME [epoch: 8.74 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00673818057441674		[learning rate: 0.0015068]
		[batch 20/20] avg loss: 0.007132226519751382		[learning rate: 0.0015045]
	Learning Rate: 0.00150453
	LOSS [training: 0.006935203547084061 | validation: 0.004287554883172978]
	TIME [epoch: 8.77 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026525108785311405		[learning rate: 0.0015022]
		[batch 20/20] avg loss: 0.007785681750124148		[learning rate: 0.0014999]
	Learning Rate: 0.00149991
	LOSS [training: 0.005219096314327644 | validation: 0.010202455660645962]
	TIME [epoch: 8.77 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019348778509567753		[learning rate: 0.0014976]
		[batch 20/20] avg loss: 0.003437195011435139		[learning rate: 0.0014953]
	Learning Rate: 0.00149532
	LOSS [training: 0.011392986760501444 | validation: 0.0046441693053458424]
	TIME [epoch: 8.73 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034202566752203636		[learning rate: 0.001493]
		[batch 20/20] avg loss: 0.013318780601970101		[learning rate: 0.0014907]
	Learning Rate: 0.00149073
	LOSS [training: 0.008369518638595233 | validation: 0.02280591884624037]
	TIME [epoch: 8.71 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009227199925908681		[learning rate: 0.0014884]
		[batch 20/20] avg loss: 0.011175821974638084		[learning rate: 0.0014862]
	Learning Rate: 0.00148616
	LOSS [training: 0.010201510950273381 | validation: 0.009432766896961724]
	TIME [epoch: 8.73 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011669706742497362		[learning rate: 0.0014839]
		[batch 20/20] avg loss: 0.009169884742768378		[learning rate: 0.0014816]
	Learning Rate: 0.00148161
	LOSS [training: 0.010419795742632871 | validation: 0.007220742590631962]
	TIME [epoch: 8.75 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012557476011541757		[learning rate: 0.0014793]
		[batch 20/20] avg loss: 0.012669420181370841		[learning rate: 0.0014771]
	Learning Rate: 0.00147707
	LOSS [training: 0.0126134480964563 | validation: 0.018416305137146913]
	TIME [epoch: 8.77 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011435024135577392		[learning rate: 0.0014748]
		[batch 20/20] avg loss: 0.010408842257483647		[learning rate: 0.0014725]
	Learning Rate: 0.00147254
	LOSS [training: 0.01092193319653052 | validation: 0.019794375892437762]
	TIME [epoch: 8.72 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017426550643187343		[learning rate: 0.0014703]
		[batch 20/20] avg loss: 0.009089644810531946		[learning rate: 0.001468]
	Learning Rate: 0.00146802
	LOSS [training: 0.013258097726859644 | validation: 0.013210645220337847]
	TIME [epoch: 8.71 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010995821708150836		[learning rate: 0.0014658]
		[batch 20/20] avg loss: 0.006942464533669085		[learning rate: 0.0014635]
	Learning Rate: 0.00146352
	LOSS [training: 0.00896914312090996 | validation: 0.009699236406704635]
	TIME [epoch: 8.74 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019209995525744775		[learning rate: 0.0014613]
		[batch 20/20] avg loss: 0.010444790966797117		[learning rate: 0.001459]
	Learning Rate: 0.00145904
	LOSS [training: 0.014827393246270948 | validation: 0.01931731406452761]
	TIME [epoch: 8.79 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003960514582758314		[learning rate: 0.0014568]
		[batch 20/20] avg loss: 0.007355881187324978		[learning rate: 0.0014546]
	Learning Rate: 0.00145457
	LOSS [training: 0.005658197885041646 | validation: 0.0060653859751724866]
	TIME [epoch: 8.74 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010068693232484507		[learning rate: 0.0014523]
		[batch 20/20] avg loss: 0.001965853444762847		[learning rate: 0.0014501]
	Learning Rate: 0.00145011
	LOSS [training: 0.006017273338623678 | validation: -0.001471033476186026]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_1129.pth
	Model improved!!!
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007571795903469581		[learning rate: 0.0014479]
		[batch 20/20] avg loss: 0.007822150270376124		[learning rate: 0.0014457]
	Learning Rate: 0.00144566
	LOSS [training: 0.007696973086922854 | validation: 0.01392562316259956]
	TIME [epoch: 8.78 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008059713364899126		[learning rate: 0.0014434]
		[batch 20/20] avg loss: 0.01287023720904334		[learning rate: 0.0014412]
	Learning Rate: 0.00144123
	LOSS [training: 0.010464975286971233 | validation: 0.01625414198757689]
	TIME [epoch: 8.74 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01546001401240402		[learning rate: 0.001439]
		[batch 20/20] avg loss: -0.0024628491967269026		[learning rate: 0.0014368]
	Learning Rate: 0.00143681
	LOSS [training: 0.006498582407838559 | validation: 0.00405010708979424]
	TIME [epoch: 8.74 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022315508499781934		[learning rate: 0.0014346]
		[batch 20/20] avg loss: 0.001743385199177112		[learning rate: 0.0014324]
	Learning Rate: 0.00143241
	LOSS [training: 0.001987468024577653 | validation: 0.0065819852320274635]
	TIME [epoch: 8.75 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010689187010699699		[learning rate: 0.0014302]
		[batch 20/20] avg loss: 0.0034756851832998753		[learning rate: 0.001428]
	Learning Rate: 0.00142802
	LOSS [training: 0.007082436096999788 | validation: 0.009089289788191564]
	TIME [epoch: 8.74 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006889825358726028		[learning rate: 0.0014258]
		[batch 20/20] avg loss: 0.0050895190174826774		[learning rate: 0.0014236]
	Learning Rate: 0.00142364
	LOSS [training: 0.005989672188104353 | validation: 0.006806221333352114]
	TIME [epoch: 8.71 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029213744443769285		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.013157777033998735		[learning rate: 0.0014193]
	Learning Rate: 0.00141928
	LOSS [training: 0.008039575739187831 | validation: 0.01898541786598014]
	TIME [epoch: 8.72 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007392804049161135		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.00350620139780143		[learning rate: 0.0014149]
	Learning Rate: 0.00141492
	LOSS [training: 0.005449502723481282 | validation: 0.0059890331649565505]
	TIME [epoch: 8.75 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014701720499572294		[learning rate: 0.0014128]
		[batch 20/20] avg loss: 0.003233068453729385		[learning rate: 0.0014106]
	Learning Rate: 0.00141059
	LOSS [training: 0.002351620251843307 | validation: 0.00720816049236727]
	TIME [epoch: 8.72 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007956621928033762		[learning rate: 0.0014084]
		[batch 20/20] avg loss: -0.002494345546674781		[learning rate: 0.0014063]
	Learning Rate: 0.00140626
	LOSS [training: -0.0008493416769357025 | validation: 0.006620924654380728]
	TIME [epoch: 8.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0051224639124942154		[learning rate: 0.0014041]
		[batch 20/20] avg loss: 0.00935357808998359		[learning rate: 0.001402]
	Learning Rate: 0.00140195
	LOSS [training: 0.007238021001238902 | validation: 0.010475051261354618]
	TIME [epoch: 8.71 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010406761009677676		[learning rate: 0.0013998]
		[batch 20/20] avg loss: 0.0038695536328882812		[learning rate: 0.0013977]
	Learning Rate: 0.00139765
	LOSS [training: 0.0071381573212829785 | validation: 0.01622227129682753]
	TIME [epoch: 8.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011829963161776468		[learning rate: 0.0013955]
		[batch 20/20] avg loss: 0.0019492336838573873		[learning rate: 0.0013934]
	Learning Rate: 0.00139337
	LOSS [training: 0.006889598422816928 | validation: 0.023137940249447354]
	TIME [epoch: 8.73 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013717464016435796		[learning rate: 0.0013912]
		[batch 20/20] avg loss: 0.00370122537423584		[learning rate: 0.0013891]
	Learning Rate: 0.0013891
	LOSS [training: 0.008709344695335816 | validation: 0.016491151240861777]
	TIME [epoch: 8.73 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012660450992906167		[learning rate: 0.001387]
		[batch 20/20] avg loss: 0.009273875785576608		[learning rate: 0.0013848]
	Learning Rate: 0.00138484
	LOSS [training: 0.010967163389241388 | validation: 0.005311937828905508]
	TIME [epoch: 8.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005363443919889237		[learning rate: 0.0013827]
		[batch 20/20] avg loss: 0.011417771031217626		[learning rate: 0.0013806]
	Learning Rate: 0.0013806
	LOSS [training: 0.008390607475553433 | validation: 0.016729445722045627]
	TIME [epoch: 8.71 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030688659347809025		[learning rate: 0.0013785]
		[batch 20/20] avg loss: 0.006006109339885829		[learning rate: 0.0013764]
	Learning Rate: 0.00137636
	LOSS [training: 0.004537487637333366 | validation: 0.01504044195491752]
	TIME [epoch: 8.72 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006194883215489079		[learning rate: 0.0013743]
		[batch 20/20] avg loss: 0.00634994777946402		[learning rate: 0.0013721]
	Learning Rate: 0.00137214
	LOSS [training: 0.006272415497476551 | validation: 0.0056184859533035215]
	TIME [epoch: 8.74 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004553596632344248		[learning rate: 0.00137]
		[batch 20/20] avg loss: 0.016833356048542702		[learning rate: 0.0013679]
	Learning Rate: 0.00136794
	LOSS [training: 0.010693476340443476 | validation: 0.020369163266031037]
	TIME [epoch: 8.75 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007114074848536737		[learning rate: 0.0013658]
		[batch 20/20] avg loss: 0.0039011819502298914		[learning rate: 0.0013637]
	Learning Rate: 0.00136375
	LOSS [training: 0.005507628399383313 | validation: 0.010889870658972066]
	TIME [epoch: 8.7 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00447951441427045		[learning rate: 0.0013617]
		[batch 20/20] avg loss: 0.007946744314183456		[learning rate: 0.0013596]
	Learning Rate: 0.00135956
	LOSS [training: 0.006213129364226953 | validation: 0.018020995115365392]
	TIME [epoch: 8.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010732569077415087		[learning rate: 0.0013575]
		[batch 20/20] avg loss: 0.004676036845927227		[learning rate: 0.0013554]
	Learning Rate: 0.0013554
	LOSS [training: 0.007704302961671157 | validation: 0.0034313187995990905]
	TIME [epoch: 8.69 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014059171092074172		[learning rate: 0.0013533]
		[batch 20/20] avg loss: 0.0103058682060228		[learning rate: 0.0013512]
	Learning Rate: 0.00135124
	LOSS [training: 0.012182519649048484 | validation: 0.013879384902887933]
	TIME [epoch: 8.74 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010053131021350867		[learning rate: 0.0013492]
		[batch 20/20] avg loss: 0.0013056701235656758		[learning rate: 0.0013471]
	Learning Rate: 0.0013471
	LOSS [training: 0.005679400572458271 | validation: 0.0008401904327870831]
	TIME [epoch: 8.7 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0047933839962759125		[learning rate: 0.001345]
		[batch 20/20] avg loss: -0.002641856470778928		[learning rate: 0.001343]
	Learning Rate: 0.00134297
	LOSS [training: 0.0010757637627484926 | validation: 0.0006839953811901229]
	TIME [epoch: 8.74 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025099189535264265		[learning rate: 0.0013409]
		[batch 20/20] avg loss: 0.008310696418333407		[learning rate: 0.0013389]
	Learning Rate: 0.00133885
	LOSS [training: 0.0054103076859299175 | validation: 0.006785679732375317]
	TIME [epoch: 8.74 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005624799184619814		[learning rate: 0.0013368]
		[batch 20/20] avg loss: 0.00973134002284121		[learning rate: 0.0013348]
	Learning Rate: 0.00133475
	LOSS [training: 0.007678069603730511 | validation: 0.01835955781073811]
	TIME [epoch: 8.76 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008600073623504668		[learning rate: 0.0013327]
		[batch 20/20] avg loss: 0.0016555381236057118		[learning rate: 0.0013307]
	Learning Rate: 0.00133066
	LOSS [training: 0.005127805873555191 | validation: 0.004643049947302507]
	TIME [epoch: 8.73 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004637056889278003		[learning rate: 0.0013286]
		[batch 20/20] avg loss: 0.005054314766618058		[learning rate: 0.0013266]
	Learning Rate: 0.00132658
	LOSS [training: 0.00484568582794803 | validation: 0.0005929131333311643]
	TIME [epoch: 8.72 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011168876336206095		[learning rate: 0.0013245]
		[batch 20/20] avg loss: 0.008038392772345943		[learning rate: 0.0013225]
	Learning Rate: 0.00132251
	LOSS [training: 0.009603634554276017 | validation: 0.001583809522981385]
	TIME [epoch: 8.74 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008067609922164784		[learning rate: 0.0013205]
		[batch 20/20] avg loss: 0.009199072215418623		[learning rate: 0.0013185]
	Learning Rate: 0.00131846
	LOSS [training: 0.0086333410687917 | validation: 0.007785906420675044]
	TIME [epoch: 8.73 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009784062123673226		[learning rate: 0.0013164]
		[batch 20/20] avg loss: 0.010182480806863469		[learning rate: 0.0013144]
	Learning Rate: 0.00131442
	LOSS [training: 0.009983271465268348 | validation: 0.015902013505675654]
	TIME [epoch: 8.75 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018754264873936142		[learning rate: 0.0013124]
		[batch 20/20] avg loss: 0.020818729961909616		[learning rate: 0.0013104]
	Learning Rate: 0.00131039
	LOSS [training: 0.01978649741792288 | validation: 0.010821018938130327]
	TIME [epoch: 8.73 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015155071006066107		[learning rate: 0.0013084]
		[batch 20/20] avg loss: 0.01881961259412817		[learning rate: 0.0013064]
	Learning Rate: 0.00130637
	LOSS [training: 0.016987341800097137 | validation: 0.006859809173081648]
	TIME [epoch: 8.75 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003109061956598221		[learning rate: 0.0013044]
		[batch 20/20] avg loss: 0.006025562893767959		[learning rate: 0.0013024]
	Learning Rate: 0.00130237
	LOSS [training: 0.004567312425183089 | validation: 0.007387843866745063]
	TIME [epoch: 8.73 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00183084957936619		[learning rate: 0.0013004]
		[batch 20/20] avg loss: 0.007487567468468485		[learning rate: 0.0012984]
	Learning Rate: 0.00129837
	LOSS [training: 0.004659208523917337 | validation: 0.005059545784466459]
	TIME [epoch: 8.74 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004210800735354204		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.0070709428331120015		[learning rate: 0.0012944]
	Learning Rate: 0.00129439
	LOSS [training: 0.00332493137978829 | validation: -0.0016022023763019727]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_1166.pth
	Model improved!!!
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007455303789065619		[learning rate: 0.0012924]
		[batch 20/20] avg loss: 0.0026427551826060667		[learning rate: 0.0012904]
	Learning Rate: 0.00129043
	LOSS [training: 0.005049029485835843 | validation: 0.014568300164977998]
	TIME [epoch: 8.73 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01754239773569239		[learning rate: 0.0012884]
		[batch 20/20] avg loss: 0.025024232901470257		[learning rate: 0.0012865]
	Learning Rate: 0.00128647
	LOSS [training: 0.02128331531858133 | validation: 0.00734161523085476]
	TIME [epoch: 8.74 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005939487757611833		[learning rate: 0.0012845]
		[batch 20/20] avg loss: 0.0031088162500281405		[learning rate: 0.0012825]
	Learning Rate: 0.00128253
	LOSS [training: 0.004524152003819988 | validation: 0.011876957241494134]
	TIME [epoch: 8.72 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005569299608847136		[learning rate: 0.0012806]
		[batch 20/20] avg loss: 0.0023615025612230833		[learning rate: 0.0012786]
	Learning Rate: 0.0012786
	LOSS [training: 0.00396540108503511 | validation: 0.010913866294265214]
	TIME [epoch: 8.74 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00328538636929248		[learning rate: 0.0012766]
		[batch 20/20] avg loss: 0.006499259047103469		[learning rate: 0.0012747]
	Learning Rate: 0.00127468
	LOSS [training: 0.0048923227081979735 | validation: -1.722297453213685e-05]
	TIME [epoch: 8.77 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017184203490450834		[learning rate: 0.0012727]
		[batch 20/20] avg loss: 0.007523253665401482		[learning rate: 0.0012708]
	Learning Rate: 0.00127077
	LOSS [training: 0.004620837007223283 | validation: 0.0025613977742326172]
	TIME [epoch: 8.73 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005550384072712627		[learning rate: 0.0012688]
		[batch 20/20] avg loss: 0.0036782358138152975		[learning rate: 0.0012669]
	Learning Rate: 0.00126687
	LOSS [training: 0.004614309943263962 | validation: 0.0019015204625794347]
	TIME [epoch: 8.73 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017132812660798694		[learning rate: 0.0012649]
		[batch 20/20] avg loss: 0.007134708116529102		[learning rate: 0.001263]
	Learning Rate: 0.00126299
	LOSS [training: 0.0044239946913044855 | validation: 0.00609748973833301]
	TIME [epoch: 8.73 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021937226311591904		[learning rate: 0.0012611]
		[batch 20/20] avg loss: 0.0030292725325649548		[learning rate: 0.0012591]
	Learning Rate: 0.00125912
	LOSS [training: 0.0026114975818620724 | validation: 0.0036183452834096922]
	TIME [epoch: 9.75 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003429831584432498		[learning rate: 0.0012572]
		[batch 20/20] avg loss: 0.003895413383697887		[learning rate: 0.0012553]
	Learning Rate: 0.00125526
	LOSS [training: 0.0036626224840651924 | validation: 0.003924789942941334]
	TIME [epoch: 8.75 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015522839114087111		[learning rate: 0.0012533]
		[batch 20/20] avg loss: 0.006255839799072819		[learning rate: 0.0012514]
	Learning Rate: 0.00125141
	LOSS [training: 0.010889339456579964 | validation: 0.0022357090363871717]
	TIME [epoch: 8.74 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077235637562582645		[learning rate: 0.0012495]
		[batch 20/20] avg loss: -0.004216409580518525		[learning rate: 0.0012476]
	Learning Rate: 0.00124757
	LOSS [training: 0.0017535770878698686 | validation: 0.00924655159473358]
	TIME [epoch: 8.71 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008039218869296413		[learning rate: 0.0012457]
		[batch 20/20] avg loss: 0.004067482053413035		[learning rate: 0.0012438]
	Learning Rate: 0.00124375
	LOSS [training: 0.006053350461354725 | validation: -0.0035015135160146134]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_1179.pth
	Model improved!!!
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026168843121425313		[learning rate: 0.0012418]
		[batch 20/20] avg loss: 0.004171720040090783		[learning rate: 0.0012399]
	Learning Rate: 0.00123994
	LOSS [training: 0.003394302176116658 | validation: 0.008040512509954774]
	TIME [epoch: 8.72 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005482039455226578		[learning rate: 0.001238]
		[batch 20/20] avg loss: 0.013043740782883362		[learning rate: 0.0012361]
	Learning Rate: 0.00123614
	LOSS [training: 0.009262890119054972 | validation: 0.010680168822153885]
	TIME [epoch: 8.76 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007975570803472772		[learning rate: 0.0012342]
		[batch 20/20] avg loss: -0.0009279910760301241		[learning rate: 0.0012323]
	Learning Rate: 0.00123235
	LOSS [training: 0.0035237898637213243 | validation: 0.004698922711162351]
	TIME [epoch: 8.74 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009841869822300283		[learning rate: 0.0012305]
		[batch 20/20] avg loss: 0.004372455971749868		[learning rate: 0.0012286]
	Learning Rate: 0.00122857
	LOSS [training: 0.007107162897025077 | validation: 0.0017136641054221829]
	TIME [epoch: 8.73 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028719443460932916		[learning rate: 0.0012267]
		[batch 20/20] avg loss: 0.0006240343386562823		[learning rate: 0.0012248]
	Learning Rate: 0.0012248
	LOSS [training: 0.0017479893423747873 | validation: 0.01117896847799249]
	TIME [epoch: 8.73 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034978648674035343		[learning rate: 0.0012229]
		[batch 20/20] avg loss: 0.004771698101092075		[learning rate: 0.001221]
	Learning Rate: 0.00122105
	LOSS [training: 0.004134781484247805 | validation: 0.006043972711819857]
	TIME [epoch: 8.73 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031549973546012556		[learning rate: 0.0012192]
		[batch 20/20] avg loss: 0.0029541906297530394		[learning rate: 0.0012173]
	Learning Rate: 0.00121731
	LOSS [training: -0.00010040336242410804 | validation: 0.005303761850145005]
	TIME [epoch: 8.75 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004322996010465122		[learning rate: 0.0012154]
		[batch 20/20] avg loss: 0.002231409394171298		[learning rate: 0.0012136]
	Learning Rate: 0.00121357
	LOSS [training: 0.0032772027023182096 | validation: 0.0024043243423633176]
	TIME [epoch: 8.72 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002381487290063991		[learning rate: 0.0012117]
		[batch 20/20] avg loss: 0.0009390416256449822		[learning rate: 0.0012099]
	Learning Rate: 0.00120985
	LOSS [training: 0.00035044644831929166 | validation: 0.0023734657401612894]
	TIME [epoch: 8.74 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038392409071886053		[learning rate: 0.001208]
		[batch 20/20] avg loss: 0.005372148397023767		[learning rate: 0.0012061]
	Learning Rate: 0.00120615
	LOSS [training: 0.0007664537449175806 | validation: 0.006688562188794132]
	TIME [epoch: 8.71 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003662489627828186		[learning rate: 0.0012043]
		[batch 20/20] avg loss: 0.0055106756731729874		[learning rate: 0.0012024]
	Learning Rate: 0.00120245
	LOSS [training: 0.004586582650500586 | validation: 0.008362956511853759]
	TIME [epoch: 8.73 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0070709670156053174		[learning rate: 0.0012006]
		[batch 20/20] avg loss: 0.0008356262762232072		[learning rate: 0.0011988]
	Learning Rate: 0.00119876
	LOSS [training: 0.0039532966459142616 | validation: 0.0030637805254761707]
	TIME [epoch: 8.76 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003085505355685135		[learning rate: 0.0011969]
		[batch 20/20] avg loss: 0.004878338511072898		[learning rate: 0.0011951]
	Learning Rate: 0.00119509
	LOSS [training: 0.0039819219333790176 | validation: 0.002845320343573748]
	TIME [epoch: 8.72 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009903632136724998		[learning rate: 0.0011933]
		[batch 20/20] avg loss: -0.002701742716874517		[learning rate: 0.0011914]
	Learning Rate: 0.00119142
	LOSS [training: -0.0008556897516010084 | validation: 0.0039413724510193445]
	TIME [epoch: 8.7 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005633780976089667		[learning rate: 0.0011896]
		[batch 20/20] avg loss: 0.004978852770009969		[learning rate: 0.0011878]
	Learning Rate: 0.00118777
	LOSS [training: 0.005306316873049819 | validation: 0.00020963365652842307]
	TIME [epoch: 8.73 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003871189870158589		[learning rate: 0.001186]
		[batch 20/20] avg loss: 0.002574407867245169		[learning rate: 0.0011841]
	Learning Rate: 0.00118413
	LOSS [training: 0.001480763427130514 | validation: 0.005386372357862509]
	TIME [epoch: 8.73 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009988766149050731		[learning rate: 0.0011823]
		[batch 20/20] avg loss: -0.0004163751887312266		[learning rate: 0.0011805]
	Learning Rate: 0.0011805
	LOSS [training: 0.004786195480159751 | validation: 0.0049803168516180454]
	TIME [epoch: 8.72 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000551889564098313		[learning rate: 0.0011787]
		[batch 20/20] avg loss: 0.0056630410470638794		[learning rate: 0.0011769]
	Learning Rate: 0.00117688
	LOSS [training: 0.002555575741482783 | validation: 0.011803340327272356]
	TIME [epoch: 8.72 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005104521970727418		[learning rate: 0.0011751]
		[batch 20/20] avg loss: 0.006945042769632173		[learning rate: 0.0011733]
	Learning Rate: 0.00117328
	LOSS [training: 0.006024782370179794 | validation: -0.004622311006278449]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_1198.pth
	Model improved!!!
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004104904045899948		[learning rate: 0.0011715]
		[batch 20/20] avg loss: 0.010745822513813391		[learning rate: 0.0011697]
	Learning Rate: 0.00116968
	LOSS [training: 0.00742536327985667 | validation: 0.006265491755105572]
	TIME [epoch: 8.74 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024508784353873513		[learning rate: 0.0011679]
		[batch 20/20] avg loss: 0.0039559149822112885		[learning rate: 0.0011661]
	Learning Rate: 0.00116609
	LOSS [training: 0.0032033967087993194 | validation: 0.01557242469728118]
	TIME [epoch: 8.75 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00677456216929888		[learning rate: 0.0011643]
		[batch 20/20] avg loss: 0.018219878330728093		[learning rate: 0.0011625]
	Learning Rate: 0.00116252
	LOSS [training: 0.012497220250013487 | validation: 0.018146371199619604]
	TIME [epoch: 8.74 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009045797225659829		[learning rate: 0.0011607]
		[batch 20/20] avg loss: 0.007735038439343388		[learning rate: 0.001159]
	Learning Rate: 0.00115896
	LOSS [training: 0.00839041783250161 | validation: 0.011758011817386116]
	TIME [epoch: 8.75 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007017322027575256		[learning rate: 0.0011572]
		[batch 20/20] avg loss: 0.010586517555008364		[learning rate: 0.0011554]
	Learning Rate: 0.0011554
	LOSS [training: 0.008801919791291812 | validation: 0.0010046289527061419]
	TIME [epoch: 8.72 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004274933346607078		[learning rate: 0.0011536]
		[batch 20/20] avg loss: 0.0013491425071296566		[learning rate: 0.0011519]
	Learning Rate: 0.00115186
	LOSS [training: -0.0014628954197387102 | validation: 0.006843948385214351]
	TIME [epoch: 8.76 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005834709920204407		[learning rate: 0.0011501]
		[batch 20/20] avg loss: 0.01508211124754039		[learning rate: 0.0011483]
	Learning Rate: 0.00114833
	LOSS [training: 0.0104584105838724 | validation: 0.0071307429926908165]
	TIME [epoch: 8.76 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005116268751490823		[learning rate: 0.0011466]
		[batch 20/20] avg loss: 0.003906463329873269		[learning rate: 0.0011448]
	Learning Rate: 0.00114481
	LOSS [training: 0.004511366040682046 | validation: 0.0068945588271718174]
	TIME [epoch: 8.76 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005328096168537576		[learning rate: 0.0011431]
		[batch 20/20] avg loss: 0.002724159161296106		[learning rate: 0.0011413]
	Learning Rate: 0.0011413
	LOSS [training: 0.004026127664916841 | validation: 0.0023423118562001647]
	TIME [epoch: 8.76 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023066519690489442		[learning rate: 0.0011395]
		[batch 20/20] avg loss: 0.007359440790583734		[learning rate: 0.0011378]
	Learning Rate: 0.0011378
	LOSS [training: 0.0048330463798163395 | validation: 0.007655959219633173]
	TIME [epoch: 8.74 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004529024886525067		[learning rate: 0.0011361]
		[batch 20/20] avg loss: 0.01110977779043645		[learning rate: 0.0011343]
	Learning Rate: 0.00113431
	LOSS [training: 0.007819401338480757 | validation: 0.012829710828829817]
	TIME [epoch: 8.73 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001082866798330567		[learning rate: 0.0011326]
		[batch 20/20] avg loss: 0.00407953666776556		[learning rate: 0.0011308]
	Learning Rate: 0.00113084
	LOSS [training: 0.002581201733048063 | validation: 0.0033255232338357515]
	TIME [epoch: 8.74 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019679289720740995		[learning rate: 0.0011291]
		[batch 20/20] avg loss: 6.722394892495638e-05		[learning rate: 0.0011274]
	Learning Rate: 0.00112737
	LOSS [training: 0.0010175764604995283 | validation: 0.004079043852410006]
	TIME [epoch: 8.72 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003035584912425396		[learning rate: 0.0011256]
		[batch 20/20] avg loss: 0.01375985551314203		[learning rate: 0.0011239]
	Learning Rate: 0.00112391
	LOSS [training: 0.008397720212783713 | validation: 0.02107347926167221]
	TIME [epoch: 8.75 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007262702356573457		[learning rate: 0.0011222]
		[batch 20/20] avg loss: 0.008098874437369131		[learning rate: 0.0011205]
	Learning Rate: 0.00112047
	LOSS [training: 0.007680788396971293 | validation: 0.00790103312142018]
	TIME [epoch: 8.73 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009278914999810945		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.010985788981373236		[learning rate: 0.001117]
	Learning Rate: 0.00111703
	LOSS [training: 0.0059568402406771645 | validation: 0.04408090527337192]
	TIME [epoch: 8.74 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01807137660182984		[learning rate: 0.0011153]
		[batch 20/20] avg loss: 0.008946486756238128		[learning rate: 0.0011136]
	Learning Rate: 0.00111361
	LOSS [training: 0.013508931679033984 | validation: 0.01466396368553487]
	TIME [epoch: 8.77 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013678810739394639		[learning rate: 0.0011119]
		[batch 20/20] avg loss: 0.021332416629332385		[learning rate: 0.0011102]
	Learning Rate: 0.0011102
	LOSS [training: 0.017505613684363514 | validation: 0.026548939943852524]
	TIME [epoch: 8.74 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010108933519279158		[learning rate: 0.0011085]
		[batch 20/20] avg loss: 0.003654445609576199		[learning rate: 0.0011068]
	Learning Rate: 0.00110679
	LOSS [training: 0.006881689564427679 | validation: 0.009385804011817312]
	TIME [epoch: 8.75 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009944531963485598		[learning rate: 0.0011051]
		[batch 20/20] avg loss: 0.0027463413809863156		[learning rate: 0.0011034]
	Learning Rate: 0.0011034
	LOSS [training: 0.006345436672235957 | validation: 0.019521911327928066]
	TIME [epoch: 8.73 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005689562882986178		[learning rate: 0.0011017]
		[batch 20/20] avg loss: 0.011835532829750538		[learning rate: 0.0011]
	Learning Rate: 0.00110002
	LOSS [training: 0.008762547856368357 | validation: 0.011152245632139384]
	TIME [epoch: 8.73 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01612012750012768		[learning rate: 0.0010983]
		[batch 20/20] avg loss: 0.005825191839764368		[learning rate: 0.0010966]
	Learning Rate: 0.00109665
	LOSS [training: 0.010972659669946027 | validation: 0.01886329831585218]
	TIME [epoch: 8.77 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012690199416235688		[learning rate: 0.001095]
		[batch 20/20] avg loss: 0.012075256778527617		[learning rate: 0.0010933]
	Learning Rate: 0.00109328
	LOSS [training: 0.01238272809738165 | validation: 0.010612416240489862]
	TIME [epoch: 8.74 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012239009783702574		[learning rate: 0.0010916]
		[batch 20/20] avg loss: 0.0013819505774292154		[learning rate: 0.0010899]
	Learning Rate: 0.00108993
	LOSS [training: 0.006810480180565894 | validation: 0.011699988342269301]
	TIME [epoch: 8.72 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007067884906787121		[learning rate: 0.0010883]
		[batch 20/20] avg loss: 0.006337537520196815		[learning rate: 0.0010866]
	Learning Rate: 0.00108659
	LOSS [training: 0.006702711213491969 | validation: 0.010530606678274188]
	TIME [epoch: 8.73 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00011538917219744348		[learning rate: 0.0010849]
		[batch 20/20] avg loss: 0.0036129652278927213		[learning rate: 0.0010833]
	Learning Rate: 0.00108326
	LOSS [training: 0.001748788027847639 | validation: 0.0180847782593681]
	TIME [epoch: 8.75 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0069024245386660925		[learning rate: 0.0010816]
		[batch 20/20] avg loss: 0.00603977922792327		[learning rate: 0.0010799]
	Learning Rate: 0.00107994
	LOSS [training: 0.006471101883294682 | validation: -0.00017559654756766317]
	TIME [epoch: 8.72 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005059727251187708		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.010203604857575207		[learning rate: 0.0010766]
	Learning Rate: 0.00107663
	LOSS [training: 0.007631666054381457 | validation: 0.003176485460646006]
	TIME [epoch: 8.71 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004839929975903607		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.00042365925354689415		[learning rate: 0.0010733]
	Learning Rate: 0.00107333
	LOSS [training: 0.0026317946147252505 | validation: 0.013117941639211136]
	TIME [epoch: 8.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007978373852458009		[learning rate: 0.0010717]
		[batch 20/20] avg loss: 0.0010769572724668333		[learning rate: 0.00107]
	Learning Rate: 0.00107004
	LOSS [training: 0.004527665562462421 | validation: 0.021964543646927525]
	TIME [epoch: 8.74 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026440866506618208		[learning rate: 0.0010684]
		[batch 20/20] avg loss: 0.003725483918901734		[learning rate: 0.0010668]
	Learning Rate: 0.00106676
	LOSS [training: 0.003184785284781778 | validation: 0.004701438070792636]
	TIME [epoch: 8.74 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00837435450688195		[learning rate: 0.0010651]
		[batch 20/20] avg loss: 0.0035803680475424		[learning rate: 0.0010635]
	Learning Rate: 0.00106349
	LOSS [training: 0.005977361277212176 | validation: 0.005632324416518562]
	TIME [epoch: 8.73 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005477090568665488		[learning rate: 0.0010619]
		[batch 20/20] avg loss: 0.0006697804189000494		[learning rate: 0.0010602]
	Learning Rate: 0.00106023
	LOSS [training: 0.0030734354937827684 | validation: 0.009427884969209812]
	TIME [epoch: 8.73 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014790767371193425		[learning rate: 0.0010586]
		[batch 20/20] avg loss: 0.00330913386782035		[learning rate: 0.001057]
	Learning Rate: 0.00105698
	LOSS [training: 0.00904995061950689 | validation: 0.005566845758626449]
	TIME [epoch: 8.71 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001653776713891126		[learning rate: 0.0010554]
		[batch 20/20] avg loss: 0.000865026091782362		[learning rate: 0.0010537]
	Learning Rate: 0.00105374
	LOSS [training: -0.0003943753110543822 | validation: 0.0083884375395414]
	TIME [epoch: 8.73 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003973827099866841		[learning rate: 0.0010521]
		[batch 20/20] avg loss: 0.0069634070527389985		[learning rate: 0.0010505]
	Learning Rate: 0.00105051
	LOSS [training: 0.0032830121713761573 | validation: 0.01315555573131186]
	TIME [epoch: 8.73 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009650280301051887		[learning rate: 0.0010489]
		[batch 20/20] avg loss: 0.00451537011742029		[learning rate: 0.0010473]
	Learning Rate: 0.00104729
	LOSS [training: 0.0070828252092360895 | validation: 0.005077133633813206]
	TIME [epoch: 8.73 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012760376452316175		[learning rate: 0.0010457]
		[batch 20/20] avg loss: -0.0005330213496538965		[learning rate: 0.0010441]
	Learning Rate: 0.00104408
	LOSS [training: 0.0003715081477888604 | validation: 0.004176432649941785]
	TIME [epoch: 8.73 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013553584929889628		[learning rate: 0.0010425]
		[batch 20/20] avg loss: 0.008375448330927175		[learning rate: 0.0010409]
	Learning Rate: 0.00104088
	LOSS [training: 0.010964516630408403 | validation: 0.014566244816237693]
	TIME [epoch: 8.76 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01358798342886855		[learning rate: 0.0010393]
		[batch 20/20] avg loss: 0.013171510011554779		[learning rate: 0.0010377]
	Learning Rate: 0.00103769
	LOSS [training: 0.013379746720211666 | validation: 0.019801737343198077]
	TIME [epoch: 8.73 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013238316438685931		[learning rate: 0.0010361]
		[batch 20/20] avg loss: 0.010820459864237273		[learning rate: 0.0010345]
	Learning Rate: 0.00103451
	LOSS [training: 0.012029388151461602 | validation: 0.007142344789290259]
	TIME [epoch: 8.77 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005186088647447335		[learning rate: 0.0010329]
		[batch 20/20] avg loss: 0.0007388646702790053		[learning rate: 0.0010313]
	Learning Rate: 0.00103134
	LOSS [training: 0.0029624766588631705 | validation: 0.0038594934439910994]
	TIME [epoch: 8.74 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00045622555241677865		[learning rate: 0.0010298]
		[batch 20/20] avg loss: -0.0027476289360005147		[learning rate: 0.0010282]
	Learning Rate: 0.00102817
	LOSS [training: -0.0011457016917918682 | validation: 0.0017145429900504066]
	TIME [epoch: 8.74 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012862279874270507		[learning rate: 0.0010266]
		[batch 20/20] avg loss: 0.011440236382429261		[learning rate: 0.001025]
	Learning Rate: 0.00102502
	LOSS [training: 0.006363232184928155 | validation: 0.01740899523718588]
	TIME [epoch: 8.71 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008388775808276457		[learning rate: 0.0010234]
		[batch 20/20] avg loss: 0.009209841918205074		[learning rate: 0.0010219]
	Learning Rate: 0.00102188
	LOSS [training: 0.008799308863240766 | validation: 0.0018965110385002954]
	TIME [epoch: 8.76 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006973683435524083		[learning rate: 0.0010203]
		[batch 20/20] avg loss: 0.0025785090412224335		[learning rate: 0.0010187]
	Learning Rate: 0.00101875
	LOSS [training: 0.004776096238373258 | validation: 0.002090660290218377]
	TIME [epoch: 8.72 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036079315241852473		[learning rate: 0.0010172]
		[batch 20/20] avg loss: 0.009413339958133943		[learning rate: 0.0010156]
	Learning Rate: 0.00101562
	LOSS [training: 0.00452627340285771 | validation: 0.04144543089181756]
	TIME [epoch: 8.72 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007324326386184091		[learning rate: 0.0010141]
		[batch 20/20] avg loss: 0.0051114128640465115		[learning rate: 0.0010125]
	Learning Rate: 0.00101251
	LOSS [training: 0.006217869625115299 | validation: 0.005803688187034873]
	TIME [epoch: 8.71 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005280518035060433		[learning rate: 0.001011]
		[batch 20/20] avg loss: 0.004392619791620732		[learning rate: 0.0010094]
	Learning Rate: 0.00100941
	LOSS [training: 0.004836568913340582 | validation: 0.013284092735244083]
	TIME [epoch: 8.69 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0157259360088656		[learning rate: 0.0010079]
		[batch 20/20] avg loss: 0.01637808692745107		[learning rate: 0.0010063]
	Learning Rate: 0.00100631
	LOSS [training: 0.016052011468158336 | validation: 0.029287390619448204]
	TIME [epoch: 8.77 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0160240533398811		[learning rate: 0.0010048]
		[batch 20/20] avg loss: 0.008959483947799201		[learning rate: 0.0010032]
	Learning Rate: 0.00100323
	LOSS [training: 0.01249176864384015 | validation: 0.004128823214735673]
	TIME [epoch: 8.72 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005529847058853229		[learning rate: 0.0010017]
		[batch 20/20] avg loss: -0.0014379542866064389		[learning rate: 0.0010002]
	Learning Rate: 0.00100015
	LOSS [training: -0.00044248479036055786 | validation: 0.005802498955057874]
	TIME [epoch: 8.72 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003833662745630301		[learning rate: 0.00099862]
		[batch 20/20] avg loss: 0.0028720290188456754		[learning rate: 0.00099709]
	Learning Rate: 0.000997087
	LOSS [training: 0.0033528458822379877 | validation: 0.006606226737413787]
	TIME [epoch: 8.73 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007032752314205304		[learning rate: 0.00099556]
		[batch 20/20] avg loss: -0.00024641283901553093		[learning rate: 0.00099403]
	Learning Rate: 0.000994031
	LOSS [training: 0.0033931697375948868 | validation: 0.015331522120995015]
	TIME [epoch: 8.73 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01270674669990352		[learning rate: 0.00099251]
		[batch 20/20] avg loss: 0.0031987519357025533		[learning rate: 0.00099098]
	Learning Rate: 0.000990984
	LOSS [training: 0.007952749317803038 | validation: 0.013985099719558677]
	TIME [epoch: 8.73 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00933468154688442		[learning rate: 0.00098946]
		[batch 20/20] avg loss: 0.013695152338377061		[learning rate: 0.00098795]
	Learning Rate: 0.000987946
	LOSS [training: 0.011514916942630743 | validation: 0.024620961069891915]
	TIME [epoch: 8.72 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009480016513528705		[learning rate: 0.00098643]
		[batch 20/20] avg loss: 0.001674674051757561		[learning rate: 0.00098492]
	Learning Rate: 0.000984918
	LOSS [training: 0.005577345282643133 | validation: 0.0033378892919901136]
	TIME [epoch: 8.72 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006361162314973026		[learning rate: 0.00098341]
		[batch 20/20] avg loss: -0.0018023931834545385		[learning rate: 0.0009819]
	Learning Rate: 0.000981899
	LOSS [training: -0.0005831384759786178 | validation: 0.0049572748370481125]
	TIME [epoch: 8.71 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008359238258362205		[learning rate: 0.00098039]
		[batch 20/20] avg loss: 0.0027317038709702168		[learning rate: 0.00097889]
	Learning Rate: 0.000978889
	LOSS [training: 0.005545471064666211 | validation: 0.014372476247407503]
	TIME [epoch: 8.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004145723155431505		[learning rate: 0.00097739]
		[batch 20/20] avg loss: 0.0022067029605725656		[learning rate: 0.00097589]
	Learning Rate: 0.000975888
	LOSS [training: 0.0031762130580020344 | validation: 0.005465207592033633]
	TIME [epoch: 8.73 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035638786474297872		[learning rate: 0.00097439]
		[batch 20/20] avg loss: 0.013660742198567211		[learning rate: 0.0009729]
	Learning Rate: 0.000972897
	LOSS [training: 0.008612310422998499 | validation: 0.03441480958470752]
	TIME [epoch: 8.73 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022576914749706106		[learning rate: 0.0009714]
		[batch 20/20] avg loss: 0.001974464657272408		[learning rate: 0.00096991]
	Learning Rate: 0.000969914
	LOSS [training: 0.012275689703489256 | validation: 0.013561561648525575]
	TIME [epoch: 8.7 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042908090080637		[learning rate: 0.00096843]
		[batch 20/20] avg loss: 0.002173083139070844		[learning rate: 0.00096694]
	Learning Rate: 0.000966941
	LOSS [training: -0.001058862934496428 | validation: 0.0030578704138976946]
	TIME [epoch: 8.71 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00014746240930204403		[learning rate: 0.00096546]
		[batch 20/20] avg loss: 0.00313979641639743		[learning rate: 0.00096398]
	Learning Rate: 0.000963977
	LOSS [training: 0.001643629412849737 | validation: 0.010011840099680768]
	TIME [epoch: 8.7 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006878420600354829		[learning rate: 0.0009625]
		[batch 20/20] avg loss: -0.0024206320582090763		[learning rate: 0.00096102]
	Learning Rate: 0.000961022
	LOSS [training: 0.0022288942710728767 | validation: -0.0011856070195218422]
	TIME [epoch: 8.74 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019918064619009095		[learning rate: 0.00095955]
		[batch 20/20] avg loss: 0.00836307634610037		[learning rate: 0.00095808]
	Learning Rate: 0.000958076
	LOSS [training: 0.00318563494209973 | validation: 0.005438299790789831]
	TIME [epoch: 8.72 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002354256935279077		[learning rate: 0.00095661]
		[batch 20/20] avg loss: 0.0024763523538798835		[learning rate: 0.00095514]
	Learning Rate: 0.000955139
	LOSS [training: 0.00241530464457948 | validation: 0.015305029419708637]
	TIME [epoch: 8.71 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0067091695842158156		[learning rate: 0.00095367]
		[batch 20/20] avg loss: 0.002625424587503282		[learning rate: 0.00095221]
	Learning Rate: 0.000952211
	LOSS [training: 0.004667297085859549 | validation: 0.014250906108214398]
	TIME [epoch: 8.72 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010601954844594688		[learning rate: 0.00095075]
		[batch 20/20] avg loss: 0.000829224703142269		[learning rate: 0.00094929]
	Learning Rate: 0.000949292
	LOSS [training: 0.005715589773868479 | validation: 0.005203172596748167]
	TIME [epoch: 8.72 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00019473531575578399		[learning rate: 0.00094784]
		[batch 20/20] avg loss: 0.0014549674383365523		[learning rate: 0.00094638]
	Learning Rate: 0.000946382
	LOSS [training: 0.0008248513770461683 | validation: 0.005250088138776967]
	TIME [epoch: 8.75 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012762867670998847		[learning rate: 0.00094493]
		[batch 20/20] avg loss: 0.0028450463158716695		[learning rate: 0.00094348]
	Learning Rate: 0.000943481
	LOSS [training: 0.000784379774385892 | validation: 0.02138814485163427]
	TIME [epoch: 8.73 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004208854402847282		[learning rate: 0.00094203]
		[batch 20/20] avg loss: 0.005960908443688359		[learning rate: 0.00094059]
	Learning Rate: 0.000940589
	LOSS [training: 0.00508488142326782 | validation: 0.0025649150879016288]
	TIME [epoch: 8.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014714660616753643		[learning rate: 0.00093915]
		[batch 20/20] avg loss: 0.0033436391802531334		[learning rate: 0.00093771]
	Learning Rate: 0.000937706
	LOSS [training: 0.0009360865592888847 | validation: 0.011483434545497646]
	TIME [epoch: 8.72 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034373222130612696		[learning rate: 0.00093627]
		[batch 20/20] avg loss: 0.004772748040304698		[learning rate: 0.00093483]
	Learning Rate: 0.000934831
	LOSS [training: 0.0006677129136217139 | validation: 0.004619476445427102]
	TIME [epoch: 8.75 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006744657941345295		[learning rate: 0.0009334]
		[batch 20/20] avg loss: -0.0005733548682560268		[learning rate: 0.00093197]
	Learning Rate: 0.000931966
	LOSS [training: 5.05554629392513e-05 | validation: 0.00433516276619963]
	TIME [epoch: 8.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006776526788196055		[learning rate: 0.00093054]
		[batch 20/20] avg loss: 0.0008320894183757868		[learning rate: 0.00092911]
	Learning Rate: 0.000929109
	LOSS [training: 0.003804308103285921 | validation: 0.011904138027952569]
	TIME [epoch: 8.69 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021653660910266057		[learning rate: 0.00092768]
		[batch 20/20] avg loss: 0.008822422193008812		[learning rate: 0.00092626]
	Learning Rate: 0.000926261
	LOSS [training: 0.0033285280509911036 | validation: 0.005505305778447685]
	TIME [epoch: 8.72 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004187807324227466		[learning rate: 0.00092484]
		[batch 20/20] avg loss: 0.0035039422498940654		[learning rate: 0.00092342]
	Learning Rate: 0.000923421
	LOSS [training: 0.0015425807587356594 | validation: 0.015499511903038788]
	TIME [epoch: 8.72 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035136238998231123		[learning rate: 0.00092201]
		[batch 20/20] avg loss: 0.002171128755453983		[learning rate: 0.00092059]
	Learning Rate: 0.000920591
	LOSS [training: 0.0028423763276385476 | validation: 0.006860172627531557]
	TIME [epoch: 8.73 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006032218721444265		[learning rate: 0.00091918]
		[batch 20/20] avg loss: -0.00018381614680693163		[learning rate: 0.00091777]
	Learning Rate: 0.000917769
	LOSS [training: 0.00020970286266874775 | validation: 0.005403801219785432]
	TIME [epoch: 8.73 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022614808095890813		[learning rate: 0.00091636]
		[batch 20/20] avg loss: 0.005660793941009251		[learning rate: 0.00091496]
	Learning Rate: 0.000914956
	LOSS [training: 0.003961137375299165 | validation: -0.003533710760104367]
	TIME [epoch: 8.71 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002562014655363068		[learning rate: 0.00091355]
		[batch 20/20] avg loss: 0.0069779929098761		[learning rate: 0.00091215]
	Learning Rate: 0.000912151
	LOSS [training: 0.004770003782619585 | validation: 0.009125880687283684]
	TIME [epoch: 8.74 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0061169898042032055		[learning rate: 0.00091075]
		[batch 20/20] avg loss: 0.006295441167650709		[learning rate: 0.00090935]
	Learning Rate: 0.000909355
	LOSS [training: 0.006206215485926958 | validation: 0.011795902702058922]
	TIME [epoch: 8.71 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006618899139177603		[learning rate: 0.00090796]
		[batch 20/20] avg loss: 0.0006218544850691377		[learning rate: 0.00090657]
	Learning Rate: 0.000906567
	LOSS [training: -2.0017714424311327e-05 | validation: 0.0006870308325992766]
	TIME [epoch: 8.73 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022034883015158296		[learning rate: 0.00090518]
		[batch 20/20] avg loss: 0.003051604542538693		[learning rate: 0.00090379]
	Learning Rate: 0.000903788
	LOSS [training: 0.00042405812051143145 | validation: 0.0012347891154651182]
	TIME [epoch: 8.71 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002205088151781941		[learning rate: 0.0009024]
		[batch 20/20] avg loss: 0.0017879469599626544		[learning rate: 0.00090102]
	Learning Rate: 0.000901018
	LOSS [training: -0.00020857059590964296 | validation: 0.003634815908004997]
	TIME [epoch: 8.71 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00129928443531043		[learning rate: 0.00089964]
		[batch 20/20] avg loss: 0.0026245395556519476		[learning rate: 0.00089826]
	Learning Rate: 0.000898256
	LOSS [training: 0.0019619119954811886 | validation: 0.019907224975954915]
	TIME [epoch: 8.73 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027423977186127823		[learning rate: 0.00089688]
		[batch 20/20] avg loss: 0.004992738857051442		[learning rate: 0.0008955]
	Learning Rate: 0.000895502
	LOSS [training: 0.003867568287832112 | validation: 0.002696710202783593]
	TIME [epoch: 8.76 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003871477862194448		[learning rate: 0.00089413]
		[batch 20/20] avg loss: 0.004763618116356017		[learning rate: 0.00089276]
	Learning Rate: 0.000892757
	LOSS [training: 0.004317547989275233 | validation: 0.005237952768522167]
	TIME [epoch: 8.72 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007891493144814209		[learning rate: 0.00089139]
		[batch 20/20] avg loss: 0.000284042899226917		[learning rate: 0.00089002]
	Learning Rate: 0.00089002
	LOSS [training: 0.000536596106854169 | validation: 0.0012415681676474766]
	TIME [epoch: 8.73 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007476618154551538		[learning rate: 0.00088866]
		[batch 20/20] avg loss: 0.0028359036223689583		[learning rate: 0.00088729]
	Learning Rate: 0.000887292
	LOSS [training: 0.0010441209034569023 | validation: 0.007042783854377413]
	TIME [epoch: 8.72 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004849686245750719		[learning rate: 0.00088593]
		[batch 20/20] avg loss: 0.00668347149223141		[learning rate: 0.00088457]
	Learning Rate: 0.000884572
	LOSS [training: 0.000916892623240346 | validation: 0.00017367647273356594]
	TIME [epoch: 8.71 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004074170846324247		[learning rate: 0.00088322]
		[batch 20/20] avg loss: -0.0022551139871599723		[learning rate: 0.00088186]
	Learning Rate: 0.000881861
	LOSS [training: -0.0013312655358961984 | validation: 0.0040292246876791026]
	TIME [epoch: 8.74 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013403678978357694		[learning rate: 0.00088051]
		[batch 20/20] avg loss: 0.02074898665128574		[learning rate: 0.00087916]
	Learning Rate: 0.000879157
	LOSS [training: 0.009704309376724986 | validation: 0.02249606278004801]
	TIME [epoch: 8.73 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006062186669059151		[learning rate: 0.00087781]
		[batch 20/20] avg loss: -0.0033213344318192713		[learning rate: 0.00087646]
	Learning Rate: 0.000876462
	LOSS [training: 0.00137042611861994 | validation: 0.005605401876966112]
	TIME [epoch: 8.72 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018467486700178312		[learning rate: 0.00087512]
		[batch 20/20] avg loss: 0.0022075025046584635		[learning rate: 0.00087378]
	Learning Rate: 0.000873776
	LOSS [training: 0.00018037691732031622 | validation: 0.007092584777127993]
	TIME [epoch: 8.7 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006556597557245273		[learning rate: 0.00087244]
		[batch 20/20] avg loss: 0.003794225953127915		[learning rate: 0.0008711]
	Learning Rate: 0.000871097
	LOSS [training: 0.0015692830987016938 | validation: -0.0028783876453399707]
	TIME [epoch: 8.72 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003097112893972929		[learning rate: 0.00086976]
		[batch 20/20] avg loss: 0.0034157773211184795		[learning rate: 0.00086843]
	Learning Rate: 0.000868427
	LOSS [training: 0.003256445107545704 | validation: -0.0008655886008736126]
	TIME [epoch: 8.75 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00016501437363637762		[learning rate: 0.00086709]
		[batch 20/20] avg loss: -0.0027550690497778764		[learning rate: 0.00086576]
	Learning Rate: 0.000865765
	LOSS [training: -0.0012950273380707496 | validation: 0.007659535061094816]
	TIME [epoch: 8.72 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001625830829685787		[learning rate: 0.00086444]
		[batch 20/20] avg loss: 0.0040756334234131174		[learning rate: 0.00086311]
	Learning Rate: 0.000863111
	LOSS [training: 0.0028507321265494523 | validation: 0.013160085896701985]
	TIME [epoch: 8.72 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021889504748486346		[learning rate: 0.00086179]
		[batch 20/20] avg loss: 0.0032373339233764315		[learning rate: 0.00086047]
	Learning Rate: 0.000860465
	LOSS [training: 0.0027131421991125337 | validation: 0.002646727240211172]
	TIME [epoch: 8.73 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022044976674635806		[learning rate: 0.00085915]
		[batch 20/20] avg loss: 0.004446080698277783		[learning rate: 0.00085783]
	Learning Rate: 0.000857828
	LOSS [training: 0.0033252891828706816 | validation: 0.004265064345549675]
	TIME [epoch: 8.73 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001913153497958952		[learning rate: 0.00085651]
		[batch 20/20] avg loss: 0.0021142857384103962		[learning rate: 0.0008552]
	Learning Rate: 0.000855198
	LOSS [training: 0.0020137196181846738 | validation: 0.000658360103068221]
	TIME [epoch: 8.73 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01199308724389365		[learning rate: 0.00085389]
		[batch 20/20] avg loss: -0.0012144929276064712		[learning rate: 0.00085258]
	Learning Rate: 0.000852576
	LOSS [training: 0.005389297158143588 | validation: 0.00471886198553898]
	TIME [epoch: 8.71 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004860218190070697		[learning rate: 0.00085127]
		[batch 20/20] avg loss: 0.0004501451107758977		[learning rate: 0.00084996]
	Learning Rate: 0.000849963
	LOSS [training: -0.0022050365396473993 | validation: 0.0034884670812047975]
	TIME [epoch: 8.73 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027101784965961454		[learning rate: 0.00084866]
		[batch 20/20] avg loss: 0.006297808393291511		[learning rate: 0.00084736]
	Learning Rate: 0.000847357
	LOSS [training: 0.0017938149483476833 | validation: 0.012383801568424899]
	TIME [epoch: 8.73 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001865893210320364		[learning rate: 0.00084606]
		[batch 20/20] avg loss: 0.00403707413922716		[learning rate: 0.00084476]
	Learning Rate: 0.00084476
	LOSS [training: 0.0029514836747737616 | validation: -0.003193003291502946]
	TIME [epoch: 8.75 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034585047244592463		[learning rate: 0.00084346]
		[batch 20/20] avg loss: 0.005088775931461996		[learning rate: 0.00084217]
	Learning Rate: 0.00084217
	LOSS [training: 0.0008151356035013748 | validation: 0.005801896008186914]
	TIME [epoch: 8.75 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000616799271073939		[learning rate: 0.00084088]
		[batch 20/20] avg loss: 0.00481068456986237		[learning rate: 0.00083959]
	Learning Rate: 0.000839589
	LOSS [training: 0.0027137419204681535 | validation: 0.003048332749727942]
	TIME [epoch: 8.73 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027845549360078917		[learning rate: 0.0008383]
		[batch 20/20] avg loss: -0.0009958834180777885		[learning rate: 0.00083702]
	Learning Rate: 0.000837015
	LOSS [training: -0.0018902191770428407 | validation: 0.0065557755007200815]
	TIME [epoch: 8.74 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025421440637971035		[learning rate: 0.00083573]
		[batch 20/20] avg loss: -0.0005341764086976302		[learning rate: 0.00083445]
	Learning Rate: 0.000834449
	LOSS [training: 0.0010039838275497364 | validation: 0.0018191081362660324]
	TIME [epoch: 8.73 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005239172938360665		[learning rate: 0.00083317]
		[batch 20/20] avg loss: -0.0043259657952461495		[learning rate: 0.00083189]
	Learning Rate: 0.000831891
	LOSS [training: 0.00045660357155725766 | validation: -0.0012954317638998731]
	TIME [epoch: 8.76 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021886027479550013		[learning rate: 0.00083062]
		[batch 20/20] avg loss: 0.002051744006531452		[learning rate: 0.00082934]
	Learning Rate: 0.000829341
	LOSS [training: 0.0021201733772432266 | validation: 0.004990920933020723]
	TIME [epoch: 8.73 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032809003958053857		[learning rate: 0.00082807]
		[batch 20/20] avg loss: -0.0023924713333060795		[learning rate: 0.0008268]
	Learning Rate: 0.000826799
	LOSS [training: 0.00044421453124965295 | validation: 0.013269484827282692]
	TIME [epoch: 8.73 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004264642004137151		[learning rate: 0.00082553]
		[batch 20/20] avg loss: 0.001139417849803165		[learning rate: 0.00082426]
	Learning Rate: 0.000824265
	LOSS [training: -0.001562612077166993 | validation: -0.0032814767071359737]
	TIME [epoch: 8.73 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010598962069637027		[learning rate: 0.000823]
		[batch 20/20] avg loss: -0.0003230737335728613		[learning rate: 0.00082174]
	Learning Rate: 0.000821738
	LOSS [training: 0.005137944168032083 | validation: -0.0018432695242980085]
	TIME [epoch: 8.71 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015502228323206158		[learning rate: 0.00082048]
		[batch 20/20] avg loss: 0.0060408483037879225		[learning rate: 0.00081922]
	Learning Rate: 0.000819219
	LOSS [training: 0.002245312735733653 | validation: -0.0003520936301170479]
	TIME [epoch: 8.73 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011444268732649015		[learning rate: 0.00081796]
		[batch 20/20] avg loss: 0.00124987607626507		[learning rate: 0.00081671]
	Learning Rate: 0.000816708
	LOSS [training: 0.006347072404457042 | validation: 0.004816482558282046]
	TIME [epoch: 8.74 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003693570338213307		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.0023357071783891557		[learning rate: 0.0008142]
	Learning Rate: 0.000814204
	LOSS [training: 0.0030146387583012317 | validation: -0.0017410047570539193]
	TIME [epoch: 8.7 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003194887767508926		[learning rate: 0.00081296]
		[batch 20/20] avg loss: 0.00036632111253480994		[learning rate: 0.00081171]
	Learning Rate: 0.000811708
	LOSS [training: -0.0014142833274870582 | validation: 0.0003800690916361414]
	TIME [epoch: 8.73 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004763623932702269		[learning rate: 0.00081046]
		[batch 20/20] avg loss: 0.0033683328776257493		[learning rate: 0.00080922]
	Learning Rate: 0.00080922
	LOSS [training: -0.0006976455275382605 | validation: 0.004059542953347461]
	TIME [epoch: 8.71 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001290574469564345		[learning rate: 0.00080798]
		[batch 20/20] avg loss: 0.006126462266233077		[learning rate: 0.00080674]
	Learning Rate: 0.000806739
	LOSS [training: 0.0024179438983343658 | validation: -0.0018099122382395348]
	TIME [epoch: 8.73 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002872940007229758		[learning rate: 0.0008055]
		[batch 20/20] avg loss: -0.0014229148932899164		[learning rate: 0.00080427]
	Learning Rate: 0.000804267
	LOSS [training: 0.0007250125569699212 | validation: -0.002378733567384289]
	TIME [epoch: 8.71 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00036123007654996		[learning rate: 0.00080303]
		[batch 20/20] avg loss: 0.002077553779573907		[learning rate: 0.0008018]
	Learning Rate: 0.000801801
	LOSS [training: 0.0008581618515119736 | validation: 0.005429509670637281]
	TIME [epoch: 8.72 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053193900342989415		[learning rate: 0.00080057]
		[batch 20/20] avg loss: -0.002331660590984508		[learning rate: 0.00079934]
	Learning Rate: 0.000799343
	LOSS [training: 0.0014938647216572165 | validation: -0.0018269692302635722]
	TIME [epoch: 8.72 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002123438305169305		[learning rate: 0.00079812]
		[batch 20/20] avg loss: -0.002376161913216802		[learning rate: 0.00079689]
	Learning Rate: 0.000796893
	LOSS [training: -0.0001263618040237488 | validation: 0.004450072879529113]
	TIME [epoch: 8.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00362687793337957		[learning rate: 0.00079567]
		[batch 20/20] avg loss: 0.0019086057858920427		[learning rate: 0.00079445]
	Learning Rate: 0.00079445
	LOSS [training: -0.0008591360737437633 | validation: 0.004520127238133106]
	TIME [epoch: 8.75 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006034152957141664		[learning rate: 0.00079323]
		[batch 20/20] avg loss: 0.0009109333082596869		[learning rate: 0.00079201]
	Learning Rate: 0.000792015
	LOSS [training: 0.0001537590062727602 | validation: 0.0048406361856541645]
	TIME [epoch: 8.71 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038181700019429		[learning rate: 0.0007908]
		[batch 20/20] avg loss: 0.0022980116304967373		[learning rate: 0.00078959]
	Learning Rate: 0.000789587
	LOSS [training: -0.0007600791857230816 | validation: 0.0046060317564033375]
	TIME [epoch: 8.72 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005198754640257394		[learning rate: 0.00078838]
		[batch 20/20] avg loss: 0.004440957125661877		[learning rate: 0.00078717]
	Learning Rate: 0.000787166
	LOSS [training: 0.002480416294843808 | validation: 0.0133373115373591]
	TIME [epoch: 8.7 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006369206531308718		[learning rate: 0.00078596]
		[batch 20/20] avg loss: 0.0035191835447933133		[learning rate: 0.00078475]
	Learning Rate: 0.000784754
	LOSS [training: 0.002078052098962092 | validation: 0.0050017691620826005]
	TIME [epoch: 8.71 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027543878682967575		[learning rate: 0.00078355]
		[batch 20/20] avg loss: 0.006289985885883842		[learning rate: 0.00078235]
	Learning Rate: 0.000782348
	LOSS [training: 0.004522186877090301 | validation: 0.008205919014131686]
	TIME [epoch: 8.7 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006400703709389869		[learning rate: 0.00078115]
		[batch 20/20] avg loss: 0.0008404214334320435		[learning rate: 0.00077995]
	Learning Rate: 0.00077995
	LOSS [training: 0.003620562571410955 | validation: 0.0037331691158787907]
	TIME [epoch: 8.69 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007954729853918343		[learning rate: 0.00077875]
		[batch 20/20] avg loss: -0.007553251969536387		[learning rate: 0.00077756]
	Learning Rate: 0.000777559
	LOSS [training: 0.00020073894219097745 | validation: 0.0025744149902742]
	TIME [epoch: 8.71 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00237006936285195		[learning rate: 0.00077637]
		[batch 20/20] avg loss: 0.0007943031608459271		[learning rate: 0.00077518]
	Learning Rate: 0.000775175
	LOSS [training: 0.0015821862618489388 | validation: 0.012591005438471303]
	TIME [epoch: 8.72 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002350120053325899		[learning rate: 0.00077399]
		[batch 20/20] avg loss: 0.0006980663552327435		[learning rate: 0.0007728]
	Learning Rate: 0.000772799
	LOSS [training: -0.0008260268490465777 | validation: 0.0024836732364756906]
	TIME [epoch: 8.75 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002318823043319316		[learning rate: 0.00077161]
		[batch 20/20] avg loss: -0.0016608601049787285		[learning rate: 0.00077043]
	Learning Rate: 0.00077043
	LOSS [training: -0.0019898415741490235 | validation: 0.0036883168927168636]
	TIME [epoch: 8.73 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000672406012931		[learning rate: 0.00076925]
		[batch 20/20] avg loss: 0.0021806578205110227		[learning rate: 0.00076807]
	Learning Rate: 0.000768068
	LOSS [training: 0.0014265319167210111 | validation: 0.007423443765465129]
	TIME [epoch: 8.73 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009310279804543093		[learning rate: 0.00076689]
		[batch 20/20] avg loss: -0.0013758137114282265		[learning rate: 0.00076571]
	Learning Rate: 0.000765714
	LOSS [training: -0.00022239286548695857 | validation: 0.000320304290473873]
	TIME [epoch: 8.72 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018476430010234302		[learning rate: 0.00076454]
		[batch 20/20] avg loss: 0.003341831037636997		[learning rate: 0.00076337]
	Learning Rate: 0.000763367
	LOSS [training: 0.0007470940183067836 | validation: 0.004887801886114247]
	TIME [epoch: 8.73 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016710254440447335		[learning rate: 0.0007622]
		[batch 20/20] avg loss: 0.0010600123957767594		[learning rate: 0.00076103]
	Learning Rate: 0.000761027
	LOSS [training: -0.000305506524133987 | validation: 0.005294618096946131]
	TIME [epoch: 8.75 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001884576130039965		[learning rate: 0.00075986]
		[batch 20/20] avg loss: -0.0015843712458808944		[learning rate: 0.00075869]
	Learning Rate: 0.000758694
	LOSS [training: 0.00015010244207953492 | validation: 0.0011305192666688272]
	TIME [epoch: 8.72 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00202788360318175		[learning rate: 0.00075753]
		[batch 20/20] avg loss: -0.004152999115547286		[learning rate: 0.00075637]
	Learning Rate: 0.000756368
	LOSS [training: -0.003090441359364518 | validation: 0.010390719267626152]
	TIME [epoch: 8.72 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00456601883960744		[learning rate: 0.00075521]
		[batch 20/20] avg loss: -0.004356011713171929		[learning rate: 0.00075405]
	Learning Rate: 0.00075405
	LOSS [training: 0.00010500356321775477 | validation: 0.005453785907181375]
	TIME [epoch: 8.72 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031786293753493303		[learning rate: 0.00075289]
		[batch 20/20] avg loss: -0.0024903299392186246		[learning rate: 0.00075174]
	Learning Rate: 0.000751738
	LOSS [training: 0.0003441497180653526 | validation: 0.004598427551849788]
	TIME [epoch: 8.72 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005069212974009788		[learning rate: 0.00075059]
		[batch 20/20] avg loss: -0.003732248410530677		[learning rate: 0.00074943]
	Learning Rate: 0.000749434
	LOSS [training: 0.000668482281739556 | validation: -0.004414212280869148]
	TIME [epoch: 8.74 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003131788958773374		[learning rate: 0.00074828]
		[batch 20/20] avg loss: 0.0030137668011195524		[learning rate: 0.00074714]
	Learning Rate: 0.000747137
	LOSS [training: -5.9011078826910495e-05 | validation: 0.008441375785233185]
	TIME [epoch: 8.72 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004548284711800654		[learning rate: 0.00074599]
		[batch 20/20] avg loss: 0.005190330424867292		[learning rate: 0.00074485]
	Learning Rate: 0.000744846
	LOSS [training: 0.0003210228565333199 | validation: 0.001124706820647284]
	TIME [epoch: 8.73 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007051724112218631		[learning rate: 0.0007437]
		[batch 20/20] avg loss: 0.00032513719183053234		[learning rate: 0.00074256]
	Learning Rate: 0.000742563
	LOSS [training: 0.0005151548015261976 | validation: 0.00384405288190402]
	TIME [epoch: 8.73 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002843156777228657		[learning rate: 0.00074142]
		[batch 20/20] avg loss: -0.00455521630280018		[learning rate: 0.00074029]
	Learning Rate: 0.000740287
	LOSS [training: -0.0008560297627857614 | validation: 0.013778465902386922]
	TIME [epoch: 8.75 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010941490575853475		[learning rate: 0.00073915]
		[batch 20/20] avg loss: -0.0011733548958353635		[learning rate: 0.00073802]
	Learning Rate: 0.000738017
	LOSS [training: -0.0011337519767103555 | validation: 0.000626669910680891]
	TIME [epoch: 8.73 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008820538858057296		[learning rate: 0.00073689]
		[batch 20/20] avg loss: -0.003946454918200996		[learning rate: 0.00073576]
	Learning Rate: 0.000735755
	LOSS [training: -0.0015322005161976337 | validation: 0.001254128470323739]
	TIME [epoch: 8.72 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012321861940650792		[learning rate: 0.00073463]
		[batch 20/20] avg loss: 0.0024651640217838618		[learning rate: 0.0007335]
	Learning Rate: 0.0007335
	LOSS [training: 0.0006164889138593913 | validation: 0.00513409188466654]
	TIME [epoch: 8.71 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001140131588601291		[learning rate: 0.00073237]
		[batch 20/20] avg loss: -0.0020085548148350853		[learning rate: 0.00073125]
	Learning Rate: 0.000731251
	LOSS [training: -0.0004342116131168971 | validation: 0.0031844847603275527]
	TIME [epoch: 8.7 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012314982521468183		[learning rate: 0.00073013]
		[batch 20/20] avg loss: -0.0005399402809624792		[learning rate: 0.00072901]
	Learning Rate: 0.00072901
	LOSS [training: 0.00034577898559216965 | validation: -0.0009556394786120569]
	TIME [epoch: 8.77 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006693754603754945		[learning rate: 0.00072789]
		[batch 20/20] avg loss: 0.007356801992832225		[learning rate: 0.00072677]
	Learning Rate: 0.000726775
	LOSS [training: 0.0003315236945386396 | validation: 0.002330008511693669]
	TIME [epoch: 8.74 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0052281467210210045		[learning rate: 0.00072566]
		[batch 20/20] avg loss: -0.0006931871774260369		[learning rate: 0.00072455]
	Learning Rate: 0.000724547
	LOSS [training: 0.002267479771797484 | validation: -0.0019389348438127634]
	TIME [epoch: 8.73 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020456186261872196		[learning rate: 0.00072344]
		[batch 20/20] avg loss: -0.0034407887293526457		[learning rate: 0.00072233]
	Learning Rate: 0.000722326
	LOSS [training: -0.0027432036777699327 | validation: -0.007975122058037546]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_1356.pth
	Model improved!!!
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013139926880299923		[learning rate: 0.00072122]
		[batch 20/20] avg loss: -0.0027903878413811853		[learning rate: 0.00072011]
	Learning Rate: 0.000720112
	LOSS [training: -0.0007381975766755967 | validation: -0.0003075039615973072]
	TIME [epoch: 8.73 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002754323947564333		[learning rate: 0.00071901]
		[batch 20/20] avg loss: 0.004163871782356253		[learning rate: 0.0007179]
	Learning Rate: 0.000717904
	LOSS [training: 0.0007047739173959607 | validation: 0.0036023087990494406]
	TIME [epoch: 8.75 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013633265261364987		[learning rate: 0.0007168]
		[batch 20/20] avg loss: 0.0004819503884196861		[learning rate: 0.0007157]
	Learning Rate: 0.000715704
	LOSS [training: 0.0009226384572780926 | validation: 0.00962594940376478]
	TIME [epoch: 8.74 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002062754471922338		[learning rate: 0.00071461]
		[batch 20/20] avg loss: -0.007272405099630684		[learning rate: 0.00071351]
	Learning Rate: 0.00071351
	LOSS [training: -0.0026048253138541725 | validation: 0.003469200536443706]
	TIME [epoch: 8.75 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006684131399679919		[learning rate: 0.00071242]
		[batch 20/20] avg loss: 0.0005991146414673622		[learning rate: 0.00071132]
	Learning Rate: 0.000711323
	LOSS [training: -3.464924925031491e-05 | validation: 0.003776076596031729]
	TIME [epoch: 8.74 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002784820835541891		[learning rate: 0.00071023]
		[batch 20/20] avg loss: 0.0016957569457462227		[learning rate: 0.00070914]
	Learning Rate: 0.000709142
	LOSS [training: 0.0022402888906440568 | validation: 0.002998190413576844]
	TIME [epoch: 8.73 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005582151343201794		[learning rate: 0.00070805]
		[batch 20/20] avg loss: 0.0018650121717907445		[learning rate: 0.00070697]
	Learning Rate: 0.000706968
	LOSS [training: 0.0006533985187352827 | validation: 0.00810649930134143]
	TIME [epoch: 8.75 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002869444263962499		[learning rate: 0.00070588]
		[batch 20/20] avg loss: 0.0003421237766754355		[learning rate: 0.0007048]
	Learning Rate: 0.000704801
	LOSS [training: 0.0016057840203189673 | validation: -0.0031172912542223865]
	TIME [epoch: 8.74 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002484287500522448		[learning rate: 0.00070372]
		[batch 20/20] avg loss: -0.002969038167490636		[learning rate: 0.00070264]
	Learning Rate: 0.000702641
	LOSS [training: -0.0013603047087191954 | validation: -0.002179984553750386]
	TIME [epoch: 8.74 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00045341894089923773		[learning rate: 0.00070156]
		[batch 20/20] avg loss: 0.002440287499747532		[learning rate: 0.00070049]
	Learning Rate: 0.000700487
	LOSS [training: 0.0009934342794241473 | validation: 0.0010981890089820465]
	TIME [epoch: 8.73 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00017191542227156848		[learning rate: 0.00069941]
		[batch 20/20] avg loss: -0.0019282108867187248		[learning rate: 0.00069834]
	Learning Rate: 0.000698339
	LOSS [training: -0.0008781477322235781 | validation: -0.0021536104382376743]
	TIME [epoch: 8.74 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00240133073347412		[learning rate: 0.00069727]
		[batch 20/20] avg loss: -0.004043746450826525		[learning rate: 0.0006962]
	Learning Rate: 0.000696199
	LOSS [training: -0.003222538592150323 | validation: 0.0003880076091543861]
	TIME [epoch: 8.77 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.247260726225931e-05		[learning rate: 0.00069513]
		[batch 20/20] avg loss: -0.0026954423973129054		[learning rate: 0.00069406]
	Learning Rate: 0.000694065
	LOSS [training: -0.0013114848950253229 | validation: 0.0045443332820108975]
	TIME [epoch: 8.78 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004489761720199185		[learning rate: 0.000693]
		[batch 20/20] avg loss: 0.003516423734293618		[learning rate: 0.00069194]
	Learning Rate: 0.000691937
	LOSS [training: -0.00048666899295278396 | validation: 0.0021457607386584435]
	TIME [epoch: 8.75 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002101622050644881		[learning rate: 0.00069088]
		[batch 20/20] avg loss: 0.0004343437119799963		[learning rate: 0.00068982]
	Learning Rate: 0.000689816
	LOSS [training: -0.0008336391693324422 | validation: 0.002195773125321835]
	TIME [epoch: 8.75 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000930383301939559		[learning rate: 0.00068876]
		[batch 20/20] avg loss: -0.0028369822324229464		[learning rate: 0.0006877]
	Learning Rate: 0.000687701
	LOSS [training: -0.0009532994652416939 | validation: -0.0013046070123980443]
	TIME [epoch: 8.75 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008886079752302789		[learning rate: 0.00068665]
		[batch 20/20] avg loss: -0.0043853357236088185		[learning rate: 0.00068559]
	Learning Rate: 0.000685593
	LOSS [training: -0.0017483638741892702 | validation: -0.00041276084278605673]
	TIME [epoch: 8.8 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036214458788000983		[learning rate: 0.00068454]
		[batch 20/20] avg loss: -0.0013737068939149854		[learning rate: 0.00068349]
	Learning Rate: 0.000683492
	LOSS [training: -0.0024975763863575415 | validation: 0.00020896344303860173]
	TIME [epoch: 8.72 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002295494436304416		[learning rate: 0.00068244]
		[batch 20/20] avg loss: 0.012191820435627988		[learning rate: 0.0006814]
	Learning Rate: 0.000681397
	LOSS [training: 0.004948162999661786 | validation: 0.007269961206841997]
	TIME [epoch: 8.79 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025942330618330856		[learning rate: 0.00068035]
		[batch 20/20] avg loss: 0.00011992562741949906		[learning rate: 0.00067931]
	Learning Rate: 0.000679308
	LOSS [training: -0.001237153717206793 | validation: -0.0017006674132616262]
	TIME [epoch: 8.73 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002644494245421995		[learning rate: 0.00067827]
		[batch 20/20] avg loss: 0.0008915804901090119		[learning rate: 0.00067723]
	Learning Rate: 0.000677225
	LOSS [training: 0.0017680373677655035 | validation: 0.005720967619366716]
	TIME [epoch: 8.74 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025890984762022		[learning rate: 0.00067619]
		[batch 20/20] avg loss: 0.0005605155415238173		[learning rate: 0.00067515]
	Learning Rate: 0.000675149
	LOSS [training: 0.001574807008863008 | validation: 0.01202012203894644]
	TIME [epoch: 8.73 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001864735954873248		[learning rate: 0.00067411]
		[batch 20/20] avg loss: 0.002381007755495245		[learning rate: 0.00067308]
	Learning Rate: 0.00067308
	LOSS [training: 0.0021228718551842465 | validation: 0.008204071757425789]
	TIME [epoch: 8.72 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0001129884643918314		[learning rate: 0.00067205]
		[batch 20/20] avg loss: 0.000519139718680906		[learning rate: 0.00067102]
	Learning Rate: 0.000671017
	LOSS [training: 0.00020307562714453738 | validation: -0.0035486922367800105]
	TIME [epoch: 8.75 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023831827786125344		[learning rate: 0.00066999]
		[batch 20/20] avg loss: 0.002347404767401793		[learning rate: 0.00066896]
	Learning Rate: 0.00066896
	LOSS [training: -1.788900560537026e-05 | validation: 0.002534231663916129]
	TIME [epoch: 8.73 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026944770297640482		[learning rate: 0.00066793]
		[batch 20/20] avg loss: 0.0011716576923422567		[learning rate: 0.00066691]
	Learning Rate: 0.000666909
	LOSS [training: -0.0007614096687108957 | validation: 0.0012085184110771766]
	TIME [epoch: 8.77 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006414658297943856		[learning rate: 0.00066589]
		[batch 20/20] avg loss: 0.0027158819222290013		[learning rate: 0.00066486]
	Learning Rate: 0.000664865
	LOSS [training: 0.00456527011008643 | validation: 0.008886861206888053]
	TIME [epoch: 8.73 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007820570446961614		[learning rate: 0.00066384]
		[batch 20/20] avg loss: 0.003385250803456392		[learning rate: 0.00066283]
	Learning Rate: 0.000662827
	LOSS [training: 0.0056029106252090035 | validation: 0.008517372963941058]
	TIME [epoch: 8.72 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003069621764138776		[learning rate: 0.00066181]
		[batch 20/20] avg loss: 4.6195115011242115e-05		[learning rate: 0.00066079]
	Learning Rate: 0.000660795
	LOSS [training: 0.001557908439575009 | validation: 0.0017910208512482463]
	TIME [epoch: 8.74 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009005746093755787		[learning rate: 0.00065978]
		[batch 20/20] avg loss: 0.005904636723212212		[learning rate: 0.00065877]
	Learning Rate: 0.000658769
	LOSS [training: 0.0034026056662938954 | validation: 0.003398487160646999]
	TIME [epoch: 8.72 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016285685125155681		[learning rate: 0.00065776]
		[batch 20/20] avg loss: -0.00366410955702879		[learning rate: 0.00065675]
	Learning Rate: 0.00065675
	LOSS [training: -0.002646339034772179 | validation: -0.004867069068859489]
	TIME [epoch: 8.75 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008869932772114623		[learning rate: 0.00065574]
		[batch 20/20] avg loss: -0.00291455559545116		[learning rate: 0.00065474]
	Learning Rate: 0.000654737
	LOSS [training: -0.001900774436331311 | validation: -0.00040481666216603295]
	TIME [epoch: 8.73 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027294784171590916		[learning rate: 0.00065373]
		[batch 20/20] avg loss: -0.00533442117170757		[learning rate: 0.00065273]
	Learning Rate: 0.00065273
	LOSS [training: -0.004031949794433332 | validation: 0.0018119933285457384]
	TIME [epoch: 8.72 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004569319565001129		[learning rate: 0.00065173]
		[batch 20/20] avg loss: -0.007568138659661003		[learning rate: 0.00065073]
	Learning Rate: 0.000650729
	LOSS [training: -0.0060687291123310665 | validation: -0.0020271122385746394]
	TIME [epoch: 8.74 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039632748534178565		[learning rate: 0.00064973]
		[batch 20/20] avg loss: -0.004914358430065646		[learning rate: 0.00064873]
	Learning Rate: 0.000648734
	LOSS [training: -0.0004755417883238946 | validation: 0.0006345614746935954]
	TIME [epoch: 8.71 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035833610783305714		[learning rate: 0.00064774]
		[batch 20/20] avg loss: 0.0033727740042737778		[learning rate: 0.00064675]
	Learning Rate: 0.000646745
	LOSS [training: -0.00010529353702839672 | validation: 0.001522683370743367]
	TIME [epoch: 8.73 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007127619824442995		[learning rate: 0.00064575]
		[batch 20/20] avg loss: 0.0011424788554665493		[learning rate: 0.00064476]
	Learning Rate: 0.000644763
	LOSS [training: -0.002992570484488223 | validation: 0.005438172959032615]
	TIME [epoch: 8.72 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011560190291535302		[learning rate: 0.00064377]
		[batch 20/20] avg loss: 0.0004049639532279074		[learning rate: 0.00064279]
	Learning Rate: 0.000642786
	LOSS [training: -0.00037552753796281135 | validation: 0.00516085537912211]
	TIME [epoch: 8.73 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007651689924551805		[learning rate: 0.0006418]
		[batch 20/20] avg loss: -0.0011397558494552242		[learning rate: 0.00064082]
	Learning Rate: 0.000640816
	LOSS [training: -0.0009524624209552021 | validation: 0.00444995590609065]
	TIME [epoch: 8.73 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002898338395623289		[learning rate: 0.00063983]
		[batch 20/20] avg loss: 0.00025430451132985537		[learning rate: 0.00063885]
	Learning Rate: 0.000638852
	LOSS [training: 0.0015763214534765722 | validation: 0.0009540778303633174]
	TIME [epoch: 8.72 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010433765758235704		[learning rate: 0.00063787]
		[batch 20/20] avg loss: -0.00021486604628492573		[learning rate: 0.00063689]
	Learning Rate: 0.000636893
	LOSS [training: -0.0006291213110542479 | validation: 0.008423508605463734]
	TIME [epoch: 8.75 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007420936191512956		[learning rate: 0.00063592]
		[batch 20/20] avg loss: -0.00322809832297231		[learning rate: 0.00063494]
	Learning Rate: 0.000634941
	LOSS [training: -0.0012430023519105072 | validation: 0.0010527318960504184]
	TIME [epoch: 8.73 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00104644902506928		[learning rate: 0.00063397]
		[batch 20/20] avg loss: -0.001344299263217269		[learning rate: 0.00063299]
	Learning Rate: 0.000632994
	LOSS [training: -0.0001489251190739943 | validation: 0.004671202745748126]
	TIME [epoch: 8.71 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021692992789883975		[learning rate: 0.00063202]
		[batch 20/20] avg loss: -0.004505027769781107		[learning rate: 0.00063105]
	Learning Rate: 0.000631054
	LOSS [training: -0.0033371635243847524 | validation: 0.009654298063019765]
	TIME [epoch: 8.74 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004313981235129758		[learning rate: 0.00063009]
		[batch 20/20] avg loss: 0.0025722495417634587		[learning rate: 0.00062912]
	Learning Rate: 0.00062912
	LOSS [training: 0.0034431153884466083 | validation: 0.0025453361261893754]
	TIME [epoch: 8.75 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004214077007006424		[learning rate: 0.00062815]
		[batch 20/20] avg loss: -0.004293333802500238		[learning rate: 0.00062719]
	Learning Rate: 0.000627191
	LOSS [training: -0.00425370540475333 | validation: 0.004967743671477887]
	TIME [epoch: 8.73 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022080115388204433		[learning rate: 0.00062623]
		[batch 20/20] avg loss: -0.000604647777619575		[learning rate: 0.00062527]
	Learning Rate: 0.000625269
	LOSS [training: -0.0014063296582200089 | validation: 0.0010233990656724934]
	TIME [epoch: 8.71 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002615666296489405		[learning rate: 0.00062431]
		[batch 20/20] avg loss: -0.003487545837715623		[learning rate: 0.00062335]
	Learning Rate: 0.000623352
	LOSS [training: -0.00043593977061310885 | validation: -0.0026636115538404836]
	TIME [epoch: 8.74 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009514063390548598		[learning rate: 0.0006224]
		[batch 20/20] avg loss: 0.0017022110816779113		[learning rate: 0.00062144]
	Learning Rate: 0.000621441
	LOSS [training: 0.005608137236113255 | validation: 0.000537497421431463]
	TIME [epoch: 8.73 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018857912774736779		[learning rate: 0.00062049]
		[batch 20/20] avg loss: -0.000594127607938251		[learning rate: 0.00061954]
	Learning Rate: 0.000619536
	LOSS [training: -0.0012399594427059644 | validation: 0.005496290777885826]
	TIME [epoch: 8.75 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002230671341919096		[learning rate: 0.00061859]
		[batch 20/20] avg loss: -0.0037051100586185685		[learning rate: 0.00061764]
	Learning Rate: 0.000617637
	LOSS [training: -0.0007372193583497363 | validation: 0.009397041587342028]
	TIME [epoch: 8.72 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019947288823624473		[learning rate: 0.00061669]
		[batch 20/20] avg loss: -0.004051035459812807		[learning rate: 0.00061574]
	Learning Rate: 0.000615744
	LOSS [training: -0.0010281532887251796 | validation: 0.0020483597035161967]
	TIME [epoch: 8.74 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001094669054607416		[learning rate: 0.0006148]
		[batch 20/20] avg loss: -0.002696869211650626		[learning rate: 0.00061386]
	Learning Rate: 0.000613856
	LOSS [training: -0.0018957691331290209 | validation: 0.007224388905732303]
	TIME [epoch: 8.73 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019783778662356504		[learning rate: 0.00061291]
		[batch 20/20] avg loss: -0.0019880827052560493		[learning rate: 0.00061197]
	Learning Rate: 0.000611974
	LOSS [training: -0.00198323028574585 | validation: 0.0017250044512992428]
	TIME [epoch: 8.74 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005493078005960039		[learning rate: 0.00061104]
		[batch 20/20] avg loss: 0.0015837693157229426		[learning rate: 0.0006101]
	Learning Rate: 0.000610099
	LOSS [training: -0.001954654345118549 | validation: 0.00661886359529073]
	TIME [epoch: 8.74 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002078923616034714		[learning rate: 0.00060916]
		[batch 20/20] avg loss: -0.0017042757323757467		[learning rate: 0.00060823]
	Learning Rate: 0.000608228
	LOSS [training: -0.0018915996742052306 | validation: 0.004926326325568545]
	TIME [epoch: 8.73 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030436491333894014		[learning rate: 0.0006073]
		[batch 20/20] avg loss: -0.0041690283789031		[learning rate: 0.00060636]
	Learning Rate: 0.000606364
	LOSS [training: -0.0005626896227568491 | validation: -0.004339991660393811]
	TIME [epoch: 8.73 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: -1.6005547153170526e-05		[learning rate: 0.00060543]
		[batch 20/20] avg loss: -0.0021529000250090436		[learning rate: 0.00060451]
	Learning Rate: 0.000604505
	LOSS [training: -0.0010844527860811072 | validation: 0.006885716973588807]
	TIME [epoch: 8.73 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002511770612163984		[learning rate: 0.00060358]
		[batch 20/20] avg loss: -0.002489987184763781		[learning rate: 0.00060265]
	Learning Rate: 0.000602652
	LOSS [training: 1.0891713700101301e-05 | validation: -1.2090028519388864e-05]
	TIME [epoch: 8.76 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017552668548739096		[learning rate: 0.00060173]
		[batch 20/20] avg loss: -0.008727825402094276		[learning rate: 0.0006008]
	Learning Rate: 0.000600805
	LOSS [training: -0.0034862792736101837 | validation: 0.0029170098336889737]
	TIME [epoch: 8.73 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0046575317628748305		[learning rate: 0.00059988]
		[batch 20/20] avg loss: 0.000870675301657869		[learning rate: 0.00059896]
	Learning Rate: 0.000598963
	LOSS [training: -0.0018934282306084804 | validation: -0.004532226590912497]
	TIME [epoch: 8.73 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033893142186608655		[learning rate: 0.00059804]
		[batch 20/20] avg loss: -0.0020623321250548823		[learning rate: 0.00059713]
	Learning Rate: 0.000597127
	LOSS [training: -0.002725823171857874 | validation: -0.0012494168811183544]
	TIME [epoch: 8.74 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009791360954840085		[learning rate: 0.00059621]
		[batch 20/20] avg loss: -0.0020896582407003567		[learning rate: 0.0005953]
	Learning Rate: 0.000595296
	LOSS [training: -0.0015343971680921828 | validation: 0.0013234102794408484]
	TIME [epoch: 8.74 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00016771311977215785		[learning rate: 0.00059438]
		[batch 20/20] avg loss: -0.003657012369225992		[learning rate: 0.00059347]
	Learning Rate: 0.000593472
	LOSS [training: -0.001912362744499075 | validation: -0.0015034214364602705]
	TIME [epoch: 8.76 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00309193024212752		[learning rate: 0.00059256]
		[batch 20/20] avg loss: -0.0034798509820820273		[learning rate: 0.00059165]
	Learning Rate: 0.000591652
	LOSS [training: -0.00019396036997725374 | validation: 0.008724342081537693]
	TIME [epoch: 8.73 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005253094107131991		[learning rate: 0.00059074]
		[batch 20/20] avg loss: 0.00428350252178411		[learning rate: 0.00058984]
	Learning Rate: 0.000589839
	LOSS [training: -0.0004847957926739402 | validation: -0.00025218671929823073]
	TIME [epoch: 8.73 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013102767762562296		[learning rate: 0.00058893]
		[batch 20/20] avg loss: 0.003815251573328875		[learning rate: 0.00058803]
	Learning Rate: 0.000588031
	LOSS [training: 0.0012524873985363227 | validation: 0.004059818403367155]
	TIME [epoch: 8.71 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003057537222546044		[learning rate: 0.00058713]
		[batch 20/20] avg loss: 0.0016716030891371914		[learning rate: 0.00058623]
	Learning Rate: 0.000586228
	LOSS [training: 0.0006829246834412935 | validation: 0.004352593618514867]
	TIME [epoch: 8.71 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006961926915630705		[learning rate: 0.00058533]
		[batch 20/20] avg loss: 0.001095667506873268		[learning rate: 0.00058443]
	Learning Rate: 0.000584431
	LOSS [training: 0.00019973740765509866 | validation: 0.0017919364466961508]
	TIME [epoch: 8.74 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002639639880302777		[learning rate: 0.00058353]
		[batch 20/20] avg loss: -0.005206434397956476		[learning rate: 0.00058264]
	Learning Rate: 0.000582639
	LOSS [training: -0.003923037139129627 | validation: 0.0036860433843513146]
	TIME [epoch: 8.71 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018357710675716461		[learning rate: 0.00058175]
		[batch 20/20] avg loss: -0.0007300631402880263		[learning rate: 0.00058085]
	Learning Rate: 0.000580854
	LOSS [training: 0.0005528539636418102 | validation: 0.0030842538275881837]
	TIME [epoch: 8.74 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020742309292333242		[learning rate: 0.00057996]
		[batch 20/20] avg loss: -0.010239465018213212		[learning rate: 0.00057907]
	Learning Rate: 0.000579073
	LOSS [training: -0.004082617044489945 | validation: 0.005202411876804911]
	TIME [epoch: 8.73 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002237297903224429		[learning rate: 0.00057818]
		[batch 20/20] avg loss: -0.006489091844482214		[learning rate: 0.0005773]
	Learning Rate: 0.000577298
	LOSS [training: -0.004363194873853322 | validation: 0.0021766247183356165]
	TIME [epoch: 8.73 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012586082165331664		[learning rate: 0.00057641]
		[batch 20/20] avg loss: 0.0013796454150585744		[learning rate: 0.00057553]
	Learning Rate: 0.000575528
	LOSS [training: 6.0518599262704074e-05 | validation: 0.007335155618945412]
	TIME [epoch: 8.73 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015419291637406684		[learning rate: 0.00057465]
		[batch 20/20] avg loss: -0.0012482888429855583		[learning rate: 0.00057376]
	Learning Rate: 0.000573764
	LOSS [training: -0.0013951090033631135 | validation: -0.0022995305199817705]
	TIME [epoch: 8.73 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020252687295960843		[learning rate: 0.00057288]
		[batch 20/20] avg loss: 0.004832613067318749		[learning rate: 0.00057201]
	Learning Rate: 0.000572005
	LOSS [training: 0.003428940898457418 | validation: 0.005622455069989367]
	TIME [epoch: 8.72 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002497758170210466		[learning rate: 0.00057113]
		[batch 20/20] avg loss: -0.005668699721471042		[learning rate: 0.00057025]
	Learning Rate: 0.000570252
	LOSS [training: -0.0015854707756302881 | validation: 0.0007660497607282196]
	TIME [epoch: 8.73 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00028902750183143494		[learning rate: 0.00056938]
		[batch 20/20] avg loss: -0.005636961140795245		[learning rate: 0.0005685]
	Learning Rate: 0.000568504
	LOSS [training: -0.0029629943213133397 | validation: 0.004665771012020968]
	TIME [epoch: 8.73 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017415256118875112		[learning rate: 0.00056763]
		[batch 20/20] avg loss: 0.0006173281656098402		[learning rate: 0.00056676]
	Learning Rate: 0.000566761
	LOSS [training: -0.0005620987231388356 | validation: -0.0033891004278523156]
	TIME [epoch: 8.77 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008184047921911588		[learning rate: 0.00056589]
		[batch 20/20] avg loss: -0.001214688376907911		[learning rate: 0.00056502]
	Learning Rate: 0.000565024
	LOSS [training: -0.001016546584549535 | validation: -0.0004554449433252435]
	TIME [epoch: 8.74 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003311617626324096		[learning rate: 0.00056416]
		[batch 20/20] avg loss: 0.003538030885959402		[learning rate: 0.00056329]
	Learning Rate: 0.000563292
	LOSS [training: 0.00011320662981765257 | validation: 0.004404219175322662]
	TIME [epoch: 8.76 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022619482162950103		[learning rate: 0.00056243]
		[batch 20/20] avg loss: 0.006847458527291681		[learning rate: 0.00056156]
	Learning Rate: 0.000561565
	LOSS [training: 0.0022927551554983347 | validation: -0.00036803511593825754]
	TIME [epoch: 8.74 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008114612629828453		[learning rate: 0.0005607]
		[batch 20/20] avg loss: 0.002782413830622471		[learning rate: 0.00055984]
	Learning Rate: 0.000559844
	LOSS [training: 0.0009854762838198128 | validation: 0.005742364855172906]
	TIME [epoch: 8.73 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018081455339887195		[learning rate: 0.00055898]
		[batch 20/20] avg loss: -0.0009436467878573644		[learning rate: 0.00055813]
	Learning Rate: 0.000558127
	LOSS [training: 0.0004322493730656774 | validation: -0.004534190045048165]
	TIME [epoch: 8.74 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00318535455923744		[learning rate: 0.00055727]
		[batch 20/20] avg loss: -0.0007765397687503579		[learning rate: 0.00055642]
	Learning Rate: 0.000556416
	LOSS [training: -0.001980947163993899 | validation: 0.008456827683425606]
	TIME [epoch: 8.75 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: -9.608882940681457e-05		[learning rate: 0.00055556]
		[batch 20/20] avg loss: 0.0051917250261056475		[learning rate: 0.00055471]
	Learning Rate: 0.000554711
	LOSS [training: 0.002547818098349416 | validation: -0.0005236270019971117]
	TIME [epoch: 8.74 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035157178115545933		[learning rate: 0.00055386]
		[batch 20/20] avg loss: 0.004476661269081764		[learning rate: 0.00055301]
	Learning Rate: 0.00055301
	LOSS [training: 0.0004804717287635853 | validation: 0.00423973150768533]
	TIME [epoch: 8.75 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00985888850440638		[learning rate: 0.00055216]
		[batch 20/20] avg loss: -0.00043905345830840963		[learning rate: 0.00055132]
	Learning Rate: 0.000551315
	LOSS [training: 0.004709917523048985 | validation: -0.0012946413700696607]
	TIME [epoch: 8.75 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004080389188255805		[learning rate: 0.00055047]
		[batch 20/20] avg loss: 0.0031256526259013386		[learning rate: 0.00054963]
	Learning Rate: 0.000549625
	LOSS [training: 0.0013588068535378792 | validation: 0.018139977796073604]
	TIME [epoch: 8.73 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006393489208980227		[learning rate: 0.00054878]
		[batch 20/20] avg loss: 0.00018568856574449326		[learning rate: 0.00054794]
	Learning Rate: 0.00054794
	LOSS [training: 0.00328958888736236 | validation: 0.0029112084809722366]
	TIME [epoch: 8.75 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002293001428433182		[learning rate: 0.0005471]
		[batch 20/20] avg loss: 0.004277959921594043		[learning rate: 0.00054626]
	Learning Rate: 0.000546261
	LOSS [training: 0.0032854806750136126 | validation: 0.003065774911675227]
	TIME [epoch: 8.73 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005106449583773056		[learning rate: 0.00054542]
		[batch 20/20] avg loss: 0.001896849559038722		[learning rate: 0.00054459]
	Learning Rate: 0.000544586
	LOSS [training: -0.001604800012367167 | validation: 0.005319866176806489]
	TIME [epoch: 8.75 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: -7.605967139912773e-05		[learning rate: 0.00054375]
		[batch 20/20] avg loss: 0.003030485895043981		[learning rate: 0.00054292]
	Learning Rate: 0.000542917
	LOSS [training: 0.0014772131118224265 | validation: 0.00578411543021033]
	TIME [epoch: 8.76 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018770458847718508		[learning rate: 0.00054208]
		[batch 20/20] avg loss: -0.0013829519388462		[learning rate: 0.00054125]
	Learning Rate: 0.000541253
	LOSS [training: 0.0002470469729628252 | validation: -0.00041653328549810754]
	TIME [epoch: 8.77 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0071817749677812685		[learning rate: 0.00054042]
		[batch 20/20] avg loss: 0.002935772655801152		[learning rate: 0.00053959]
	Learning Rate: 0.000539593
	LOSS [training: -0.0021230011559900583 | validation: -0.00025798117875334997]
	TIME [epoch: 8.76 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035844093494816634		[learning rate: 0.00053877]
		[batch 20/20] avg loss: 0.00046756542566428325		[learning rate: 0.00053794]
	Learning Rate: 0.000537939
	LOSS [training: -0.00155842196190869 | validation: -0.0012352736118422954]
	TIME [epoch: 8.75 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003043670867543748		[learning rate: 0.00053711]
		[batch 20/20] avg loss: -0.001399369840829244		[learning rate: 0.00053629]
	Learning Rate: 0.00053629
	LOSS [training: -0.0022215203541864964 | validation: -0.00038308809676108023]
	TIME [epoch: 8.75 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001068534495306435		[learning rate: 0.00053547]
		[batch 20/20] avg loss: -0.0030900139610366534		[learning rate: 0.00053465]
	Learning Rate: 0.000534646
	LOSS [training: -0.002079274228171544 | validation: 0.0017852552035805306]
	TIME [epoch: 8.77 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012165573598287177		[learning rate: 0.00053383]
		[batch 20/20] avg loss: -0.0047443219708505424		[learning rate: 0.00053301]
	Learning Rate: 0.000533007
	LOSS [training: -0.0017638823055109124 | validation: 0.009529740739714278]
	TIME [epoch: 8.77 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007501683931081404		[learning rate: 0.00053219]
		[batch 20/20] avg loss: -0.0011283396254796905		[learning rate: 0.00053137]
	Learning Rate: 0.000531374
	LOSS [training: -0.004315011778280547 | validation: -0.005309450769574927]
	TIME [epoch: 8.75 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003072777079446227		[learning rate: 0.00053056]
		[batch 20/20] avg loss: -0.0010689077268541771		[learning rate: 0.00052974]
	Learning Rate: 0.000529745
	LOSS [training: -0.0020708424031502027 | validation: 0.005132019300568737]
	TIME [epoch: 8.76 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0051286787056145176		[learning rate: 0.00052893]
		[batch 20/20] avg loss: 0.00288487128722418		[learning rate: 0.00052812]
	Learning Rate: 0.000528121
	LOSS [training: -0.001121903709195169 | validation: 0.007008055830583861]
	TIME [epoch: 8.77 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00684148044066898		[learning rate: 0.00052731]
		[batch 20/20] avg loss: 0.003438514363360646		[learning rate: 0.0005265]
	Learning Rate: 0.000526502
	LOSS [training: 0.005139997402014812 | validation: -0.0016680017509625835]
	TIME [epoch: 8.76 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007984603928176614		[learning rate: 0.00052569]
		[batch 20/20] avg loss: -8.198509184193878e-05		[learning rate: 0.00052489]
	Learning Rate: 0.000524888
	LOSS [training: -0.004033294510009276 | validation: 0.0013013368140507155]
	TIME [epoch: 8.77 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00028008636903948433		[learning rate: 0.00052408]
		[batch 20/20] avg loss: -0.002107504335539423		[learning rate: 0.00052328]
	Learning Rate: 0.000523279
	LOSS [training: -0.0009137089832499691 | validation: -0.004000092398993583]
	TIME [epoch: 8.74 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00510026574823472		[learning rate: 0.00052248]
		[batch 20/20] avg loss: -0.0005680650968616762		[learning rate: 0.00052167]
	Learning Rate: 0.000521675
	LOSS [training: -0.0028341654225481983 | validation: -0.005119784405503914]
	TIME [epoch: 8.77 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014406436226864115		[learning rate: 0.00052087]
		[batch 20/20] avg loss: 0.004318232318373043		[learning rate: 0.00052008]
	Learning Rate: 0.000520076
	LOSS [training: 0.001438794347843316 | validation: -0.005148901032431978]
	TIME [epoch: 8.79 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008086387832556689		[learning rate: 0.00051928]
		[batch 20/20] avg loss: -0.0007986632885057275		[learning rate: 0.00051848]
	Learning Rate: 0.000518482
	LOSS [training: 4.987747374970465e-06 | validation: -0.0010552699618826002]
	TIME [epoch: 8.77 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002945010613050094		[learning rate: 0.00051769]
		[batch 20/20] avg loss: -0.0002165949793933763		[learning rate: 0.00051689]
	Learning Rate: 0.000516892
	LOSS [training: -0.0015808027962217349 | validation: 0.005825683223233133]
	TIME [epoch: 8.74 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004351679770257411		[learning rate: 0.0005161]
		[batch 20/20] avg loss: -0.002874485946506856		[learning rate: 0.00051531]
	Learning Rate: 0.000515308
	LOSS [training: 0.0007385969118752771 | validation: 0.0010412588831078984]
	TIME [epoch: 8.71 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002516012438425297		[learning rate: 0.00051452]
		[batch 20/20] avg loss: -0.0018043175780965224		[learning rate: 0.00051373]
	Learning Rate: 0.000513728
	LOSS [training: -0.0021601650082609097 | validation: 0.0008238111086810483]
	TIME [epoch: 8.74 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019622346687848706		[learning rate: 0.00051294]
		[batch 20/20] avg loss: 0.0028457275308155067		[learning rate: 0.00051215]
	Learning Rate: 0.000512153
	LOSS [training: 0.000441746431015318 | validation: -0.003954029422925094]
	TIME [epoch: 8.76 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003967882650126863		[learning rate: 0.00051137]
		[batch 20/20] avg loss: -0.003642845234106419		[learning rate: 0.00051058]
	Learning Rate: 0.000510583
	LOSS [training: -0.0038053639421166408 | validation: -0.001854779502344388]
	TIME [epoch: 8.73 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006708603729603413		[learning rate: 0.0005098]
		[batch 20/20] avg loss: -0.000644131777298309		[learning rate: 0.00050902]
	Learning Rate: 0.000509018
	LOSS [training: -0.0036763677534508614 | validation: -0.0017240943510653552]
	TIME [epoch: 8.78 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002165263824161612		[learning rate: 0.00050824]
		[batch 20/20] avg loss: -0.0025264175126696385		[learning rate: 0.00050746]
	Learning Rate: 0.000507458
	LOSS [training: -0.00018057684425401327 | validation: 0.0050041064309587074]
	TIME [epoch: 8.75 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0043138595008535485		[learning rate: 0.00050668]
		[batch 20/20] avg loss: -0.0018412109296173824		[learning rate: 0.0005059]
	Learning Rate: 0.000505902
	LOSS [training: -0.0030775352152354655 | validation: 0.0034363304114194268]
	TIME [epoch: 8.74 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002090521940995442		[learning rate: 0.00050513]
		[batch 20/20] avg loss: 0.00015293928918583415		[learning rate: 0.00050435]
	Learning Rate: 0.000504351
	LOSS [training: -0.0009687913259048039 | validation: 0.004518346156202684]
	TIME [epoch: 8.76 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001806977935382395		[learning rate: 0.00050358]
		[batch 20/20] avg loss: -0.0048671891366078865		[learning rate: 0.00050281]
	Learning Rate: 0.000502806
	LOSS [training: -0.0033370835359951404 | validation: -0.0007684492540195721]
	TIME [epoch: 8.75 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013020325702240041		[learning rate: 0.00050203]
		[batch 20/20] avg loss: -0.004119979021694803		[learning rate: 0.00050126]
	Learning Rate: 0.000501264
	LOSS [training: -0.0027110057959594037 | validation: 0.001001792250558366]
	TIME [epoch: 8.73 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001050037252912415		[learning rate: 0.0005005]
		[batch 20/20] avg loss: -0.0014655754707304982		[learning rate: 0.00049973]
	Learning Rate: 0.000499728
	LOSS [training: -0.0012578063618214567 | validation: -0.0018723577262741778]
	TIME [epoch: 8.75 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018090685255615774		[learning rate: 0.00049896]
		[batch 20/20] avg loss: 0.0003784253131090154		[learning rate: 0.0004982]
	Learning Rate: 0.000498196
	LOSS [training: -0.0007153216062262809 | validation: 0.002332647676764758]
	TIME [epoch: 8.78 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008667057176386689		[learning rate: 0.00049743]
		[batch 20/20] avg loss: 0.0015058273573238714		[learning rate: 0.00049667]
	Learning Rate: 0.000496668
	LOSS [training: 0.00118626653748127 | validation: 0.004488130736728945]
	TIME [epoch: 8.78 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002780372603663173		[learning rate: 0.00049591]
		[batch 20/20] avg loss: -0.0031099046409623207		[learning rate: 0.00049515]
	Learning Rate: 0.000495146
	LOSS [training: -0.002945138622312747 | validation: -0.0005705400576905468]
	TIME [epoch: 8.75 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008386084469064268		[learning rate: 0.00049439]
		[batch 20/20] avg loss: -0.007863923949496147		[learning rate: 0.00049363]
	Learning Rate: 0.000493628
	LOSS [training: -0.0035126577512948594 | validation: 0.0014269323204556634]
	TIME [epoch: 8.76 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0073621174843299785		[learning rate: 0.00049287]
		[batch 20/20] avg loss: 0.0019007376552200735		[learning rate: 0.00049211]
	Learning Rate: 0.000492115
	LOSS [training: -0.002730689914554953 | validation: 0.0010887698164427796]
	TIME [epoch: 8.73 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000791599051097343		[learning rate: 0.00049136]
		[batch 20/20] avg loss: -0.002322040400742826		[learning rate: 0.00049061]
	Learning Rate: 0.000490606
	LOSS [training: -0.0015568197259200843 | validation: 0.00026836489827801176]
	TIME [epoch: 8.75 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005603372421933304		[learning rate: 0.00048985]
		[batch 20/20] avg loss: -0.0031352066929049684		[learning rate: 0.0004891]
	Learning Rate: 0.000489103
	LOSS [training: -0.0018477719675491493 | validation: 0.0004846743478697591]
	TIME [epoch: 8.74 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005329258464474981		[learning rate: 0.00048835]
		[batch 20/20] avg loss: -0.004277820915725268		[learning rate: 0.0004876]
	Learning Rate: 0.000487603
	LOSS [training: -0.0024053733810863826 | validation: 0.002376232593678612]
	TIME [epoch: 8.76 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0056719027526768795		[learning rate: 0.00048686]
		[batch 20/20] avg loss: 0.0007992800932222054		[learning rate: 0.00048611]
	Learning Rate: 0.000486109
	LOSS [training: -0.002436311329727337 | validation: 0.00042149313031265064]
	TIME [epoch: 8.77 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008128424499449924		[learning rate: 0.00048536]
		[batch 20/20] avg loss: -0.0027958121599396036		[learning rate: 0.00048462]
	Learning Rate: 0.000484619
	LOSS [training: -0.0009914848549973058 | validation: -0.003849478239967608]
	TIME [epoch: 8.75 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003572979063390764		[learning rate: 0.00048388]
		[batch 20/20] avg loss: -0.0025061213412781148		[learning rate: 0.00048313]
	Learning Rate: 0.000483133
	LOSS [training: -0.00303955020233444 | validation: 0.0006193764304141514]
	TIME [epoch: 8.78 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003508310636132239		[learning rate: 0.00048239]
		[batch 20/20] avg loss: -0.004893387898216923		[learning rate: 0.00048165]
	Learning Rate: 0.000481652
	LOSS [training: -0.0042008492671745815 | validation: 0.003908255784787509]
	TIME [epoch: 8.81 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0072455929468969536		[learning rate: 0.00048091]
		[batch 20/20] avg loss: -0.0027033010651480935		[learning rate: 0.00048018]
	Learning Rate: 0.000480175
	LOSS [training: -0.004974447006022524 | validation: 0.0032943089403292343]
	TIME [epoch: 8.74 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016324375139508524		[learning rate: 0.00047944]
		[batch 20/20] avg loss: -0.0018609328088002846		[learning rate: 0.0004787]
	Learning Rate: 0.000478704
	LOSS [training: -0.0017466851613755683 | validation: -0.004672003245568658]
	TIME [epoch: 8.73 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021464545878503907		[learning rate: 0.00047797]
		[batch 20/20] avg loss: 0.0023580319232332217		[learning rate: 0.00047724]
	Learning Rate: 0.000477236
	LOSS [training: 0.00010578866769141583 | validation: -0.00076444725886457]
	TIME [epoch: 8.76 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005750772066280954		[learning rate: 0.0004765]
		[batch 20/20] avg loss: -0.0050547822114181944		[learning rate: 0.00047577]
	Learning Rate: 0.000475773
	LOSS [training: -0.002814929709023144 | validation: 0.00252490054674847]
	TIME [epoch: 8.78 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021013204785477386		[learning rate: 0.00047504]
		[batch 20/20] avg loss: -0.006185896160646966		[learning rate: 0.00047431]
	Learning Rate: 0.000474315
	LOSS [training: -0.004143608319597352 | validation: -0.002865611911840633]
	TIME [epoch: 8.71 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010404616746728418		[learning rate: 0.00047359]
		[batch 20/20] avg loss: -0.004517391866990592		[learning rate: 0.00047286]
	Learning Rate: 0.000472861
	LOSS [training: -0.002778926770831717 | validation: -0.004263859626535221]
	TIME [epoch: 8.73 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00031550704734094055		[learning rate: 0.00047214]
		[batch 20/20] avg loss: 0.0005300074679000696		[learning rate: 0.00047141]
	Learning Rate: 0.000471411
	LOSS [training: 0.00010725021027956456 | validation: 0.0017052234078149587]
	TIME [epoch: 8.74 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011724931287923261		[learning rate: 0.00047069]
		[batch 20/20] avg loss: -0.003998963396992805		[learning rate: 0.00046997]
	Learning Rate: 0.000469966
	LOSS [training: -0.0014132351341002395 | validation: -0.0024344131675736276]
	TIME [epoch: 8.74 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002599613105322654		[learning rate: 0.00046925]
		[batch 20/20] avg loss: -0.003484523670211552		[learning rate: 0.00046853]
	Learning Rate: 0.000468526
	LOSS [training: -0.003042068387767104 | validation: 0.002501369668780844]
	TIME [epoch: 8.78 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005992144317950929		[learning rate: 0.00046781]
		[batch 20/20] avg loss: 0.0020658778242314776		[learning rate: 0.00046709]
	Learning Rate: 0.000467089
	LOSS [training: -0.001963133246859726 | validation: -0.0010313257284367727]
	TIME [epoch: 8.73 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002794066104484018		[learning rate: 0.00046637]
		[batch 20/20] avg loss: -0.004784984583443181		[learning rate: 0.00046566]
	Learning Rate: 0.000465658
	LOSS [training: -0.0037895253439636 | validation: 0.002916152891920044]
	TIME [epoch: 8.71 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003560789163559671		[learning rate: 0.00046494]
		[batch 20/20] avg loss: -0.005500112719479999		[learning rate: 0.00046423]
	Learning Rate: 0.00046423
	LOSS [training: -0.004530450941519834 | validation: 0.006188150230103314]
	TIME [epoch: 8.72 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005087916110197423		[learning rate: 0.00046352]
		[batch 20/20] avg loss: 0.0032348382074437534		[learning rate: 0.00046281]
	Learning Rate: 0.000462807
	LOSS [training: 0.004161377158820589 | validation: 8.890125958472953e-05]
	TIME [epoch: 8.76 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003738532140240694		[learning rate: 0.0004621]
		[batch 20/20] avg loss: 0.002520866863721907		[learning rate: 0.00046139]
	Learning Rate: 0.000461388
	LOSS [training: -0.000608832638259393 | validation: 0.005142712582412548]
	TIME [epoch: 8.74 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035054503323478287		[learning rate: 0.00046068]
		[batch 20/20] avg loss: 0.007575594171528403		[learning rate: 0.00045997]
	Learning Rate: 0.000459974
	LOSS [training: 0.005540522251938116 | validation: 0.011640958662156156]
	TIME [epoch: 8.77 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031588945378573305		[learning rate: 0.00045927]
		[batch 20/20] avg loss: 0.00033957774711796285		[learning rate: 0.00045856]
	Learning Rate: 0.000458564
	LOSS [training: 0.0017492361424876466 | validation: 0.003752037739335124]
	TIME [epoch: 8.75 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004739504901709201		[learning rate: 0.00045786]
		[batch 20/20] avg loss: 0.002024237101044028		[learning rate: 0.00045716]
	Learning Rate: 0.000457158
	LOSS [training: 0.0033818710013766144 | validation: 0.006609037443875216]
	TIME [epoch: 8.73 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004143924246065392		[learning rate: 0.00045646]
		[batch 20/20] avg loss: -0.005032687999174182		[learning rate: 0.00045576]
	Learning Rate: 0.000455757
	LOSS [training: -0.0004443818765543948 | validation: 0.0060964198174340255]
	TIME [epoch: 8.77 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020511041793756504		[learning rate: 0.00045506]
		[batch 20/20] avg loss: -0.0013209489634255379		[learning rate: 0.00045436]
	Learning Rate: 0.00045436
	LOSS [training: -0.0016860265714005943 | validation: 0.005722139444548479]
	TIME [epoch: 8.72 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022698381486517495		[learning rate: 0.00045366]
		[batch 20/20] avg loss: 0.005012204932413606		[learning rate: 0.00045297]
	Learning Rate: 0.000452967
	LOSS [training: 0.0036410215405326776 | validation: -0.0007764294026197018]
	TIME [epoch: 8.71 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011117468805341916		[learning rate: 0.00045227]
		[batch 20/20] avg loss: -0.0009132452422490354		[learning rate: 0.00045158]
	Learning Rate: 0.000451579
	LOSS [training: -0.0010124960613916132 | validation: -0.001481181024162157]
	TIME [epoch: 8.71 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003322332314241537		[learning rate: 0.00045089]
		[batch 20/20] avg loss: -0.004781841121925423		[learning rate: 0.00045019]
	Learning Rate: 0.000450194
	LOSS [training: -0.00405208671808348 | validation: 0.002290923325344048]
	TIME [epoch: 8.75 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029385896265185264		[learning rate: 0.0004495]
		[batch 20/20] avg loss: -0.005393957649252148		[learning rate: 0.00044881]
	Learning Rate: 0.000448814
	LOSS [training: -0.001227684011366811 | validation: 0.0011963922142961328]
	TIME [epoch: 8.77 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002720015634853287		[learning rate: 0.00044813]
		[batch 20/20] avg loss: -8.223006795988839e-05		[learning rate: 0.00044744]
	Learning Rate: 0.000447439
	LOSS [training: -0.0014011228514065877 | validation: 0.004408597683923149]
	TIME [epoch: 8.78 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007561198391654231		[learning rate: 0.00044675]
		[batch 20/20] avg loss: -0.005308954878345701		[learning rate: 0.00044607]
	Learning Rate: 0.000446067
	LOSS [training: -0.006435076634999967 | validation: 0.005235247604110828]
	TIME [epoch: 8.76 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033775634498145178		[learning rate: 0.00044538]
		[batch 20/20] avg loss: -1.1484051849753163e-05		[learning rate: 0.0004447]
	Learning Rate: 0.0004447
	LOSS [training: -0.0016945237508321353 | validation: 0.007351707452611311]
	TIME [epoch: 8.76 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001422333180120306		[learning rate: 0.00044402]
		[batch 20/20] avg loss: -0.0006315969856737244		[learning rate: 0.00044334]
	Learning Rate: 0.000443336
	LOSS [training: 0.0003953680972232909 | validation: 0.00035536476743432263]
	TIME [epoch: 8.73 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00022917834167874651		[learning rate: 0.00044266]
		[batch 20/20] avg loss: -0.0022061191028796756		[learning rate: 0.00044198]
	Learning Rate: 0.000441977
	LOSS [training: -0.0009884703806004643 | validation: 0.004175492852815342]
	TIME [epoch: 8.77 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004263682321190769		[learning rate: 0.0004413]
		[batch 20/20] avg loss: -0.0021133647184945186		[learning rate: 0.00044062]
	Learning Rate: 0.000440622
	LOSS [training: -0.003188523519842644 | validation: 0.003563311486950031]
	TIME [epoch: 8.78 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016978933987595723		[learning rate: 0.00043995]
		[batch 20/20] avg loss: -0.006323734832453883		[learning rate: 0.00043927]
	Learning Rate: 0.000439272
	LOSS [training: -0.0023129207168471554 | validation: -0.0005684457031190152]
	TIME [epoch: 8.72 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011192356606729713		[learning rate: 0.0004386]
		[batch 20/20] avg loss: -0.004824126190747323		[learning rate: 0.00043793]
	Learning Rate: 0.000437925
	LOSS [training: -0.0018524452650371757 | validation: -0.0014867081723554361]
	TIME [epoch: 8.72 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006062754836776094		[learning rate: 0.00043725]
		[batch 20/20] avg loss: -0.0015433251391118647		[learning rate: 0.00043658]
	Learning Rate: 0.000436583
	LOSS [training: -0.003803039987943979 | validation: -0.0035379341517821927]
	TIME [epoch: 8.73 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004136103300264047		[learning rate: 0.00043591]
		[batch 20/20] avg loss: 0.0021814244038624005		[learning rate: 0.00043524]
	Learning Rate: 0.000435244
	LOSS [training: -0.0009773394482008226 | validation: -0.004473999218436636]
	TIME [epoch: 8.76 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003955751214735607		[learning rate: 0.00043458]
		[batch 20/20] avg loss: 0.001972294596517555		[learning rate: 0.00043391]
	Learning Rate: 0.00043391
	LOSS [training: -0.0009917283091090258 | validation: -0.0017481939775467583]
	TIME [epoch: 8.72 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006171373896483806		[learning rate: 0.00043324]
		[batch 20/20] avg loss: -0.0025182400285707262		[learning rate: 0.00043258]
	Learning Rate: 0.00043258
	LOSS [training: -0.0015676887091095534 | validation: -0.0025515959906358557]
	TIME [epoch: 8.73 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007059607413448151		[learning rate: 0.00043192]
		[batch 20/20] avg loss: -0.00146836033860148		[learning rate: 0.00043125]
	Learning Rate: 0.000431254
	LOSS [training: -0.004263983876024816 | validation: -0.00303785478582592]
	TIME [epoch: 8.72 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002473277197123391		[learning rate: 0.00043059]
		[batch 20/20] avg loss: -0.006815089256656944		[learning rate: 0.00042993]
	Learning Rate: 0.000429932
	LOSS [training: -0.004644183226890168 | validation: 0.0021149816489971426]
	TIME [epoch: 8.73 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003102188042251186		[learning rate: 0.00042927]
		[batch 20/20] avg loss: -0.002437186375355841		[learning rate: 0.00042861]
	Learning Rate: 0.000428614
	LOSS [training: -0.002769687208803513 | validation: -1.9634310145074697e-05]
	TIME [epoch: 8.75 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002870908645167224		[learning rate: 0.00042796]
		[batch 20/20] avg loss: -0.0008388008063853078		[learning rate: 0.0004273]
	Learning Rate: 0.0004273
	LOSS [training: -0.0018548547257762657 | validation: -0.004764831545697965]
	TIME [epoch: 8.71 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017935087421159005		[learning rate: 0.00042664]
		[batch 20/20] avg loss: -0.0031632229886115566		[learning rate: 0.00042599]
	Learning Rate: 0.000425991
	LOSS [training: -0.0024783658653637283 | validation: 0.0044894545951095904]
	TIME [epoch: 8.73 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0057424339181305385		[learning rate: 0.00042534]
		[batch 20/20] avg loss: 0.0008279686440288353		[learning rate: 0.00042468]
	Learning Rate: 0.000424685
	LOSS [training: -0.0024572326370508523 | validation: 0.005493436477440845]
	TIME [epoch: 8.73 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008046653489406242		[learning rate: 0.00042403]
		[batch 20/20] avg loss: 0.0014917495644048598		[learning rate: 0.00042338]
	Learning Rate: 0.000423383
	LOSS [training: 0.001148207456672742 | validation: 0.0009165767211300252]
	TIME [epoch: 8.76 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004478853032786942		[learning rate: 0.00042273]
		[batch 20/20] avg loss: -0.005815312262735661		[learning rate: 0.00042208]
	Learning Rate: 0.000422085
	LOSS [training: -0.005147082647761303 | validation: -0.005701265285721845]
	TIME [epoch: 8.74 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036981876096971573		[learning rate: 0.00042144]
		[batch 20/20] avg loss: -0.0039176100439128		[learning rate: 0.00042079]
	Learning Rate: 0.000420791
	LOSS [training: -0.0038078988268049783 | validation: -0.0014086220493185358]
	TIME [epoch: 8.73 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00559091866130195		[learning rate: 0.00042015]
		[batch 20/20] avg loss: -0.0011271032077707684		[learning rate: 0.0004195]
	Learning Rate: 0.000419501
	LOSS [training: -0.0033590109345363593 | validation: 0.0032398642081516273]
	TIME [epoch: 8.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002551794795385317		[learning rate: 0.00041886]
		[batch 20/20] avg loss: 0.0014743195961832267		[learning rate: 0.00041822]
	Learning Rate: 0.000418215
	LOSS [training: -0.0005387375996010453 | validation: 0.005433437437473605]
	TIME [epoch: 8.74 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00124883915242964		[learning rate: 0.00041757]
		[batch 20/20] avg loss: 0.0003953212230617028		[learning rate: 0.00041693]
	Learning Rate: 0.000416933
	LOSS [training: 0.0008220801877456711 | validation: 0.0024443222866816043]
	TIME [epoch: 8.76 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002169042415214463		[learning rate: 0.00041629]
		[batch 20/20] avg loss: -0.003824587496724151		[learning rate: 0.00041566]
	Learning Rate: 0.000415655
	LOSS [training: -0.0018038416276013522 | validation: 0.0018277772215627864]
	TIME [epoch: 8.74 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002904229977199038		[learning rate: 0.00041502]
		[batch 20/20] avg loss: -0.0009185338483269284		[learning rate: 0.00041438]
	Learning Rate: 0.000414381
	LOSS [training: -0.0019113819127629834 | validation: 0.0015795561703520955]
	TIME [epoch: 8.71 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024109307619021134		[learning rate: 0.00041375]
		[batch 20/20] avg loss: -0.00477732514403013		[learning rate: 0.00041311]
	Learning Rate: 0.000413111
	LOSS [training: -0.0035941279529661217 | validation: 0.00477645720777967]
	TIME [epoch: 8.72 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014213397900222		[learning rate: 0.00041248]
		[batch 20/20] avg loss: -0.0017366900646811185		[learning rate: 0.00041184]
	Learning Rate: 0.000411845
	LOSS [training: -0.00015767513732945927 | validation: 0.0008317771257991878]
	TIME [epoch: 8.74 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015375301800039855		[learning rate: 0.00041121]
		[batch 20/20] avg loss: -0.004864969224365479		[learning rate: 0.00041058]
	Learning Rate: 0.000410582
	LOSS [training: -0.001663719522180747 | validation: 0.004645427582292628]
	TIME [epoch: 8.74 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002960094543975879		[learning rate: 0.00040995]
		[batch 20/20] avg loss: -0.0008005745725204325		[learning rate: 0.00040932]
	Learning Rate: 0.000409323
	LOSS [training: -0.0018803345582481552 | validation: 0.0032497307557257855]
	TIME [epoch: 8.7 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006450764112851377		[learning rate: 0.0004087]
		[batch 20/20] avg loss: -0.011044496818315564		[learning rate: 0.00040807]
	Learning Rate: 0.000408069
	LOSS [training: -0.005199710203515213 | validation: -0.002854157246306199]
	TIME [epoch: 8.73 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0052705048889662		[learning rate: 0.00040744]
		[batch 20/20] avg loss: -0.006355302952970973		[learning rate: 0.00040682]
	Learning Rate: 0.000406818
	LOSS [training: -0.005812903920968586 | validation: -0.0112242817357247]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study2/model_tr_study2_r5_20240216_214530/states/model_tr_study2_1543.pth
	Model improved!!!
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022637443941765515		[learning rate: 0.00040619]
		[batch 20/20] avg loss: -0.005777183062149909		[learning rate: 0.00040557]
	Learning Rate: 0.000405571
	LOSS [training: -0.00402046372816323 | validation: 0.0065530806715807036]
	TIME [epoch: 8.73 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007624214716207976		[learning rate: 0.00040495]
		[batch 20/20] avg loss: -0.004290232128511156		[learning rate: 0.00040433]
	Learning Rate: 0.000404328
	LOSS [training: -0.005957223422359567 | validation: 0.0018026330134088108]
	TIME [epoch: 8.72 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00044466733150885264		[learning rate: 0.00040371]
		[batch 20/20] avg loss: -0.005327025583421087		[learning rate: 0.00040309]
	Learning Rate: 0.000403088
	LOSS [training: -0.0024411791259561165 | validation: -0.002420143283809541]
	TIME [epoch: 8.71 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005422256491925544		[learning rate: 0.00040247]
		[batch 20/20] avg loss: -0.0021618045186516132		[learning rate: 0.00040185]
	Learning Rate: 0.000401852
	LOSS [training: -0.0013520150839220837 | validation: 0.0044523162932957085]
	TIME [epoch: 8.72 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004927582125381526		[learning rate: 0.00040124]
		[batch 20/20] avg loss: 0.0031603151276952575		[learning rate: 0.00040062]
	Learning Rate: 0.000400621
	LOSS [training: -0.000883633498843134 | validation: -0.002568407216245631]
	TIME [epoch: 8.73 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002009138886412909		[learning rate: 0.00040001]
		[batch 20/20] avg loss: -0.007144534944971685		[learning rate: 0.00039939]
	Learning Rate: 0.000399393
	LOSS [training: -0.004576836915692297 | validation: -0.003493798508158245]
	TIME [epoch: 8.74 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020714299458194285		[learning rate: 0.00039878]
		[batch 20/20] avg loss: -0.002700797386338406		[learning rate: 0.00039817]
	Learning Rate: 0.000398168
	LOSS [training: -0.00031468372025948927 | validation: -0.0042043069721625255]
	TIME [epoch: 8.72 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004488169988058644		[learning rate: 0.00039756]
		[batch 20/20] avg loss: -0.0036461146676494806		[learning rate: 0.00039695]
	Learning Rate: 0.000396948
	LOSS [training: -0.004067142327854062 | validation: 0.0006542899525672553]
	TIME [epoch: 8.72 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004159541737219663		[learning rate: 0.00039634]
		[batch 20/20] avg loss: -0.0008300253842173772		[learning rate: 0.00039573]
	Learning Rate: 0.000395731
	LOSS [training: -0.0024947835607185198 | validation: 0.006219479102226376]
	TIME [epoch: 8.69 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007719279570966875		[learning rate: 0.00039512]
		[batch 20/20] avg loss: 0.00016237982371186227		[learning rate: 0.00039452]
	Learning Rate: 0.000394518
	LOSS [training: -0.003778449873627507 | validation: 0.004719134167012466]
	TIME [epoch: 8.75 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006233727398780244		[learning rate: 0.00039391]
		[batch 20/20] avg loss: 0.00045902646956192804		[learning rate: 0.00039331]
	Learning Rate: 0.000393308
	LOSS [training: 0.0005411996047199764 | validation: 0.0023964324468500344]
	TIME [epoch: 8.74 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024412039071397013		[learning rate: 0.00039271]
		[batch 20/20] avg loss: -0.0009945848471767335		[learning rate: 0.0003921]
	Learning Rate: 0.000392103
	LOSS [training: -0.0017178943771582178 | validation: -0.003918375312615989]
	TIME [epoch: 8.73 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003898091895470237		[learning rate: 0.0003915]
		[batch 20/20] avg loss: -0.004270257158980769		[learning rate: 0.0003909]
	Learning Rate: 0.000390901
	LOSS [training: -0.004084174527225503 | validation: -0.005222650426189642]
	TIME [epoch: 8.72 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037533314917197343		[learning rate: 0.0003903]
		[batch 20/20] avg loss: -0.005631559962187399		[learning rate: 0.0003897]
	Learning Rate: 0.000389703
	LOSS [training: -0.004692445726953568 | validation: 0.0036888387627576]
	TIME [epoch: 8.75 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016940559328314213		[learning rate: 0.0003891]
		[batch 20/20] avg loss: -0.005131049490107895		[learning rate: 0.00038851]
	Learning Rate: 0.000388508
	LOSS [training: -0.003412552711469658 | validation: 0.0049156374445900024]
	TIME [epoch: 8.76 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005615948759372236		[learning rate: 0.00038791]
		[batch 20/20] avg loss: -0.006914759318482644		[learning rate: 0.00038732]
	Learning Rate: 0.000387317
	LOSS [training: -0.006265354038927441 | validation: -0.0029683333411947388]
	TIME [epoch: 8.74 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0063559861988139845		[learning rate: 0.00038672]
		[batch 20/20] avg loss: -0.008411260039813646		[learning rate: 0.00038613]
	Learning Rate: 0.00038613
	LOSS [training: -0.007383623119313817 | validation: -0.004696270974424502]
	TIME [epoch: 8.73 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002999071301897265		[learning rate: 0.00038554]
		[batch 20/20] avg loss: -0.005154106662230916		[learning rate: 0.00038495]
	Learning Rate: 0.000384946
	LOSS [training: -0.002727006896210321 | validation: 0.0029314043926777604]
	TIME [epoch: 8.71 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003220960161806439		[learning rate: 0.00038436]
		[batch 20/20] avg loss: -0.004481219099168586		[learning rate: 0.00038377]
	Learning Rate: 0.000383766
	LOSS [training: -0.0038510896304875123 | validation: 0.00014244566195228378]
	TIME [epoch: 8.71 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006902342190088799		[learning rate: 0.00038318]
		[batch 20/20] avg loss: -0.002935664589552865		[learning rate: 0.00038259]
	Learning Rate: 0.00038259
	LOSS [training: -0.004919003389820833 | validation: -0.0005946425227316761]
	TIME [epoch: 8.7 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006308867030565546		[learning rate: 0.000382]
		[batch 20/20] avg loss: -0.0007391086578726876		[learning rate: 0.00038142]
	Learning Rate: 0.000381417
	LOSS [training: -0.0035239878442191166 | validation: -0.00260470810989077]
	TIME [epoch: 8.74 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027376520057779486		[learning rate: 0.00038083]
		[batch 20/20] avg loss: -0.0009586974579950675		[learning rate: 0.00038025]
	Learning Rate: 0.000380248
	LOSS [training: -0.001848174731886508 | validation: 0.0007180691093268117]
	TIME [epoch: 8.72 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014263166304301106		[learning rate: 0.00037966]
		[batch 20/20] avg loss: -0.009881934527886865		[learning rate: 0.00037908]
	Learning Rate: 0.000379082
	LOSS [training: -0.005654125579158488 | validation: 0.0015559245718395848]
	TIME [epoch: 8.72 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005074540203331197		[learning rate: 0.0003785]
		[batch 20/20] avg loss: -0.0011238242481592224		[learning rate: 0.00037792]
	Learning Rate: 0.00037792
	LOSS [training: -0.0030991822257452105 | validation: 0.0077994691249138725]
	TIME [epoch: 8.7 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002668508104572434		[learning rate: 0.00037734]
		[batch 20/20] avg loss: -0.007370261513059027		[learning rate: 0.00037676]
	Learning Rate: 0.000376762
	LOSS [training: -0.0050193848088157305 | validation: 0.0033015614152517546]
	TIME [epoch: 8.73 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005452912538991481		[learning rate: 0.00037618]
		[batch 20/20] avg loss: -0.0041355908420301155		[learning rate: 0.00037561]
	Learning Rate: 0.000375607
	LOSS [training: -0.004794251690510796 | validation: 0.002983251712489859]
	TIME [epoch: 8.73 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007052604131003809		[learning rate: 0.00037503]
		[batch 20/20] avg loss: -0.00011412448923461809		[learning rate: 0.00037446]
	Learning Rate: 0.000374455
	LOSS [training: -0.0035833643101192136 | validation: -0.0022321304531195806]
	TIME [epoch: 8.71 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007551865671766099		[learning rate: 0.00037388]
		[batch 20/20] avg loss: -0.00522429645575873		[learning rate: 0.00037331]
	Learning Rate: 0.000373307
	LOSS [training: -0.0029897415114676696 | validation: 0.001371002099272461]
	TIME [epoch: 8.72 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030764002874006198		[learning rate: 0.00037273]
		[batch 20/20] avg loss: -0.005065791451137058		[learning rate: 0.00037216]
	Learning Rate: 0.000372163
	LOSS [training: -0.004071095869268839 | validation: 0.006487820957341429]
	TIME [epoch: 8.71 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007626945162641441		[learning rate: 0.00037159]
		[batch 20/20] avg loss: -0.0014555041356575714		[learning rate: 0.00037102]
	Learning Rate: 0.000371022
	LOSS [training: -0.004541224649149506 | validation: 0.0054140232443465625]
	TIME [epoch: 8.75 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000878282628534215		[learning rate: 0.00037045]
		[batch 20/20] avg loss: -0.006364408846508243		[learning rate: 0.00036988]
	Learning Rate: 0.000369885
	LOSS [training: -0.0036213457375212288 | validation: -0.0012840686667564979]
	TIME [epoch: 8.71 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021280472385853295		[learning rate: 0.00036932]
		[batch 20/20] avg loss: -0.0028097926440192296		[learning rate: 0.00036875]
	Learning Rate: 0.000368751
	LOSS [training: -0.0024689199413022796 | validation: 0.000995820128754262]
	TIME [epoch: 8.72 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001547166232847717		[learning rate: 0.00036819]
		[batch 20/20] avg loss: -0.004274643789101795		[learning rate: 0.00036762]
	Learning Rate: 0.000367621
	LOSS [training: -0.0029109050109747563 | validation: -0.0012167582809950585]
	TIME [epoch: 8.72 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005390902488084264		[learning rate: 0.00036706]
		[batch 20/20] avg loss: -0.005925703927419134		[learning rate: 0.00036649]
	Learning Rate: 0.000366494
	LOSS [training: -0.0026933068393053543 | validation: -0.002692978367926662]
	TIME [epoch: 8.72 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025200457599004237		[learning rate: 0.00036593]
		[batch 20/20] avg loss: -0.002361077414283702		[learning rate: 0.00036537]
	Learning Rate: 0.00036537
	LOSS [training: -0.0024405615870920624 | validation: -0.003757061487663526]
	TIME [epoch: 8.74 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003473112233029671		[learning rate: 0.00036481]
		[batch 20/20] avg loss: -0.003183401509582358		[learning rate: 0.00036425]
	Learning Rate: 0.00036425
	LOSS [training: -0.003328256871306015 | validation: -0.0007671817352250382]
	TIME [epoch: 8.73 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002328297944125825		[learning rate: 0.00036369]
		[batch 20/20] avg loss: -0.007413124960549808		[learning rate: 0.00036313]
	Learning Rate: 0.000363134
	LOSS [training: -0.0025424135082119915 | validation: -0.004900295215582549]
	TIME [epoch: 8.73 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006133942642718451		[learning rate: 0.00036258]
		[batch 20/20] avg loss: -0.00477502136182082		[learning rate: 0.00036202]
	Learning Rate: 0.000362021
	LOSS [training: -0.005454482002269635 | validation: -0.0016409762218602972]
	TIME [epoch: 8.73 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004112222908232432		[learning rate: 0.00036147]
		[batch 20/20] avg loss: -0.003751356293924527		[learning rate: 0.00036091]
	Learning Rate: 0.000360911
	LOSS [training: -0.003931789601078479 | validation: 0.007087858225865242]
	TIME [epoch: 8.72 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024633470453180374		[learning rate: 0.00036036]
		[batch 20/20] avg loss: -0.0033121639124206017		[learning rate: 0.0003598]
	Learning Rate: 0.000359805
	LOSS [training: -0.002887755478869319 | validation: 0.003907936844501372]
	TIME [epoch: 8.76 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002586315379762531		[learning rate: 0.00035925]
		[batch 20/20] avg loss: -0.004036275251098215		[learning rate: 0.0003587]
	Learning Rate: 0.000358702
	LOSS [training: -0.003311295315430373 | validation: 0.0008089560417612154]
	TIME [epoch: 8.73 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005671437520442935		[learning rate: 0.00035815]
		[batch 20/20] avg loss: -0.004330211458586413		[learning rate: 0.0003576]
	Learning Rate: 0.000357602
	LOSS [training: -0.0018815338532710596 | validation: 0.0020392148589026484]
	TIME [epoch: 8.73 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003954318214503913		[learning rate: 0.00035705]
		[batch 20/20] avg loss: 0.003196710216533344		[learning rate: 0.00035651]
	Learning Rate: 0.000356506
	LOSS [training: -0.0003788039989852842 | validation: -0.0005875608868585045]
	TIME [epoch: 8.7 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011285611403559835		[learning rate: 0.00035596]
		[batch 20/20] avg loss: 0.002952931143429693		[learning rate: 0.00035541]
	Learning Rate: 0.000355413
	LOSS [training: 0.000912185001536854 | validation: 0.0010645338712943328]
	TIME [epoch: 8.74 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002782871924506011		[learning rate: 0.00035487]
		[batch 20/20] avg loss: -0.0005097219463987052		[learning rate: 0.00035432]
	Learning Rate: 0.000354323
	LOSS [training: -0.00011571737697405229 | validation: 0.002714243039028439]
	TIME [epoch: 8.74 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002725725739841859		[learning rate: 0.00035378]
		[batch 20/20] avg loss: -0.0036089770060565443		[learning rate: 0.00035324]
	Learning Rate: 0.000353237
	LOSS [training: -0.001940774790020365 | validation: 0.003680457978770049]
	TIME [epoch: 8.71 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0070402727974323565		[learning rate: 0.0003527]
		[batch 20/20] avg loss: -0.0019313293921655451		[learning rate: 0.00035215]
	Learning Rate: 0.000352155
	LOSS [training: -0.004485801094798951 | validation: 0.006900032095762631]
	TIME [epoch: 8.73 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007437453548059228		[learning rate: 0.00035161]
		[batch 20/20] avg loss: -0.0022027690549435474		[learning rate: 0.00035108]
	Learning Rate: 0.000351075
	LOSS [training: -0.0048201113015013865 | validation: 0.0059622740634248834]
	TIME [epoch: 8.71 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0057747363556524415		[learning rate: 0.00035054]
		[batch 20/20] avg loss: -0.004299710552734134		[learning rate: 0.00035]
	Learning Rate: 0.000349999
	LOSS [training: -0.005037223454193287 | validation: 0.006727372109009562]
	TIME [epoch: 8.75 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011337702505249943		[learning rate: 0.00034946]
		[batch 20/20] avg loss: -0.004888665423656		[learning rate: 0.00034893]
	Learning Rate: 0.000348926
	LOSS [training: -0.0030112178370904977 | validation: 0.0007741435013936316]
	TIME [epoch: 8.72 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004493657802612328		[learning rate: 0.00034839]
		[batch 20/20] avg loss: -0.004934829169324283		[learning rate: 0.00034786]
	Learning Rate: 0.000347856
	LOSS [training: -0.004714243485968305 | validation: -0.0005114018123965863]
	TIME [epoch: 8.73 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007321563072568732		[learning rate: 0.00034732]
		[batch 20/20] avg loss: -0.006044001296885976		[learning rate: 0.00034679]
	Learning Rate: 0.00034679
	LOSS [training: -0.006682782184727354 | validation: 0.0012241817472949234]
	TIME [epoch: 8.73 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005352979423117941		[learning rate: 0.00034626]
		[batch 20/20] avg loss: -0.0003700329631305392		[learning rate: 0.00034573]
	Learning Rate: 0.000345727
	LOSS [training: -0.0028615061931242395 | validation: -0.0010812508659665374]
	TIME [epoch: 8.72 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011749289930948798		[learning rate: 0.0003452]
		[batch 20/20] avg loss: 0.0009314489121404746		[learning rate: 0.00034467]
	Learning Rate: 0.000344667
	LOSS [training: -0.00012174004047720241 | validation: 0.007052581710442225]
	TIME [epoch: 8.75 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001514595450262815		[learning rate: 0.00034414]
		[batch 20/20] avg loss: -0.004165810948176616		[learning rate: 0.00034361]
	Learning Rate: 0.000343611
	LOSS [training: -0.0028402031992197153 | validation: 0.005730498483239853]
	TIME [epoch: 8.73 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006153002735987308		[learning rate: 0.00034308]
		[batch 20/20] avg loss: -0.004552667463137075		[learning rate: 0.00034256]
	Learning Rate: 0.000342557
	LOSS [training: -0.005352835099562191 | validation: -0.0012967065050798784]
	TIME [epoch: 8.72 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017630101341440188		[learning rate: 0.00034203]
		[batch 20/20] avg loss: 0.0010834240615513195		[learning rate: 0.00034151]
	Learning Rate: 0.000341507
	LOSS [training: 0.001423217097847669 | validation: -0.002242754973815013]
	TIME [epoch: 8.73 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021492125285181166		[learning rate: 0.00034098]
		[batch 20/20] avg loss: -0.0037228584017082005		[learning rate: 0.00034046]
	Learning Rate: 0.00034046
	LOSS [training: -0.0029360354651131594 | validation: 0.008991700048827824]
	TIME [epoch: 8.7 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012717176038124192		[learning rate: 0.00033994]
		[batch 20/20] avg loss: -0.003589159389312356		[learning rate: 0.00033942]
	Learning Rate: 0.000339417
	LOSS [training: -0.0024304384965623863 | validation: 0.0005848679395858384]
	TIME [epoch: 8.74 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0062014341431470815		[learning rate: 0.0003389]
		[batch 20/20] avg loss: 0.000743158837620585		[learning rate: 0.00033838]
	Learning Rate: 0.000338376
	LOSS [training: -0.0027291376527632484 | validation: 0.007097879970256805]
	TIME [epoch: 8.7 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004066392501432642		[learning rate: 0.00033786]
		[batch 20/20] avg loss: -0.001503910267697549		[learning rate: 0.00033734]
	Learning Rate: 0.000337339
	LOSS [training: -0.0027851513845650955 | validation: 0.009553316100877368]
	TIME [epoch: 8.71 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004354903549365073		[learning rate: 0.00033682]
		[batch 20/20] avg loss: -0.001339799365443805		[learning rate: 0.0003363]
	Learning Rate: 0.000336305
	LOSS [training: -0.002847351457404439 | validation: 0.004407642377236117]
	TIME [epoch: 8.72 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006947820995309774		[learning rate: 0.00033579]
		[batch 20/20] avg loss: -0.005145592958419576		[learning rate: 0.00033527]
	Learning Rate: 0.000335274
	LOSS [training: -0.0022254054294442997 | validation: 0.005221698897628179]
	TIME [epoch: 8.72 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030390751118435895		[learning rate: 0.00033476]
		[batch 20/20] avg loss: 0.0002269650667217996		[learning rate: 0.00033425]
	Learning Rate: 0.000334246
	LOSS [training: -0.0014060550225608948 | validation: 0.0015969197312272615]
	TIME [epoch: 8.76 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004696376963815881		[learning rate: 0.00033373]
		[batch 20/20] avg loss: -0.0027299199212281245		[learning rate: 0.00033322]
	Learning Rate: 0.000333222
	LOSS [training: -0.0037131484425220033 | validation: -0.00028422114542175474]
	TIME [epoch: 8.72 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017938454208967132		[learning rate: 0.00033271]
		[batch 20/20] avg loss: -0.005888206131920536		[learning rate: 0.0003322]
	Learning Rate: 0.0003322
	LOSS [training: -0.003841025776408624 | validation: 0.006138148589909444]
	TIME [epoch: 8.68 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003495723311332526		[learning rate: 0.00033169]
		[batch 20/20] avg loss: -0.004426347778063375		[learning rate: 0.00033118]
	Learning Rate: 0.000331182
	LOSS [training: -0.0023879600545983135 | validation: 0.0032833240190380847]
	TIME [epoch: 8.71 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007598663981712092		[learning rate: 0.00033067]
		[batch 20/20] avg loss: 0.0023336001945276205		[learning rate: 0.00033017]
	Learning Rate: 0.000330167
	LOSS [training: -0.0026325318935922363 | validation: 0.006505760478782269]
	TIME [epoch: 8.71 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021764458671039892		[learning rate: 0.00032966]
		[batch 20/20] avg loss: -0.0003168476744396265		[learning rate: 0.00032915]
	Learning Rate: 0.000329155
	LOSS [training: -0.0012466467707718083 | validation: 0.0032297072919377032]
	TIME [epoch: 8.74 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005911553200188621		[learning rate: 0.00032865]
		[batch 20/20] avg loss: -0.004175552585733945		[learning rate: 0.00032815]
	Learning Rate: 0.000328146
	LOSS [training: -0.005043552892961282 | validation: 0.005239992469132444]
	TIME [epoch: 8.71 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003652086873267351		[learning rate: 0.00032764]
		[batch 20/20] avg loss: -0.0010121688202929576		[learning rate: 0.00032714]
	Learning Rate: 0.00032714
	LOSS [training: -0.002332127846780154 | validation: 0.0013877054145433854]
	TIME [epoch: 8.72 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004924166880406774		[learning rate: 0.00032664]
		[batch 20/20] avg loss: -0.004687941678788581		[learning rate: 0.00032614]
	Learning Rate: 0.000326137
	LOSS [training: -0.004806054279597678 | validation: 0.00016741660337369225]
	TIME [epoch: 8.71 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0051851313565502944		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 0.00044915592853909773		[learning rate: 0.00032514]
	Learning Rate: 0.000325137
	LOSS [training: -0.002367987714005599 | validation: 0.0036923195942994438]
	TIME [epoch: 8.74 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00403030053062162		[learning rate: 0.00032464]
		[batch 20/20] avg loss: -0.0008062882906714478		[learning rate: 0.00032414]
	Learning Rate: 0.000324141
	LOSS [training: -0.0024182944106465342 | validation: 0.0022778061460523216]
	TIME [epoch: 8.72 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001816518954758246		[learning rate: 0.00032364]
		[batch 20/20] avg loss: -0.002945568819857991		[learning rate: 0.00032315]
	Learning Rate: 0.000323147
	LOSS [training: -0.0023810438873081187 | validation: -0.0033519924137008633]
	TIME [epoch: 8.71 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0071839302798893155		[learning rate: 0.00032265]
		[batch 20/20] avg loss: -0.005265752780609474		[learning rate: 0.00032216]
	Learning Rate: 0.000322156
	LOSS [training: -0.006224841530249394 | validation: 0.0002912791078680863]
	TIME [epoch: 8.72 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021783895483873167		[learning rate: 0.00032166]
		[batch 20/20] avg loss: -0.0018568333607338026		[learning rate: 0.00032117]
	Learning Rate: 0.000321169
	LOSS [training: 0.00016077809382675727 | validation: 0.0041402020997701825]
	TIME [epoch: 8.7 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008470690632022802		[learning rate: 0.00032068]
		[batch 20/20] avg loss: -0.003236226075160324		[learning rate: 0.00032018]
	Learning Rate: 0.000320184
	LOSS [training: -0.0020416475691813025 | validation: -0.001030321479948806]
	TIME [epoch: 8.72 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016652887386059793		[learning rate: 0.00031969]
		[batch 20/20] avg loss: -0.002368126429942046		[learning rate: 0.0003192]
	Learning Rate: 0.000319203
	LOSS [training: -0.002016707584274012 | validation: -0.0024536894811951126]
	TIME [epoch: 8.7 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005695998793934412		[learning rate: 0.00031871]
		[batch 20/20] avg loss: -0.003286151894283064		[learning rate: 0.00031822]
	Learning Rate: 0.000318224
	LOSS [training: 0.0012049234498256742 | validation: -0.007106891249591012]
	TIME [epoch: 8.71 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002058523209680808		[learning rate: 0.00031774]
		[batch 20/20] avg loss: -0.005221362227390002		[learning rate: 0.00031725]
	Learning Rate: 0.000317249
	LOSS [training: -0.0015814195088545968 | validation: -0.003961244237969244]
	TIME [epoch: 8.74 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017043183828269154		[learning rate: 0.00031676]
		[batch 20/20] avg loss: -0.0027271915008549915		[learning rate: 0.00031628]
	Learning Rate: 0.000316276
	LOSS [training: -0.0022157549418409537 | validation: -0.0024303368085412734]
	TIME [epoch: 8.73 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005285418186632591		[learning rate: 0.00031579]
		[batch 20/20] avg loss: -0.0037738957992350134		[learning rate: 0.00031531]
	Learning Rate: 0.000315307
	LOSS [training: -0.004529656992933802 | validation: -0.00614298286307775]
	TIME [epoch: 8.76 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004624676673900162		[learning rate: 0.00031482]
		[batch 20/20] avg loss: -0.0012595600252774722		[learning rate: 0.00031434]
	Learning Rate: 0.00031434
	LOSS [training: -0.002942118349588817 | validation: -0.006685681442615722]
	TIME [epoch: 8.72 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008086102035048674		[learning rate: 0.00031386]
		[batch 20/20] avg loss: -0.005620112361788806		[learning rate: 0.00031338]
	Learning Rate: 0.000313377
	LOSS [training: -0.0068531071984187395 | validation: -0.0029897775945022164]
	TIME [epoch: 8.72 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029558714330639806		[learning rate: 0.0003129]
		[batch 20/20] avg loss: -0.007482469435521627		[learning rate: 0.00031242]
	Learning Rate: 0.000312416
	LOSS [training: -0.005219170434292803 | validation: 0.001458517519205933]
	TIME [epoch: 8.73 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0074210946402055765		[learning rate: 0.00031194]
		[batch 20/20] avg loss: -0.001058343772382671		[learning rate: 0.00031146]
	Learning Rate: 0.000311458
	LOSS [training: -0.0042397192062941244 | validation: -0.008070900809299883]
	TIME [epoch: 8.73 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038630691542146504		[learning rate: 0.00031098]
		[batch 20/20] avg loss: 0.00452886284328929		[learning rate: 0.0003105]
	Learning Rate: 0.000310504
	LOSS [training: 0.0003328968445373199 | validation: 0.0033684378732610955]
	TIME [epoch: 8.74 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030094772960126857		[learning rate: 0.00031003]
		[batch 20/20] avg loss: -0.0007188521268259149		[learning rate: 0.00030955]
	Learning Rate: 0.000309552
	LOSS [training: -0.0018641647114193 | validation: 0.0066801886213719835]
	TIME [epoch: 8.73 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0069237521390868775		[learning rate: 0.00030908]
		[batch 20/20] avg loss: -0.00017453096825579203		[learning rate: 0.0003086]
	Learning Rate: 0.000308603
	LOSS [training: -0.003549141553671336 | validation: 0.001864278743649347]
	TIME [epoch: 8.74 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00030347071095258545		[learning rate: 0.00030813]
		[batch 20/20] avg loss: -0.0026523256016417456		[learning rate: 0.00030766]
	Learning Rate: 0.000307657
	LOSS [training: -0.0011744274453445798 | validation: -0.001147410915438455]
	TIME [epoch: 8.73 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032828461974039685		[learning rate: 0.00030718]
		[batch 20/20] avg loss: -0.002958373020511462		[learning rate: 0.00030671]
	Learning Rate: 0.000306714
	LOSS [training: -0.003120609608957715 | validation: -0.0016284812551938211]
	TIME [epoch: 8.73 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0067438431036844744		[learning rate: 0.00030624]
		[batch 20/20] avg loss: -0.0012737185083116771		[learning rate: 0.00030577]
	Learning Rate: 0.000305774
	LOSS [training: -0.0040087808059980756 | validation: -0.0017154305235488206]
	TIME [epoch: 8.71 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013510311561467918		[learning rate: 0.0003053]
		[batch 20/20] avg loss: -0.005631875358162215		[learning rate: 0.00030484]
	Learning Rate: 0.000304836
	LOSS [training: -0.0034914532571545037 | validation: -0.0008227436623719295]
	TIME [epoch: 8.73 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013208361914179329		[learning rate: 0.00030437]
		[batch 20/20] avg loss: -0.002186481623924879		[learning rate: 0.0003039]
	Learning Rate: 0.000303902
	LOSS [training: -0.001753658907671406 | validation: -0.004734980552223212]
	TIME [epoch: 8.69 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007578052799898273		[learning rate: 0.00030344]
		[batch 20/20] avg loss: -0.0012904466707374356		[learning rate: 0.00030297]
	Learning Rate: 0.00030297
	LOSS [training: -0.004434249735317855 | validation: -0.002178173341861084]
	TIME [epoch: 8.7 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006977598184737277		[learning rate: 0.00030251]
		[batch 20/20] avg loss: -0.0076973500165379485		[learning rate: 0.00030204]
	Learning Rate: 0.000302042
	LOSS [training: -0.007337474100637611 | validation: -0.00035482608810747307]
	TIME [epoch: 8.75 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004687263885107552		[learning rate: 0.00030158]
		[batch 20/20] avg loss: -0.012037665229911966		[learning rate: 0.00030112]
	Learning Rate: 0.000301116
	LOSS [training: -0.0036752006724022077 | validation: 0.0033892058383115584]
	TIME [epoch: 8.74 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00026153385822513017		[learning rate: 0.00030065]
		[batch 20/20] avg loss: -0.003816592448088811		[learning rate: 0.00030019]
	Learning Rate: 0.000300193
	LOSS [training: -0.0017775292949318404 | validation: -0.0023068110770659137]
	TIME [epoch: 8.74 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021347381656836506		[learning rate: 0.00029973]
		[batch 20/20] avg loss: -0.000252936089338888		[learning rate: 0.00029927]
	Learning Rate: 0.000299272
	LOSS [training: -0.0011938371275112692 | validation: 0.0066474227456491345]
	TIME [epoch: 8.72 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00266658865931484		[learning rate: 0.00029881]
		[batch 20/20] avg loss: -0.006111216071271172		[learning rate: 0.00029835]
	Learning Rate: 0.000298355
	LOSS [training: -0.004388902365293007 | validation: -0.0020416856308466303]
	TIME [epoch: 8.71 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007285525296120632		[learning rate: 0.0002979]
		[batch 20/20] avg loss: 0.0007710460952699196		[learning rate: 0.00029744]
	Learning Rate: 0.00029744
	LOSS [training: -0.003257239600425356 | validation: -0.0019753872351517742]
	TIME [epoch: 8.72 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004728075383582274		[learning rate: 0.00029698]
		[batch 20/20] avg loss: -0.0008089912288614767		[learning rate: 0.00029653]
	Learning Rate: 0.000296529
	LOSS [training: -0.0027685333062218752 | validation: 0.00825315783900624]
	TIME [epoch: 8.74 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007448655723033472		[learning rate: 0.00029607]
		[batch 20/20] avg loss: -0.0007792650957487198		[learning rate: 0.00029562]
	Learning Rate: 0.00029562
	LOSS [training: -0.0041139604093910955 | validation: -0.0013883832673031744]
	TIME [epoch: 8.71 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005259308448802845		[learning rate: 0.00029517]
		[batch 20/20] avg loss: -0.005532750239550012		[learning rate: 0.00029471]
	Learning Rate: 0.000294713
	LOSS [training: -0.005396029344176429 | validation: -0.0016177801628984023]
	TIME [epoch: 8.71 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006179067419258886		[learning rate: 0.00029426]
		[batch 20/20] avg loss: -0.0021292609019613767		[learning rate: 0.00029381]
	Learning Rate: 0.00029381
	LOSS [training: -0.004154164160610131 | validation: -0.0016825955838699611]
	TIME [epoch: 8.73 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005363769245764156		[learning rate: 0.00029336]
		[batch 20/20] avg loss: -0.0020645497801366325		[learning rate: 0.00029291]
	Learning Rate: 0.000292909
	LOSS [training: -0.0037141595129503943 | validation: -0.0009640151043781483]
	TIME [epoch: 8.76 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004887125234812752		[learning rate: 0.00029246]
		[batch 20/20] avg loss: -0.005015026298321		[learning rate: 0.00029201]
	Learning Rate: 0.000292011
	LOSS [training: -0.004951075766566877 | validation: -0.0011989748893397273]
	TIME [epoch: 8.7 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032020039805288544		[learning rate: 0.00029156]
		[batch 20/20] avg loss: 0.0007213938816174067		[learning rate: 0.00029112]
	Learning Rate: 0.000291116
	LOSS [training: -0.0012403050494557242 | validation: -0.0017540864463635919]
	TIME [epoch: 8.71 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005679431871929353		[learning rate: 0.00029067]
		[batch 20/20] avg loss: -0.0016182956721513416		[learning rate: 0.00029022]
	Learning Rate: 0.000290224
	LOSS [training: -0.003648863772040347 | validation: -7.737858115791966e-05]
	TIME [epoch: 8.71 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010408782632735356		[learning rate: 0.00028978]
		[batch 20/20] avg loss: -0.006475004368669541		[learning rate: 0.00028933]
	Learning Rate: 0.000289334
	LOSS [training: -0.008441893500702447 | validation: -0.0014602037383343973]
	TIME [epoch: 8.73 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00037189436569534803		[learning rate: 0.00028889]
		[batch 20/20] avg loss: -0.005931594689792737		[learning rate: 0.00028845]
	Learning Rate: 0.000288447
	LOSS [training: -0.0027798501620486943 | validation: -0.0038999364294099526]
	TIME [epoch: 8.72 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042431379312058825		[learning rate: 0.000288]
		[batch 20/20] avg loss: -0.005092701215523189		[learning rate: 0.00028756]
	Learning Rate: 0.000287563
	LOSS [training: -0.004667919573364537 | validation: 0.0014357677074673786]
	TIME [epoch: 8.69 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010333486671402943		[learning rate: 0.00028712]
		[batch 20/20] avg loss: -0.002120793270622268		[learning rate: 0.00028668]
	Learning Rate: 0.000286682
	LOSS [training: -0.001577070968881281 | validation: 0.000662112610372764]
	TIME [epoch: 8.73 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00010583214926843919		[learning rate: 0.00028624]
		[batch 20/20] avg loss: -0.010084423669931075		[learning rate: 0.0002858]
	Learning Rate: 0.000285803
	LOSS [training: -0.005095127909599756 | validation: 0.0007900265769833288]
	TIME [epoch: 8.71 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003756703060679687		[learning rate: 0.00028536]
		[batch 20/20] avg loss: -0.0027791306829011548		[learning rate: 0.00028493]
	Learning Rate: 0.000284927
	LOSS [training: -0.003267916871790421 | validation: -0.0011000426018473114]
	TIME [epoch: 8.72 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00023049914123819942		[learning rate: 0.00028449]
		[batch 20/20] avg loss: -0.009498102964945573		[learning rate: 0.00028405]
	Learning Rate: 0.000284053
	LOSS [training: -0.004864301053091886 | validation: -0.00014220767054142327]
	TIME [epoch: 8.74 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006429526975864113		[learning rate: 0.00028362]
		[batch 20/20] avg loss: -0.006806236261588186		[learning rate: 0.00028318]
	Learning Rate: 0.000283183
	LOSS [training: -0.003724594479587299 | validation: -0.0036851107322914524]
	TIME [epoch: 8.71 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004475394357707984		[learning rate: 0.00028275]
		[batch 20/20] avg loss: -0.0009460720325156234		[learning rate: 0.00028231]
	Learning Rate: 0.000282315
	LOSS [training: -0.0027107331951118033 | validation: 3.574968089270979e-05]
	TIME [epoch: 8.7 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003545564320777779		[learning rate: 0.00028188]
		[batch 20/20] avg loss: -0.004045197679924786		[learning rate: 0.00028145]
	Learning Rate: 0.000281449
	LOSS [training: -0.003795381000351282 | validation: -0.0005458946127956193]
	TIME [epoch: 8.72 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018314869166746266		[learning rate: 0.00028102]
		[batch 20/20] avg loss: -0.00020883808926545818		[learning rate: 0.00028059]
	Learning Rate: 0.000280586
	LOSS [training: 0.0008113244137045842 | validation: -0.005520448756334227]
	TIME [epoch: 8.75 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002844482051774289		[learning rate: 0.00028016]
		[batch 20/20] avg loss: -0.003285223406150306		[learning rate: 0.00027973]
	Learning Rate: 0.000279726
	LOSS [training: -0.0030648527289622975 | validation: -0.00023626389072875737]
	TIME [epoch: 8.75 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002281658018132237		[learning rate: 0.0002793]
		[batch 20/20] avg loss: -0.002252091809746304		[learning rate: 0.00027887]
	Learning Rate: 0.000278869
	LOSS [training: -0.00226687491393927 | validation: -0.004533699369856721]
	TIME [epoch: 8.7 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035108527259787156		[learning rate: 0.00027844]
		[batch 20/20] avg loss: -0.008876771111948966		[learning rate: 0.00027801]
	Learning Rate: 0.000278014
	LOSS [training: -0.006193811918963842 | validation: 0.0024636279577325343]
	TIME [epoch: 8.71 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007924502959082957		[learning rate: 0.00027759]
		[batch 20/20] avg loss: -0.0020024054466237853		[learning rate: 0.00027716]
	Learning Rate: 0.000277162
	LOSS [training: -0.00496345420285337 | validation: 0.00558109250996165]
	TIME [epoch: 8.7 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005201872651675357		[learning rate: 0.00027674]
		[batch 20/20] avg loss: -0.003455290437458871		[learning rate: 0.00027631]
	Learning Rate: 0.000276312
	LOSS [training: -0.0019877388513132033 | validation: -0.0028615952225050792]
	TIME [epoch: 8.73 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005145249660286249		[learning rate: 0.00027589]
		[batch 20/20] avg loss: -0.0028924637605570445		[learning rate: 0.00027547]
	Learning Rate: 0.000275465
	LOSS [training: -0.004018856710421646 | validation: 0.0014368903529142346]
	TIME [epoch: 8.71 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0052013766354389555		[learning rate: 0.00027504]
		[batch 20/20] avg loss: -0.002505871408161243		[learning rate: 0.00027462]
	Learning Rate: 0.000274621
	LOSS [training: -0.0038536240218001007 | validation: 0.001273571568920311]
	TIME [epoch: 8.71 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: -3.616672145527779e-05		[learning rate: 0.0002742]
		[batch 20/20] avg loss: -0.0015023499190512115		[learning rate: 0.00027378]
	Learning Rate: 0.000273779
	LOSS [training: -0.0007692583202532447 | validation: -0.0030756846732918]
	TIME [epoch: 8.73 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006116210153059232		[learning rate: 0.00027336]
		[batch 20/20] avg loss: -0.00596478266532274		[learning rate: 0.00027294]
	Learning Rate: 0.00027294
	LOSS [training: -0.0026765808250084084 | validation: -0.004438520444712339]
	TIME [epoch: 8.72 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007275518435052991		[learning rate: 0.00027252]
		[batch 20/20] avg loss: -0.003038864977443557		[learning rate: 0.0002721]
	Learning Rate: 0.000272103
	LOSS [training: -0.0051571917062482745 | validation: -1.4039945108934235e-05]
	TIME [epoch: 8.71 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00508261625358981		[learning rate: 0.00027169]
		[batch 20/20] avg loss: -0.004077817439060853		[learning rate: 0.00027127]
	Learning Rate: 0.000271269
	LOSS [training: -0.004580216846325332 | validation: 0.0007498985314983589]
	TIME [epoch: 8.73 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008827976811053616		[learning rate: 0.00027085]
		[batch 20/20] avg loss: -0.0015736183941630771		[learning rate: 0.00027044]
	Learning Rate: 0.000270437
	LOSS [training: -0.005200797602608347 | validation: -0.0005321753088069103]
	TIME [epoch: 8.71 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026070704175874143		[learning rate: 0.00027002]
		[batch 20/20] avg loss: -0.00031378641858468377		[learning rate: 0.00026961]
	Learning Rate: 0.000269608
	LOSS [training: -0.0014604284180860492 | validation: -0.0020727554451819372]
	TIME [epoch: 8.71 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004498136769128234		[learning rate: 0.00026919]
		[batch 20/20] avg loss: -0.006443924587414569		[learning rate: 0.00026878]
	Learning Rate: 0.000268782
	LOSS [training: -0.003446869132163696 | validation: -0.0009691639369085301]
	TIME [epoch: 8.73 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015585223602292881		[learning rate: 0.00026837]
		[batch 20/20] avg loss: -0.005372086205045362		[learning rate: 0.00026796]
	Learning Rate: 0.000267958
	LOSS [training: -0.0034653042826373248 | validation: -0.002764612207265588]
	TIME [epoch: 8.73 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001556953270228189		[learning rate: 0.00026755]
		[batch 20/20] avg loss: -0.005246163198603336		[learning rate: 0.00026714]
	Learning Rate: 0.000267137
	LOSS [training: -0.0034015582344157622 | validation: 0.0005204311960524192]
	TIME [epoch: 8.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005506131142314216		[learning rate: 0.00026673]
		[batch 20/20] avg loss: -0.0057053643655540325		[learning rate: 0.00026632]
	Learning Rate: 0.000266318
	LOSS [training: -0.0031279887398927265 | validation: -0.0009234624243785908]
	TIME [epoch: 8.71 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00466632221775369		[learning rate: 0.00026591]
		[batch 20/20] avg loss: -0.0028528976384516154		[learning rate: 0.0002655]
	Learning Rate: 0.000265501
	LOSS [training: -0.003759609928102653 | validation: 0.0010874412490397562]
	TIME [epoch: 8.73 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006103704475761013		[learning rate: 0.00026509]
		[batch 20/20] avg loss: -0.007792292194896376		[learning rate: 0.00026469]
	Learning Rate: 0.000264687
	LOSS [training: -0.004201331321236239 | validation: 0.0021642812279514955]
	TIME [epoch: 8.73 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00011563936697283044		[learning rate: 0.00026428]
		[batch 20/20] avg loss: -0.00542027193051132		[learning rate: 0.00026388]
	Learning Rate: 0.000263876
	LOSS [training: -0.0026523162817692447 | validation: -0.0020415028982207727]
	TIME [epoch: 8.71 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008529681954894678		[learning rate: 0.00026347]
		[batch 20/20] avg loss: -0.0028682728712499136		[learning rate: 0.00026307]
	Learning Rate: 0.000263067
	LOSS [training: -0.0018606205333696912 | validation: 0.005165568450546635]
	TIME [epoch: 8.7 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003293281683492308		[learning rate: 0.00026266]
		[batch 20/20] avg loss: -0.004336005099810019		[learning rate: 0.00026226]
	Learning Rate: 0.000262261
	LOSS [training: -0.0038146433916511637 | validation: 0.0031317464622507233]
	TIME [epoch: 8.69 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014661286455958098		[learning rate: 0.00026186]
		[batch 20/20] avg loss: 0.005469103608864131		[learning rate: 0.00026146]
	Learning Rate: 0.000261457
	LOSS [training: 0.002001487481634161 | validation: 0.00724740437197384]
	TIME [epoch: 8.71 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00032378397145699635		[learning rate: 0.00026106]
		[batch 20/20] avg loss: -0.003619067248602633		[learning rate: 0.00026066]
	Learning Rate: 0.000260655
	LOSS [training: -0.0019714256100298146 | validation: 0.0012068312850309223]
	TIME [epoch: 8.74 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: -5.896801575430654e-05		[learning rate: 0.00026026]
		[batch 20/20] avg loss: 0.0014852173147412912		[learning rate: 0.00025986]
	Learning Rate: 0.000259856
	LOSS [training: 0.0007131246494934925 | validation: 0.001203848370677437]
	TIME [epoch: 8.71 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025900184676798605		[learning rate: 0.00025946]
		[batch 20/20] avg loss: -0.00583609768944141		[learning rate: 0.00025906]
	Learning Rate: 0.00025906
	LOSS [training: -0.0016230396108807752 | validation: 0.002555380192419349]
	TIME [epoch: 8.73 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007776869517393726		[learning rate: 0.00025866]
		[batch 20/20] avg loss: -0.0029477326268127673		[learning rate: 0.00025827]
	Learning Rate: 0.000258266
	LOSS [training: -0.005362301072103245 | validation: 0.0020990316252173335]
	TIME [epoch: 8.71 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001181093139209925		[learning rate: 0.00025787]
		[batch 20/20] avg loss: -0.009173700417331473		[learning rate: 0.00025747]
	Learning Rate: 0.000257474
	LOSS [training: -0.0039963036390607744 | validation: 0.0007960889757672829]
	TIME [epoch: 8.71 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008668095190618012		[learning rate: 0.00025708]
		[batch 20/20] avg loss: -0.0050045897354669515		[learning rate: 0.00025668]
	Learning Rate: 0.000256685
	LOSS [training: -0.006836342463042483 | validation: 0.0068298800493519956]
	TIME [epoch: 8.74 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006209609289041374		[learning rate: 0.00025629]
		[batch 20/20] avg loss: -0.0009689887229137132		[learning rate: 0.0002559]
	Learning Rate: 0.000255898
	LOSS [training: -0.00017401389700478782 | validation: 0.006072647048314891]
	TIME [epoch: 8.73 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020173307543140334		[learning rate: 0.00025551]
		[batch 20/20] avg loss: -0.003485026885806532		[learning rate: 0.00025511]
	Learning Rate: 0.000255113
	LOSS [training: -0.0027511788200602827 | validation: 0.003623473118512409]
	TIME [epoch: 8.71 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003954819733188697		[learning rate: 0.00025472]
		[batch 20/20] avg loss: -0.004893769505345563		[learning rate: 0.00025433]
	Learning Rate: 0.000254331
	LOSS [training: -0.0044242946192671285 | validation: -0.00214584732191621]
	TIME [epoch: 8.72 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004163465631891626		[learning rate: 0.00025394]
		[batch 20/20] avg loss: -0.0069832436734366095		[learning rate: 0.00025355]
	Learning Rate: 0.000253552
	LOSS [training: -0.005573354652664117 | validation: 0.0006400990987842148]
	TIME [epoch: 8.74 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.757255335127393e-05		[learning rate: 0.00025316]
		[batch 20/20] avg loss: -0.002881497749032462		[learning rate: 0.00025277]
	Learning Rate: 0.000252775
	LOSS [training: -0.0013919625978405937 | validation: -0.00534250500554415]
	TIME [epoch: 8.69 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00492391509556876		[learning rate: 0.00025239]
		[batch 20/20] avg loss: -0.0035261321532469503		[learning rate: 0.000252]
	Learning Rate: 0.000252
	LOSS [training: -0.004225023624407856 | validation: 0.001232703346798618]
	TIME [epoch: 8.7 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013091224118781486		[learning rate: 0.00025161]
		[batch 20/20] avg loss: -0.005719868679619673		[learning rate: 0.00025123]
	Learning Rate: 0.000251227
	LOSS [training: -0.0035144955457489108 | validation: -0.006658758913890006]
	TIME [epoch: 8.71 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005060202934524385		[learning rate: 0.00025084]
		[batch 20/20] avg loss: -0.0017475603146963384		[learning rate: 0.00025046]
	Learning Rate: 0.000250457
	LOSS [training: -0.003403881624610362 | validation: -0.0017855172961153756]
	TIME [epoch: 8.71 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035727937372950097		[learning rate: 0.00025007]
		[batch 20/20] avg loss: -0.005209928378820761		[learning rate: 0.00024969]
	Learning Rate: 0.000249689
	LOSS [training: -0.004391361058057886 | validation: -4.399656077319191e-06]
	TIME [epoch: 8.74 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020198438984100602		[learning rate: 0.00024931]
		[batch 20/20] avg loss: -0.0030377229539234967		[learning rate: 0.00024892]
	Learning Rate: 0.000248924
	LOSS [training: -0.0025287834261667787 | validation: -0.001034326876063407]
	TIME [epoch: 8.72 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003030009020046116		[learning rate: 0.00024854]
		[batch 20/20] avg loss: -0.0075489233080044335		[learning rate: 0.00024816]
	Learning Rate: 0.000248161
	LOSS [training: -0.005289466164025275 | validation: -0.0040322861607447]
	TIME [epoch: 8.73 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026856501529846596		[learning rate: 0.00024778]
		[batch 20/20] avg loss: -0.004904860662535584		[learning rate: 0.0002474]
	Learning Rate: 0.0002474
	LOSS [training: -0.0037952554077601217 | validation: 0.0037963391855668533]
	TIME [epoch: 8.69 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031944405756006724		[learning rate: 0.00024702]
		[batch 20/20] avg loss: -0.003745639360507922		[learning rate: 0.00024664]
	Learning Rate: 0.000246642
	LOSS [training: -0.0034700399680542967 | validation: -0.005067844066149223]
	TIME [epoch: 8.71 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004562333199916332		[learning rate: 0.00024626]
		[batch 20/20] avg loss: -0.0033630411044067846		[learning rate: 0.00024589]
	Learning Rate: 0.000245886
	LOSS [training: -0.003962687152161559 | validation: 0.00019656762527549892]
	TIME [epoch: 8.73 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009312284354232861		[learning rate: 0.00024551]
		[batch 20/20] avg loss: -0.006541132914055445		[learning rate: 0.00024513]
	Learning Rate: 0.000245132
	LOSS [training: -0.003736180674739365 | validation: -0.003880848174305052]
	TIME [epoch: 8.72 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009952970214305158		[learning rate: 0.00024476]
		[batch 20/20] avg loss: -0.004558470225658336		[learning rate: 0.00024438]
	Learning Rate: 0.000244381
	LOSS [training: -0.002776883623544426 | validation: 0.0012061261191423658]
	TIME [epoch: 8.73 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002052156308021199		[learning rate: 0.00024401]
		[batch 20/20] avg loss: 0.0005211392903723521		[learning rate: 0.00024363]
	Learning Rate: 0.000243631
	LOSS [training: -0.0007655085088244233 | validation: 0.004616304951422005]
	TIME [epoch: 8.71 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018483421898516888		[learning rate: 0.00024326]
		[batch 20/20] avg loss: -0.0022872944980770285		[learning rate: 0.00024288]
	Learning Rate: 0.000242885
	LOSS [training: -0.0020678183439643586 | validation: 0.0026652041305748326]
	TIME [epoch: 8.72 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008461679899864686		[learning rate: 0.00024251]
		[batch 20/20] avg loss: -0.0052399081699437905		[learning rate: 0.00024214]
	Learning Rate: 0.00024214
	LOSS [training: -0.006850794034904238 | validation: 0.0030652739674883957]
	TIME [epoch: 8.73 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004029666947306159		[learning rate: 0.00024177]
		[batch 20/20] avg loss: -0.004240439495058723		[learning rate: 0.0002414]
	Learning Rate: 0.000241398
	LOSS [training: -0.004135053221182442 | validation: 0.0004525194812818067]
	TIME [epoch: 8.7 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002668900869319527		[learning rate: 0.00024103]
		[batch 20/20] avg loss: -0.005570601648705767		[learning rate: 0.00024066]
	Learning Rate: 0.000240658
	LOSS [training: -0.004119751259012648 | validation: -0.0023912666818344773]
	TIME [epoch: 8.71 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030891862891593838		[learning rate: 0.00024029]
		[batch 20/20] avg loss: -0.007012031265167587		[learning rate: 0.00023992]
	Learning Rate: 0.00023992
	LOSS [training: -0.005050608777163486 | validation: -0.003926459467597877]
	TIME [epoch: 8.72 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005800892635896492		[learning rate: 0.00023955]
		[batch 20/20] avg loss: 0.00045150852977465577		[learning rate: 0.00023918]
	Learning Rate: 0.000239185
	LOSS [training: -0.0026746920530609187 | validation: -0.0013708402398490633]
	TIME [epoch: 8.73 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00502182635981027		[learning rate: 0.00023882]
		[batch 20/20] avg loss: -0.004826264496729691		[learning rate: 0.00023845]
	Learning Rate: 0.000238451
	LOSS [training: -0.004924045428269981 | validation: -0.001796181324787374]
	TIME [epoch: 8.71 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005139971560804086		[learning rate: 0.00023809]
		[batch 20/20] avg loss: -0.00026905004751515626		[learning rate: 0.00023772]
	Learning Rate: 0.000237721
	LOSS [training: -0.0027045108041596215 | validation: -0.0007111339927297214]
	TIME [epoch: 8.71 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003774397539268839		[learning rate: 0.00023736]
		[batch 20/20] avg loss: -0.003439701893935591		[learning rate: 0.00023699]
	Learning Rate: 0.000236992
	LOSS [training: -0.003607049716602215 | validation: 0.008144305052433682]
	TIME [epoch: 8.72 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002965056859793151		[learning rate: 0.00023663]
		[batch 20/20] avg loss: 0.0018528473833180122		[learning rate: 0.00023627]
	Learning Rate: 0.000236265
	LOSS [training: -0.0005561047382375694 | validation: 0.005059988200294082]
	TIME [epoch: 8.7 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009359863509983306		[learning rate: 0.0002359]
		[batch 20/20] avg loss: -0.008480751210492193		[learning rate: 0.00023554]
	Learning Rate: 0.000235541
	LOSS [training: -0.004708368780745263 | validation: 0.005096443120850778]
	TIME [epoch: 8.73 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028590248988661316		[learning rate: 0.00023518]
		[batch 20/20] avg loss: -0.0040303715442203585		[learning rate: 0.00023482]
	Learning Rate: 0.000234819
	LOSS [training: -0.003444698221543244 | validation: 0.00031013374283315445]
	TIME [epoch: 8.72 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006095361501377028		[learning rate: 0.00023446]
		[batch 20/20] avg loss: -0.00703812058201672		[learning rate: 0.0002341]
	Learning Rate: 0.000234099
	LOSS [training: -0.0065667410416968735 | validation: -0.003127991825157616]
	TIME [epoch: 8.69 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00397433386516396		[learning rate: 0.00023374]
		[batch 20/20] avg loss: -0.0031178541007050803		[learning rate: 0.00023338]
	Learning Rate: 0.000233382
	LOSS [training: -0.003546093982934521 | validation: 0.0046335938573396265]
	TIME [epoch: 8.71 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006689991208595307		[learning rate: 0.00023302]
		[batch 20/20] avg loss: -0.0023461360209131327		[learning rate: 0.00023267]
	Learning Rate: 0.000232666
	LOSS [training: -0.004518063614754221 | validation: -0.0051818935677671015]
	TIME [epoch: 8.71 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000340605558210854		[learning rate: 0.00023231]
		[batch 20/20] avg loss: -0.0023258556658792206		[learning rate: 0.00023195]
	Learning Rate: 0.000231953
	LOSS [training: -0.0009926250538341834 | validation: 0.002706137579984666]
	TIME [epoch: 8.73 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003916328716408816		[learning rate: 0.0002316]
		[batch 20/20] avg loss: -0.0006411683873166143		[learning rate: 0.00023124]
	Learning Rate: 0.000231242
	LOSS [training: -0.002278748551862716 | validation: 0.00478602397262603]
	TIME [epoch: 8.72 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003067787541595404		[learning rate: 0.00023089]
		[batch 20/20] avg loss: 0.0038687914334332623		[learning rate: 0.00023053]
	Learning Rate: 0.000230533
	LOSS [training: 0.00040050194591892874 | validation: 3.225749190657011e-05]
	TIME [epoch: 8.71 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005166353430472362		[learning rate: 0.00023018]
		[batch 20/20] avg loss: -0.0016909168080604935		[learning rate: 0.00022983]
	Learning Rate: 0.000229826
	LOSS [training: -0.003428635119266426 | validation: 0.0009043613918323912]
	TIME [epoch: 8.71 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00029125742828475825		[learning rate: 0.00022947]
		[batch 20/20] avg loss: -0.0014525692669569404		[learning rate: 0.00022912]
	Learning Rate: 0.000229122
	LOSS [training: -0.0005806559193360913 | validation: -0.0016138122368919417]
	TIME [epoch: 8.72 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005944878316584909		[learning rate: 0.00022877]
		[batch 20/20] avg loss: -0.004655253744720089		[learning rate: 0.00022842]
	Learning Rate: 0.00022842
	LOSS [training: -0.005300066030652499 | validation: -0.004246378985485862]
	TIME [epoch: 8.73 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005416179052098074		[learning rate: 0.00022807]
		[batch 20/20] avg loss: -0.008498206631020968		[learning rate: 0.00022772]
	Learning Rate: 0.000227719
	LOSS [training: -0.00397829436290558 | validation: -0.003399314868604662]
	TIME [epoch: 8.71 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: -4.8530099881825654e-05		[learning rate: 0.00022737]
		[batch 20/20] avg loss: -0.005152564880423976		[learning rate: 0.00022702]
	Learning Rate: 0.000227021
	LOSS [training: -0.0026005474901529 | validation: 0.0015307899191678298]
	TIME [epoch: 8.69 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010642469096920563		[learning rate: 0.00022667]
		[batch 20/20] avg loss: -0.0052805289273667915		[learning rate: 0.00022633]
	Learning Rate: 0.000226325
	LOSS [training: -0.003172387918529424 | validation: 0.0012460059304799805]
	TIME [epoch: 8.72 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003100751707611578		[learning rate: 0.00022598]
		[batch 20/20] avg loss: -0.003873475765137589		[learning rate: 0.00022563]
	Learning Rate: 0.000225632
	LOSS [training: -0.003487113736374584 | validation: -0.005875720668408982]
	TIME [epoch: 8.7 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009761772744480285		[learning rate: 0.00022529]
		[batch 20/20] avg loss: -0.004554862252200547		[learning rate: 0.00022494]
	Learning Rate: 0.00022494
	LOSS [training: -0.0027655197633242876 | validation: 0.0030624233439307336]
	TIME [epoch: 8.73 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004617826756361061		[learning rate: 0.00022459]
		[batch 20/20] avg loss: -0.004756069624369034		[learning rate: 0.00022425]
	Learning Rate: 0.00022425
	LOSS [training: -0.004686948190365047 | validation: 0.011333802874118818]
	TIME [epoch: 8.7 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006943323177718114		[learning rate: 0.00022391]
		[batch 20/20] avg loss: 0.001407631541196982		[learning rate: 0.00022356]
	Learning Rate: 0.000223563
	LOSS [training: -0.0027678458182605658 | validation: -0.0030616461813944176]
	TIME [epoch: 8.73 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004019029978437461		[learning rate: 0.00022322]
		[batch 20/20] avg loss: -0.006166864355971706		[learning rate: 0.00022288]
	Learning Rate: 0.000222878
	LOSS [training: -0.005092947167204583 | validation: -0.002427988784129369]
	TIME [epoch: 8.7 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007435946986399515		[learning rate: 0.00022254]
		[batch 20/20] avg loss: -0.002402843317452907		[learning rate: 0.00022219]
	Learning Rate: 0.000222195
	LOSS [training: -0.004919395151926211 | validation: -0.0016292592592162878]
	TIME [epoch: 8.73 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017160799085950216		[learning rate: 0.00022185]
		[batch 20/20] avg loss: 0.0015121119273580725		[learning rate: 0.00022151]
	Learning Rate: 0.000221513
	LOSS [training: -0.00010198399061847453 | validation: -0.0013965367321290638]
	TIME [epoch: 8.71 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002242309744375732		[learning rate: 0.00022117]
		[batch 20/20] avg loss: -0.004906760695061576		[learning rate: 0.00022083]
	Learning Rate: 0.000220834
	LOSS [training: -0.003574535219718654 | validation: 0.001144683584791053]
	TIME [epoch: 8.73 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008243435788948953		[learning rate: 0.0002205]
		[batch 20/20] avg loss: -0.004276564379884518		[learning rate: 0.00022016]
	Learning Rate: 0.000220157
	LOSS [training: -0.0062600000844167366 | validation: 0.0012305102636706922]
	TIME [epoch: 8.72 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032579328009927342		[learning rate: 0.00021982]
		[batch 20/20] avg loss: -0.0005961716456130866		[learning rate: 0.00021948]
	Learning Rate: 0.000219483
	LOSS [training: -0.00192705222330291 | validation: 0.0006327038846801686]
	TIME [epoch: 8.71 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005053742745752148		[learning rate: 0.00021915]
		[batch 20/20] avg loss: -0.002101547054917385		[learning rate: 0.00021881]
	Learning Rate: 0.00021881
	LOSS [training: -0.0035776449003347673 | validation: 0.0007398065889454162]
	TIME [epoch: 8.74 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003361073475139569		[learning rate: 0.00021847]
		[batch 20/20] avg loss: -0.010374432191825822		[learning rate: 0.00021814]
	Learning Rate: 0.000218139
	LOSS [training: -0.003506679358343126 | validation: -0.004550579129226306]
	TIME [epoch: 8.7 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021881378633724813		[learning rate: 0.0002178]
		[batch 20/20] avg loss: -0.009679490613711424		[learning rate: 0.00021747]
	Learning Rate: 0.00021747
	LOSS [training: -0.0059338142385419535 | validation: -0.004264646494250883]
	TIME [epoch: 8.71 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005996599248497467		[learning rate: 0.00021714]
		[batch 20/20] avg loss: -0.001507719549785081		[learning rate: 0.0002168]
	Learning Rate: 0.000216804
	LOSS [training: -0.0037521593991412744 | validation: -0.006602477118708677]
	TIME [epoch: 8.72 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009105780441991802		[learning rate: 0.00021647]
		[batch 20/20] avg loss: -0.001640028827368392		[learning rate: 0.00021614]
	Learning Rate: 0.000216139
	LOSS [training: -0.005372904634680098 | validation: -0.003056552791276015]
	TIME [epoch: 8.72 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00018512366018448768		[learning rate: 0.00021581]
		[batch 20/20] avg loss: -0.002278674576412002		[learning rate: 0.00021548]
	Learning Rate: 0.000215477
	LOSS [training: -0.001046775458113757 | validation: 0.0008347329685494608]
	TIME [epoch: 8.73 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025561121665761766		[learning rate: 0.00021515]
		[batch 20/20] avg loss: 0.0034096024126097995		[learning rate: 0.00021482]
	Learning Rate: 0.000214816
	LOSS [training: 0.0004267451230168114 | validation: 0.0008688270054155942]
	TIME [epoch: 8.71 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002787552946575662		[learning rate: 0.00021449]
		[batch 20/20] avg loss: -0.003529080524677791		[learning rate: 0.00021416]
	Learning Rate: 0.000214157
	LOSS [training: -0.0019039179096676788 | validation: 0.005389124728899251]
	TIME [epoch: 8.72 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024458088695215583		[learning rate: 0.00021383]
		[batch 20/20] avg loss: -0.003640841700160462		[learning rate: 0.0002135]
	Learning Rate: 0.000213501
	LOSS [training: -0.00304332528484101 | validation: 0.003916891089725035]
	TIME [epoch: 8.72 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014258128420544523		[learning rate: 0.00021317]
		[batch 20/20] avg loss: -0.002905226311289499		[learning rate: 0.00021285]
	Learning Rate: 0.000212847
	LOSS [training: -0.0007397067346175229 | validation: 0.0014971931453334844]
	TIME [epoch: 8.69 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016186981423201691		[learning rate: 0.00021252]
		[batch 20/20] avg loss: -0.0015128103587506742		[learning rate: 0.00021219]
	Learning Rate: 0.000212194
	LOSS [training: -0.0015657542505354218 | validation: -0.0023554354779076634]
	TIME [epoch: 8.74 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004835928028070735		[learning rate: 0.00021187]
		[batch 20/20] avg loss: -0.0060647152027539005		[learning rate: 0.00021154]
	Learning Rate: 0.000211544
	LOSS [training: -0.0054503216154123185 | validation: 0.005092402471897597]
	TIME [epoch: 8.72 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002443501661385181		[learning rate: 0.00021122]
		[batch 20/20] avg loss: -0.0023960498985723068		[learning rate: 0.0002109]
	Learning Rate: 0.000210895
	LOSS [training: -0.0010758498662168943 | validation: 0.0007507240397581602]
	TIME [epoch: 8.73 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00315495862764388		[learning rate: 0.00021057]
		[batch 20/20] avg loss: -0.0019781413461687604		[learning rate: 0.00021025]
	Learning Rate: 0.000210249
	LOSS [training: -0.00256654998690632 | validation: -0.0012706022265726227]
	TIME [epoch: 8.72 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004367745663888185		[learning rate: 0.00020993]
		[batch 20/20] avg loss: -0.0018065528457439253		[learning rate: 0.0002096]
	Learning Rate: 0.000209604
	LOSS [training: -0.003087149254816055 | validation: -0.0005247280061022961]
	TIME [epoch: 8.75 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006519064527861551		[learning rate: 0.00020928]
		[batch 20/20] avg loss: -0.002884028699313743		[learning rate: 0.00020896]
	Learning Rate: 0.000208962
	LOSS [training: -0.0011160611232637941 | validation: -0.0012103287199535953]
	TIME [epoch: 8.7 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005861412035860268		[learning rate: 0.00020864]
		[batch 20/20] avg loss: -0.0010465817331386025		[learning rate: 0.00020832]
	Learning Rate: 0.000208321
	LOSS [training: -0.0034539968844994355 | validation: -0.0010117247598800073]
	TIME [epoch: 8.74 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002276743123779746		[learning rate: 0.000208]
		[batch 20/20] avg loss: -0.0003141582451760508		[learning rate: 0.00020768]
	Learning Rate: 0.000207683
	LOSS [training: -0.0012954506844778985 | validation: -0.001172904088569609]
	TIME [epoch: 8.71 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028675958689359676		[learning rate: 0.00020736]
		[batch 20/20] avg loss: -0.0030908304995811106		[learning rate: 0.00020705]
	Learning Rate: 0.000207046
	LOSS [training: -0.0029792131842585386 | validation: 0.0005290999998536577]
	TIME [epoch: 8.73 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003070762664993988		[learning rate: 0.00020673]
		[batch 20/20] avg loss: -0.006261760938395161		[learning rate: 0.00020641]
	Learning Rate: 0.000206411
	LOSS [training: -0.0032844186024472794 | validation: 0.0032800074469582843]
	TIME [epoch: 8.75 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010788269165334496		[learning rate: 0.00020609]
		[batch 20/20] avg loss: -0.003386890370967917		[learning rate: 0.00020578]
	Learning Rate: 0.000205778
	LOSS [training: -0.0011540317272172333 | validation: -0.006709148755909568]
	TIME [epoch: 8.73 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015384532827463844		[learning rate: 0.00020546]
		[batch 20/20] avg loss: -0.0008034515678646732		[learning rate: 0.00020515]
	Learning Rate: 0.000205148
	LOSS [training: -0.0011709524253055287 | validation: -4.659873523199401e-05]
	TIME [epoch: 8.74 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008359177137370435		[learning rate: 0.00020483]
		[batch 20/20] avg loss: -0.00018683649083500124		[learning rate: 0.00020452]
	Learning Rate: 0.000204519
	LOSS [training: -0.004273006814102719 | validation: 0.0013838419637101783]
	TIME [epoch: 8.69 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005081965367463318		[learning rate: 0.00020421]
		[batch 20/20] avg loss: -0.0029122386736004898		[learning rate: 0.00020389]
	Learning Rate: 0.000203892
	LOSS [training: -0.003997102020531904 | validation: -0.006308051379587534]
	TIME [epoch: 8.73 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006232815097597701		[learning rate: 0.00020358]
		[batch 20/20] avg loss: -0.0019852663767318097		[learning rate: 0.00020327]
	Learning Rate: 0.000203267
	LOSS [training: -0.004109040737164756 | validation: -0.0028601428619937693]
	TIME [epoch: 8.73 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003786573623131018		[learning rate: 0.00020296]
		[batch 20/20] avg loss: -0.0023313090734444117		[learning rate: 0.00020264]
	Learning Rate: 0.000202644
	LOSS [training: -0.0030589413482877146 | validation: -0.001345974105412243]
	TIME [epoch: 8.71 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002425821921242503		[learning rate: 0.00020233]
		[batch 20/20] avg loss: 0.00022285044121917737		[learning rate: 0.00020202]
	Learning Rate: 0.000202023
	LOSS [training: -0.0011014857400116628 | validation: 0.0014081880438169174]
	TIME [epoch: 8.75 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0045663728634616485		[learning rate: 0.00020171]
		[batch 20/20] avg loss: -0.0028925305692963027		[learning rate: 0.0002014]
	Learning Rate: 0.000201403
	LOSS [training: -0.003729451716378976 | validation: 0.0007408732657945335]
	TIME [epoch: 8.73 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003440815029063159		[learning rate: 0.00020109]
		[batch 20/20] avg loss: -0.003933130735642019		[learning rate: 0.00020079]
	Learning Rate: 0.000200786
	LOSS [training: -0.0036869728823525896 | validation: 0.0007559381523347159]
	TIME [epoch: 8.7 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0055926042874195675		[learning rate: 0.00020048]
		[batch 20/20] avg loss: -0.004244100394881503		[learning rate: 0.00020017]
	Learning Rate: 0.00020017
	LOSS [training: -0.004918352341150536 | validation: -0.0014400133947735118]
	TIME [epoch: 8.75 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004544111085845452		[learning rate: 0.00019986]
		[batch 20/20] avg loss: -0.003971299913084372		[learning rate: 0.00019956]
	Learning Rate: 0.000199557
	LOSS [training: -0.004257705499464913 | validation: -0.0032602784256411032]
	TIME [epoch: 8.71 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000826301392506839		[learning rate: 0.00019925]
		[batch 20/20] avg loss: -0.00022414091653345862		[learning rate: 0.00019895]
	Learning Rate: 0.000198945
	LOSS [training: -0.0005252211545201489 | validation: 0.003162686408525648]
	TIME [epoch: 8.73 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0067632375620728215		[learning rate: 0.00019864]
		[batch 20/20] avg loss: -0.0006053286950500097		[learning rate: 0.00019834]
	Learning Rate: 0.000198335
	LOSS [training: -0.003684283128561415 | validation: 0.005855779101137239]
	TIME [epoch: 8.71 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0058117257448437095		[learning rate: 0.00019803]
		[batch 20/20] avg loss: -0.004185349667977993		[learning rate: 0.00019773]
	Learning Rate: 0.000197727
	LOSS [training: -0.004998537706410852 | validation: 0.011041172556382988]
	TIME [epoch: 8.74 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005515945945749405		[learning rate: 0.00019742]
		[batch 20/20] avg loss: -0.006903380261277024		[learning rate: 0.00019712]
	Learning Rate: 0.000197121
	LOSS [training: -0.006209663103513213 | validation: 3.311832293699415e-05]
	TIME [epoch: 8.71 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005862878370107476		[learning rate: 0.00019682]
		[batch 20/20] avg loss: -0.0013358977910004558		[learning rate: 0.00019652]
	Learning Rate: 0.000196517
	LOSS [training: -0.0035993880805539656 | validation: 0.004588173092391612]
	TIME [epoch: 8.71 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002193414505874719		[learning rate: 0.00019622]
		[batch 20/20] avg loss: -0.0015132476221651962		[learning rate: 0.00019591]
	Learning Rate: 0.000195915
	LOSS [training: -0.001853331064019958 | validation: -1.6935281031115303e-05]
	TIME [epoch: 8.69 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011054035919442818		[learning rate: 0.00019561]
		[batch 20/20] avg loss: -0.004228656489371044		[learning rate: 0.00019531]
	Learning Rate: 0.000195314
	LOSS [training: -0.002667030040657663 | validation: -0.008430817683165778]
	TIME [epoch: 8.7 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004205365520486893		[learning rate: 0.00019501]
		[batch 20/20] avg loss: -0.00812930575290984		[learning rate: 0.00019472]
	Learning Rate: 0.000194715
	LOSS [training: -0.006167335636698366 | validation: 0.0019525427366263223]
	TIME [epoch: 8.72 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011439347027359039		[learning rate: 0.00019442]
		[batch 20/20] avg loss: -0.0029023084273918118		[learning rate: 0.00019412]
	Learning Rate: 0.000194118
	LOSS [training: -0.002023121565063857 | validation: 0.000641886409694667]
	TIME [epoch: 8.72 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005025740519775685		[learning rate: 0.00019382]
		[batch 20/20] avg loss: -0.0019869428918965035		[learning rate: 0.00019352]
	Learning Rate: 0.000193523
	LOSS [training: -0.003506341705836094 | validation: 0.0030631022390139756]
	TIME [epoch: 8.72 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00457242842278792		[learning rate: 0.00019323]
		[batch 20/20] avg loss: -0.005090160066541468		[learning rate: 0.00019293]
	Learning Rate: 0.00019293
	LOSS [training: -0.004831294244664694 | validation: -0.002621192180714929]
	TIME [epoch: 8.7 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005185886252424261		[learning rate: 0.00019263]
		[batch 20/20] avg loss: 0.0012192395268411948		[learning rate: 0.00019234]
	Learning Rate: 0.000192339
	LOSS [training: -0.0019833233627915337 | validation: -0.0013114018851764653]
	TIME [epoch: 8.72 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001591370111200381		[learning rate: 0.00019204]
		[batch 20/20] avg loss: -0.0061583216539657145		[learning rate: 0.00019175]
	Learning Rate: 0.000191749
	LOSS [training: -0.0038748458825830475 | validation: -0.00322671790517257]
	TIME [epoch: 8.74 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01233966676664967		[learning rate: 0.00019145]
		[batch 20/20] avg loss: 0.0005819866681047308		[learning rate: 0.00019116]
	Learning Rate: 0.000191161
	LOSS [training: -0.00587884004927247 | validation: -0.0014485725439742118]
	TIME [epoch: 8.71 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007672996447515371		[learning rate: 0.00019087]
		[batch 20/20] avg loss: -0.002895947233527784		[learning rate: 0.00019058]
	Learning Rate: 0.000190575
	LOSS [training: -0.005284471840521577 | validation: 0.001928837417009187]
	TIME [epoch: 8.71 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009506493816202735		[learning rate: 0.00019028]
		[batch 20/20] avg loss: -0.0001352678829947761		[learning rate: 0.00018999]
	Learning Rate: 0.000189991
	LOSS [training: -0.004820880849598757 | validation: -0.0010758742447000477]
	TIME [epoch: 8.71 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004548479882965204		[learning rate: 0.0001897]
		[batch 20/20] avg loss: -0.007426981046586821		[learning rate: 0.00018941]
	Learning Rate: 0.000189409
	LOSS [training: -0.0039409145174416705 | validation: -0.0005348792227849473]
	TIME [epoch: 8.71 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001357480795387099		[learning rate: 0.00018912]
		[batch 20/20] avg loss: -0.0005809691603875318		[learning rate: 0.00018883]
	Learning Rate: 0.000188828
	LOSS [training: -0.0009692249778873154 | validation: -0.0030258734382327714]
	TIME [epoch: 8.76 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035301817605853463		[learning rate: 0.00018854]
		[batch 20/20] avg loss: -0.0057475295641403025		[learning rate: 0.00018825]
	Learning Rate: 0.000188249
	LOSS [training: -0.004638855662362824 | validation: -0.004438394042888412]
	TIME [epoch: 8.7 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004192606425081925		[learning rate: 0.00018796]
		[batch 20/20] avg loss: -0.002902164578709312		[learning rate: 0.00018767]
	Learning Rate: 0.000187672
	LOSS [training: -0.00354738550189562 | validation: 0.004520587898507551]
	TIME [epoch: 8.73 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007530016560520918		[learning rate: 0.00018738]
		[batch 20/20] avg loss: -0.0074115294542431275		[learning rate: 0.0001871]
	Learning Rate: 0.000187097
	LOSS [training: -0.003329263899095518 | validation: 0.000657688944206037]
	TIME [epoch: 8.7 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007525925827645065		[learning rate: 0.00018681]
		[batch 20/20] avg loss: -0.004927707692661177		[learning rate: 0.00018652]
	Learning Rate: 0.000186523
	LOSS [training: -0.006226816760153119 | validation: -0.0001749587220608636]
	TIME [epoch: 8.72 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006667626000754813		[learning rate: 0.00018624]
		[batch 20/20] avg loss: -0.004683092188869509		[learning rate: 0.00018595]
	Learning Rate: 0.000185952
	LOSS [training: -0.00567535909481216 | validation: -0.0011324389034230039]
	TIME [epoch: 8.74 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0057318294737854795		[learning rate: 0.00018567]
		[batch 20/20] avg loss: -0.005706750777917408		[learning rate: 0.00018538]
	Learning Rate: 0.000185382
	LOSS [training: -0.005719290125851443 | validation: -0.0009292783969235717]
	TIME [epoch: 8.72 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006984033149533188		[learning rate: 0.0001851]
		[batch 20/20] avg loss: -0.0011413908814540677		[learning rate: 0.00018481]
	Learning Rate: 0.000184813
	LOSS [training: -0.0040627120154936285 | validation: -0.009654452481129254]
	TIME [epoch: 8.73 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004182166630261772		[learning rate: 0.00018453]
		[batch 20/20] avg loss: -0.006035520073952286		[learning rate: 0.00018425]
	Learning Rate: 0.000184247
	LOSS [training: -0.0051088433521070285 | validation: -0.00149461257086315]
	TIME [epoch: 8.69 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015308610956507034		[learning rate: 0.00018396]
		[batch 20/20] avg loss: -0.007117967416359646		[learning rate: 0.00018368]
	Learning Rate: 0.000183682
	LOSS [training: -0.004324414256005174 | validation: -0.006131215787343764]
	TIME [epoch: 8.76 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004388548402255853		[learning rate: 0.0001834]
		[batch 20/20] avg loss: -0.0014228337485971518		[learning rate: 0.00018312]
	Learning Rate: 0.000183119
	LOSS [training: -0.002905691075426502 | validation: 0.0006615583954938615]
	TIME [epoch: 8.73 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014358085012574418		[learning rate: 0.00018284]
		[batch 20/20] avg loss: -0.004128072474515783		[learning rate: 0.00018256]
	Learning Rate: 0.000182558
	LOSS [training: -0.0027819404878866127 | validation: 0.008266603870492923]
	TIME [epoch: 8.74 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014090335457229701		[learning rate: 0.00018228]
		[batch 20/20] avg loss: -0.008520605369384742		[learning rate: 0.000182]
	Learning Rate: 0.000181998
	LOSS [training: -0.0049648194575538564 | validation: 0.003727455228365137]
	TIME [epoch: 8.76 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026363539856043942		[learning rate: 0.00018172]
		[batch 20/20] avg loss: -0.0011826373933351826		[learning rate: 0.00018144]
	Learning Rate: 0.00018144
	LOSS [training: -0.0019094956894697888 | validation: -0.002266544130391992]
	TIME [epoch: 8.76 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003298725189497967		[learning rate: 0.00018116]
		[batch 20/20] avg loss: -0.00270959408308823		[learning rate: 0.00018088]
	Learning Rate: 0.000180884
	LOSS [training: -0.0030041596362930982 | validation: 0.0009107974513356441]
	TIME [epoch: 8.79 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007329116114938752		[learning rate: 0.00018061]
		[batch 20/20] avg loss: -0.0039639835265288526		[learning rate: 0.00018033]
	Learning Rate: 0.000180329
	LOSS [training: -0.005646549820733803 | validation: 0.0010659261759155922]
	TIME [epoch: 8.72 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030133534658557453		[learning rate: 0.00018005]
		[batch 20/20] avg loss: -0.006760874074060161		[learning rate: 0.00017978]
	Learning Rate: 0.000179777
	LOSS [training: -0.004887113769957953 | validation: 0.004494268928056391]
	TIME [epoch: 8.75 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005270306955247856		[learning rate: 0.0001795]
		[batch 20/20] avg loss: -0.0034031379985529084		[learning rate: 0.00017923]
	Learning Rate: 0.000179226
	LOSS [training: -0.004336722476900382 | validation: 0.0017892267286539246]
	TIME [epoch: 8.77 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038168294052282952		[learning rate: 0.00017895]
		[batch 20/20] avg loss: -0.0012718756836140102		[learning rate: 0.00017868]
	Learning Rate: 0.000178676
	LOSS [training: -0.002544352544421153 | validation: -0.006837237142954305]
	TIME [epoch: 8.72 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008386644530323364		[learning rate: 0.0001784]
		[batch 20/20] avg loss: -0.0025254765336354466		[learning rate: 0.00017813]
	Learning Rate: 0.000178128
	LOSS [training: -0.005456060531979405 | validation: -0.005889027367524637]
	TIME [epoch: 8.76 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006699181188742067		[learning rate: 0.00017786]
		[batch 20/20] avg loss: -0.00026360059982531746		[learning rate: 0.00017758]
	Learning Rate: 0.000177582
	LOSS [training: -0.0034813908942836922 | validation: 0.00023586289631870815]
	TIME [epoch: 8.73 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006818238249635245		[learning rate: 0.00017731]
		[batch 20/20] avg loss: -0.0060451174220525344		[learning rate: 0.00017704]
	Learning Rate: 0.000177038
	LOSS [training: -0.006431677835843889 | validation: 0.0038441334436587534]
	TIME [epoch: 8.71 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00551642911872585		[learning rate: 0.00017677]
		[batch 20/20] avg loss: -0.0040422868515112564		[learning rate: 0.0001765]
	Learning Rate: 0.000176495
	LOSS [training: -0.004779357985118553 | validation: -0.003458404741747795]
	TIME [epoch: 8.73 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009698798718513034		[learning rate: 0.00017622]
		[batch 20/20] avg loss: -0.004938210980575006		[learning rate: 0.00017595]
	Learning Rate: 0.000175954
	LOSS [training: -0.00731850484954402 | validation: 0.0032310539174498993]
	TIME [epoch: 8.74 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006301397508566067		[learning rate: 0.00017568]
		[batch 20/20] avg loss: -0.003599672727075463		[learning rate: 0.00017541]
	Learning Rate: 0.000175415
	LOSS [training: -0.004950535117820765 | validation: -0.0009050609663929261]
	TIME [epoch: 8.74 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00315550766496731		[learning rate: 0.00017515]
		[batch 20/20] avg loss: 0.00040173938207317174		[learning rate: 0.00017488]
	Learning Rate: 0.000174877
	LOSS [training: -0.001376884141447069 | validation: 0.00012165661282587158]
	TIME [epoch: 8.71 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030342091597625543		[learning rate: 0.00017461]
		[batch 20/20] avg loss: -0.00696368151568853		[learning rate: 0.00017434]
	Learning Rate: 0.000174341
	LOSS [training: -0.004998945337725542 | validation: -0.00774781279470559]
	TIME [epoch: 8.71 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006404998196043217		[learning rate: 0.00017407]
		[batch 20/20] avg loss: -0.0061284951445904085		[learning rate: 0.00017381]
	Learning Rate: 0.000173807
	LOSS [training: -0.006266746670316814 | validation: -0.0039084692408063865]
	TIME [epoch: 8.73 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00583764362399866		[learning rate: 0.00017354]
		[batch 20/20] avg loss: 0.0003904892425393265		[learning rate: 0.00017327]
	Learning Rate: 0.000173274
	LOSS [training: -0.002723577190729667 | validation: -0.004912300958425493]
	TIME [epoch: 8.75 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00598992741328232		[learning rate: 0.00017301]
		[batch 20/20] avg loss: -0.006562705610315023		[learning rate: 0.00017274]
	Learning Rate: 0.000172743
	LOSS [training: -0.006276316511798672 | validation: -0.00019636921569064964]
	TIME [epoch: 8.71 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006325089694088386		[learning rate: 0.00017248]
		[batch 20/20] avg loss: -0.006882361998555317		[learning rate: 0.00017221]
	Learning Rate: 0.000172213
	LOSS [training: -0.006603725846321852 | validation: 0.001537061865159579]
	TIME [epoch: 8.71 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006935301748481554		[learning rate: 0.00017195]
		[batch 20/20] avg loss: -0.006111111888583895		[learning rate: 0.00017169]
	Learning Rate: 0.000171685
	LOSS [training: -0.006523206818532723 | validation: -0.002899512443757212]
	TIME [epoch: 8.73 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004884112293622143		[learning rate: 0.00017142]
		[batch 20/20] avg loss: -0.006415505051517538		[learning rate: 0.00017116]
	Learning Rate: 0.000171159
	LOSS [training: -0.005649808672569839 | validation: -0.00420013130905157]
	TIME [epoch: 8.71 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004734029883846715		[learning rate: 0.0001709]
		[batch 20/20] avg loss: -0.008833642430206729		[learning rate: 0.00017063]
	Learning Rate: 0.000170634
	LOSS [training: -0.006783836157026722 | validation: -0.0055708032263819445]
	TIME [epoch: 8.73 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007308867238355643		[learning rate: 0.00017037]
		[batch 20/20] avg loss: -0.00338217527273301		[learning rate: 0.00017011]
	Learning Rate: 0.000170111
	LOSS [training: -0.005345521255544326 | validation: 0.00162908803539225]
	TIME [epoch: 8.74 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031872662449953953		[learning rate: 0.00016985]
		[batch 20/20] avg loss: -0.0013937759204914843		[learning rate: 0.00016959]
	Learning Rate: 0.00016959
	LOSS [training: -0.0022905210827434395 | validation: -0.000362401232815251]
	TIME [epoch: 8.72 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022662121120258067		[learning rate: 0.00016933]
		[batch 20/20] avg loss: -0.00915390979127224		[learning rate: 0.00016907]
	Learning Rate: 0.00016907
	LOSS [training: -0.005710060951649023 | validation: -0.0019808505987972454]
	TIME [epoch: 8.71 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008038801483324677		[learning rate: 0.00016881]
		[batch 20/20] avg loss: -0.0009456281159617569		[learning rate: 0.00016855]
	Learning Rate: 0.000168552
	LOSS [training: -0.0044922147996432175 | validation: -0.008249634332869814]
	TIME [epoch: 8.72 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038032489327573444		[learning rate: 0.00016829]
		[batch 20/20] avg loss: -0.005079313883051052		[learning rate: 0.00016804]
	Learning Rate: 0.000168035
	LOSS [training: -0.004441281407904197 | validation: 0.0006331229104595468]
	TIME [epoch: 8.74 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004558592891311316		[learning rate: 0.00016778]
		[batch 20/20] avg loss: -0.004205964796613105		[learning rate: 0.00016752]
	Learning Rate: 0.00016752
	LOSS [training: -0.004382278843962211 | validation: -0.005760207432721575]
	TIME [epoch: 8.7 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003352951571187945		[learning rate: 0.00016726]
		[batch 20/20] avg loss: -0.005934687875016572		[learning rate: 0.00016701]
	Learning Rate: 0.000167006
	LOSS [training: -0.004643819723102259 | validation: -0.005109780627214483]
	TIME [epoch: 8.73 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005629288522385802		[learning rate: 0.00016675]
		[batch 20/20] avg loss: -0.0036128121927393336		[learning rate: 0.00016649]
	Learning Rate: 0.000166495
	LOSS [training: -0.004621050357562568 | validation: -0.005500986062626504]
	TIME [epoch: 8.72 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004683615555875177		[learning rate: 0.00016624]
		[batch 20/20] avg loss: -0.004923398219687325		[learning rate: 0.00016598]
	Learning Rate: 0.000165984
	LOSS [training: -0.00480350688778125 | validation: -0.0003793703378242551]
	TIME [epoch: 8.7 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003996003898566137		[learning rate: 0.00016573]
		[batch 20/20] avg loss: -0.003008823388327496		[learning rate: 0.00016548]
	Learning Rate: 0.000165475
	LOSS [training: -0.003502413643446816 | validation: 0.0013594851311840373]
	TIME [epoch: 8.73 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020085566525573466		[learning rate: 0.00016522]
		[batch 20/20] avg loss: -0.00529398450114338		[learning rate: 0.00016497]
	Learning Rate: 0.000164968
	LOSS [training: -0.001642713924293017 | validation: -0.002219768417243587]
	TIME [epoch: 8.71 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006633855305134227		[learning rate: 0.00016472]
		[batch 20/20] avg loss: -0.004179618651545039		[learning rate: 0.00016446]
	Learning Rate: 0.000164462
	LOSS [training: -0.005406736978339635 | validation: -0.00506552514432806]
	TIME [epoch: 8.72 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006621565007357587		[learning rate: 0.00016421]
		[batch 20/20] avg loss: -0.0020412166488015648		[learning rate: 0.00016396]
	Learning Rate: 0.000163958
	LOSS [training: -0.004331390828079576 | validation: -0.005631878670854098]
	TIME [epoch: 8.71 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004005431392544183		[learning rate: 0.00016371]
		[batch 20/20] avg loss: -0.006435448201837671		[learning rate: 0.00016346]
	Learning Rate: 0.000163456
	LOSS [training: -0.005220439797190926 | validation: -0.004607680040963768]
	TIME [epoch: 8.73 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004909629492199317		[learning rate: 0.0001632]
		[batch 20/20] avg loss: 0.0009102668065342111		[learning rate: 0.00016295]
	Learning Rate: 0.000162955
	LOSS [training: -0.0019996813428325537 | validation: -0.0025457051406126835]
	TIME [epoch: 8.72 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007583658431473702		[learning rate: 0.0001627]
		[batch 20/20] avg loss: 0.0037191047069676498		[learning rate: 0.00016246]
	Learning Rate: 0.000162455
	LOSS [training: -0.0019322768622530254 | validation: -0.0009875262934822887]
	TIME [epoch: 8.71 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00528377818120476		[learning rate: 0.00016221]
		[batch 20/20] avg loss: -0.009136520307625646		[learning rate: 0.00016196]
	Learning Rate: 0.000161957
	LOSS [training: -0.007210149244415202 | validation: -0.004228809831308884]
	TIME [epoch: 8.7 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004089116999608034		[learning rate: 0.00016171]
		[batch 20/20] avg loss: -0.004574657677536803		[learning rate: 0.00016146]
	Learning Rate: 0.000161461
	LOSS [training: -0.004331887338572418 | validation: 0.0010849395860242916]
	TIME [epoch: 8.73 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009120373750197766		[learning rate: 0.00016121]
		[batch 20/20] avg loss: -0.0022851438179349857		[learning rate: 0.00016097]
	Learning Rate: 0.000160966
	LOSS [training: -0.0006865532214576045 | validation: -0.0015290630286822955]
	TIME [epoch: 8.75 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023493437989623328		[learning rate: 0.00016072]
		[batch 20/20] avg loss: -0.005632149800121065		[learning rate: 0.00016047]
	Learning Rate: 0.000160472
	LOSS [training: -0.003990746799541699 | validation: -0.006291498445893262]
	TIME [epoch: 8.73 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001330297576712003		[learning rate: 0.00016023]
		[batch 20/20] avg loss: -0.0038406797053959856		[learning rate: 0.00015998]
	Learning Rate: 0.00015998
	LOSS [training: -0.0025854886410539945 | validation: 0.001797395330565285]
	TIME [epoch: 8.73 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0069808959003637245		[learning rate: 0.00015973]
		[batch 20/20] avg loss: -0.004585752126013235		[learning rate: 0.00015949]
	Learning Rate: 0.00015949
	LOSS [training: -0.00578332401318848 | validation: 0.006390258933557548]
	TIME [epoch: 8.73 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00980163064528954		[learning rate: 0.00015925]
		[batch 20/20] avg loss: -0.001494067578319519		[learning rate: 0.000159]
	Learning Rate: 0.000159001
	LOSS [training: -0.005647849111804529 | validation: 0.001905248077120985]
	TIME [epoch: 8.71 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034220207482274097		[learning rate: 0.00015876]
		[batch 20/20] avg loss: -0.0024982937371957285		[learning rate: 0.00015851]
	Learning Rate: 0.000158514
	LOSS [training: -0.0029601572427115684 | validation: -0.0004576580776554425]
	TIME [epoch: 8.75 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042344927268291145		[learning rate: 0.00015827]
		[batch 20/20] avg loss: -0.005500718160572418		[learning rate: 0.00015803]
	Learning Rate: 0.000158028
	LOSS [training: -0.004867605443700766 | validation: 0.002675575638457619]
	TIME [epoch: 8.71 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003484888130334189		[learning rate: 0.00015779]
		[batch 20/20] avg loss: -0.005720126487801958		[learning rate: 0.00015754]
	Learning Rate: 0.000157543
	LOSS [training: -0.004602507309068074 | validation: 0.0028216951301023663]
	TIME [epoch: 8.72 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023931680394500662		[learning rate: 0.0001573]
		[batch 20/20] avg loss: -0.004552363942743976		[learning rate: 0.00015706]
	Learning Rate: 0.00015706
	LOSS [training: -0.003472765991097021 | validation: 0.004642844826810606]
	TIME [epoch: 8.72 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004568810988624744		[learning rate: 0.00015682]
		[batch 20/20] avg loss: -0.008877458956155123		[learning rate: 0.00015658]
	Learning Rate: 0.000156579
	LOSS [training: -0.006723134972389934 | validation: 0.0002467356606467937]
	TIME [epoch: 8.72 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004094393977413165		[learning rate: 0.00015634]
		[batch 20/20] avg loss: -0.0032765200297716636		[learning rate: 0.0001561]
	Learning Rate: 0.000156099
	LOSS [training: -0.003685457003592414 | validation: -0.002070465668832376]
	TIME [epoch: 8.74 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0051877772551531896		[learning rate: 0.00015586]
		[batch 20/20] avg loss: -0.005900131043816417		[learning rate: 0.00015562]
	Learning Rate: 0.00015562
	LOSS [training: -0.005543954149484804 | validation: 0.0018815746754029287]
	TIME [epoch: 8.71 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003553445449002961		[learning rate: 0.00015538]
		[batch 20/20] avg loss: -0.00410036551273744		[learning rate: 0.00015514]
	Learning Rate: 0.000155143
	LOSS [training: -0.003826905480870201 | validation: -0.0013096108818948997]
	TIME [epoch: 8.69 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008986077094357841		[learning rate: 0.00015491]
		[batch 20/20] avg loss: -0.004706416730010717		[learning rate: 0.00015467]
	Learning Rate: 0.000154668
	LOSS [training: -0.006846246912184278 | validation: -0.005263582701900545]
	TIME [epoch: 8.71 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004475448278402208		[learning rate: 0.00015443]
		[batch 20/20] avg loss: -0.004665585261146138		[learning rate: 0.00015419]
	Learning Rate: 0.000154194
	LOSS [training: -0.004570516769774173 | validation: -0.001555792545321913]
	TIME [epoch: 8.72 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005384740661810734		[learning rate: 0.00015396]
		[batch 20/20] avg loss: -0.00481083131194068		[learning rate: 0.00015372]
	Learning Rate: 0.000153721
	LOSS [training: -0.005097785986875707 | validation: 0.0029074218206424107]
	TIME [epoch: 8.75 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019045364170950214		[learning rate: 0.00015349]
		[batch 20/20] avg loss: -0.003756439301505513		[learning rate: 0.00015325]
	Learning Rate: 0.00015325
	LOSS [training: -0.0028304878593002672 | validation: 0.0019827434242601512]
	TIME [epoch: 8.72 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007952828224942433		[learning rate: 0.00015301]
		[batch 20/20] avg loss: -0.007596223773735739		[learning rate: 0.00015278]
	Learning Rate: 0.00015278
	LOSS [training: -0.0077745259993390855 | validation: 1.438922145054135e-05]
	TIME [epoch: 8.72 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005357246577623229		[learning rate: 0.00015255]
		[batch 20/20] avg loss: -0.004778846954394576		[learning rate: 0.00015231]
	Learning Rate: 0.000152312
	LOSS [training: -0.005068046766008903 | validation: 0.0019983157581171082]
	TIME [epoch: 8.72 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032382314254418136		[learning rate: 0.00015208]
		[batch 20/20] avg loss: -0.0067198677003365796		[learning rate: 0.00015184]
	Learning Rate: 0.000151845
	LOSS [training: -0.0049790495628891955 | validation: -0.0050449364655376335]
	TIME [epoch: 8.73 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007915535755611817		[learning rate: 0.00015161]
		[batch 20/20] avg loss: -0.003992240661236136		[learning rate: 0.00015138]
	Learning Rate: 0.000151379
	LOSS [training: -0.005953888208423976 | validation: -0.0022071912277959946]
	TIME [epoch: 8.72 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005388334087613052		[learning rate: 0.00015115]
		[batch 20/20] avg loss: -0.0023443466157594775		[learning rate: 0.00015092]
	Learning Rate: 0.000150915
	LOSS [training: -0.003866340351686265 | validation: 0.0031270145584429486]
	TIME [epoch: 8.73 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003281463445816591		[learning rate: 0.00015068]
		[batch 20/20] avg loss: -0.005200567813052339		[learning rate: 0.00015045]
	Learning Rate: 0.000150453
	LOSS [training: -0.000959552183617873 | validation: -0.0010586032704513818]
	TIME [epoch: 8.73 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007238036875117489		[learning rate: 0.00015022]
		[batch 20/20] avg loss: 0.000747471406209486		[learning rate: 0.00014999]
	Learning Rate: 0.000149991
	LOSS [training: -0.003245282734454002 | validation: -0.006184000861248428]
	TIME [epoch: 8.72 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002984717832726953		[learning rate: 0.00014976]
		[batch 20/20] avg loss: -0.004788423106714946		[learning rate: 0.00014953]
	Learning Rate: 0.000149532
	LOSS [training: -0.00254344744499382 | validation: -0.004018418083943781]
	TIME [epoch: 8.72 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005368139617982783		[learning rate: 0.0001493]
		[batch 20/20] avg loss: -0.005880074326403884		[learning rate: 0.00014907]
	Learning Rate: 0.000149073
	LOSS [training: -0.005624106972193335 | validation: 0.00021364322344577554]
	TIME [epoch: 8.72 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007461558483893996		[learning rate: 0.00014884]
		[batch 20/20] avg loss: -0.007172920942788996		[learning rate: 0.00014862]
	Learning Rate: 0.000148616
	LOSS [training: -0.007317239713341496 | validation: -0.003664095898022193]
	TIME [epoch: 8.72 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009357394316179218		[learning rate: 0.00014839]
		[batch 20/20] avg loss: -0.0025229740331678533		[learning rate: 0.00014816]
	Learning Rate: 0.000148161
	LOSS [training: -0.0059401841746735354 | validation: -0.0060035541012269595]
	TIME [epoch: 8.74 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004148928259053858		[learning rate: 0.00014793]
		[batch 20/20] avg loss: -0.0041650764119148444		[learning rate: 0.00014771]
	Learning Rate: 0.000147707
	LOSS [training: -0.004157002335484351 | validation: 0.003320197559374246]
	TIME [epoch: 8.72 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006595941434441784		[learning rate: 0.00014748]
		[batch 20/20] avg loss: -0.00593010179301784		[learning rate: 0.00014725]
	Learning Rate: 0.000147254
	LOSS [training: -0.006263021613729812 | validation: 0.0003776377752945487]
	TIME [epoch: 8.73 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0058525904081945124		[learning rate: 0.00014703]
		[batch 20/20] avg loss: -0.0061902314257071215		[learning rate: 0.0001468]
	Learning Rate: 0.000146802
	LOSS [training: -0.006021410916950817 | validation: -0.0049369862096587565]
	TIME [epoch: 8.71 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009032605993743559		[learning rate: 0.00014658]
		[batch 20/20] avg loss: -0.0029326779761675533		[learning rate: 0.00014635]
	Learning Rate: 0.000146352
	LOSS [training: -0.005982641984955558 | validation: 0.003134900802039898]
	TIME [epoch: 8.71 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005053442370120659		[learning rate: 0.00014613]
		[batch 20/20] avg loss: -0.005874626396925485		[learning rate: 0.0001459]
	Learning Rate: 0.000145904
	LOSS [training: -0.005464034383523071 | validation: -0.0027609729207059373]
	TIME [epoch: 8.73 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003920377639410314		[learning rate: 0.00014568]
		[batch 20/20] avg loss: -0.004182173203653598		[learning rate: 0.00014546]
	Learning Rate: 0.000145457
	LOSS [training: -0.004051275421531955 | validation: -0.0026604439547463794]
	TIME [epoch: 8.71 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008011719749745443		[learning rate: 0.00014523]
		[batch 20/20] avg loss: -0.0008625670211053222		[learning rate: 0.00014501]
	Learning Rate: 0.000145011
	LOSS [training: -0.004437143385425382 | validation: -0.0028404204584852218]
	TIME [epoch: 8.74 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003780849398103989		[learning rate: 0.00014479]
		[batch 20/20] avg loss: -0.007683981052763608		[learning rate: 0.00014457]
	Learning Rate: 0.000144566
	LOSS [training: -0.005732415225433799 | validation: 0.0007189619293640533]
	TIME [epoch: 8.74 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004657262173449218		[learning rate: 0.00014434]
		[batch 20/20] avg loss: -0.0048235136789091535		[learning rate: 0.00014412]
	Learning Rate: 0.000144123
	LOSS [training: -0.004740387926179186 | validation: 0.0009993639748485917]
	TIME [epoch: 8.72 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002962402672267726		[learning rate: 0.0001439]
		[batch 20/20] avg loss: -0.004927278527682088		[learning rate: 0.00014368]
	Learning Rate: 0.000143681
	LOSS [training: -0.003944840599974906 | validation: -0.0019972128641827774]
	TIME [epoch: 8.74 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006592081425950927		[learning rate: 0.00014346]
		[batch 20/20] avg loss: -0.0044941744170880595		[learning rate: 0.00014324]
	Learning Rate: 0.000143241
	LOSS [training: -0.005543127921519493 | validation: 0.0021701775462901907]
	TIME [epoch: 8.74 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034605152344192925		[learning rate: 0.00014302]
		[batch 20/20] avg loss: -0.005917424208874323		[learning rate: 0.0001428]
	Learning Rate: 0.000142802
	LOSS [training: -0.004688969721646808 | validation: -0.0014981528714611797]
	TIME [epoch: 8.73 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002356594109667679		[learning rate: 0.00014258]
		[batch 20/20] avg loss: -0.0061403989448082395		[learning rate: 0.00014236]
	Learning Rate: 0.000142364
	LOSS [training: -0.00424849652723796 | validation: -0.00451859462932992]
	TIME [epoch: 8.72 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004502399982017429		[learning rate: 0.00014215]
		[batch 20/20] avg loss: -0.00933153097710288		[learning rate: 0.00014193]
	Learning Rate: 0.000141928
	LOSS [training: -0.004440645489450569 | validation: 0.0017432925927382827]
	TIME [epoch: 8.71 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005829725571277581		[learning rate: 0.00014171]
		[batch 20/20] avg loss: -0.003427063248765392		[learning rate: 0.00014149]
	Learning Rate: 0.000141492
	LOSS [training: -0.004628394410021487 | validation: -0.0065347464823094035]
	TIME [epoch: 8.73 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006402395561985014		[learning rate: 0.00014128]
		[batch 20/20] avg loss: 0.001635295320175103		[learning rate: 0.00014106]
	Learning Rate: 0.000141059
	LOSS [training: -0.0023835501209049557 | validation: -0.002515802959547983]
	TIME [epoch: 8.75 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005774731658288692		[learning rate: 0.00014084]
		[batch 20/20] avg loss: -0.008005091685288096		[learning rate: 0.00014063]
	Learning Rate: 0.000140626
	LOSS [training: -0.006889911671788393 | validation: 0.00350766207338129]
	TIME [epoch: 8.74 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002573335950742234		[learning rate: 0.00014041]
		[batch 20/20] avg loss: -0.003305013347705477		[learning rate: 0.0001402]
	Learning Rate: 0.000140195
	LOSS [training: -0.002939174649223856 | validation: -0.006575508359503371]
	TIME [epoch: 8.7 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008115387695767732		[learning rate: 0.00013998]
		[batch 20/20] avg loss: -0.006380125164057081		[learning rate: 0.00013977]
	Learning Rate: 0.000139765
	LOSS [training: -0.007247756429912405 | validation: -0.005750789570661845]
	TIME [epoch: 8.73 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007495217491961886		[learning rate: 0.00013955]
		[batch 20/20] avg loss: -0.00382723493240684		[learning rate: 0.00013934]
	Learning Rate: 0.000139337
	LOSS [training: -0.005661226212184362 | validation: -0.0041400345426529]
	TIME [epoch: 8.7 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005612438876295778		[learning rate: 0.00013912]
		[batch 20/20] avg loss: -0.004846403001801354		[learning rate: 0.00013891]
	Learning Rate: 0.00013891
	LOSS [training: -0.005229420939048566 | validation: -0.00012565625579257032]
	TIME [epoch: 8.76 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010430545167040097		[learning rate: 0.0001387]
		[batch 20/20] avg loss: -0.006921133901396417		[learning rate: 0.00013848]
	Learning Rate: 0.000138484
	LOSS [training: -0.002939039692346204 | validation: 0.001059903864897975]
	TIME [epoch: 8.73 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009485131720728686		[learning rate: 0.00013827]
		[batch 20/20] avg loss: -0.005277758555427663		[learning rate: 0.00013806]
	Learning Rate: 0.00013806
	LOSS [training: -0.007381445138078174 | validation: -0.0013457275025825727]
	TIME [epoch: 8.7 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003775831973941701		[learning rate: 0.00013785]
		[batch 20/20] avg loss: -0.006494271305822577		[learning rate: 0.00013764]
	Learning Rate: 0.000137636
	LOSS [training: -0.005135051639882139 | validation: -0.0071266368311748915]
	TIME [epoch: 8.71 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003822962159154055		[learning rate: 0.00013743]
		[batch 20/20] avg loss: -0.0021155661963118415		[learning rate: 0.00013721]
	Learning Rate: 0.000137215
	LOSS [training: -0.0029692641777329483 | validation: -0.001011972623198922]
	TIME [epoch: 8.7 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006981853006930006		[learning rate: 0.000137]
		[batch 20/20] avg loss: -0.004336654814618486		[learning rate: 0.00013679]
	Learning Rate: 0.000136794
	LOSS [training: -0.0056592539107742455 | validation: -0.0021466998674468077]
	TIME [epoch: 8.71 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017420639593627098		[learning rate: 0.00013658]
		[batch 20/20] avg loss: -0.005113929995365217		[learning rate: 0.00013637]
	Learning Rate: 0.000136375
	LOSS [training: -0.003427996977363964 | validation: 0.0006550371377280141]
	TIME [epoch: 8.7 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038282011903172106		[learning rate: 0.00013617]
		[batch 20/20] avg loss: -0.004930611768139917		[learning rate: 0.00013596]
	Learning Rate: 0.000135956
	LOSS [training: -0.004379406479228564 | validation: -0.0031435776557070657]
	TIME [epoch: 8.71 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014856642606722037		[learning rate: 0.00013575]
		[batch 20/20] avg loss: -0.010663656008404267		[learning rate: 0.00013554]
	Learning Rate: 0.00013554
	LOSS [training: -0.0060746601345382355 | validation: -0.0007014514705137499]
	TIME [epoch: 8.69 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004615062373150391		[learning rate: 0.00013533]
		[batch 20/20] avg loss: -0.0026509536552778443		[learning rate: 0.00013512]
	Learning Rate: 0.000135124
	LOSS [training: -0.0036330080142141182 | validation: 0.00040493642267530475]
	TIME [epoch: 8.71 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006639569455954371		[learning rate: 0.00013492]
		[batch 20/20] avg loss: -0.007040079777424093		[learning rate: 0.00013471]
	Learning Rate: 0.00013471
	LOSS [training: -0.006839824616689231 | validation: -0.004165264307202526]
	TIME [epoch: 8.7 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004649630856995719		[learning rate: 0.0001345]
		[batch 20/20] avg loss: -0.00642204800889464		[learning rate: 0.0001343]
	Learning Rate: 0.000134297
	LOSS [training: -0.0055358394329451804 | validation: 0.0010265527191183244]
	TIME [epoch: 8.69 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01149277143463895		[learning rate: 0.00013409]
		[batch 20/20] avg loss: -0.00013798885395497246		[learning rate: 0.00013389]
	Learning Rate: 0.000133885
	LOSS [training: -0.00581538014429696 | validation: -0.0030502787487886687]
	TIME [epoch: 8.69 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020209001029895873		[learning rate: 0.00013368]
		[batch 20/20] avg loss: -0.0023133063989548277		[learning rate: 0.00013347]
	Learning Rate: 0.000133475
	LOSS [training: -0.0021671032509722075 | validation: -7.188634324997766e-05]
	TIME [epoch: 8.71 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009954110433731175		[learning rate: 0.00013327]
		[batch 20/20] avg loss: -0.004087148343915379		[learning rate: 0.00013307]
	Learning Rate: 0.000133066
	LOSS [training: -0.007020629388823278 | validation: -0.0018181472996263081]
	TIME [epoch: 8.72 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0030730009581927302		[learning rate: 0.00013286]
		[batch 20/20] avg loss: -0.00788305614510272		[learning rate: 0.00013266]
	Learning Rate: 0.000132658
	LOSS [training: -0.005478028551647725 | validation: -5.2287723752928936e-05]
	TIME [epoch: 8.7 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005500816591167764		[learning rate: 0.00013245]
		[batch 20/20] avg loss: -0.005255244339261046		[learning rate: 0.00013225]
	Learning Rate: 0.000132251
	LOSS [training: -0.005378030465214406 | validation: 0.010355520053854794]
	TIME [epoch: 8.69 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0039756133783186275		[learning rate: 0.00013205]
		[batch 20/20] avg loss: -0.005222793091856845		[learning rate: 0.00013185]
	Learning Rate: 0.000131846
	LOSS [training: -0.0045992032350877345 | validation: -0.00016723004753934332]
	TIME [epoch: 8.68 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005962015795047318		[learning rate: 0.00013164]
		[batch 20/20] avg loss: -0.005543321875850704		[learning rate: 0.00013144]
	Learning Rate: 0.000131442
	LOSS [training: -0.005752668835449012 | validation: -0.0024443345163359972]
	TIME [epoch: 8.69 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004245963167813669		[learning rate: 0.00013124]
		[batch 20/20] avg loss: 0.0008105275207819287		[learning rate: 0.00013104]
	Learning Rate: 0.000131039
	LOSS [training: -0.0017177178235158706 | validation: -0.004200523344126084]
	TIME [epoch: 8.72 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006501729413553934		[learning rate: 0.00013084]
		[batch 20/20] avg loss: -0.0019300326590802309		[learning rate: 0.00013064]
	Learning Rate: 0.000130637
	LOSS [training: -0.0042158810363170825 | validation: -0.0003267730433646076]
	TIME [epoch: 8.7 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008209230505465128		[learning rate: 0.00013044]
		[batch 20/20] avg loss: -0.00035536156993326676		[learning rate: 0.00013024]
	Learning Rate: 0.000130237
	LOSS [training: -0.004282296037699197 | validation: -0.0016586654413436879]
	TIME [epoch: 8.69 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006957703103040444		[learning rate: 0.00013004]
		[batch 20/20] avg loss: -0.0012555539785468355		[learning rate: 0.00012984]
	Learning Rate: 0.000129837
	LOSS [training: -0.004106628540793639 | validation: -0.005676585321513584]
	TIME [epoch: 8.72 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009246362053162394		[learning rate: 0.00012964]
		[batch 20/20] avg loss: -0.003067466722645611		[learning rate: 0.00012944]
	Learning Rate: 0.000129439
	LOSS [training: -0.006156914387904002 | validation: -0.008896339579695326]
	TIME [epoch: 8.69 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027991198271086495		[learning rate: 0.00012924]
		[batch 20/20] avg loss: -0.004279628336592336		[learning rate: 0.00012904]
	Learning Rate: 0.000129043
	LOSS [training: -0.003539374081850492 | validation: 0.0006335615040886586]
	TIME [epoch: 8.72 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010049829643940047		[learning rate: 0.00012884]
		[batch 20/20] avg loss: -0.004189119826823761		[learning rate: 0.00012865]
	Learning Rate: 0.000128647
	LOSS [training: -0.007119474735381904 | validation: -0.00391011467803178]
	TIME [epoch: 8.68 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009558069670671684		[learning rate: 0.00012845]
		[batch 20/20] avg loss: -0.005546512858495995		[learning rate: 0.00012825]
	Learning Rate: 0.000128253
	LOSS [training: -0.007552291264583837 | validation: -0.004509492183480461]
	TIME [epoch: 8.67 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005910684145637847		[learning rate: 0.00012806]
		[batch 20/20] avg loss: -0.005930060573227451		[learning rate: 0.00012786]
	Learning Rate: 0.00012786
	LOSS [training: -0.005920372359432649 | validation: -0.0011005374464536856]
	TIME [epoch: 8.66 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0057395674460982435		[learning rate: 0.00012766]
		[batch 20/20] avg loss: -0.001167642589484558		[learning rate: 0.00012747]
	Learning Rate: 0.000127468
	LOSS [training: -0.0034536050177914 | validation: 0.001479020621131406]
	TIME [epoch: 8.69 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005373255233923228		[learning rate: 0.00012727]
		[batch 20/20] avg loss: -0.006474557776536161		[learning rate: 0.00012708]
	Learning Rate: 0.000127077
	LOSS [training: -0.0059239065052296945 | validation: -0.002469633988977588]
	TIME [epoch: 8.71 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005897762189847407		[learning rate: 0.00012688]
		[batch 20/20] avg loss: -0.0033851581953110803		[learning rate: 0.00012669]
	Learning Rate: 0.000126687
	LOSS [training: -0.004641460192579244 | validation: -0.0018121341182624402]
	TIME [epoch: 8.72 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009512539804622919		[learning rate: 0.00012649]
		[batch 20/20] avg loss: -0.0034169280749446486		[learning rate: 0.0001263]
	Learning Rate: 0.000126299
	LOSS [training: -0.0021840910277034697 | validation: -0.0010157354223134377]
	TIME [epoch: 8.69 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036906004078073123		[learning rate: 0.00012611]
		[batch 20/20] avg loss: -0.005440401510170207		[learning rate: 0.00012591]
	Learning Rate: 0.000125912
	LOSS [training: -0.00456550095898876 | validation: 0.0003448712936200888]
	TIME [epoch: 8.68 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004017541092130579		[learning rate: 0.00012572]
		[batch 20/20] avg loss: -0.008201662117087633		[learning rate: 0.00012553]
	Learning Rate: 0.000125526
	LOSS [training: -0.006109601604609105 | validation: -0.003572124002554667]
	TIME [epoch: 8.7 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032270786520601238		[learning rate: 0.00012533]
		[batch 20/20] avg loss: -0.008100943946501682		[learning rate: 0.00012514]
	Learning Rate: 0.000125141
	LOSS [training: -0.005664011299280902 | validation: -0.0034260161151101493]
	TIME [epoch: 8.69 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008175835400132366		[learning rate: 0.00012495]
		[batch 20/20] avg loss: -0.0053801971991211586		[learning rate: 0.00012476]
	Learning Rate: 0.000124757
	LOSS [training: -0.006778016299626761 | validation: 0.002343273174716003]
	TIME [epoch: 8.71 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032204699346443677		[learning rate: 0.00012457]
		[batch 20/20] avg loss: -0.007114335787923182		[learning rate: 0.00012438]
	Learning Rate: 0.000124375
	LOSS [training: -0.005167402861283775 | validation: -0.0032935785151351213]
	TIME [epoch: 8.68 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006957984386850795		[learning rate: 0.00012418]
		[batch 20/20] avg loss: -0.004311402596846575		[learning rate: 0.00012399]
	Learning Rate: 0.000123994
	LOSS [training: -0.005634693491848685 | validation: -0.0011233709239866433]
	TIME [epoch: 8.7 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022849705787766515		[learning rate: 0.0001238]
		[batch 20/20] avg loss: -0.0011014740924981821		[learning rate: 0.00012361]
	Learning Rate: 0.000123614
	LOSS [training: -0.001693222335637417 | validation: -0.003377566802226533]
	TIME [epoch: 8.72 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016197831215842046		[learning rate: 0.00012342]
		[batch 20/20] avg loss: -0.00918693173650573		[learning rate: 0.00012323]
	Learning Rate: 0.000123235
	LOSS [training: -0.005403357429044967 | validation: -0.000760169581810488]
	TIME [epoch: 8.68 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005549637712239784		[learning rate: 0.00012305]
		[batch 20/20] avg loss: -0.002765130908690262		[learning rate: 0.00012286]
	Learning Rate: 0.000122857
	LOSS [training: -0.004157384310465022 | validation: -0.0043810757979728335]
	TIME [epoch: 8.7 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004772413611438072		[learning rate: 0.00012267]
		[batch 20/20] avg loss: -0.008687101428851993		[learning rate: 0.00012248]
	Learning Rate: 0.00012248
	LOSS [training: -0.006729757520145032 | validation: -0.001359776676948394]
	TIME [epoch: 8.7 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015727308324288724		[learning rate: 0.00012229]
		[batch 20/20] avg loss: -0.006021482987555326		[learning rate: 0.0001221]
	Learning Rate: 0.000122105
	LOSS [training: -0.0037971069099920996 | validation: -0.0025343230925972036]
	TIME [epoch: 8.72 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016085444699369064		[learning rate: 0.00012192]
		[batch 20/20] avg loss: -0.0023910396685319557		[learning rate: 0.00012173]
	Learning Rate: 0.000121731
	LOSS [training: -0.001999792069234431 | validation: 0.0016246295847415397]
	TIME [epoch: 8.74 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003330459177245555		[learning rate: 0.00012154]
		[batch 20/20] avg loss: -0.008072860888150346		[learning rate: 0.00012136]
	Learning Rate: 0.000121357
	LOSS [training: -0.005701660032697951 | validation: -0.00367166770116506]
	TIME [epoch: 8.7 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005472338110917236		[learning rate: 0.00012117]
		[batch 20/20] avg loss: -9.323418848572427e-05		[learning rate: 0.00012099]
	Learning Rate: 0.000120985
	LOSS [training: -0.00278278614970148 | validation: -0.004718741192530025]
	TIME [epoch: 8.73 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022324008851250243		[learning rate: 0.0001208]
		[batch 20/20] avg loss: -0.007282472629333325		[learning rate: 0.00012061]
	Learning Rate: 0.000120615
	LOSS [training: -0.004757436757229174 | validation: -0.004880973653719761]
	TIME [epoch: 8.7 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00451361042688694		[learning rate: 0.00012043]
		[batch 20/20] avg loss: -0.006254794865800601		[learning rate: 0.00012024]
	Learning Rate: 0.000120245
	LOSS [training: -0.005384202646343771 | validation: -0.003719213494307973]
	TIME [epoch: 8.71 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004344337365909918		[learning rate: 0.00012006]
		[batch 20/20] avg loss: 0.00029416953393122133		[learning rate: 0.00011988]
	Learning Rate: 0.000119876
	LOSS [training: -0.002025083915989348 | validation: -0.0006874800122185236]
	TIME [epoch: 8.72 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.7020671649257945e-05		[learning rate: 0.00011969]
		[batch 20/20] avg loss: -0.004445125166335674		[learning rate: 0.00011951]
	Learning Rate: 0.000119509
	LOSS [training: -0.002194052247343208 | validation: 0.00168403895905244]
	TIME [epoch: 8.72 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004939880465388342		[learning rate: 0.00011933]
		[batch 20/20] avg loss: -0.003794754200292796		[learning rate: 0.00011914]
	Learning Rate: 0.000119142
	LOSS [training: -0.004367317332840569 | validation: 0.0020215665760150246]
	TIME [epoch: 8.71 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010442746372132267		[learning rate: 0.00011896]
		[batch 20/20] avg loss: -0.0050165549613669395		[learning rate: 0.00011878]
	Learning Rate: 0.000118777
	LOSS [training: -0.007729650666749602 | validation: -0.0012575865839036635]
	TIME [epoch: 8.69 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036513200717240963		[learning rate: 0.0001186]
		[batch 20/20] avg loss: -0.007025393206081487		[learning rate: 0.00011841]
	Learning Rate: 0.000118413
	LOSS [training: -0.0016870365671786958 | validation: 0.0023631582291641275]
	TIME [epoch: 8.71 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00261824003183062		[learning rate: 0.00011823]
		[batch 20/20] avg loss: -0.007314131578627346		[learning rate: 0.00011805]
	Learning Rate: 0.00011805
	LOSS [training: -0.004966185805228984 | validation: -0.0024275072261994318]
	TIME [epoch: 8.72 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0040629773241728975		[learning rate: 0.00011787]
		[batch 20/20] avg loss: -0.004292350938245147		[learning rate: 0.00011769]
	Learning Rate: 0.000117688
	LOSS [training: -0.004177664131209022 | validation: 0.00040765608794405796]
	TIME [epoch: 8.71 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036649471929282784		[learning rate: 0.00011751]
		[batch 20/20] avg loss: -0.002435103403930214		[learning rate: 0.00011733]
	Learning Rate: 0.000117328
	LOSS [training: -0.003050025298429247 | validation: 0.0007493885427467593]
	TIME [epoch: 8.72 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037157917248189396		[learning rate: 0.00011715]
		[batch 20/20] avg loss: -0.008543648294125347		[learning rate: 0.00011697]
	Learning Rate: 0.000116968
	LOSS [training: -0.006129720009472142 | validation: -0.00041833456625379675]
	TIME [epoch: 8.72 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004519832555136804		[learning rate: 0.00011679]
		[batch 20/20] avg loss: -0.0010704610924199404		[learning rate: 0.00011661]
	Learning Rate: 0.000116609
	LOSS [training: -0.0027951468237783724 | validation: -0.003646449863687105]
	TIME [epoch: 8.73 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014637230135606232		[learning rate: 0.00011643]
		[batch 20/20] avg loss: -0.003145720465253898		[learning rate: 0.00011625]
	Learning Rate: 0.000116252
	LOSS [training: -0.002304721739407261 | validation: 0.0002335297764359094]
	TIME [epoch: 8.74 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021780687874014006		[learning rate: 0.00011607]
		[batch 20/20] avg loss: -0.009199651504062914		[learning rate: 0.0001159]
	Learning Rate: 0.000115896
	LOSS [training: -0.005688860145732157 | validation: 0.0007589373863267652]
	TIME [epoch: 8.72 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00824019327650451		[learning rate: 0.00011572]
		[batch 20/20] avg loss: 0.0008056339364873865		[learning rate: 0.00011554]
	Learning Rate: 0.00011554
	LOSS [training: -0.0037172796700085616 | validation: -0.001756034072286538]
	TIME [epoch: 8.73 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002380666778918474		[learning rate: 0.00011536]
		[batch 20/20] avg loss: -0.0033330137827743767		[learning rate: 0.00011519]
	Learning Rate: 0.000115186
	LOSS [training: -0.0028568402808464254 | validation: -0.003636391276529252]
	TIME [epoch: 8.73 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005780576317996199		[learning rate: 0.00011501]
		[batch 20/20] avg loss: -0.007050987808940976		[learning rate: 0.00011483]
	Learning Rate: 0.000114833
	LOSS [training: -0.006415782063468586 | validation: 0.0037552601051494276]
	TIME [epoch: 8.72 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00621730124458036		[learning rate: 0.00011466]
		[batch 20/20] avg loss: -0.0030737326768759936		[learning rate: 0.00011448]
	Learning Rate: 0.000114481
	LOSS [training: -0.0046455169607281755 | validation: 0.005985092070458675]
	TIME [epoch: 8.72 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008055675993469781		[learning rate: 0.00011431]
		[batch 20/20] avg loss: -0.010002479844856133		[learning rate: 0.00011413]
	Learning Rate: 0.00011413
	LOSS [training: -0.005404023722101555 | validation: 0.0036762463285973193]
	TIME [epoch: 8.71 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001433757839147452		[learning rate: 0.00011395]
		[batch 20/20] avg loss: -0.006996895127354572		[learning rate: 0.00011378]
	Learning Rate: 0.00011378
	LOSS [training: -0.004215326483251012 | validation: -0.005967950543824309]
	TIME [epoch: 8.7 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005673653299070901		[learning rate: 0.00011361]
		[batch 20/20] avg loss: -0.004555397630127305		[learning rate: 0.00011343]
	Learning Rate: 0.000113431
	LOSS [training: -0.005114525464599102 | validation: 0.00012349872491440453]
	TIME [epoch: 8.71 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007735679123194445		[learning rate: 0.00011326]
		[batch 20/20] avg loss: -0.0008571358304862896		[learning rate: 0.00011308]
	Learning Rate: 0.000113084
	LOSS [training: -0.004296407476840366 | validation: 0.002683430040339363]
	TIME [epoch: 8.72 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009792288069927404		[learning rate: 0.00011291]
		[batch 20/20] avg loss: -0.00459748004145078		[learning rate: 0.00011274]
	Learning Rate: 0.000112737
	LOSS [training: -0.007194884055689093 | validation: -0.0009777427640367746]
	TIME [epoch: 8.74 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004026936396829479		[learning rate: 0.00011256]
		[batch 20/20] avg loss: -0.0023545953812668776		[learning rate: 0.00011239]
	Learning Rate: 0.000112391
	LOSS [training: -0.003190765889048179 | validation: -0.00452583020768175]
	TIME [epoch: 8.71 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004358500609114387		[learning rate: 0.00011222]
		[batch 20/20] avg loss: -0.00710007750332467		[learning rate: 0.00011205]
	Learning Rate: 0.000112047
	LOSS [training: -0.005729289056219528 | validation: 0.0044234202968749465]
	TIME [epoch: 8.75 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006572059950349074		[learning rate: 0.00011188]
		[batch 20/20] avg loss: -0.007525509260298735		[learning rate: 0.0001117]
	Learning Rate: 0.000111703
	LOSS [training: -0.0070487846053239045 | validation: -0.002302303442189781]
	TIME [epoch: 8.73 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007633892037196698		[learning rate: 0.00011153]
		[batch 20/20] avg loss: -0.0001618403452041613		[learning rate: 0.00011136]
	Learning Rate: 0.000111361
	LOSS [training: -0.003897866191200429 | validation: 0.0009582818883358357]
	TIME [epoch: 8.73 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005346010522659597		[learning rate: 0.00011119]
		[batch 20/20] avg loss: -0.006909436426466536		[learning rate: 0.00011102]
	Learning Rate: 0.00011102
	LOSS [training: -0.006127723474563066 | validation: -0.0015987029334194127]
	TIME [epoch: 8.73 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006760719083262064		[learning rate: 0.00011085]
		[batch 20/20] avg loss: -0.0042725091761735		[learning rate: 0.00011068]
	Learning Rate: 0.000110679
	LOSS [training: -0.005516614129717782 | validation: -0.007104102029780709]
	TIME [epoch: 8.73 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015878826424220096		[learning rate: 0.00011051]
		[batch 20/20] avg loss: -0.00783804511556396		[learning rate: 0.00011034]
	Learning Rate: 0.00011034
	LOSS [training: -0.004712963878992985 | validation: -0.005226717761838091]
	TIME [epoch: 8.74 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006721769363040577		[learning rate: 0.00011017]
		[batch 20/20] avg loss: -0.006753153895858117		[learning rate: 0.00011]
	Learning Rate: 0.000110002
	LOSS [training: -0.006737461629449345 | validation: 0.0008868320785905715]
	TIME [epoch: 8.73 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005090303797079998		[learning rate: 0.00010983]
		[batch 20/20] avg loss: -0.0031915311962245076		[learning rate: 0.00010966]
	Learning Rate: 0.000109665
	LOSS [training: -0.004140917496652253 | validation: 0.0026615028015026816]
	TIME [epoch: 8.71 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0045769605076528665		[learning rate: 0.0001095]
		[batch 20/20] avg loss: -0.006036237883207404		[learning rate: 0.00010933]
	Learning Rate: 0.000109328
	LOSS [training: -0.005306599195430135 | validation: -0.0009599993741506948]
	TIME [epoch: 8.72 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006374200191570115		[learning rate: 0.00010916]
		[batch 20/20] avg loss: -0.0026346718542807454		[learning rate: 0.00010899]
	Learning Rate: 0.000108993
	LOSS [training: -0.00450443602292543 | validation: -0.0020184155878687202]
	TIME [epoch: 8.72 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004055929989118033		[learning rate: 0.00010883]
		[batch 20/20] avg loss: -0.005106304873198771		[learning rate: 0.00010866]
	Learning Rate: 0.000108659
	LOSS [training: -0.0045811174311584015 | validation: -0.0006186358572409097]
	TIME [epoch: 8.73 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006676525152391639		[learning rate: 0.00010849]
		[batch 20/20] avg loss: -0.0050864366493539185		[learning rate: 0.00010833]
	Learning Rate: 0.000108326
	LOSS [training: -0.005881480900872779 | validation: 0.0001008853531752674]
	TIME [epoch: 8.74 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008118582264737852		[learning rate: 0.00010816]
		[batch 20/20] avg loss: -0.004441313175948013		[learning rate: 0.00010799]
	Learning Rate: 0.000107994
	LOSS [training: -0.006279947720342932 | validation: -0.006424241019255956]
	TIME [epoch: 8.73 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006490827330109314		[learning rate: 0.00010783]
		[batch 20/20] avg loss: -0.0020751757244375614		[learning rate: 0.00010766]
	Learning Rate: 0.000107663
	LOSS [training: -0.004283001527273437 | validation: -0.0020166262804391056]
	TIME [epoch: 8.72 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031981525016834683		[learning rate: 0.0001075]
		[batch 20/20] avg loss: -0.003513437075333506		[learning rate: 0.00010733]
	Learning Rate: 0.000107333
	LOSS [training: -0.0033557947885084864 | validation: -0.00395486118723248]
	TIME [epoch: 8.73 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004115335024141778		[learning rate: 0.00010717]
		[batch 20/20] avg loss: -0.004125009723555413		[learning rate: 0.000107]
	Learning Rate: 0.000107004
	LOSS [training: -0.004120172373848595 | validation: -0.009031306852312033]
	TIME [epoch: 8.71 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004591331582181617		[learning rate: 0.00010684]
		[batch 20/20] avg loss: -0.002923663979265466		[learning rate: 0.00010668]
	Learning Rate: 0.000106676
	LOSS [training: -0.003757497780723541 | validation: -0.0022845929511251277]
	TIME [epoch: 8.74 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010737186652209291		[learning rate: 0.00010651]
		[batch 20/20] avg loss: -0.003300428340157713		[learning rate: 0.00010635]
	Learning Rate: 0.000106349
	LOSS [training: -0.007018807496183502 | validation: 0.002658983600232916]
	TIME [epoch: 8.74 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005207909629541789		[learning rate: 0.00010619]
		[batch 20/20] avg loss: -0.0049000736835006446		[learning rate: 0.00010602]
	Learning Rate: 0.000106023
	LOSS [training: -0.005053991656521217 | validation: 0.0004424166861420505]
	TIME [epoch: 8.73 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005122816955613367		[learning rate: 0.00010586]
		[batch 20/20] avg loss: -0.004726245147413704		[learning rate: 0.0001057]
	Learning Rate: 0.000105698
	LOSS [training: -0.004924531051513537 | validation: 0.0009761889265369893]
	TIME [epoch: 8.73 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00047035255368278316		[learning rate: 0.00010554]
		[batch 20/20] avg loss: -0.0023300727242097944		[learning rate: 0.00010537]
	Learning Rate: 0.000105374
	LOSS [training: -0.0014002126389462887 | validation: -0.004123473715344011]
	TIME [epoch: 8.71 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035918228701152704		[learning rate: 0.00010521]
		[batch 20/20] avg loss: -0.004046094154781063		[learning rate: 0.00010505]
	Learning Rate: 0.000105051
	LOSS [training: -0.0038189585124481667 | validation: 0.0004007422349200222]
	TIME [epoch: 8.75 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022368358859368017		[learning rate: 0.00010489]
		[batch 20/20] avg loss: -0.0057833586681356065		[learning rate: 0.00010473]
	Learning Rate: 0.000104729
	LOSS [training: -0.004010097277036204 | validation: -0.005910061060476379]
	TIME [epoch: 8.72 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003883080799371921		[learning rate: 0.00010457]
		[batch 20/20] avg loss: -0.008702271738841994		[learning rate: 0.00010441]
	Learning Rate: 0.000104408
	LOSS [training: -0.006292676269106958 | validation: -0.002782358996596435]
	TIME [epoch: 8.72 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004059527150059516		[learning rate: 0.00010425]
		[batch 20/20] avg loss: -0.005035422811189231		[learning rate: 0.00010409]
	Learning Rate: 0.000104088
	LOSS [training: -0.0045474749806243735 | validation: -0.00778611920284164]
	TIME [epoch: 8.71 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008168410762975536		[learning rate: 0.00010393]
		[batch 20/20] avg loss: -0.0056327716955118685		[learning rate: 0.00010377]
	Learning Rate: 0.000103769
	LOSS [training: -0.006900591229243702 | validation: -0.0008933287548462237]
	TIME [epoch: 8.74 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00812641684474684		[learning rate: 0.00010361]
		[batch 20/20] avg loss: -0.003080194046703132		[learning rate: 0.00010345]
	Learning Rate: 0.000103451
	LOSS [training: -0.005603305445724985 | validation: -0.0051380876122953095]
	TIME [epoch: 8.72 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001664083764818199		[learning rate: 0.00010329]
		[batch 20/20] avg loss: -0.008423698257726812		[learning rate: 0.00010313]
	Learning Rate: 0.000103134
	LOSS [training: -0.003379807246454307 | validation: -0.004393221209224186]
	TIME [epoch: 8.71 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005851165403617034		[learning rate: 0.00010298]
		[batch 20/20] avg loss: -0.00271895431088807		[learning rate: 0.00010282]
	Learning Rate: 0.000102817
	LOSS [training: -0.004285059857252552 | validation: -0.005136635563119109]
	TIME [epoch: 8.71 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014261773341685978		[learning rate: 0.00010266]
		[batch 20/20] avg loss: -0.005435685493495895		[learning rate: 0.0001025]
	Learning Rate: 0.000102502
	LOSS [training: -0.0034309314138322457 | validation: -0.0005471462268424676]
	TIME [epoch: 8.7 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002661954874337407		[learning rate: 0.00010235]
		[batch 20/20] avg loss: -0.013042653696106332		[learning rate: 0.00010219]
	Learning Rate: 0.000102188
	LOSS [training: -0.007852304285221868 | validation: -0.0007841556224489863]
	TIME [epoch: 8.73 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003816827547729146		[learning rate: 0.00010203]
		[batch 20/20] avg loss: -0.002110054092430028		[learning rate: 0.00010187]
	Learning Rate: 0.000101875
	LOSS [training: -0.002963440820079588 | validation: -8.642881595532928e-05]
	TIME [epoch: 8.72 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00633114685348686		[learning rate: 0.00010172]
		[batch 20/20] avg loss: -0.001620405015644702		[learning rate: 0.00010156]
	Learning Rate: 0.000101562
	LOSS [training: -0.0039757759345657815 | validation: -0.00026892645357199166]
	TIME [epoch: 8.72 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004267655836479761		[learning rate: 0.00010141]
		[batch 20/20] avg loss: -0.006689362120912171		[learning rate: 0.00010125]
	Learning Rate: 0.000101251
	LOSS [training: -0.005478508978695966 | validation: -0.007509869543216257]
	TIME [epoch: 8.71 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004663413527995237		[learning rate: 0.0001011]
		[batch 20/20] avg loss: -0.0027513289906778344		[learning rate: 0.00010094]
	Learning Rate: 0.000100941
	LOSS [training: -0.003707371259336535 | validation: 0.000356133915280453]
	TIME [epoch: 8.7 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026924630973047597		[learning rate: 0.00010079]
		[batch 20/20] avg loss: -0.00784400918575831		[learning rate: 0.00010063]
	Learning Rate: 0.000100631
	LOSS [training: -0.0052682361415315355 | validation: -0.003305744461509644]
	TIME [epoch: 8.73 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003953027913962057		[learning rate: 0.00010048]
		[batch 20/20] avg loss: -0.003746515615425073		[learning rate: 0.00010032]
	Learning Rate: 0.000100323
	LOSS [training: -0.0038497717646935646 | validation: 0.00042621760833265677]
	TIME [epoch: 8.69 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010426918164608457		[learning rate: 0.00010017]
		[batch 20/20] avg loss: -0.0030860282888512274		[learning rate: 0.00010002]
	Learning Rate: 0.000100015
	LOSS [training: -0.0067564732267298405 | validation: -0.001623981714355727]
	TIME [epoch: 8.68 sec]
Finished training in 17558.967 seconds.
