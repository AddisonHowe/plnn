Args:
Namespace(name='model_tr_study5', outdir='out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0', training_data='data/transition_rate_studies/tr_study5/tr_study5_training/r0', validation_data='data/transition_rate_studies/tr_study5/tr_study5_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2361566117

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.76207576407798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.76207576407798 | validation: 9.521663213243633]
	TIME [epoch: 49.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.885323828645026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.885323828645026 | validation: 8.071111485834033]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.952005212419751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.952005212419751 | validation: 8.02991413404549]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.844255633819774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.844255633819774 | validation: 8.606911124842524]
	TIME [epoch: 10.4 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.277229982884228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.277229982884228 | validation: 7.930373116826593]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.084666526474377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.084666526474377 | validation: 6.951620951606578]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.301854357681078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.301854357681078 | validation: 7.053212853733498]
	TIME [epoch: 10.5 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.127395452816283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.127395452816283 | validation: 7.352495703475045]
	TIME [epoch: 10.5 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.804964902209798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.804964902209798 | validation: 8.182470643210813]
	TIME [epoch: 10.4 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.8804166076947695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.8804166076947695 | validation: 6.82477213740719]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.61374135065005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.61374135065005 | validation: 5.872606094377]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.813662606277832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.813662606277832 | validation: 5.494380746378804]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.448798122360176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.448798122360176 | validation: 6.381258698615928]
	TIME [epoch: 10.5 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.973627562240095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.973627562240095 | validation: 5.034048235014244]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.418735458473842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.418735458473842 | validation: 5.997834417200902]
	TIME [epoch: 10.5 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.690237458534268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.690237458534268 | validation: 5.13903095679746]
	TIME [epoch: 10.4 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.578855191191478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.578855191191478 | validation: 5.237166363990515]
	TIME [epoch: 10.5 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.569433034121924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.569433034121924 | validation: 5.218901684461899]
	TIME [epoch: 10.5 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.347741864501717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.347741864501717 | validation: 4.755883094917601]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.411291588923236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.411291588923236 | validation: 5.299473989204143]
	TIME [epoch: 10.4 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155890650237365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.155890650237365 | validation: 4.751690463776142]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2703618801564955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2703618801564955 | validation: 4.456753679965834]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.068167838024925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.068167838024925 | validation: 5.557921439766732]
	TIME [epoch: 10.4 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.337855475452183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.337855475452183 | validation: 4.206312185599353]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.302775732139095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.302775732139095 | validation: 4.541390413609204]
	TIME [epoch: 10.4 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.796369148643018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.796369148643018 | validation: 4.384852877899153]
	TIME [epoch: 10.4 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.059402331607296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.059402331607296 | validation: 6.484078453294892]
	TIME [epoch: 10.4 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.970856490162038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.970856490162038 | validation: 4.934710637161094]
	TIME [epoch: 10.4 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172439901283635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.172439901283635 | validation: 4.094306598023225]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18195894666186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.18195894666186 | validation: 4.1741321652325665]
	TIME [epoch: 10.4 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.211534515070087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.211534515070087 | validation: 4.940535334231448]
	TIME [epoch: 10.4 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.511670432101887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.511670432101887 | validation: 3.988492087576331]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.061951623550593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.061951623550593 | validation: 4.044520640749207]
	TIME [epoch: 10.5 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.854542703842702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.854542703842702 | validation: 4.458464811669866]
	TIME [epoch: 10.4 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165026191219939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.165026191219939 | validation: 4.552311664837847]
	TIME [epoch: 10.4 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.996489298588041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.996489298588041 | validation: 3.947699906645186]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.275929256838694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.275929256838694 | validation: 6.731045842655247]
	TIME [epoch: 10.4 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.656628595729191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.656628595729191 | validation: 5.305525122007158]
	TIME [epoch: 10.4 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.332773882248203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.332773882248203 | validation: 4.493698435430159]
	TIME [epoch: 10.5 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.232046729829344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.232046729829344 | validation: 4.71702168476741]
	TIME [epoch: 10.4 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.237894504405629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.237894504405629 | validation: 4.456938037616719]
	TIME [epoch: 10.5 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.271073718354706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.271073718354706 | validation: 4.2545601776237785]
	TIME [epoch: 10.4 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.139497810672025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.139497810672025 | validation: 4.169869264026936]
	TIME [epoch: 10.5 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182521872966616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.182521872966616 | validation: 4.075940822866537]
	TIME [epoch: 10.4 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.041809003594277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.041809003594277 | validation: 4.18441347769639]
	TIME [epoch: 10.5 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.898743961265227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.898743961265227 | validation: 3.8125823224137747]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.682741224827397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.682741224827397 | validation: 3.944552544280955]
	TIME [epoch: 10.4 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171989785560947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.171989785560947 | validation: 3.603786073417368]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.640769921339185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.640769921339185 | validation: 3.8198453037120603]
	TIME [epoch: 10.4 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.762958800622241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.762958800622241 | validation: 3.461715020779112]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.745397002439002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.745397002439002 | validation: 3.7568297569998097]
	TIME [epoch: 10.4 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.563317311682285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.563317311682285 | validation: 3.4219402283223563]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4221874608556675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4221874608556675 | validation: 4.010019157026862]
	TIME [epoch: 10.5 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0446258522354555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0446258522354555 | validation: 3.3668622622421185]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.490425473172749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.490425473172749 | validation: 3.858569434067994]
	TIME [epoch: 10.4 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5915731879235615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5915731879235615 | validation: 3.5683150956417102]
	TIME [epoch: 10.5 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.686831297555525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.686831297555525 | validation: 3.905054215011732]
	TIME [epoch: 10.4 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.51745403515223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.51745403515223 | validation: 3.4035362530583018]
	TIME [epoch: 10.5 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.549325992418682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.549325992418682 | validation: 3.476142460210659]
	TIME [epoch: 10.4 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.475816018112304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.475816018112304 | validation: 3.490783931309682]
	TIME [epoch: 10.4 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.404957860500174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.404957860500174 | validation: 3.355764555569708]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5014445099800895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5014445099800895 | validation: 3.2281033789095797]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.518821744877035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.518821744877035 | validation: 3.5453422700958495]
	TIME [epoch: 10.4 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.432066188723845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.432066188723845 | validation: 3.412534222861408]
	TIME [epoch: 10.4 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2255277044411885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2255277044411885 | validation: 3.307490390942186]
	TIME [epoch: 10.4 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.176549629449104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.176549629449104 | validation: 3.5729233610137454]
	TIME [epoch: 10.4 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0831500020660165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0831500020660165 | validation: 3.1069251520064474]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5854643448640195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5854643448640195 | validation: 2.470523550244634]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.273081378045758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.273081378045758 | validation: 3.2311669409131047]
	TIME [epoch: 10.5 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8000988860713654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8000988860713654 | validation: 3.0495287818179193]
	TIME [epoch: 10.4 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2215317125276632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2215317125276632 | validation: 2.813561736511862]
	TIME [epoch: 10.5 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5959329525627046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5959329525627046 | validation: 2.370420189351349]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.740557635896911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.740557635896911 | validation: 2.385056331886167]
	TIME [epoch: 10.4 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.532248280166383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.532248280166383 | validation: 3.239391552161018]
	TIME [epoch: 10.4 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0941953516605536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0941953516605536 | validation: 3.331007423429114]
	TIME [epoch: 10.4 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6812187770803284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6812187770803284 | validation: 5.448076531550531]
	TIME [epoch: 10.5 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7943127433804698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7943127433804698 | validation: 2.4142798367038134]
	TIME [epoch: 10.5 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5848588734655302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5848588734655302 | validation: 2.1657769901712944]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1101132301107315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1101132301107315 | validation: 2.4454454869109052]
	TIME [epoch: 10.5 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4141206199076173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4141206199076173 | validation: 2.15025473874361]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2573789323184323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2573789323184323 | validation: 2.095218132988238]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1929677636571623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1929677636571623 | validation: 2.378369177954306]
	TIME [epoch: 10.5 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3626916892634364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3626916892634364 | validation: 2.8116301963529704]
	TIME [epoch: 10.5 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.652466579631425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.652466579631425 | validation: 2.021992807117669]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.419513466471554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.419513466471554 | validation: 1.8430740324362354]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6040817883859924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6040817883859924 | validation: 4.144989842766275]
	TIME [epoch: 10.5 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6965699396488114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6965699396488114 | validation: 2.381536157236503]
	TIME [epoch: 10.5 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4017822769104553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4017822769104553 | validation: 2.5971014392411003]
	TIME [epoch: 10.5 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5343552716238427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5343552716238427 | validation: 2.237339273361152]
	TIME [epoch: 10.5 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2095002698013815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2095002698013815 | validation: 1.9597331522852164]
	TIME [epoch: 10.5 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.307375249274154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.307375249274154 | validation: 1.9840483703257592]
	TIME [epoch: 10.5 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.375987029322558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.375987029322558 | validation: 3.1554765990988187]
	TIME [epoch: 10.5 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.070389575323815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.070389575323815 | validation: 2.014217170386048]
	TIME [epoch: 10.5 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1391602722015604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1391602722015604 | validation: 2.8080346818641004]
	TIME [epoch: 10.5 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.171679920902105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.171679920902105 | validation: 2.0096735685295934]
	TIME [epoch: 10.5 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4819535079551533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4819535079551533 | validation: 1.9528471423214182]
	TIME [epoch: 10.5 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0269401510074188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0269401510074188 | validation: 1.7742973209533295]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2348906059009552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2348906059009552 | validation: 2.721194957319442]
	TIME [epoch: 10.4 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5757895579831445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5757895579831445 | validation: 1.9369260991466097]
	TIME [epoch: 10.5 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0556004083783384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0556004083783384 | validation: 1.9321889558345444]
	TIME [epoch: 10.5 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8212221019077464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8212221019077464 | validation: 2.213458054437963]
	TIME [epoch: 10.5 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5917507647703864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5917507647703864 | validation: 1.9193661568217915]
	TIME [epoch: 10.5 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2239609654278736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2239609654278736 | validation: 1.9869676550161866]
	TIME [epoch: 10.4 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8643320059455657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8643320059455657 | validation: 1.749572899340182]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8161169515662507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8161169515662507 | validation: 4.064068479701916]
	TIME [epoch: 10.4 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.519184754021193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.519184754021193 | validation: 1.5731064484141308]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8190937661384168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8190937661384168 | validation: 1.5393632943915037]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7044407833714683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7044407833714683 | validation: 1.8943020037540919]
	TIME [epoch: 10.4 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7058186863341291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7058186863341291 | validation: 1.6454028207853015]
	TIME [epoch: 10.5 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0954310195847383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0954310195847383 | validation: 1.7675972198602095]
	TIME [epoch: 10.5 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6589789286106282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6589789286106282 | validation: 1.8657440510760313]
	TIME [epoch: 10.5 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1201108831174578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1201108831174578 | validation: 1.541404867906329]
	TIME [epoch: 10.5 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6053417617340782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6053417617340782 | validation: 2.030857583794549]
	TIME [epoch: 10.5 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7596680911847282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7596680911847282 | validation: 1.8698071801442382]
	TIME [epoch: 10.5 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.732981206670567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.732981206670567 | validation: 1.4475355581710394]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.630876384626974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.630876384626974 | validation: 1.6375324426350084]
	TIME [epoch: 10.5 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8601142216381072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8601142216381072 | validation: 5.683441974998898]
	TIME [epoch: 10.5 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.655310207588859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.655310207588859 | validation: 1.8283691998283402]
	TIME [epoch: 10.5 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9385545307437397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9385545307437397 | validation: 1.8183839924130172]
	TIME [epoch: 10.5 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.729084898935723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.729084898935723 | validation: 1.5033710403386573]
	TIME [epoch: 10.5 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.824510833696258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.824510833696258 | validation: 1.754037986015049]
	TIME [epoch: 10.5 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9024060807662941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9024060807662941 | validation: 1.767802166875149]
	TIME [epoch: 10.5 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6719780400249324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6719780400249324 | validation: 1.6144475901539341]
	TIME [epoch: 10.5 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4519805027461081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4519805027461081 | validation: 1.307413350402669]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2871415156449924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2871415156449924 | validation: 1.4268641216598719]
	TIME [epoch: 10.5 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7663924451131685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7663924451131685 | validation: 1.625647746452363]
	TIME [epoch: 10.5 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7846223468627442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7846223468627442 | validation: 2.5890414215324276]
	TIME [epoch: 10.5 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8067244586631326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8067244586631326 | validation: 1.6148542767999068]
	TIME [epoch: 10.5 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7781765398682132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7781765398682132 | validation: 1.570947610682287]
	TIME [epoch: 10.5 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.495054133199588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.495054133199588 | validation: 1.3313284737482745]
	TIME [epoch: 10.5 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7862550529853227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7862550529853227 | validation: 2.185306726788886]
	TIME [epoch: 10.5 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7791900498380784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7791900498380784 | validation: 1.470958097195217]
	TIME [epoch: 10.5 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.093925504707459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.093925504707459 | validation: 2.7774215616150935]
	TIME [epoch: 10.5 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0779187092832982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0779187092832982 | validation: 2.172587376701988]
	TIME [epoch: 10.5 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9198696352827593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9198696352827593 | validation: 2.036751196254624]
	TIME [epoch: 10.5 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8027930253623683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8027930253623683 | validation: 1.826607730343012]
	TIME [epoch: 10.5 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6310076373438833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6310076373438833 | validation: 1.3479890353080575]
	TIME [epoch: 10.5 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4435724651022437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4435724651022437 | validation: 1.5032927026624878]
	TIME [epoch: 10.5 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.631386610279662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.631386610279662 | validation: 1.4085211627668557]
	TIME [epoch: 10.5 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5045445404000712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5045445404000712 | validation: 1.4486852735569884]
	TIME [epoch: 10.5 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9965978549344037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9965978549344037 | validation: 1.8534940920718443]
	TIME [epoch: 10.5 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9252543074157067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9252543074157067 | validation: 1.7108157054436088]
	TIME [epoch: 10.5 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.579922618726537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.579922618726537 | validation: 1.4879873298877084]
	TIME [epoch: 10.5 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5237742356313428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5237742356313428 | validation: 1.4062702567360537]
	TIME [epoch: 10.5 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6888622360602112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6888622360602112 | validation: 1.5727386452214824]
	TIME [epoch: 10.5 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1550572309640423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1550572309640423 | validation: 2.049794015975981]
	TIME [epoch: 10.5 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9358319613147437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9358319613147437 | validation: 1.730765643548263]
	TIME [epoch: 10.5 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4228409333477596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4228409333477596 | validation: 1.556692960601751]
	TIME [epoch: 10.5 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.406109018141936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.406109018141936 | validation: 1.5374752331390011]
	TIME [epoch: 10.5 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8077657682877626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8077657682877626 | validation: 1.4525592621842247]
	TIME [epoch: 10.5 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5581428805698319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5581428805698319 | validation: 2.1144916523385566]
	TIME [epoch: 10.5 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9617457389799697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9617457389799697 | validation: 1.6103097718677077]
	TIME [epoch: 10.5 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4557429338595722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4557429338595722 | validation: 1.5142781518417812]
	TIME [epoch: 10.5 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4991361682627413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4991361682627413 | validation: 1.8247684021866137]
	TIME [epoch: 10.5 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4555148001239324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4555148001239324 | validation: 3.4446706980571156]
	TIME [epoch: 10.5 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4756277751374776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4756277751374776 | validation: 1.2893572643959692]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6816571104675817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6816571104675817 | validation: 1.6265067944999385]
	TIME [epoch: 10.5 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6961614575643236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6961614575643236 | validation: 2.353380832144794]
	TIME [epoch: 10.5 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.401053094220464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.401053094220464 | validation: 1.8959101353857941]
	TIME [epoch: 10.5 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8144436830034554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8144436830034554 | validation: 1.6448824343809412]
	TIME [epoch: 10.5 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9804515984287854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9804515984287854 | validation: 2.1970403685843287]
	TIME [epoch: 10.5 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9129161434176887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9129161434176887 | validation: 1.7688371506964335]
	TIME [epoch: 10.5 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6275307688648861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6275307688648861 | validation: 2.385768875864039]
	TIME [epoch: 10.5 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1237056850527285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1237056850527285 | validation: 1.7389443556116184]
	TIME [epoch: 10.5 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6905555579104417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6905555579104417 | validation: 1.682812974493367]
	TIME [epoch: 10.5 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.639500938531722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.639500938531722 | validation: 2.3284529203577478]
	TIME [epoch: 10.5 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.828868262766451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.828868262766451 | validation: 1.6770570743436586]
	TIME [epoch: 10.5 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.831817229331513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.831817229331513 | validation: 1.635531140756826]
	TIME [epoch: 10.6 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7228536735708893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7228536735708893 | validation: 1.682653476121965]
	TIME [epoch: 10.5 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.579663252605626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.579663252605626 | validation: 1.6293848642421755]
	TIME [epoch: 10.5 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.011383931884171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.011383931884171 | validation: 1.577155899614501]
	TIME [epoch: 10.5 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7871480532417034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7871480532417034 | validation: 1.2820588179927046]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3455026453852135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3455026453852135 | validation: 1.247702043115403]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4535982317857798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4535982317857798 | validation: 1.9514903742964027]
	TIME [epoch: 10.6 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.040728762744005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.040728762744005 | validation: 1.7045002176866126]
	TIME [epoch: 10.5 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8587044224140918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8587044224140918 | validation: 1.5335827912032114]
	TIME [epoch: 10.5 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7915477366858519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7915477366858519 | validation: 1.41741784202431]
	TIME [epoch: 10.5 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0040748960194827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0040748960194827 | validation: 1.7156260554369755]
	TIME [epoch: 10.5 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9981665326482272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9981665326482272 | validation: 1.8043415820819002]
	TIME [epoch: 10.5 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7107897124187574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7107897124187574 | validation: 1.5321622858130732]
	TIME [epoch: 10.5 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9777491747690625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9777491747690625 | validation: 1.4036231156086934]
	TIME [epoch: 10.5 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5535911954529091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5535911954529091 | validation: 1.3850599416932545]
	TIME [epoch: 10.5 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5163039436062355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5163039436062355 | validation: 1.7567588594507928]
	TIME [epoch: 10.5 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4744985602822205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4744985602822205 | validation: 1.6204685029348278]
	TIME [epoch: 10.5 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.652420637781224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.652420637781224 | validation: 1.6931989428605547]
	TIME [epoch: 10.5 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5981132338226014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5981132338226014 | validation: 1.2586445633507677]
	TIME [epoch: 10.5 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5449491375123854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5449491375123854 | validation: 1.5627679728883246]
	TIME [epoch: 10.5 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5016393749116481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5016393749116481 | validation: 1.3391022486829576]
	TIME [epoch: 10.5 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8607773248591144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8607773248591144 | validation: 1.9498776549354404]
	TIME [epoch: 10.5 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6499040451648295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6499040451648295 | validation: 1.6990999305157746]
	TIME [epoch: 10.5 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4938687259685608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4938687259685608 | validation: 1.4198913191744749]
	TIME [epoch: 10.5 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5005712889389706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5005712889389706 | validation: 1.5153805426112263]
	TIME [epoch: 10.5 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5260943787035417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5260943787035417 | validation: 1.7500676242131896]
	TIME [epoch: 10.5 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.712122924449897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.712122924449897 | validation: 1.4518281302542224]
	TIME [epoch: 10.5 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.649325951535209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.649325951535209 | validation: 1.6582435472468076]
	TIME [epoch: 10.5 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6812972681353728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6812972681353728 | validation: 1.3943400779379507]
	TIME [epoch: 10.5 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.42761358701761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.42761358701761 | validation: 1.491392267997136]
	TIME [epoch: 10.5 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3798701498047563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3798701498047563 | validation: 2.2584364350551445]
	TIME [epoch: 10.5 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.82318703258075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.82318703258075 | validation: 1.3140826842115823]
	TIME [epoch: 10.5 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.368534587266394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.368534587266394 | validation: 1.2089848999316168]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3700130252511495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3700130252511495 | validation: 1.6078431095108494]
	TIME [epoch: 10.5 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.652597667257303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.652597667257303 | validation: 1.5700009697247839]
	TIME [epoch: 10.5 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4338735324146503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4338735324146503 | validation: 1.1978848528815984]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3788081691621272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3788081691621272 | validation: 1.205587392916203]
	TIME [epoch: 10.6 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4267195000271808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4267195000271808 | validation: 1.3041600661027228]
	TIME [epoch: 10.5 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4848612063422604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4848612063422604 | validation: 1.9837887218392576]
	TIME [epoch: 10.5 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5808166693143888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5808166693143888 | validation: 1.44175349225693]
	TIME [epoch: 10.5 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.446607832435656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.446607832435656 | validation: 1.1840652045785194]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5353344694954638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5353344694954638 | validation: 1.1544715606297506]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.361546878788836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.361546878788836 | validation: 1.5627215561909729]
	TIME [epoch: 10.5 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8878765468382297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8878765468382297 | validation: 1.5676591470579524]
	TIME [epoch: 10.5 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6825182668579877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6825182668579877 | validation: 1.091617165376803]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4192189197590919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4192189197590919 | validation: 1.7824139639978773]
	TIME [epoch: 10.5 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.435105105146903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.435105105146903 | validation: 1.2537512309782455]
	TIME [epoch: 10.5 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.532408540405769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.532408540405769 | validation: 1.2076858959489833]
	TIME [epoch: 10.5 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4348783993691816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4348783993691816 | validation: 1.4371969117216457]
	TIME [epoch: 10.5 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5979220416280349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5979220416280349 | validation: 1.7230482834176126]
	TIME [epoch: 10.5 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9521828664094714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9521828664094714 | validation: 1.835737861414695]
	TIME [epoch: 10.5 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8694013605058035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8694013605058035 | validation: 1.2798018973648986]
	TIME [epoch: 10.5 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5837853127148072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5837853127148072 | validation: 1.4786592246423738]
	TIME [epoch: 10.5 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74135964771116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.74135964771116 | validation: 1.4931950208083191]
	TIME [epoch: 10.5 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4259286075790438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4259286075790438 | validation: 1.263825514498985]
	TIME [epoch: 10.5 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.407633133658463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.407633133658463 | validation: 1.4599570574214016]
	TIME [epoch: 10.5 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.590377924973002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.590377924973002 | validation: 2.270171831947198]
	TIME [epoch: 10.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5164357470369882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5164357470369882 | validation: 1.5660113001704394]
	TIME [epoch: 10.5 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7015113404466198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7015113404466198 | validation: 1.1437163023714898]
	TIME [epoch: 10.5 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3819366138274947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3819366138274947 | validation: 2.077616600986115]
	TIME [epoch: 10.5 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.924824725639592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.924824725639592 | validation: 2.2411000089742346]
	TIME [epoch: 10.5 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194000114313435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.194000114313435 | validation: 2.053973358810018]
	TIME [epoch: 10.5 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1860129476727423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1860129476727423 | validation: 2.8107288421243095]
	TIME [epoch: 10.5 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.120605752042673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.120605752042673 | validation: 1.9887172055091014]
	TIME [epoch: 10.5 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5784632284095108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5784632284095108 | validation: 1.2260151177678724]
	TIME [epoch: 10.5 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5127983077751488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5127983077751488 | validation: 1.302065868207556]
	TIME [epoch: 10.5 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.388316153685445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.388316153685445 | validation: 1.6862632557530806]
	TIME [epoch: 10.5 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5263814357768823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5263814357768823 | validation: 1.2716686465244242]
	TIME [epoch: 10.5 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2799181492704652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2799181492704652 | validation: 1.252306310754861]
	TIME [epoch: 10.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3066064841601683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3066064841601683 | validation: 1.3943284650084633]
	TIME [epoch: 10.5 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.431889918682496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.431889918682496 | validation: 1.242296283531574]
	TIME [epoch: 10.5 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.65988105472186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.65988105472186 | validation: 1.6024805715414356]
	TIME [epoch: 10.5 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.600038470629657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.600038470629657 | validation: 1.615434800041068]
	TIME [epoch: 10.5 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3932428060777513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3932428060777513 | validation: 1.8028528932577674]
	TIME [epoch: 10.5 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.476126563960476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.476126563960476 | validation: 1.1849536059087615]
	TIME [epoch: 10.5 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4109582436263948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4109582436263948 | validation: 1.5811773907834386]
	TIME [epoch: 10.5 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4290736187480584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4290736187480584 | validation: 1.4819392088925947]
	TIME [epoch: 10.5 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3140903015004604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3140903015004604 | validation: 0.9619395855668327]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2574135479636042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2574135479636042 | validation: 1.7093593851033837]
	TIME [epoch: 10.5 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9672851663678064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9672851663678064 | validation: 1.3532856964146966]
	TIME [epoch: 10.6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3242920386556443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3242920386556443 | validation: 1.6520956600687509]
	TIME [epoch: 10.5 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6236178062221314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6236178062221314 | validation: 1.3598209356606605]
	TIME [epoch: 10.5 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.59755848590082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.59755848590082 | validation: 1.9209369789019297]
	TIME [epoch: 10.5 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7687615810770843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7687615810770843 | validation: 1.3688266213465496]
	TIME [epoch: 10.5 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4035445797387411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4035445797387411 | validation: 2.018164249936207]
	TIME [epoch: 10.5 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5219421193401645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5219421193401645 | validation: 1.4321844600905826]
	TIME [epoch: 10.5 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.58647830525489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.58647830525489 | validation: 1.649028463365393]
	TIME [epoch: 10.5 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.556482213650934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.556482213650934 | validation: 1.2681776817753294]
	TIME [epoch: 10.5 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3603706607327606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3603706607327606 | validation: 1.0441395326544858]
	TIME [epoch: 10.5 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4107410454913398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4107410454913398 | validation: 1.5689907894981818]
	TIME [epoch: 10.5 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.493881194747766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.493881194747766 | validation: 2.0386031086825884]
	TIME [epoch: 10.5 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8981475535749914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8981475535749914 | validation: 1.2557017283943277]
	TIME [epoch: 10.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8994300610969415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8994300610969415 | validation: 1.500008124434944]
	TIME [epoch: 10.5 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2745944078680005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2745944078680005 | validation: 1.341956134397069]
	TIME [epoch: 10.5 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300349547120163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.300349547120163 | validation: 1.4360303137677892]
	TIME [epoch: 10.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4367862627607093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4367862627607093 | validation: 1.5166912787701696]
	TIME [epoch: 10.5 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5433359458236093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5433359458236093 | validation: 1.519561613388241]
	TIME [epoch: 10.5 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4187785497327423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4187785497327423 | validation: 1.095971004413526]
	TIME [epoch: 10.5 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.320112909390049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.320112909390049 | validation: 0.9060870110481138]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5411450107824503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5411450107824503 | validation: 1.4939136144648764]
	TIME [epoch: 10.5 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9254631601067362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9254631601067362 | validation: 1.2784065951702943]
	TIME [epoch: 10.5 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.466273654976453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.466273654976453 | validation: 1.8132049063890725]
	TIME [epoch: 10.5 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.702552155032635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.702552155032635 | validation: 1.699836413869254]
	TIME [epoch: 10.5 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.404819795427461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.404819795427461 | validation: 1.4067367450766615]
	TIME [epoch: 10.5 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1954018433422524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1954018433422524 | validation: 1.8462523219373992]
	TIME [epoch: 10.5 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6137202624744336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6137202624744336 | validation: 1.4777604933097732]
	TIME [epoch: 10.5 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6761255809746758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6761255809746758 | validation: 2.2788759712910953]
	TIME [epoch: 10.5 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9874978699334416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9874978699334416 | validation: 1.2519756410056162]
	TIME [epoch: 10.5 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6101901458337102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6101901458337102 | validation: 1.532882911814144]
	TIME [epoch: 10.5 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7489931808320756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7489931808320756 | validation: 1.4091054179318099]
	TIME [epoch: 10.5 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.721000079958214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.721000079958214 | validation: 1.2367179345108401]
	TIME [epoch: 10.5 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.428123694884944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.428123694884944 | validation: 1.4882292843985823]
	TIME [epoch: 10.5 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.641568354562374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.641568354562374 | validation: 1.3475640585058535]
	TIME [epoch: 10.5 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4140762919729435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4140762919729435 | validation: 1.182159073989198]
	TIME [epoch: 10.5 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3273896145580952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3273896145580952 | validation: 1.1957390070014153]
	TIME [epoch: 10.5 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.437346006485236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.437346006485236 | validation: 1.9353697980592506]
	TIME [epoch: 10.5 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6305765624716642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6305765624716642 | validation: 1.538432962049313]
	TIME [epoch: 10.5 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.516741954207895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.516741954207895 | validation: 1.129660504452314]
	TIME [epoch: 10.5 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4240944794519896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4240944794519896 | validation: 1.1885034485260182]
	TIME [epoch: 10.4 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2351629826610526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2351629826610526 | validation: 1.2223099516399072]
	TIME [epoch: 10.5 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5144550609297482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5144550609297482 | validation: 1.7610210188415556]
	TIME [epoch: 10.5 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5279727887133894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5279727887133894 | validation: 1.936782757823113]
	TIME [epoch: 10.5 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.861243281424498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.861243281424498 | validation: 1.5131751252069894]
	TIME [epoch: 10.5 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5273684898685813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5273684898685813 | validation: 1.4457102823166446]
	TIME [epoch: 10.5 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3212869154077764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3212869154077764 | validation: 0.9842519429535002]
	TIME [epoch: 10.5 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4600445660570216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4600445660570216 | validation: 1.4307820271153113]
	TIME [epoch: 10.5 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6150796538474743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6150796538474743 | validation: 1.585840566809471]
	TIME [epoch: 10.5 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5272908288073752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5272908288073752 | validation: 1.1972409460767315]
	TIME [epoch: 10.5 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4384398434428511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4384398434428511 | validation: 1.9565910455345932]
	TIME [epoch: 10.5 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.233195622908219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.233195622908219 | validation: 1.9241351583822444]
	TIME [epoch: 10.5 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9826054046993573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9826054046993573 | validation: 1.7657337945076352]
	TIME [epoch: 10.5 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.756769837173937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.756769837173937 | validation: 1.455462461749113]
	TIME [epoch: 10.5 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6624950358455863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6624950358455863 | validation: 2.4864184035850245]
	TIME [epoch: 10.5 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9710001761292588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9710001761292588 | validation: 1.7634368623944676]
	TIME [epoch: 10.5 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.703684284799116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.703684284799116 | validation: 1.2542213119214334]
	TIME [epoch: 10.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5713800423891942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5713800423891942 | validation: 1.4290174875727555]
	TIME [epoch: 10.5 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7466780190594378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7466780190594378 | validation: 1.1135820624964397]
	TIME [epoch: 10.5 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.389078865166024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.389078865166024 | validation: 1.6232032144704829]
	TIME [epoch: 10.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.393049415360625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.393049415360625 | validation: 1.6646151327417795]
	TIME [epoch: 10.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5350449534811053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5350449534811053 | validation: 1.4473757722978895]
	TIME [epoch: 10.5 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.026718234700655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.026718234700655 | validation: 1.9858732494323215]
	TIME [epoch: 10.5 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.55419056305935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.55419056305935 | validation: 1.641575723621136]
	TIME [epoch: 10.5 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8614340979489028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8614340979489028 | validation: 2.6547276418281025]
	TIME [epoch: 10.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2625220446055336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2625220446055336 | validation: 1.6918989166919327]
	TIME [epoch: 10.5 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.660594204207256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.660594204207256 | validation: 1.8685734937791072]
	TIME [epoch: 10.5 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3998039166431002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3998039166431002 | validation: 1.5009028579634072]
	TIME [epoch: 10.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.236414226470767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.236414226470767 | validation: 1.1973989048566815]
	TIME [epoch: 10.5 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5316906449379377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5316906449379377 | validation: 1.1625100875289607]
	TIME [epoch: 10.5 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3175378355369818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3175378355369818 | validation: 1.0964133907714664]
	TIME [epoch: 10.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2368367769728414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2368367769728414 | validation: 1.136624755060734]
	TIME [epoch: 10.5 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2368365328709519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2368365328709519 | validation: 1.8599570424321261]
	TIME [epoch: 10.5 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9163660092346202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9163660092346202 | validation: 4.9284377289733445]
	TIME [epoch: 10.5 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4556230091273483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4556230091273483 | validation: 1.1599992678430522]
	TIME [epoch: 10.5 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.413023433662455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.413023433662455 | validation: 1.5357384462060513]
	TIME [epoch: 10.5 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5148332271191158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5148332271191158 | validation: 1.6547380961873654]
	TIME [epoch: 10.5 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6432278253817993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6432278253817993 | validation: 1.202567193722967]
	TIME [epoch: 10.5 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8102273210353474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8102273210353474 | validation: 1.9933005797503405]
	TIME [epoch: 10.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.120301427459494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.120301427459494 | validation: 2.1686661222635473]
	TIME [epoch: 10.5 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.834878516181437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.834878516181437 | validation: 1.5250043019688786]
	TIME [epoch: 10.5 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9527678626498677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9527678626498677 | validation: 1.1707571561117138]
	TIME [epoch: 10.5 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.57639080652742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.57639080652742 | validation: 1.8141512345818303]
	TIME [epoch: 10.5 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6640111251592722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6640111251592722 | validation: 1.2741446116048964]
	TIME [epoch: 10.5 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.664399026897923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.664399026897923 | validation: 1.3055900784235237]
	TIME [epoch: 10.5 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3307686161601517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3307686161601517 | validation: 1.1639697737927261]
	TIME [epoch: 10.5 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5371088705413583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5371088705413583 | validation: 1.13147220971573]
	TIME [epoch: 10.5 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6750698472296466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6750698472296466 | validation: 1.4785421976532878]
	TIME [epoch: 10.5 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3187943967834836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3187943967834836 | validation: 1.363996531966829]
	TIME [epoch: 10.5 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5158563070119828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5158563070119828 | validation: 1.4009874361728363]
	TIME [epoch: 10.5 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.379601789576538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.379601789576538 | validation: 2.037058780428066]
	TIME [epoch: 10.5 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8358236218442268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8358236218442268 | validation: 2.463065171792256]
	TIME [epoch: 10.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0862365052595613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0862365052595613 | validation: 2.358043893004274]
	TIME [epoch: 10.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9240726765289597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9240726765289597 | validation: 1.3131164819337096]
	TIME [epoch: 10.5 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5372555359670925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5372555359670925 | validation: 1.4136875403437734]
	TIME [epoch: 10.5 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0809285734239595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0809285734239595 | validation: 2.53535839978713]
	TIME [epoch: 10.5 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6713375196289662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6713375196289662 | validation: 1.397558347052083]
	TIME [epoch: 10.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9811237001564854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9811237001564854 | validation: 1.474846977346137]
	TIME [epoch: 10.5 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8323876875037957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8323876875037957 | validation: 1.3291042786192966]
	TIME [epoch: 10.5 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9054329998901174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9054329998901174 | validation: 1.410473034160647]
	TIME [epoch: 10.5 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5856446380898137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5856446380898137 | validation: 1.4735708705707893]
	TIME [epoch: 10.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.471376691276298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.471376691276298 | validation: 0.9735475213084446]
	TIME [epoch: 10.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5053575201484457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5053575201484457 | validation: 2.8129654634538226]
	TIME [epoch: 10.5 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4804808402437204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4804808402437204 | validation: 2.6560670797553434]
	TIME [epoch: 10.5 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.876888797803906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.876888797803906 | validation: 3.5476803858349455]
	TIME [epoch: 10.5 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.011922702660013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.011922702660013 | validation: 2.0930269697459494]
	TIME [epoch: 10.5 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7573848426208234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7573848426208234 | validation: 2.316703248690798]
	TIME [epoch: 10.5 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.217078490017415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.217078490017415 | validation: 5.957261413792757]
	TIME [epoch: 10.5 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.723206113466458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.723206113466458 | validation: 6.42090866455307]
	TIME [epoch: 10.5 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.837550634786636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.837550634786636 | validation: 5.973040722524831]
	TIME [epoch: 10.5 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.691771281038093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.691771281038093 | validation: 6.182112422053385]
	TIME [epoch: 10.5 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.734330029768417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.734330029768417 | validation: 5.911684314756776]
	TIME [epoch: 10.5 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.497980732629081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.497980732629081 | validation: 5.964200288511913]
	TIME [epoch: 10.5 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.277648953491206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.277648953491206 | validation: 5.923119468398304]
	TIME [epoch: 10.5 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.661518218457422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.661518218457422 | validation: 5.8293808386862525]
	TIME [epoch: 10.5 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.843974829416437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.843974829416437 | validation: 3.0613108244246003]
	TIME [epoch: 10.5 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.349649611663648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.349649611663648 | validation: 6.260096968716257]
	TIME [epoch: 10.5 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.652590011179481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.652590011179481 | validation: 6.141425616493231]
	TIME [epoch: 10.5 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.701377124644038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.701377124644038 | validation: 6.011328734670918]
	TIME [epoch: 10.5 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.7373279748216195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.7373279748216195 | validation: 6.105275098245134]
	TIME [epoch: 10.5 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.724080325821115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.724080325821115 | validation: 5.992665086583397]
	TIME [epoch: 10.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.587680550755115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.587680550755115 | validation: 6.0434575626457265]
	TIME [epoch: 10.5 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799103105803814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.799103105803814 | validation: 4.124331337774189]
	TIME [epoch: 10.5 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.477409438288324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.477409438288324 | validation: 3.3221105165924114]
	TIME [epoch: 10.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1585719933334997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1585719933334997 | validation: 3.0727534900826097]
	TIME [epoch: 10.5 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.180960776032473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.180960776032473 | validation: 3.842996186161366]
	TIME [epoch: 10.5 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.69494943362715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.69494943362715 | validation: 2.423144931200607]
	TIME [epoch: 10.5 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.342811304734245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.342811304734245 | validation: 2.7184443311150437]
	TIME [epoch: 10.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2231286243033037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2231286243033037 | validation: 2.9532430282068565]
	TIME [epoch: 10.5 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3803701796595673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3803701796595673 | validation: 1.738337757166591]
	TIME [epoch: 10.5 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.682326538068523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.682326538068523 | validation: 2.461410060802041]
	TIME [epoch: 10.5 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9851929187218444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9851929187218444 | validation: 1.8803956761499825]
	TIME [epoch: 10.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0141863434266574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0141863434266574 | validation: 1.3557664123677966]
	TIME [epoch: 10.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.923909663320207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.923909663320207 | validation: 2.345453852066199]
	TIME [epoch: 10.5 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7649663521279528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7649663521279528 | validation: 1.518213266311542]
	TIME [epoch: 10.5 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7747838483815996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7747838483815996 | validation: 1.3804133654070054]
	TIME [epoch: 10.5 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8709374114900612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8709374114900612 | validation: 2.850458323272377]
	TIME [epoch: 10.5 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8400390590474816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8400390590474816 | validation: 2.850818323363267]
	TIME [epoch: 10.5 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0512144843385878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0512144843385878 | validation: 1.0837799670621326]
	TIME [epoch: 10.5 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4911356277297352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4911356277297352 | validation: 1.9730292760566916]
	TIME [epoch: 10.5 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6740303027648644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6740303027648644 | validation: 2.1930045544982018]
	TIME [epoch: 10.5 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183183506856762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.183183506856762 | validation: 3.735010854738127]
	TIME [epoch: 10.5 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8718863381990802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8718863381990802 | validation: 1.2798483056643897]
	TIME [epoch: 10.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4987854292863847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4987854292863847 | validation: 1.2821868105753678]
	TIME [epoch: 10.5 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5974047957319195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5974047957319195 | validation: 1.8630567744321116]
	TIME [epoch: 10.5 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6589377644553405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6589377644553405 | validation: 5.738305337786482]
	TIME [epoch: 10.5 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6971732454883908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6971732454883908 | validation: 1.6358298666280562]
	TIME [epoch: 10.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.346985624234384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.346985624234384 | validation: 1.4882000024827369]
	TIME [epoch: 10.5 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7531819290537212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7531819290537212 | validation: 1.3328090871526537]
	TIME [epoch: 10.5 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5737246082774952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5737246082774952 | validation: 2.414908412168369]
	TIME [epoch: 10.5 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1452390058614634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1452390058614634 | validation: 2.0437801102581]
	TIME [epoch: 10.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7409615881433518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7409615881433518 | validation: 1.385216828167284]
	TIME [epoch: 10.5 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4431532528286308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4431532528286308 | validation: 1.4695422355049124]
	TIME [epoch: 10.5 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7197704807827372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7197704807827372 | validation: 2.1735689328568872]
	TIME [epoch: 10.5 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0806474578166556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0806474578166556 | validation: 2.786110103688384]
	TIME [epoch: 10.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.328308192451602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.328308192451602 | validation: 1.9050628830814662]
	TIME [epoch: 10.5 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.904412210851507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.904412210851507 | validation: 1.650952598397862]
	TIME [epoch: 10.5 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.460948217264924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.460948217264924 | validation: 1.6290173264371026]
	TIME [epoch: 10.5 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2370618983970365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2370618983970365 | validation: 1.5341518291582088]
	TIME [epoch: 10.5 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7782581133413686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7782581133413686 | validation: 1.547161534967918]
	TIME [epoch: 10.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4802001384487788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4802001384487788 | validation: 1.2167716235940007]
	TIME [epoch: 10.5 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6099225817346994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6099225817346994 | validation: 1.4917515018029355]
	TIME [epoch: 10.5 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.51563685989072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.51563685989072 | validation: 1.3777177419210456]
	TIME [epoch: 10.5 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4543806145811768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4543806145811768 | validation: 1.444823154658946]
	TIME [epoch: 10.5 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.59072888674434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.59072888674434 | validation: 1.4382975271880793]
	TIME [epoch: 10.5 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2610104737251846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2610104737251846 | validation: 1.3248415698159426]
	TIME [epoch: 10.5 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.531398004507345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.531398004507345 | validation: 1.5990523261757499]
	TIME [epoch: 10.5 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4302736099804327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4302736099804327 | validation: 1.319523636258526]
	TIME [epoch: 10.5 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5483646513085807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5483646513085807 | validation: 1.5184837715759472]
	TIME [epoch: 10.5 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6184276697833515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6184276697833515 | validation: 1.4393309784468284]
	TIME [epoch: 10.5 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5350754531296666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5350754531296666 | validation: 1.1734004279046542]
	TIME [epoch: 10.5 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5835193672369954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5835193672369954 | validation: 1.6758082327329684]
	TIME [epoch: 10.5 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4864519135958205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4864519135958205 | validation: 1.5261549019126022]
	TIME [epoch: 10.5 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4163065593070965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4163065593070965 | validation: 1.2186815999085072]
	TIME [epoch: 10.5 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8766973674761875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8766973674761875 | validation: 1.8500231097839288]
	TIME [epoch: 10.5 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9385714274073418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9385714274073418 | validation: 1.3482557056324331]
	TIME [epoch: 10.5 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5533946034697632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5533946034697632 | validation: 1.4605456200742202]
	TIME [epoch: 10.5 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4562306001871161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4562306001871161 | validation: 1.150816679547591]
	TIME [epoch: 10.5 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4031201759389527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4031201759389527 | validation: 1.5521681717918563]
	TIME [epoch: 10.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7647852225114533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7647852225114533 | validation: 1.771668000615683]
	TIME [epoch: 10.5 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.713190156520109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.713190156520109 | validation: 1.3717417500786553]
	TIME [epoch: 10.5 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.471109041434574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.471109041434574 | validation: 1.0984872643070585]
	TIME [epoch: 10.5 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4513555357859849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4513555357859849 | validation: 1.1660751856018126]
	TIME [epoch: 10.5 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5128302334360193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5128302334360193 | validation: 1.5465840245184226]
	TIME [epoch: 10.5 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.498721798900001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.498721798900001 | validation: 1.2080786590613801]
	TIME [epoch: 10.5 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3313083082252568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3313083082252568 | validation: 1.448787183882421]
	TIME [epoch: 10.5 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.31406654144081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.31406654144081 | validation: 1.1443756223672992]
	TIME [epoch: 10.5 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5307735337067006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5307735337067006 | validation: 4.124877910103721]
	TIME [epoch: 10.5 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.367042364727899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.367042364727899 | validation: 2.1895637515734774]
	TIME [epoch: 10.5 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.933371556363553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.933371556363553 | validation: 1.442092692158331]
	TIME [epoch: 10.5 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5528831018832252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5528831018832252 | validation: 1.7711540812619349]
	TIME [epoch: 10.5 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2385799616928685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2385799616928685 | validation: 1.8436518530782524]
	TIME [epoch: 10.5 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9694534862877855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9694534862877855 | validation: 1.867417979842761]
	TIME [epoch: 10.5 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75009209838746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.75009209838746 | validation: 1.610360085710054]
	TIME [epoch: 10.5 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5318659052420818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5318659052420818 | validation: 1.3321866520928876]
	TIME [epoch: 10.5 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.342300959952005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.342300959952005 | validation: 1.1634607323127981]
	TIME [epoch: 10.5 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2874725773214326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2874725773214326 | validation: 2.1454889100812586]
	TIME [epoch: 10.5 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0433695895680337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0433695895680337 | validation: 1.7346484522570234]
	TIME [epoch: 10.5 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7527508126500801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7527508126500801 | validation: 3.493156479130755]
	TIME [epoch: 10.5 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.423747336718397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.423747336718397 | validation: 1.3876368692880896]
	TIME [epoch: 10.5 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4370208925037442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4370208925037442 | validation: 1.1435272838403745]
	TIME [epoch: 10.6 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8280079480930354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8280079480930354 | validation: 1.174808727755661]
	TIME [epoch: 10.5 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5219711859697012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5219711859697012 | validation: 1.371172622452229]
	TIME [epoch: 10.5 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4209765558930847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4209765558930847 | validation: 1.1831101944315745]
	TIME [epoch: 10.5 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.409230951263018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.409230951263018 | validation: 1.1943607427868534]
	TIME [epoch: 10.5 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6656441521786227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6656441521786227 | validation: 1.3268069985897832]
	TIME [epoch: 10.5 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4614845500683358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4614845500683358 | validation: 1.3416006105265041]
	TIME [epoch: 10.5 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4292000579098183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4292000579098183 | validation: 1.8177506782948087]
	TIME [epoch: 10.5 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.517508401922322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.517508401922322 | validation: 1.425794700139127]
	TIME [epoch: 10.5 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.389441085671402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.389441085671402 | validation: 1.065029666790058]
	TIME [epoch: 10.5 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1047701235212477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1047701235212477 | validation: 1.775263797295604]
	TIME [epoch: 10.5 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6890401054539097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6890401054539097 | validation: 1.197014826222452]
	TIME [epoch: 10.5 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6468699757460352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6468699757460352 | validation: 1.1998261358191935]
	TIME [epoch: 10.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6643950024497935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6643950024497935 | validation: 1.7100768285675927]
	TIME [epoch: 10.5 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4809703650296993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4809703650296993 | validation: 1.0414275514090092]
	TIME [epoch: 10.5 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3867941679129832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3867941679129832 | validation: 1.6584521709381137]
	TIME [epoch: 10.5 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3436247128538004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3436247128538004 | validation: 1.3161271036279858]
	TIME [epoch: 10.5 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2186509036096305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2186509036096305 | validation: 1.3912322995542912]
	TIME [epoch: 10.5 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3457262067979827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3457262067979827 | validation: 1.0821225299652968]
	TIME [epoch: 10.5 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4141873881936582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4141873881936582 | validation: 1.6071337239346775]
	TIME [epoch: 10.5 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9376020468170476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9376020468170476 | validation: 1.2757839282965169]
	TIME [epoch: 10.5 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4697890529707878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4697890529707878 | validation: 1.191700780945352]
	TIME [epoch: 10.5 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3639911257297723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3639911257297723 | validation: 1.356847242174437]
	TIME [epoch: 10.5 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.449645740486261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.449645740486261 | validation: 0.9984922078907257]
	TIME [epoch: 10.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3742502847887053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3742502847887053 | validation: 4.112190721257088]
	TIME [epoch: 10.5 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5036025247773432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5036025247773432 | validation: 1.0395625363388783]
	TIME [epoch: 10.5 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8098119457560506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8098119457560506 | validation: 5.243473818479528]
	TIME [epoch: 10.5 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7498433190316565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7498433190316565 | validation: 5.166062588580019]
	TIME [epoch: 10.5 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.372662245335512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.372662245335512 | validation: 5.950101503862491]
	TIME [epoch: 10.5 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.640908369213809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.640908369213809 | validation: 6.150365951764633]
	TIME [epoch: 10.5 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.663846008662969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.663846008662969 | validation: 5.836544640974482]
	TIME [epoch: 10.5 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.713491275703507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.713491275703507 | validation: 6.514601500521893]
	TIME [epoch: 10.5 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.57862465691907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.57862465691907 | validation: 5.7497429596941]
	TIME [epoch: 10.5 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3225530214047465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3225530214047465 | validation: 6.19481026295153]
	TIME [epoch: 10.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.704511792327945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.704511792327945 | validation: 6.176922694642529]
	TIME [epoch: 10.5 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.6857463078489845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6857463078489845 | validation: 6.2602273437072204]
	TIME [epoch: 10.5 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.760187083288694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.760187083288694 | validation: 5.875315903965743]
	TIME [epoch: 10.5 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.693751699719547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.693751699719547 | validation: 6.0462797084112765]
	TIME [epoch: 10.5 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.4863130882882825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4863130882882825 | validation: 5.937916733173511]
	TIME [epoch: 10.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.404160530066524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.404160530066524 | validation: 5.893940438553922]
	TIME [epoch: 10.5 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.398939559182238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.398939559182238 | validation: 5.656965810323967]
	TIME [epoch: 10.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.396804286326736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.396804286326736 | validation: 5.919668426250929]
	TIME [epoch: 10.5 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.2897565295514095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2897565295514095 | validation: 5.53106636671269]
	TIME [epoch: 10.5 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.950382541727836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.950382541727836 | validation: 5.130874282666389]
	TIME [epoch: 10.5 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.28804943133149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.28804943133149 | validation: 4.44374367565734]
	TIME [epoch: 10.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6071967796000486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6071967796000486 | validation: 3.8110553392478015]
	TIME [epoch: 10.5 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.26099715982323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.26099715982323 | validation: 2.7962240327296537]
	TIME [epoch: 10.5 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.446850148890592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.446850148890592 | validation: 2.801326049923625]
	TIME [epoch: 10.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3054706261159645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3054706261159645 | validation: 2.7838737064904433]
	TIME [epoch: 10.5 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2353988478947264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2353988478947264 | validation: 2.512465966199018]
	TIME [epoch: 10.5 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0890263603016583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0890263603016583 | validation: 2.2491386244167417]
	TIME [epoch: 10.5 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0870540767283456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0870540767283456 | validation: 2.681076012396932]
	TIME [epoch: 10.5 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3981767788147446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3981767788147446 | validation: 2.3690124173987366]
	TIME [epoch: 10.5 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1046369555188216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1046369555188216 | validation: 2.0098917854491463]
	TIME [epoch: 10.5 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9443892325515826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9443892325515826 | validation: 2.1623660726579286]
	TIME [epoch: 10.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9959014124353847		[learning rate: 0.0099755]
	Learning Rate: 0.00997547
	LOSS [training: 1.9959014124353847 | validation: 2.009810891224468]
	TIME [epoch: 10.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.436996444293828		[learning rate: 0.0099449]
	Learning Rate: 0.00994489
	LOSS [training: 2.436996444293828 | validation: 2.3191292401652936]
	TIME [epoch: 10.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9611071533561624		[learning rate: 0.0099144]
	Learning Rate: 0.0099144
	LOSS [training: 1.9611071533561624 | validation: 2.4525761104267]
	TIME [epoch: 10.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7771041584399567		[learning rate: 0.009884]
	Learning Rate: 0.00988401
	LOSS [training: 2.7771041584399567 | validation: 2.359171854739673]
	TIME [epoch: 10.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8984970144672009		[learning rate: 0.0098537]
	Learning Rate: 0.00985371
	LOSS [training: 1.8984970144672009 | validation: 2.035526602232169]
	TIME [epoch: 10.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8060708801102254		[learning rate: 0.0098235]
	Learning Rate: 0.00982351
	LOSS [training: 1.8060708801102254 | validation: 1.9797114225924524]
	TIME [epoch: 10.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1032047291527705		[learning rate: 0.0097934]
	Learning Rate: 0.0097934
	LOSS [training: 2.1032047291527705 | validation: 1.952332873701725]
	TIME [epoch: 10.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.101564061192689		[learning rate: 0.0097634]
	Learning Rate: 0.00976337
	LOSS [training: 2.101564061192689 | validation: 1.8865777308941887]
	TIME [epoch: 10.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7471508733444285		[learning rate: 0.0097334]
	Learning Rate: 0.00973345
	LOSS [training: 1.7471508733444285 | validation: 2.206258165489178]
	TIME [epoch: 10.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.827926041272237		[learning rate: 0.0097036]
	Learning Rate: 0.00970361
	LOSS [training: 1.827926041272237 | validation: 2.475596467307836]
	TIME [epoch: 10.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8803433695787235		[learning rate: 0.0096739]
	Learning Rate: 0.00967386
	LOSS [training: 1.8803433695787235 | validation: 1.8620928792996478]
	TIME [epoch: 10.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7041031974708443		[learning rate: 0.0096442]
	Learning Rate: 0.00964421
	LOSS [training: 1.7041031974708443 | validation: 2.317535707676212]
	TIME [epoch: 10.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0153544044570144		[learning rate: 0.0096146]
	Learning Rate: 0.00961465
	LOSS [training: 2.0153544044570144 | validation: 2.209403062439057]
	TIME [epoch: 10.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6922276410013297		[learning rate: 0.0095852]
	Learning Rate: 0.00958517
	LOSS [training: 1.6922276410013297 | validation: 1.5932142763307622]
	TIME [epoch: 10.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5706053622189784		[learning rate: 0.0095558]
	Learning Rate: 0.00955579
	LOSS [training: 1.5706053622189784 | validation: 1.9233019729456766]
	TIME [epoch: 10.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8430470682084188		[learning rate: 0.0095265]
	Learning Rate: 0.0095265
	LOSS [training: 1.8430470682084188 | validation: 2.1091269207600516]
	TIME [epoch: 10.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6875986441014803		[learning rate: 0.0094973]
	Learning Rate: 0.0094973
	LOSS [training: 1.6875986441014803 | validation: 1.9657544688141753]
	TIME [epoch: 10.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.620289078697682		[learning rate: 0.0094682]
	Learning Rate: 0.00946818
	LOSS [training: 1.620289078697682 | validation: 1.9086416490316134]
	TIME [epoch: 10.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5414257890290544		[learning rate: 0.0094392]
	Learning Rate: 0.00943916
	LOSS [training: 1.5414257890290544 | validation: 1.973798670599934]
	TIME [epoch: 10.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5059320949780717		[learning rate: 0.0094102]
	Learning Rate: 0.00941022
	LOSS [training: 1.5059320949780717 | validation: 1.856003533164359]
	TIME [epoch: 10.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5542807901572595		[learning rate: 0.0093814]
	Learning Rate: 0.00938138
	LOSS [training: 1.5542807901572595 | validation: 1.6781613785993785]
	TIME [epoch: 10.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4814225298387336		[learning rate: 0.0093526]
	Learning Rate: 0.00935262
	LOSS [training: 1.4814225298387336 | validation: 1.8431286719015934]
	TIME [epoch: 10.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6873631403808567		[learning rate: 0.009324]
	Learning Rate: 0.00932395
	LOSS [training: 1.6873631403808567 | validation: 1.6851128419293977]
	TIME [epoch: 10.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7229562525267645		[learning rate: 0.0092954]
	Learning Rate: 0.00929537
	LOSS [training: 1.7229562525267645 | validation: 1.8676571092351992]
	TIME [epoch: 10.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.488123486957775		[learning rate: 0.0092669]
	Learning Rate: 0.00926687
	LOSS [training: 1.488123486957775 | validation: 1.6257567190811193]
	TIME [epoch: 10.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5393497999638464		[learning rate: 0.0092385]
	Learning Rate: 0.00923847
	LOSS [training: 1.5393497999638464 | validation: 2.1930718849061424]
	TIME [epoch: 10.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7063396115116227		[learning rate: 0.0092101]
	Learning Rate: 0.00921015
	LOSS [training: 1.7063396115116227 | validation: 1.7286050619109683]
	TIME [epoch: 10.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4274746329743269		[learning rate: 0.0091819]
	Learning Rate: 0.00918192
	LOSS [training: 1.4274746329743269 | validation: 1.488952295583868]
	TIME [epoch: 10.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4312108539684234		[learning rate: 0.0091538]
	Learning Rate: 0.00915377
	LOSS [training: 1.4312108539684234 | validation: 1.5531726631930187]
	TIME [epoch: 10.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.886162975299315		[learning rate: 0.0091257]
	Learning Rate: 0.00912571
	LOSS [training: 2.886162975299315 | validation: 0.9959994854225954]
	TIME [epoch: 10.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.878178986295877		[learning rate: 0.0090977]
	Learning Rate: 0.00909774
	LOSS [training: 1.878178986295877 | validation: 3.4173450877417237]
	TIME [epoch: 10.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4471660002629552		[learning rate: 0.0090698]
	Learning Rate: 0.00906985
	LOSS [training: 1.4471660002629552 | validation: 2.331489076198766]
	TIME [epoch: 10.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5361166199535297		[learning rate: 0.009042]
	Learning Rate: 0.00904205
	LOSS [training: 1.5361166199535297 | validation: 0.7364527608959722]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5034847589658966		[learning rate: 0.0090143]
	Learning Rate: 0.00901433
	LOSS [training: 1.5034847589658966 | validation: 1.0164936816110506]
	TIME [epoch: 10.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2770508121972448		[learning rate: 0.0089867]
	Learning Rate: 0.00898669
	LOSS [training: 1.2770508121972448 | validation: 1.1422363174367476]
	TIME [epoch: 10.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5213255054931425		[learning rate: 0.0089591]
	Learning Rate: 0.00895915
	LOSS [training: 2.5213255054931425 | validation: 1.5711320123996972]
	TIME [epoch: 10.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1950305681689732		[learning rate: 0.0089317]
	Learning Rate: 0.00893168
	LOSS [training: 1.1950305681689732 | validation: 1.3078292672891445]
	TIME [epoch: 10.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2738873898958074		[learning rate: 0.0089043]
	Learning Rate: 0.0089043
	LOSS [training: 2.2738873898958074 | validation: 5.056862419874475]
	TIME [epoch: 10.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.910413524338037		[learning rate: 0.008877]
	Learning Rate: 0.00887701
	LOSS [training: 2.910413524338037 | validation: 1.2898841166537682]
	TIME [epoch: 10.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5461237235162746		[learning rate: 0.0088498]
	Learning Rate: 0.0088498
	LOSS [training: 1.5461237235162746 | validation: 1.604452288791611]
	TIME [epoch: 10.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.418002399511464		[learning rate: 0.0088227]
	Learning Rate: 0.00882267
	LOSS [training: 1.418002399511464 | validation: 0.8704953352058852]
	TIME [epoch: 10.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0288986521529508		[learning rate: 0.0087956]
	Learning Rate: 0.00879562
	LOSS [training: 1.0288986521529508 | validation: 0.9366170156688602]
	TIME [epoch: 10.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9422970378869288		[learning rate: 0.0087687]
	Learning Rate: 0.00876866
	LOSS [training: 0.9422970378869288 | validation: 1.4658035505662432]
	TIME [epoch: 10.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9489095742127676		[learning rate: 0.0087418]
	Learning Rate: 0.00874178
	LOSS [training: 0.9489095742127676 | validation: 0.9141031284202719]
	TIME [epoch: 10.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1802646565285397		[learning rate: 0.008715]
	Learning Rate: 0.00871499
	LOSS [training: 3.1802646565285397 | validation: 3.9339870443093714]
	TIME [epoch: 10.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.17360108813324		[learning rate: 0.0086883]
	Learning Rate: 0.00868827
	LOSS [training: 2.17360108813324 | validation: 1.0222098504349817]
	TIME [epoch: 10.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4226562341705897		[learning rate: 0.0086616]
	Learning Rate: 0.00866164
	LOSS [training: 1.4226562341705897 | validation: 1.5672832776657106]
	TIME [epoch: 10.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.539476060721215		[learning rate: 0.0086351]
	Learning Rate: 0.00863509
	LOSS [training: 1.539476060721215 | validation: 1.9938051781363124]
	TIME [epoch: 10.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1220237209099622		[learning rate: 0.0086086]
	Learning Rate: 0.00860862
	LOSS [training: 1.1220237209099622 | validation: 1.4139515756134122]
	TIME [epoch: 10.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.051648719978108		[learning rate: 0.0085822]
	Learning Rate: 0.00858223
	LOSS [training: 1.051648719978108 | validation: 0.6839762732348725]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3209485491544506		[learning rate: 0.0085559]
	Learning Rate: 0.00855592
	LOSS [training: 1.3209485491544506 | validation: 1.0827111886415466]
	TIME [epoch: 10.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9166116022828122		[learning rate: 0.0085297]
	Learning Rate: 0.00852969
	LOSS [training: 0.9166116022828122 | validation: 1.0569822638132085]
	TIME [epoch: 10.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0620292837628538		[learning rate: 0.0085035]
	Learning Rate: 0.00850354
	LOSS [training: 1.0620292837628538 | validation: 2.3280683611160096]
	TIME [epoch: 10.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3462904362742374		[learning rate: 0.0084775]
	Learning Rate: 0.00847748
	LOSS [training: 1.3462904362742374 | validation: 1.175249970397393]
	TIME [epoch: 10.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1394806466717846		[learning rate: 0.0084515]
	Learning Rate: 0.00845149
	LOSS [training: 1.1394806466717846 | validation: 0.9784222581198848]
	TIME [epoch: 10.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8520953398293679		[learning rate: 0.0084256]
	Learning Rate: 0.00842558
	LOSS [training: 0.8520953398293679 | validation: 0.7369249067829778]
	TIME [epoch: 10.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9120352892178856		[learning rate: 0.0083998]
	Learning Rate: 0.00839976
	LOSS [training: 0.9120352892178856 | validation: 1.4845689391091819]
	TIME [epoch: 10.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.13097268400258		[learning rate: 0.008374]
	Learning Rate: 0.00837401
	LOSS [training: 1.13097268400258 | validation: 1.372395031760433]
	TIME [epoch: 10.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9552537170954197		[learning rate: 0.0083483]
	Learning Rate: 0.00834834
	LOSS [training: 0.9552537170954197 | validation: 1.2071954944533076]
	TIME [epoch: 10.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0958490396146106		[learning rate: 0.0083227]
	Learning Rate: 0.00832275
	LOSS [training: 1.0958490396146106 | validation: 1.1009207248254196]
	TIME [epoch: 10.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7897403589802814		[learning rate: 0.0082972]
	Learning Rate: 0.00829723
	LOSS [training: 0.7897403589802814 | validation: 1.0975102909818284]
	TIME [epoch: 10.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9502521944471818		[learning rate: 0.0082718]
	Learning Rate: 0.0082718
	LOSS [training: 0.9502521944471818 | validation: 0.7302773906273984]
	TIME [epoch: 10.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1815074911757897		[learning rate: 0.0082464]
	Learning Rate: 0.00824644
	LOSS [training: 1.1815074911757897 | validation: 1.4040893744791452]
	TIME [epoch: 10.5 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8004483615136367		[learning rate: 0.0082212]
	Learning Rate: 0.00822116
	LOSS [training: 0.8004483615136367 | validation: 0.8048577005129233]
	TIME [epoch: 10.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1366917545707536		[learning rate: 0.008196]
	Learning Rate: 0.00819596
	LOSS [training: 1.1366917545707536 | validation: 1.137632897090425]
	TIME [epoch: 10.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0395018637123419		[learning rate: 0.0081708]
	Learning Rate: 0.00817084
	LOSS [training: 1.0395018637123419 | validation: 0.8236331363276024]
	TIME [epoch: 10.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6590494399798146		[learning rate: 0.0081458]
	Learning Rate: 0.00814579
	LOSS [training: 0.6590494399798146 | validation: 0.9468899859361583]
	TIME [epoch: 10.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8781086576291216		[learning rate: 0.0081208]
	Learning Rate: 0.00812082
	LOSS [training: 0.8781086576291216 | validation: 0.6704973465288764]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8104241220545159		[learning rate: 0.0080959]
	Learning Rate: 0.00809593
	LOSS [training: 0.8104241220545159 | validation: 0.6118159824871254]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9971877248170917		[learning rate: 0.0080711]
	Learning Rate: 0.00807111
	LOSS [training: 0.9971877248170917 | validation: 5.1672090219626865]
	TIME [epoch: 10.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.700029568600738		[learning rate: 0.0080464]
	Learning Rate: 0.00804637
	LOSS [training: 2.700029568600738 | validation: 1.500509409310874]
	TIME [epoch: 10.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9207065813365602		[learning rate: 0.0080217]
	Learning Rate: 0.0080217
	LOSS [training: 0.9207065813365602 | validation: 1.743084061618809]
	TIME [epoch: 10.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1586276916016165		[learning rate: 0.0079971]
	Learning Rate: 0.00799711
	LOSS [training: 1.1586276916016165 | validation: 1.3587392854380766]
	TIME [epoch: 10.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1154078227834776		[learning rate: 0.0079726]
	Learning Rate: 0.0079726
	LOSS [training: 1.1154078227834776 | validation: 1.4302208200881648]
	TIME [epoch: 10.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9170460871406089		[learning rate: 0.0079482]
	Learning Rate: 0.00794816
	LOSS [training: 0.9170460871406089 | validation: 0.8160919675352568]
	TIME [epoch: 10.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.856950326969408		[learning rate: 0.0079238]
	Learning Rate: 0.0079238
	LOSS [training: 0.856950326969408 | validation: 0.854533921325468]
	TIME [epoch: 10.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.742398816957782		[learning rate: 0.0078995]
	Learning Rate: 0.00789951
	LOSS [training: 0.742398816957782 | validation: 0.8510465290065342]
	TIME [epoch: 10.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6778100772533697		[learning rate: 0.0078753]
	Learning Rate: 0.00787529
	LOSS [training: 0.6778100772533697 | validation: 0.6780820375030424]
	TIME [epoch: 10.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9697657508113592		[learning rate: 0.0078512]
	Learning Rate: 0.00785115
	LOSS [training: 0.9697657508113592 | validation: 2.778106435026136]
	TIME [epoch: 10.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3904171897312165		[learning rate: 0.0078271]
	Learning Rate: 0.00782708
	LOSS [training: 1.3904171897312165 | validation: 0.6829863264560737]
	TIME [epoch: 10.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7801752226610311		[learning rate: 0.0078031]
	Learning Rate: 0.00780309
	LOSS [training: 0.7801752226610311 | validation: 1.0810746683349428]
	TIME [epoch: 10.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0869594721819427		[learning rate: 0.0077792]
	Learning Rate: 0.00777917
	LOSS [training: 1.0869594721819427 | validation: 0.8843968648852163]
	TIME [epoch: 10.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.172987462167073		[learning rate: 0.0077553]
	Learning Rate: 0.00775532
	LOSS [training: 1.172987462167073 | validation: 0.7678740710635812]
	TIME [epoch: 10.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8634428641842403		[learning rate: 0.0077316]
	Learning Rate: 0.00773155
	LOSS [training: 0.8634428641842403 | validation: 0.9442732329261648]
	TIME [epoch: 10.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7266279321888789		[learning rate: 0.0077079]
	Learning Rate: 0.00770785
	LOSS [training: 0.7266279321888789 | validation: 0.7028135183734489]
	TIME [epoch: 10.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8293876975038025		[learning rate: 0.0076842]
	Learning Rate: 0.00768422
	LOSS [training: 0.8293876975038025 | validation: 0.585268778121258]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3191945277364077		[learning rate: 0.0076607]
	Learning Rate: 0.00766067
	LOSS [training: 1.3191945277364077 | validation: 1.067237651427713]
	TIME [epoch: 10.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2940828021183992		[learning rate: 0.0076372]
	Learning Rate: 0.00763718
	LOSS [training: 1.2940828021183992 | validation: 1.9272800993381751]
	TIME [epoch: 10.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2359298017270457		[learning rate: 0.0076138]
	Learning Rate: 0.00761377
	LOSS [training: 1.2359298017270457 | validation: 0.9648016649606151]
	TIME [epoch: 10.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.09033682763804		[learning rate: 0.0075904]
	Learning Rate: 0.00759043
	LOSS [training: 1.09033682763804 | validation: 1.3453454276454333]
	TIME [epoch: 10.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9552216443788121		[learning rate: 0.0075672]
	Learning Rate: 0.00756717
	LOSS [training: 0.9552216443788121 | validation: 0.8249660847879469]
	TIME [epoch: 10.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976112338992398		[learning rate: 0.007544]
	Learning Rate: 0.00754397
	LOSS [training: 0.6976112338992398 | validation: 0.43527202383248564]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8866438149736242		[learning rate: 0.0075208]
	Learning Rate: 0.00752085
	LOSS [training: 0.8866438149736242 | validation: 0.7570646438336394]
	TIME [epoch: 10.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8067716981067978		[learning rate: 0.0074978]
	Learning Rate: 0.00749779
	LOSS [training: 0.8067716981067978 | validation: 1.8909917325467154]
	TIME [epoch: 10.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9242937764040817		[learning rate: 0.0074748]
	Learning Rate: 0.00747481
	LOSS [training: 0.9242937764040817 | validation: 0.8318417838990972]
	TIME [epoch: 10.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9878691995641505		[learning rate: 0.0074519]
	Learning Rate: 0.00745189
	LOSS [training: 0.9878691995641505 | validation: 0.566786861314921]
	TIME [epoch: 10.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0207324873589125		[learning rate: 0.0074291]
	Learning Rate: 0.00742905
	LOSS [training: 1.0207324873589125 | validation: 0.7118236183564578]
	TIME [epoch: 10.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6701769319956357		[learning rate: 0.0074063]
	Learning Rate: 0.00740628
	LOSS [training: 0.6701769319956357 | validation: 0.8078982147319772]
	TIME [epoch: 10.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7101351874788128		[learning rate: 0.0073836]
	Learning Rate: 0.00738357
	LOSS [training: 0.7101351874788128 | validation: 2.5708135398606577]
	TIME [epoch: 10.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6308594987326874		[learning rate: 0.0073609]
	Learning Rate: 0.00736094
	LOSS [training: 1.6308594987326874 | validation: 2.0444806180371113]
	TIME [epoch: 10.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0640890626847497		[learning rate: 0.0073384]
	Learning Rate: 0.00733838
	LOSS [training: 1.0640890626847497 | validation: 0.9383719750982007]
	TIME [epoch: 10.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7830224342336318		[learning rate: 0.0073159]
	Learning Rate: 0.00731588
	LOSS [training: 0.7830224342336318 | validation: 0.9188270172127986]
	TIME [epoch: 10.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6495958404678076		[learning rate: 0.0072935]
	Learning Rate: 0.00729345
	LOSS [training: 0.6495958404678076 | validation: 0.4404400533288183]
	TIME [epoch: 10.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.650524958887208		[learning rate: 0.0072711]
	Learning Rate: 0.0072711
	LOSS [training: 0.650524958887208 | validation: 0.48912481723742945]
	TIME [epoch: 10.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.730423674024092		[learning rate: 0.0072488]
	Learning Rate: 0.00724881
	LOSS [training: 0.730423674024092 | validation: 0.9082577502873269]
	TIME [epoch: 10.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0592966484072597		[learning rate: 0.0072266]
	Learning Rate: 0.00722659
	LOSS [training: 1.0592966484072597 | validation: 1.195782549349301]
	TIME [epoch: 10.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7849640141023801		[learning rate: 0.0072044]
	Learning Rate: 0.00720444
	LOSS [training: 0.7849640141023801 | validation: 1.3750774074575438]
	TIME [epoch: 10.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7659873494176688		[learning rate: 0.0071824]
	Learning Rate: 0.00718235
	LOSS [training: 0.7659873494176688 | validation: 0.721669449884509]
	TIME [epoch: 10.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7878624905149018		[learning rate: 0.0071603]
	Learning Rate: 0.00716033
	LOSS [training: 0.7878624905149018 | validation: 0.5852790390644976]
	TIME [epoch: 10.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8434594709343761		[learning rate: 0.0071384]
	Learning Rate: 0.00713838
	LOSS [training: 0.8434594709343761 | validation: 0.7146895080480948]
	TIME [epoch: 10.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7366809831050635		[learning rate: 0.0071165]
	Learning Rate: 0.0071165
	LOSS [training: 0.7366809831050635 | validation: 1.2910364713594238]
	TIME [epoch: 10.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9247298192399583		[learning rate: 0.0070947]
	Learning Rate: 0.00709469
	LOSS [training: 0.9247298192399583 | validation: 0.6235751397481044]
	TIME [epoch: 10.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8216910004490099		[learning rate: 0.0070729]
	Learning Rate: 0.00707294
	LOSS [training: 0.8216910004490099 | validation: 2.0799970634261435]
	TIME [epoch: 10.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5788339348520497		[learning rate: 0.0070513]
	Learning Rate: 0.00705126
	LOSS [training: 1.5788339348520497 | validation: 1.228063928824003]
	TIME [epoch: 10.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0968555377530718		[learning rate: 0.0070296]
	Learning Rate: 0.00702964
	LOSS [training: 1.0968555377530718 | validation: 0.7951349382487544]
	TIME [epoch: 10.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8054120102231084		[learning rate: 0.0070081]
	Learning Rate: 0.00700809
	LOSS [training: 0.8054120102231084 | validation: 0.7268791141885756]
	TIME [epoch: 10.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6047469123141436		[learning rate: 0.0069866]
	Learning Rate: 0.00698661
	LOSS [training: 0.6047469123141436 | validation: 0.7052332850278552]
	TIME [epoch: 10.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7439907611762087		[learning rate: 0.0069652]
	Learning Rate: 0.0069652
	LOSS [training: 0.7439907611762087 | validation: 0.7013794019773044]
	TIME [epoch: 10.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1306170506299504		[learning rate: 0.0069438]
	Learning Rate: 0.00694384
	LOSS [training: 1.1306170506299504 | validation: 2.1450781464582507]
	TIME [epoch: 10.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2167000436268922		[learning rate: 0.0069226]
	Learning Rate: 0.00692256
	LOSS [training: 1.2167000436268922 | validation: 0.6464636370971695]
	TIME [epoch: 10.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7372529234340213		[learning rate: 0.0069013]
	Learning Rate: 0.00690134
	LOSS [training: 0.7372529234340213 | validation: 0.5202231824246419]
	TIME [epoch: 10.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5747119453383794		[learning rate: 0.0068802]
	Learning Rate: 0.00688018
	LOSS [training: 0.5747119453383794 | validation: 0.8601183437246411]
	TIME [epoch: 10.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.659504551917444		[learning rate: 0.0068591]
	Learning Rate: 0.00685909
	LOSS [training: 0.659504551917444 | validation: 0.6919005401549367]
	TIME [epoch: 10.5 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.85410859605417		[learning rate: 0.0068381]
	Learning Rate: 0.00683807
	LOSS [training: 0.85410859605417 | validation: 0.6101727125456652]
	TIME [epoch: 10.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5741260838028468		[learning rate: 0.0068171]
	Learning Rate: 0.0068171
	LOSS [training: 0.5741260838028468 | validation: 0.42701776174499556]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r0_20240217_073951/states/model_tr_study5_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6868415469117962		[learning rate: 0.0067962]
	Learning Rate: 0.00679621
	LOSS [training: 0.6868415469117962 | validation: 1.0750080246774787]
	TIME [epoch: 10.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7698019354168231		[learning rate: 0.0067754]
	Learning Rate: 0.00677537
	LOSS [training: 0.7698019354168231 | validation: 0.743850921267368]
	TIME [epoch: 10.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934059864375348		[learning rate: 0.0067546]
	Learning Rate: 0.00675461
	LOSS [training: 0.6934059864375348 | validation: 0.48723977054822326]
	TIME [epoch: 10.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7805002163858171		[learning rate: 0.0067339]
	Learning Rate: 0.0067339
	LOSS [training: 0.7805002163858171 | validation: 0.4408456785558467]
	TIME [epoch: 10.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.752748209279898		[learning rate: 0.0067133]
	Learning Rate: 0.00671326
	LOSS [training: 0.752748209279898 | validation: 0.7882311401331025]
	TIME [epoch: 10.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7532486473889192		[learning rate: 0.0066927]
	Learning Rate: 0.00669268
	LOSS [training: 0.7532486473889192 | validation: 0.5050906835691883]
	TIME [epoch: 10.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6665521663564824		[learning rate: 0.0066722]
	Learning Rate: 0.00667216
	LOSS [training: 0.6665521663564824 | validation: 1.0037106727706688]
	TIME [epoch: 10.5 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: nan		[learning rate: 0.0066517]
ERROR:
nan encountered in epoch 632 (training loss).
