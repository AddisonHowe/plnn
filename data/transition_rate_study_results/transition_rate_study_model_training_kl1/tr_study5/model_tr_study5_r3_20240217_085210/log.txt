Args:
Namespace(name='model_tr_study5', outdir='out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3', training_data='data/transition_rate_studies/tr_study5/tr_study5_training/r3', validation_data='data/transition_rate_studies/tr_study5/tr_study5_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3641458755

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.040558039344019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.040558039344019 | validation: 11.60577507794308]
	TIME [epoch: 49.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.264070214819549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.264070214819549 | validation: 11.285866152414888]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.981143290484917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.981143290484917 | validation: 10.260104221751304]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.306321141367029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.306321141367029 | validation: 9.878012467258952]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.288501899254923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.288501899254923 | validation: 9.807050397958879]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.513297142798853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.513297142798853 | validation: 8.913002715900985]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.896248767155607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.896248767155607 | validation: 8.354956880515282]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.898939141834603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.898939141834603 | validation: 8.96457886073076]
	TIME [epoch: 10.3 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.348569774843456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.348569774843456 | validation: 8.004361289534685]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.058694805734293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.058694805734293 | validation: 7.920918954526309]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.987146202505672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.987146202505672 | validation: 8.28362274186305]
	TIME [epoch: 10.3 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.857931422277355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.857931422277355 | validation: 7.42364836665411]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.755373677017067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.755373677017067 | validation: 7.638878899209722]
	TIME [epoch: 10.3 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.327083874586888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.327083874586888 | validation: 7.187213833968785]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.991706593079849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.991706593079849 | validation: 6.967554064181513]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.546662929594707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.546662929594707 | validation: 6.79954717713358]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.465500546520651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.465500546520651 | validation: 7.026885044341151]
	TIME [epoch: 10.3 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.417415275345147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.417415275345147 | validation: 6.966865948371151]
	TIME [epoch: 10.3 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.247354613873152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.247354613873152 | validation: 7.073028505402671]
	TIME [epoch: 10.3 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.506463010733087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.506463010733087 | validation: 6.82818359059334]
	TIME [epoch: 10.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.380263522226061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.380263522226061 | validation: 6.843038965245219]
	TIME [epoch: 10.3 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.33996121478763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.33996121478763 | validation: 6.914841587803799]
	TIME [epoch: 10.3 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.30230654089908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.30230654089908 | validation: 6.842975144218136]
	TIME [epoch: 10.3 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.396458072727979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.396458072727979 | validation: 7.483526679376563]
	TIME [epoch: 10.3 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.601607439106141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.601607439106141 | validation: 6.726694076572244]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3366848653734875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3366848653734875 | validation: 6.83493959440809]
	TIME [epoch: 10.3 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.290166763876757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.290166763876757 | validation: 6.746773059027646]
	TIME [epoch: 10.3 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.546900330005082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.546900330005082 | validation: 7.326855851830754]
	TIME [epoch: 10.3 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.56261346147316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.56261346147316 | validation: 6.831189032453958]
	TIME [epoch: 10.3 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.2753848533719605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2753848533719605 | validation: 6.841306003993415]
	TIME [epoch: 10.3 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.324638140706677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.324638140706677 | validation: 6.766960790452357]
	TIME [epoch: 10.3 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.174209072590541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.174209072590541 | validation: 6.986157136375944]
	TIME [epoch: 10.3 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.172749473308026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.172749473308026 | validation: 6.604085582396099]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.322894525452301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.322894525452301 | validation: 6.7090831470109995]
	TIME [epoch: 10.3 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.440350633532773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.440350633532773 | validation: 6.590454486821114]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.114682827630361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.114682827630361 | validation: 6.79561894279548]
	TIME [epoch: 10.4 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.710288999916661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.710288999916661 | validation: 6.681570756729973]
	TIME [epoch: 10.3 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.317110674052367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.317110674052367 | validation: 6.813454624263346]
	TIME [epoch: 10.3 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1365447873571854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1365447873571854 | validation: 6.74270296056726]
	TIME [epoch: 10.3 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.193355116671084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.193355116671084 | validation: 6.5205511993631955]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3530878547381215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3530878547381215 | validation: 6.679625038682884]
	TIME [epoch: 10.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.531341133030827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.531341133030827 | validation: 6.59639880053747]
	TIME [epoch: 10.3 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.125163344609895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.125163344609895 | validation: 6.725139002086196]
	TIME [epoch: 10.3 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3896629883113905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3896629883113905 | validation: 6.70088296541491]
	TIME [epoch: 10.3 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3418929544624065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3418929544624065 | validation: 6.854276536205657]
	TIME [epoch: 10.3 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.095216125754365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.095216125754365 | validation: 6.658939415441879]
	TIME [epoch: 10.3 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.935593270467398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.935593270467398 | validation: 6.5673186262233205]
	TIME [epoch: 10.3 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.132412236371105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.132412236371105 | validation: 6.710542638013826]
	TIME [epoch: 10.3 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.298546548881055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.298546548881055 | validation: 6.914756954709177]
	TIME [epoch: 10.3 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.026837243777389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.026837243777389 | validation: 6.647333351294906]
	TIME [epoch: 10.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.060721739191509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.060721739191509 | validation: 6.500696895492292]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.204291185107311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.204291185107311 | validation: 6.642469950770257]
	TIME [epoch: 10.3 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.958888866220112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.958888866220112 | validation: 6.667783400486571]
	TIME [epoch: 10.3 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0173644693587605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0173644693587605 | validation: 6.514803344305008]
	TIME [epoch: 10.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1212237657676525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1212237657676525 | validation: 6.557439280648445]
	TIME [epoch: 10.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.2759783294506075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2759783294506075 | validation: 6.674126470302224]
	TIME [epoch: 10.3 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.117434266713493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.117434266713493 | validation: 6.532042734227932]
	TIME [epoch: 10.3 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.942827370702727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.942827370702727 | validation: 6.896662704929408]
	TIME [epoch: 10.3 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.082637967820502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.082637967820502 | validation: 6.481853103430113]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.049321895715674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.049321895715674 | validation: 6.46269534903441]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.983960512619801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.983960512619801 | validation: 6.487239532437809]
	TIME [epoch: 10.3 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.03947648353881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.03947648353881 | validation: 6.6044806278215065]
	TIME [epoch: 10.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0762493065318735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0762493065318735 | validation: 6.571745393283908]
	TIME [epoch: 10.3 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1444692927438656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1444692927438656 | validation: 6.609852474478883]
	TIME [epoch: 10.3 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8948461810068995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8948461810068995 | validation: 6.7708499636098365]
	TIME [epoch: 10.3 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.074006009239957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.074006009239957 | validation: 6.567991248651721]
	TIME [epoch: 10.3 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.869598035276445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.869598035276445 | validation: 6.56493124425445]
	TIME [epoch: 10.3 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.941701150327451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.941701150327451 | validation: 6.521635434635234]
	TIME [epoch: 10.3 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.101869938505033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.101869938505033 | validation: 6.460224118193551]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.011139912590159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.011139912590159 | validation: 6.49401545502179]
	TIME [epoch: 10.3 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.906368626199362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.906368626199362 | validation: 6.426352779609823]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.852048662539202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.852048662539202 | validation: 6.4455825887454035]
	TIME [epoch: 10.3 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.969220208922856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.969220208922856 | validation: 6.566996647328717]
	TIME [epoch: 10.3 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.898838027490993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.898838027490993 | validation: 6.5017309825789065]
	TIME [epoch: 10.3 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.807763577808283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.807763577808283 | validation: 6.9679028080625836]
	TIME [epoch: 10.3 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.97503946445839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.97503946445839 | validation: 6.528613325345775]
	TIME [epoch: 10.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8028201534795185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8028201534795185 | validation: 6.369105472132831]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.773996941446366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.773996941446366 | validation: 6.757533794816841]
	TIME [epoch: 10.3 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.890985069900715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.890985069900715 | validation: 6.5386332997961105]
	TIME [epoch: 10.3 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.934972979952175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.934972979952175 | validation: 6.309621795529452]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.746411983785514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.746411983785514 | validation: 6.374222048776747]
	TIME [epoch: 10.3 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.883002189737353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.883002189737353 | validation: 6.396674642423781]
	TIME [epoch: 10.3 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7989592207233205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7989592207233205 | validation: 6.562963146583647]
	TIME [epoch: 10.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.852636226173975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.852636226173975 | validation: 6.333309920709387]
	TIME [epoch: 10.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.845963248052454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.845963248052454 | validation: 6.413110015421887]
	TIME [epoch: 10.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.749894348025993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.749894348025993 | validation: 6.22822255709521]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.778346559197589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.778346559197589 | validation: 6.7605876641304565]
	TIME [epoch: 10.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.897988960856606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.897988960856606 | validation: 6.419233067171204]
	TIME [epoch: 10.3 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.905590544496112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.905590544496112 | validation: 6.476920096027226]
	TIME [epoch: 10.3 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8176004991011885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8176004991011885 | validation: 6.533563052548581]
	TIME [epoch: 10.3 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.745472700090078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.745472700090078 | validation: 6.366521502558073]
	TIME [epoch: 10.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6453002654087925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6453002654087925 | validation: 6.693358967796057]
	TIME [epoch: 10.3 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.683636300703244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.683636300703244 | validation: 6.142139917578711]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.743525742714608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.743525742714608 | validation: 6.5884916490152685]
	TIME [epoch: 10.3 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.795063614174302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.795063614174302 | validation: 6.2230836161250025]
	TIME [epoch: 10.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.613004040359991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.613004040359991 | validation: 6.230348309261538]
	TIME [epoch: 10.3 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.603024605936246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.603024605936246 | validation: 6.189731668027583]
	TIME [epoch: 10.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7738626415292895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7738626415292895 | validation: 6.969561551914302]
	TIME [epoch: 10.3 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.901227006632576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.901227006632576 | validation: 6.389097753527152]
	TIME [epoch: 10.3 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.626560930881439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.626560930881439 | validation: 6.150729994283982]
	TIME [epoch: 10.3 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.623968992483121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.623968992483121 | validation: 6.317022909540235]
	TIME [epoch: 10.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.626158773055627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.626158773055627 | validation: 6.00282875314874]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.523700918188842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.523700918188842 | validation: 6.8036186727738]
	TIME [epoch: 10.3 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.621559309647301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.621559309647301 | validation: 6.055558290140141]
	TIME [epoch: 10.3 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.40533211832462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.40533211832462 | validation: 6.337383137993375]
	TIME [epoch: 10.3 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.435601333268453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.435601333268453 | validation: 5.915501176045368]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.304159496574779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.304159496574779 | validation: 6.039976227569593]
	TIME [epoch: 10.3 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4197983454907614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4197983454907614 | validation: 6.240026733499542]
	TIME [epoch: 10.3 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.389324229497509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.389324229497509 | validation: 5.886718631086126]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.293904941470092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.293904941470092 | validation: 5.893641656877494]
	TIME [epoch: 10.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2709871626452856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2709871626452856 | validation: 5.982052905220328]
	TIME [epoch: 10.3 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.291494867307113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.291494867307113 | validation: 5.62716883908105]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.037888105753317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.037888105753317 | validation: 5.5747468315562765]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2911458439603525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2911458439603525 | validation: 6.008164281213058]
	TIME [epoch: 10.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.288366127750658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.288366127750658 | validation: 6.883302068878702]
	TIME [epoch: 10.3 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.540401270897909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.540401270897909 | validation: 5.560900730544079]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.387625251861657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.387625251861657 | validation: 5.612796342184267]
	TIME [epoch: 10.3 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.505905744770814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.505905744770814 | validation: 5.67254519253533]
	TIME [epoch: 10.3 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.043782915527123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.043782915527123 | validation: 5.636906665956777]
	TIME [epoch: 10.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.385866330278214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.385866330278214 | validation: 6.037483591174058]
	TIME [epoch: 10.3 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.161424815736699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.161424815736699 | validation: 5.659831228795781]
	TIME [epoch: 10.4 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.095103648298934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.095103648298934 | validation: 5.8124372303387855]
	TIME [epoch: 10.3 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.032150741239762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.032150741239762 | validation: 5.859604058605437]
	TIME [epoch: 10.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.039142783657745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.039142783657745 | validation: 5.468235504854647]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.315878229811231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.315878229811231 | validation: 5.552942413940046]
	TIME [epoch: 10.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.794757071513222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.794757071513222 | validation: 6.309103280298882]
	TIME [epoch: 10.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.406741277558502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.406741277558502 | validation: 5.958352901729787]
	TIME [epoch: 10.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.917640932562769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.917640932562769 | validation: 5.435236897988309]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9184920027643315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9184920027643315 | validation: 8.39824471546502]
	TIME [epoch: 10.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.146387121057279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.146387121057279 | validation: 5.7847984700694175]
	TIME [epoch: 10.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.618429413831869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.618429413831869 | validation: 6.138698361270006]
	TIME [epoch: 10.3 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.036746575786615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.036746575786615 | validation: 5.694922904977104]
	TIME [epoch: 10.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802769253302794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.802769253302794 | validation: 5.767820659618525]
	TIME [epoch: 10.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14789554173306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.14789554173306 | validation: 6.221023317659444]
	TIME [epoch: 10.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1426798870506705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1426798870506705 | validation: 8.753015560334703]
	TIME [epoch: 10.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.799736145399736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.799736145399736 | validation: 5.52508811256703]
	TIME [epoch: 10.3 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.908940674641647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.908940674641647 | validation: 5.4191695894658185]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.692274763297961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.692274763297961 | validation: 5.919054796770886]
	TIME [epoch: 10.3 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.86161220957277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.86161220957277 | validation: 5.3733383076572885]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.746140009871436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.746140009871436 | validation: 5.463040460074122]
	TIME [epoch: 10.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.674585737005959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.674585737005959 | validation: 5.962649749975335]
	TIME [epoch: 10.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.400590583711697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.400590583711697 | validation: 6.076292825740447]
	TIME [epoch: 10.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3282948001486155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3282948001486155 | validation: 5.382903449670096]
	TIME [epoch: 10.3 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.6756765465474555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6756765465474555 | validation: 5.80222905884551]
	TIME [epoch: 10.3 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.693892450696526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.693892450696526 | validation: 5.206769892222696]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.451838418070853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.451838418070853 | validation: 5.825426201610886]
	TIME [epoch: 10.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.233275820465398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.233275820465398 | validation: 5.611947426966342]
	TIME [epoch: 10.3 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.288179790139898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.288179790139898 | validation: 6.266682372117294]
	TIME [epoch: 10.3 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30262555697003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.30262555697003 | validation: 5.249336917146081]
	TIME [epoch: 10.3 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.595284457040594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.595284457040594 | validation: 5.1968673256854405]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.421194637315012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.421194637315012 | validation: 5.3639380556735095]
	TIME [epoch: 10.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.741555980998913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.741555980998913 | validation: 5.708147605478383]
	TIME [epoch: 10.3 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.609896412572895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.609896412572895 | validation: 5.768455260745234]
	TIME [epoch: 10.3 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.916028162951801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.916028162951801 | validation: 5.426629134946422]
	TIME [epoch: 10.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.837664180217327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.837664180217327 | validation: 6.8643464889413295]
	TIME [epoch: 10.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7882951007269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7882951007269 | validation: 5.3147027043046]
	TIME [epoch: 10.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4801089269299945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4801089269299945 | validation: 5.077989489823941]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.526010031002739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.526010031002739 | validation: 5.2858786172525924]
	TIME [epoch: 10.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.512248338461908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.512248338461908 | validation: 5.226711558314637]
	TIME [epoch: 10.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773896924568806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.773896924568806 | validation: 5.47532234926133]
	TIME [epoch: 10.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.436423192153933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.436423192153933 | validation: 5.54568287701622]
	TIME [epoch: 10.3 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5729345704436435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5729345704436435 | validation: 5.22566275863451]
	TIME [epoch: 10.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.439690791111305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.439690791111305 | validation: 5.2651098245664425]
	TIME [epoch: 10.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.451224140489494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.451224140489494 | validation: 5.089857000198815]
	TIME [epoch: 10.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3775153162580205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3775153162580205 | validation: 5.122563791936255]
	TIME [epoch: 10.3 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774856785694805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.774856785694805 | validation: 4.873591856723006]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.345926139098685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.345926139098685 | validation: 5.411497594316532]
	TIME [epoch: 10.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.357728245104015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.357728245104015 | validation: 5.057571094922067]
	TIME [epoch: 10.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.268066777988523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.268066777988523 | validation: 5.418165252093092]
	TIME [epoch: 10.3 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.451549862614333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.451549862614333 | validation: 5.194182157882765]
	TIME [epoch: 10.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.355399691554898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.355399691554898 | validation: 4.936321033418834]
	TIME [epoch: 10.3 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.167092588477006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.167092588477006 | validation: 5.370171621103168]
	TIME [epoch: 10.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4356555044841235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4356555044841235 | validation: 4.706291788812717]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.022913487643405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.022913487643405 | validation: 4.604285573820319]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.145886710886325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.145886710886325 | validation: 4.645162364959823]
	TIME [epoch: 10.3 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.189996752424664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.189996752424664 | validation: 4.955548848274939]
	TIME [epoch: 10.3 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.007276407028913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.007276407028913 | validation: 4.094424484697251]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.004063082949324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.004063082949324 | validation: 4.349954132243028]
	TIME [epoch: 10.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.581267183895261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.581267183895261 | validation: 1.5208276226358215]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7091226151484584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7091226151484584 | validation: 1.2998550087737732]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8615195709595014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8615195709595014 | validation: 3.249684524738641]
	TIME [epoch: 10.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.27651396489692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.27651396489692 | validation: 2.5946894037016808]
	TIME [epoch: 10.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4955623108277196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4955623108277196 | validation: 1.4751612877898561]
	TIME [epoch: 10.3 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4136768989453135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4136768989453135 | validation: 1.4671313558884043]
	TIME [epoch: 10.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.384198084728344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.384198084728344 | validation: 1.7477419279880615]
	TIME [epoch: 10.3 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6096429683271611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6096429683271611 | validation: 2.5824962290286373]
	TIME [epoch: 10.3 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.111819702966153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.111819702966153 | validation: 3.0463972981457177]
	TIME [epoch: 10.3 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.402635297849621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.402635297849621 | validation: 2.085338729747124]
	TIME [epoch: 10.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9520563665602289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9520563665602289 | validation: 2.372412383812468]
	TIME [epoch: 10.3 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.061499587056602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.061499587056602 | validation: 2.5726494287513537]
	TIME [epoch: 10.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.154469073418196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.154469073418196 | validation: 2.1573263650306007]
	TIME [epoch: 10.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3828453263200857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3828453263200857 | validation: 1.96490978774356]
	TIME [epoch: 10.3 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9675229174752544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9675229174752544 | validation: 2.1651320392702393]
	TIME [epoch: 10.3 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.433145968694869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.433145968694869 | validation: 2.571894310584554]
	TIME [epoch: 10.3 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0221478957036503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0221478957036503 | validation: 2.0533072586079544]
	TIME [epoch: 10.3 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9034988027269297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9034988027269297 | validation: 1.8255273368814506]
	TIME [epoch: 10.3 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.269949429405199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.269949429405199 | validation: 3.623393724724954]
	TIME [epoch: 10.3 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3913145082175977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3913145082175977 | validation: 1.1989519515504705]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4827154141376941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4827154141376941 | validation: 1.6759598201596264]
	TIME [epoch: 10.3 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949362695477119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.949362695477119 | validation: 1.9028989308521043]
	TIME [epoch: 10.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.852635219020644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.852635219020644 | validation: 1.4404214138400147]
	TIME [epoch: 10.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2275581038492829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2275581038492829 | validation: 1.0472444255423496]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.089188091184489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.089188091184489 | validation: 1.1053847383706825]
	TIME [epoch: 10.3 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7123504372169154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7123504372169154 | validation: 2.0789398656730884]
	TIME [epoch: 10.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7405994439639745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7405994439639745 | validation: 1.3925073960445145]
	TIME [epoch: 10.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5499250724096485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5499250724096485 | validation: 1.2364176973901806]
	TIME [epoch: 10.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5164696147994157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5164696147994157 | validation: 1.1941665108431552]
	TIME [epoch: 10.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2967234537142684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2967234537142684 | validation: 1.5233553969366365]
	TIME [epoch: 10.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.399356773891323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.399356773891323 | validation: 1.555263490492326]
	TIME [epoch: 10.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.27098206279538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.27098206279538 | validation: 2.0114513928262854]
	TIME [epoch: 10.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.312434224941553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.312434224941553 | validation: 0.9701316470451243]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1774581037214573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1774581037214573 | validation: 1.2291741855872271]
	TIME [epoch: 10.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7230718285710573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7230718285710573 | validation: 0.9722147733674297]
	TIME [epoch: 10.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2140150418960196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2140150418960196 | validation: 0.8251830756442626]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.467994467269847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.467994467269847 | validation: 1.8931619012787382]
	TIME [epoch: 10.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0272112153225073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0272112153225073 | validation: 1.8493410289023924]
	TIME [epoch: 10.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4123874580226814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4123874580226814 | validation: 0.9291861776131674]
	TIME [epoch: 10.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2177861091920508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2177861091920508 | validation: 0.8298071932345167]
	TIME [epoch: 10.3 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0315271143064704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0315271143064704 | validation: 4.644576370151805]
	TIME [epoch: 10.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.284433194341408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.284433194341408 | validation: 1.0904427980458726]
	TIME [epoch: 10.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3794862019642156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3794862019642156 | validation: 1.2538536255007229]
	TIME [epoch: 10.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.17261664644657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.17261664644657 | validation: 1.1061855289775622]
	TIME [epoch: 10.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8156269340457818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8156269340457818 | validation: 1.1499625059746281]
	TIME [epoch: 10.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4047105094319838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4047105094319838 | validation: 5.451740149795324]
	TIME [epoch: 10.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7680374244483614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7680374244483614 | validation: 1.5470843353470098]
	TIME [epoch: 10.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3879378291444637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3879378291444637 | validation: 1.6843336467551597]
	TIME [epoch: 10.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7515352952291767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7515352952291767 | validation: 1.3283527166305515]
	TIME [epoch: 10.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2613423610046035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2613423610046035 | validation: 1.1495939936763788]
	TIME [epoch: 10.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1492237822690798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1492237822690798 | validation: 1.981697479829424]
	TIME [epoch: 10.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3128673735125125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3128673735125125 | validation: 1.1580249034283987]
	TIME [epoch: 10.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2400631173224115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2400631173224115 | validation: 1.1740871803843624]
	TIME [epoch: 10.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5413746471713625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5413746471713625 | validation: 1.4613479533728428]
	TIME [epoch: 10.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1880301533182558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1880301533182558 | validation: 1.0270718571986197]
	TIME [epoch: 10.3 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.226020424177657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.226020424177657 | validation: 1.3447399545377707]
	TIME [epoch: 10.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1880839439931385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1880839439931385 | validation: 1.7710378033254734]
	TIME [epoch: 10.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1811857966118615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1811857966118615 | validation: 0.8907185670137928]
	TIME [epoch: 10.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0081393822465081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0081393822465081 | validation: 0.7191805489636396]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8993656512552753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8993656512552753 | validation: 1.5931463490972408]
	TIME [epoch: 10.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2411864343841643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2411864343841643 | validation: 0.8702650071833525]
	TIME [epoch: 10.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.493941478657876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.493941478657876 | validation: 2.9636240698898]
	TIME [epoch: 10.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8402531050735464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8402531050735464 | validation: 0.7535189883884238]
	TIME [epoch: 10.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0960989293070984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0960989293070984 | validation: 0.9466147414024084]
	TIME [epoch: 10.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.082457712315033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.082457712315033 | validation: 1.1393824552778151]
	TIME [epoch: 10.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3376743324559583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3376743324559583 | validation: 0.8701225569038153]
	TIME [epoch: 10.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0306711965321003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0306711965321003 | validation: 1.2368948357341274]
	TIME [epoch: 10.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1858238118502968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1858238118502968 | validation: 0.6306755017684865]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0929434625968573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0929434625968573 | validation: 1.4593092458370314]
	TIME [epoch: 10.3 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1997334218803923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1997334218803923 | validation: 1.791276837752746]
	TIME [epoch: 10.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.81587220071311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.81587220071311 | validation: 1.381771045144037]
	TIME [epoch: 10.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1422476340664187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1422476340664187 | validation: 0.8176192507862672]
	TIME [epoch: 10.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0990600510278656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0990600510278656 | validation: 0.988679413410842]
	TIME [epoch: 10.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.119548787438212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.119548787438212 | validation: 0.873898117851457]
	TIME [epoch: 10.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0201467778237103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0201467778237103 | validation: 5.806879360474422]
	TIME [epoch: 10.3 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3010532419566063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3010532419566063 | validation: 1.4648728832827738]
	TIME [epoch: 10.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2115774393392311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2115774393392311 | validation: 2.2124746606289176]
	TIME [epoch: 10.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6883419688137793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6883419688137793 | validation: 1.1488147149576744]
	TIME [epoch: 10.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0679493690845245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0679493690845245 | validation: 2.0374051394185697]
	TIME [epoch: 10.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5204420840921626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5204420840921626 | validation: 0.9588326865726944]
	TIME [epoch: 10.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4073959832927596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4073959832927596 | validation: 2.1458076929349748]
	TIME [epoch: 10.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.403439329960789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.403439329960789 | validation: 1.5451629685055956]
	TIME [epoch: 10.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4335316678576582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4335316678576582 | validation: 1.0923011198798682]
	TIME [epoch: 10.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2206479136638326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2206479136638326 | validation: 0.8066331848016464]
	TIME [epoch: 10.3 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9332883627483923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9332883627483923 | validation: 1.0061543143690603]
	TIME [epoch: 10.3 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5174237632691603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5174237632691603 | validation: 1.5843556892672417]
	TIME [epoch: 10.3 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4299527574108235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4299527574108235 | validation: 0.8355941717954116]
	TIME [epoch: 10.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.199269158065817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.199269158065817 | validation: 1.325013075889849]
	TIME [epoch: 10.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.091137714179046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.091137714179046 | validation: 0.997928559397019]
	TIME [epoch: 10.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4237363237098462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4237363237098462 | validation: 4.380628025681865]
	TIME [epoch: 10.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2589937455939637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2589937455939637 | validation: 0.9385406899252078]
	TIME [epoch: 10.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2353131532442636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2353131532442636 | validation: 0.9827990388750942]
	TIME [epoch: 10.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0420806294509743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0420806294509743 | validation: 1.9110031588364609]
	TIME [epoch: 10.3 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.291382276952584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.291382276952584 | validation: 1.0593429283579763]
	TIME [epoch: 10.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2201005053366452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2201005053366452 | validation: 0.9424745085990359]
	TIME [epoch: 10.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.098180651343016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.098180651343016 | validation: 0.8305118157918107]
	TIME [epoch: 10.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1486515389334215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1486515389334215 | validation: 0.9029784090933043]
	TIME [epoch: 10.3 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9321471434765687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9321471434765687 | validation: 0.7657744104235263]
	TIME [epoch: 10.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0435832251563835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0435832251563835 | validation: 1.1067293593352967]
	TIME [epoch: 10.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.100781797490726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.100781797490726 | validation: 0.6259234059004344]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7753203519396573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7753203519396573 | validation: 0.9753266915250141]
	TIME [epoch: 10.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4382696687225525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4382696687225525 | validation: 1.9099615745105587]
	TIME [epoch: 10.3 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2583688958429375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2583688958429375 | validation: 0.9403432374958045]
	TIME [epoch: 10.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9944200376362058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9944200376362058 | validation: 0.7986664921075886]
	TIME [epoch: 10.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9816260989242636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9816260989242636 | validation: 1.1731780907708795]
	TIME [epoch: 10.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3089024287013964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3089024287013964 | validation: 0.8282052972760824]
	TIME [epoch: 10.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9524437971137371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9524437971137371 | validation: 1.293689374565808]
	TIME [epoch: 10.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0679026756144443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0679026756144443 | validation: 0.9109936174600662]
	TIME [epoch: 10.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9679032016489678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9679032016489678 | validation: 1.491939215044489]
	TIME [epoch: 10.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0533976190716725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0533976190716725 | validation: 1.0015390355529201]
	TIME [epoch: 10.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1115475229824778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1115475229824778 | validation: 1.3374696245844906]
	TIME [epoch: 10.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1257947853462134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1257947853462134 | validation: 1.0211899076811972]
	TIME [epoch: 10.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.043665688023751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.043665688023751 | validation: 1.4060136733963662]
	TIME [epoch: 10.3 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2578352094049399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2578352094049399 | validation: 0.8992078840016251]
	TIME [epoch: 10.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8003718536182738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8003718536182738 | validation: 1.470965345458766]
	TIME [epoch: 10.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0745773568534362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0745773568534362 | validation: 0.9870847242038865]
	TIME [epoch: 10.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9020002008422143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9020002008422143 | validation: 1.9750556081804818]
	TIME [epoch: 10.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6724037744650677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6724037744650677 | validation: 2.608906528118104]
	TIME [epoch: 10.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8291192874051792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8291192874051792 | validation: 1.6228154409960525]
	TIME [epoch: 10.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3891609065604051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3891609065604051 | validation: 1.2947595893907735]
	TIME [epoch: 10.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0802809761723473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0802809761723473 | validation: 0.7543407187596342]
	TIME [epoch: 10.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8500748148911319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8500748148911319 | validation: 1.3743084424456953]
	TIME [epoch: 10.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.057330841179946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.057330841179946 | validation: 6.368696972903397]
	TIME [epoch: 10.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.630304408956677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.630304408956677 | validation: 5.83962576793304]
	TIME [epoch: 10.3 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.994620184906048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.994620184906048 | validation: 4.924626763522079]
	TIME [epoch: 10.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.082212867346279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.082212867346279 | validation: 1.6394137450389192]
	TIME [epoch: 10.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5846314832775446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5846314832775446 | validation: 1.2945861770752436]
	TIME [epoch: 10.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2930315433178652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2930315433178652 | validation: 0.8523121557235862]
	TIME [epoch: 10.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0807642814963756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0807642814963756 | validation: 1.329491261852069]
	TIME [epoch: 10.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.193594710478957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.193594710478957 | validation: 1.3738955304491323]
	TIME [epoch: 10.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1647864920259128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1647864920259128 | validation: 0.8389215769221119]
	TIME [epoch: 10.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.040786371785752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.040786371785752 | validation: 2.0206947894521106]
	TIME [epoch: 10.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5441874343276651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5441874343276651 | validation: 0.9581885144193854]
	TIME [epoch: 10.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1220404568926987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1220404568926987 | validation: 1.049308131972217]
	TIME [epoch: 10.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.085875131826682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.085875131826682 | validation: 2.2609871343622423]
	TIME [epoch: 10.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2764558281976868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2764558281976868 | validation: 1.2415979845346339]
	TIME [epoch: 10.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4711136748583393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4711136748583393 | validation: 2.1216935109108785]
	TIME [epoch: 10.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.379380037392282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.379380037392282 | validation: 1.7028553395543684]
	TIME [epoch: 10.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1230483814001537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1230483814001537 | validation: 1.6156573259410574]
	TIME [epoch: 10.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2820031853844154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2820031853844154 | validation: 0.941575773068048]
	TIME [epoch: 10.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9150526893032499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9150526893032499 | validation: 0.9881717131077044]
	TIME [epoch: 10.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.135402950511851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.135402950511851 | validation: 0.6733274321784961]
	TIME [epoch: 10.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9245057739439828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9245057739439828 | validation: 0.9160843941439116]
	TIME [epoch: 10.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5412404987874713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5412404987874713 | validation: 0.9789557070914149]
	TIME [epoch: 10.3 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300559158999065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.300559158999065 | validation: 0.9696815291482637]
	TIME [epoch: 10.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7378373616753326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7378373616753326 | validation: 1.19381888018599]
	TIME [epoch: 10.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2710049101902257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2710049101902257 | validation: 3.0437989737748388]
	TIME [epoch: 10.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0329031348540676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0329031348540676 | validation: 1.0729256793772268]
	TIME [epoch: 10.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.280242852709904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.280242852709904 | validation: 1.942964466719592]
	TIME [epoch: 10.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0832575187552402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0832575187552402 | validation: 1.5567716709043753]
	TIME [epoch: 10.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5327636006492367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5327636006492367 | validation: 1.333102830546142]
	TIME [epoch: 10.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1887588969761975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1887588969761975 | validation: 0.8482551374859586]
	TIME [epoch: 10.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1510345873645527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1510345873645527 | validation: 1.1974251278264274]
	TIME [epoch: 10.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.161831152451333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.161831152451333 | validation: 1.0977714124444147]
	TIME [epoch: 10.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1870037813575147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1870037813575147 | validation: 1.0541390490854778]
	TIME [epoch: 10.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.22805553506578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.22805553506578 | validation: 0.9023601935203118]
	TIME [epoch: 10.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8412027432666219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8412027432666219 | validation: 0.9469638352513539]
	TIME [epoch: 10.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9436836734390948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9436836734390948 | validation: 1.1733598840094432]
	TIME [epoch: 10.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.079100749694951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.079100749694951 | validation: 0.8935800656403025]
	TIME [epoch: 10.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299387156832865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.299387156832865 | validation: 0.8436283423086158]
	TIME [epoch: 10.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8927191610557014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8927191610557014 | validation: 0.9799680048808378]
	TIME [epoch: 10.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9599316726637277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9599316726637277 | validation: 1.5389561439784512]
	TIME [epoch: 10.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2342232283638332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2342232283638332 | validation: 2.69707202813126]
	TIME [epoch: 10.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3461041330203356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3461041330203356 | validation: 1.9968553876503807]
	TIME [epoch: 10.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3003334085522193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3003334085522193 | validation: 1.2296843336865575]
	TIME [epoch: 10.3 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9293881554819542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9293881554819542 | validation: 0.7930174312332229]
	TIME [epoch: 10.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9228046591192225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9228046591192225 | validation: 0.5947404767028271]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9597337184655903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9597337184655903 | validation: 1.970426449611887]
	TIME [epoch: 10.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.469768501730973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.469768501730973 | validation: 0.7717449306674371]
	TIME [epoch: 10.3 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9124400425371313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9124400425371313 | validation: 1.7181098720191303]
	TIME [epoch: 10.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.887934191301186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.887934191301186 | validation: 0.8404423767665923]
	TIME [epoch: 10.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8627067319685878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8627067319685878 | validation: 1.226583553806863]
	TIME [epoch: 10.3 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.999161057453494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.999161057453494 | validation: 0.840976024515132]
	TIME [epoch: 10.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.027810826738391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.027810826738391 | validation: 0.7024580258599242]
	TIME [epoch: 10.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.105287861551019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.105287861551019 | validation: 3.5709180563935226]
	TIME [epoch: 10.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1684471019796305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1684471019796305 | validation: 2.0399406003737726]
	TIME [epoch: 10.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3784582062475026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3784582062475026 | validation: 0.7998934809470231]
	TIME [epoch: 10.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2788421022729057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2788421022729057 | validation: 0.8453432248224173]
	TIME [epoch: 10.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9102419805114754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9102419805114754 | validation: 0.9436867505460734]
	TIME [epoch: 10.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.01792984481084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.01792984481084 | validation: 1.0836963187646098]
	TIME [epoch: 10.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2155782417714085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2155782417714085 | validation: 3.4372189161905213]
	TIME [epoch: 10.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8782611460143102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8782611460143102 | validation: 0.9923219837620532]
	TIME [epoch: 10.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9650051608297104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9650051608297104 | validation: 1.0439586330738697]
	TIME [epoch: 10.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.010955772048514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010955772048514 | validation: 0.8536181404118687]
	TIME [epoch: 10.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.093371702682607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.093371702682607 | validation: 0.9753276998029727]
	TIME [epoch: 10.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9603499085441205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9603499085441205 | validation: 0.8347805719015305]
	TIME [epoch: 10.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4796699052313287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4796699052313287 | validation: 1.2659155872281824]
	TIME [epoch: 10.3 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.071644542543058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.071644542543058 | validation: 0.8197880397991143]
	TIME [epoch: 10.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2544757157529012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2544757157529012 | validation: 0.9426675140982395]
	TIME [epoch: 10.4 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3052855209776375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3052855209776375 | validation: 1.3952775527947037]
	TIME [epoch: 10.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5223089136550123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5223089136550123 | validation: 1.7627249253737676]
	TIME [epoch: 10.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4823760298253732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4823760298253732 | validation: 1.3263815275642998]
	TIME [epoch: 10.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8868234648464874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8868234648464874 | validation: 1.4771730930136027]
	TIME [epoch: 10.4 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4955277068628885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4955277068628885 | validation: 1.2635291608960104]
	TIME [epoch: 10.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4437149062083316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4437149062083316 | validation: 1.3284079776978939]
	TIME [epoch: 10.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4883919382948467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4883919382948467 | validation: 1.4078762087862062]
	TIME [epoch: 10.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2836678436538502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2836678436538502 | validation: 1.2679867297857432]
	TIME [epoch: 10.4 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9471268634491625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9471268634491625 | validation: 1.840387813263985]
	TIME [epoch: 10.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.36773852660087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.36773852660087 | validation: 1.155972877326091]
	TIME [epoch: 10.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.167813967949089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.167813967949089 | validation: 1.5859722929178768]
	TIME [epoch: 10.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2137804389467817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2137804389467817 | validation: 1.4688680343269556]
	TIME [epoch: 10.4 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1697288907711187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1697288907711187 | validation: 3.4179941542928054]
	TIME [epoch: 10.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8859900928970075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8859900928970075 | validation: 1.1138147474955382]
	TIME [epoch: 10.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4815059412475864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4815059412475864 | validation: 2.0809236205450454]
	TIME [epoch: 10.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3844487132446937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3844487132446937 | validation: 0.8900476256538249]
	TIME [epoch: 10.4 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3078083101454379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3078083101454379 | validation: 1.3201147209008142]
	TIME [epoch: 10.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.110292931380091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.110292931380091 | validation: 1.5757083550876467]
	TIME [epoch: 10.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.367870630326308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.367870630326308 | validation: 1.0069483769938883]
	TIME [epoch: 10.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0354932618676855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0354932618676855 | validation: 1.1308633269505626]
	TIME [epoch: 10.3 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9646632062623228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9646632062623228 | validation: 1.5977188363775587]
	TIME [epoch: 10.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1941089472948172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1941089472948172 | validation: 1.2468061617793882]
	TIME [epoch: 10.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.121471954504681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.121471954504681 | validation: 1.056353057678952]
	TIME [epoch: 10.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0153482319820974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0153482319820974 | validation: 1.058889485481031]
	TIME [epoch: 10.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1984195554555142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1984195554555142 | validation: 0.8097432454577589]
	TIME [epoch: 10.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.33808659344802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.33808659344802 | validation: 1.2314912969961085]
	TIME [epoch: 10.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.577607677080357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.577607677080357 | validation: 1.6464545471930723]
	TIME [epoch: 10.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2753948436146307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2753948436146307 | validation: 1.0450955693511603]
	TIME [epoch: 10.4 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2359658502344706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2359658502344706 | validation: 1.9537588681338178]
	TIME [epoch: 10.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.984805515013041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.984805515013041 | validation: 4.690716837278207]
	TIME [epoch: 10.3 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.794009961532151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.794009961532151 | validation: 7.076014739409902]
	TIME [epoch: 10.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.111058161348727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.111058161348727 | validation: 1.086662178663178]
	TIME [epoch: 10.4 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.499550388820369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.499550388820369 | validation: 1.7050406298884746]
	TIME [epoch: 10.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5688118152907329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5688118152907329 | validation: 1.2469109235349631]
	TIME [epoch: 10.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2829789093260409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2829789093260409 | validation: 1.137050888817013]
	TIME [epoch: 10.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5835895667914537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5835895667914537 | validation: 2.8471912671936326]
	TIME [epoch: 10.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5413722982695892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5413722982695892 | validation: 1.4767122422488113]
	TIME [epoch: 10.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.187668825880091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.187668825880091 | validation: 2.0248085593638625]
	TIME [epoch: 10.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240466056582354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.240466056582354 | validation: 1.5812433607398972]
	TIME [epoch: 10.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2082512162073684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2082512162073684 | validation: 0.9258304530255181]
	TIME [epoch: 10.4 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.060653361412866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.060653361412866 | validation: 2.5462518729541985]
	TIME [epoch: 10.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0685003675679816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0685003675679816 | validation: 1.8009942349571315]
	TIME [epoch: 10.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6197062608421977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6197062608421977 | validation: 1.7628209298554354]
	TIME [epoch: 10.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6441890450910268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6441890450910268 | validation: 1.7822814122161816]
	TIME [epoch: 10.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5770181324041608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5770181324041608 | validation: 2.48362027497301]
	TIME [epoch: 10.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5331814096142542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5331814096142542 | validation: 1.351781743503574]
	TIME [epoch: 10.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5243669042543968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5243669042543968 | validation: 1.6077331972823004]
	TIME [epoch: 10.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2332770990317115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2332770990317115 | validation: 0.7132933691760546]
	TIME [epoch: 10.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6014591189456548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6014591189456548 | validation: 4.813543083217176]
	TIME [epoch: 10.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196983603841333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.196983603841333 | validation: 0.9798704966624614]
	TIME [epoch: 10.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7085267622824596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7085267622824596 | validation: 1.0135958215821905]
	TIME [epoch: 10.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.045803543110083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.045803543110083 | validation: 0.6884406003640061]
	TIME [epoch: 10.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1148271481383234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1148271481383234 | validation: 0.8375222819425092]
	TIME [epoch: 10.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9011734112161601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9011734112161601 | validation: 0.4987445312555572]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r3_20240217_085210/states/model_tr_study5_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2280656168602335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2280656168602335 | validation: 1.2864662211438003]
	TIME [epoch: 10.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8794286423966908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8794286423966908 | validation: 3.2502561630466222]
	TIME [epoch: 10.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8636786717063512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8636786717063512 | validation: 2.9183793658082857]
	TIME [epoch: 10.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.695472604960679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.695472604960679 | validation: 0.9894679417098271]
	TIME [epoch: 10.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.372022605331739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.372022605331739 | validation: 1.7432942150338437]
	TIME [epoch: 10.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3607527576556857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3607527576556857 | validation: 0.8542986639556424]
	TIME [epoch: 10.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.025691086353384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.025691086353384 | validation: 0.9778537126372598]
	TIME [epoch: 10.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2885887011537087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2885887011537087 | validation: 0.9978794346603681]
	TIME [epoch: 10.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4691017549496272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4691017549496272 | validation: 1.5329698193266348]
	TIME [epoch: 10.3 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3776484348274847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3776484348274847 | validation: 1.5331137262768573]
	TIME [epoch: 10.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.418546770032457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.418546770032457 | validation: 1.305589438424951]
	TIME [epoch: 10.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.379606672345175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.379606672345175 | validation: 1.5705493612037456]
	TIME [epoch: 10.3 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.406774468425748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.406774468425748 | validation: 1.4609535471301893]
	TIME [epoch: 10.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2478232999031644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2478232999031644 | validation: 2.6631328210970797]
	TIME [epoch: 10.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9286605700751676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9286605700751676 | validation: 1.8196824399911569]
	TIME [epoch: 10.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6921119943093477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6921119943093477 | validation: 3.5605067172142335]
	TIME [epoch: 10.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.853504732951097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.853504732951097 | validation: 2.660949505912303]
	TIME [epoch: 10.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9527138280412017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9527138280412017 | validation: 6.538544285795344]
	TIME [epoch: 10.3 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9842946157294556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9842946157294556 | validation: 4.828219009275632]
	TIME [epoch: 10.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.208967698880804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.208967698880804 | validation: 2.005168004278352]
	TIME [epoch: 10.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.96141853476093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.96141853476093 | validation: 1.564658024704417]
	TIME [epoch: 10.3 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.160110238167055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.160110238167055 | validation: 6.912040938617755]
	TIME [epoch: 10.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1760461190900795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1760461190900795 | validation: 3.078354179657747]
	TIME [epoch: 10.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.492730099362068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.492730099362068 | validation: 5.087373544564065]
	TIME [epoch: 10.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7289415156757304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7289415156757304 | validation: 2.9699725745937156]
	TIME [epoch: 10.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6437914798931974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6437914798931974 | validation: 4.496742732353692]
	TIME [epoch: 10.3 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.917901683002796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.917901683002796 | validation: 5.541739788933508]
	TIME [epoch: 10.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.907369314883076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.907369314883076 | validation: 6.329702593657835]
	TIME [epoch: 10.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.606451556181412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.606451556181412 | validation: 4.963113279532443]
	TIME [epoch: 10.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.494236896922646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.494236896922646 | validation: 6.299366072221987]
	TIME [epoch: 10.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: nan		[learning rate: 0.01]
ERROR:
nan encountered in epoch 451 (training loss).
