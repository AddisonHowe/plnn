Args:
Namespace(name='model_tr_study5', outdir='out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5', training_data='data/transition_rate_studies/tr_study5/tr_study5_training/r5', validation_data='data/transition_rate_studies/tr_study5/tr_study5_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3645122928

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.912503575860738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.912503575860738 | validation: 11.423869166417358]
	TIME [epoch: 49.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.312768416605866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.312768416605866 | validation: 11.797583559220016]
	TIME [epoch: 10.5 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.203470710687455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.203470710687455 | validation: 11.344392857852165]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.270515699858413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.270515699858413 | validation: 11.024837161333995]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.658568991617702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.658568991617702 | validation: 10.52042040069563]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.274814919559304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.274814919559304 | validation: 10.276603042728885]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.098310644491855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.098310644491855 | validation: 10.170951458519898]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.073883454576016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.073883454576016 | validation: 10.061969754733417]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.09086926370604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.09086926370604 | validation: 10.093650363098142]
	TIME [epoch: 10.6 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.935256720561842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.935256720561842 | validation: 9.99354226445855]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.912881841624156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.912881841624156 | validation: 10.236971647524578]
	TIME [epoch: 10.6 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.95606422293009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.95606422293009 | validation: 10.232661233927255]
	TIME [epoch: 10.5 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.011666514055166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.011666514055166 | validation: 9.894774124371283]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.800248065926603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.800248065926603 | validation: 9.84680341733189]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.722827820335983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.722827820335983 | validation: 10.217163860496772]
	TIME [epoch: 10.5 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.772951161676463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.772951161676463 | validation: 9.862696212886856]
	TIME [epoch: 10.5 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.593928253849498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.593928253849498 | validation: 9.294929986275376]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.316535181057052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.316535181057052 | validation: 9.658743392255557]
	TIME [epoch: 10.5 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.056089977783788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.056089977783788 | validation: 9.211181701372386]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.233634706071232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.233634706071232 | validation: 9.93828843648608]
	TIME [epoch: 10.5 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.526988658286127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.526988658286127 | validation: 10.888729881231134]
	TIME [epoch: 10.5 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.05717664192043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.05717664192043 | validation: 8.494034293185168]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.194458085424207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.194458085424207 | validation: 7.989809894123042]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.948282824918868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.948282824918868 | validation: 8.250025470915554]
	TIME [epoch: 10.5 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.809711993316431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.809711993316431 | validation: 7.5300243517916625]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.68784971517591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.68784971517591 | validation: 7.512861270583805]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.433308598516564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.433308598516564 | validation: 8.082768486236962]
	TIME [epoch: 10.5 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.612908516959729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.612908516959729 | validation: 7.250832811995487]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.348624438416081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.348624438416081 | validation: 7.131526293158949]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.317480339016539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.317480339016539 | validation: 7.316796157193778]
	TIME [epoch: 10.6 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.194143002454274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.194143002454274 | validation: 7.916994145476734]
	TIME [epoch: 10.5 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.241210192779239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.241210192779239 | validation: 7.1627398223858805]
	TIME [epoch: 10.6 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.052385662325802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.052385662325802 | validation: 6.671777927550432]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.98844707821613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.98844707821613 | validation: 8.070588949965522]
	TIME [epoch: 10.6 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.2412285881034055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2412285881034055 | validation: 6.761782861239633]
	TIME [epoch: 10.6 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.763066361263293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.763066361263293 | validation: 6.447562041697317]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8983452507671945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8983452507671945 | validation: 6.798871346365963]
	TIME [epoch: 10.5 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.826192795013851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.826192795013851 | validation: 6.5357963478586205]
	TIME [epoch: 10.6 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.560892714888562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.560892714888562 | validation: 6.294979486727548]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.716745397128721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.716745397128721 | validation: 7.640150642403442]
	TIME [epoch: 10.6 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.134880252961979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.134880252961979 | validation: 6.474526098730433]
	TIME [epoch: 10.6 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5874624892781615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5874624892781615 | validation: 6.186704481379343]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.557679537606363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.557679537606363 | validation: 6.632524373369099]
	TIME [epoch: 10.5 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.523441218374839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.523441218374839 | validation: 6.833185386785012]
	TIME [epoch: 10.5 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5039456526403905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5039456526403905 | validation: 6.9458966457473315]
	TIME [epoch: 10.6 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5146276899593385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5146276899593385 | validation: 6.312687651089813]
	TIME [epoch: 10.5 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.475216124592388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.475216124592388 | validation: 6.122383521776308]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.485320740161034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.485320740161034 | validation: 6.307427853968529]
	TIME [epoch: 10.5 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.357485686078922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.357485686078922 | validation: 7.0876027045980345]
	TIME [epoch: 10.6 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.610153774208712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.610153774208712 | validation: 5.976839255904462]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.047705524127019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.047705524127019 | validation: 6.270468076307216]
	TIME [epoch: 10.6 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5744492256642095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5744492256642095 | validation: 5.950730075457724]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.281818608696206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.281818608696206 | validation: 5.730979386510609]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159037325775314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.159037325775314 | validation: 6.549687078191322]
	TIME [epoch: 10.6 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.416253199954579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.416253199954579 | validation: 5.90123664513731]
	TIME [epoch: 10.5 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9353630093724945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9353630093724945 | validation: 5.135318849226385]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.030968889837285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.030968889837285 | validation: 6.113950516162143]
	TIME [epoch: 10.6 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.46089469815861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.46089469815861 | validation: 5.535572125930471]
	TIME [epoch: 10.6 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.539361794330185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.539361794330185 | validation: 6.654355834028498]
	TIME [epoch: 10.6 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0205487215285896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0205487215285896 | validation: 4.9201493237748135]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.284266219810876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.284266219810876 | validation: 4.747210548402822]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.304189088507021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.304189088507021 | validation: 4.887630978844138]
	TIME [epoch: 10.6 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.50875647059104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.50875647059104 | validation: 5.315776361036801]
	TIME [epoch: 10.6 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.49316061831736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.49316061831736 | validation: 4.390703917511484]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.210754428557581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.210754428557581 | validation: 4.559749234697501]
	TIME [epoch: 10.6 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.370643209097414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.370643209097414 | validation: 11.274969362861889]
	TIME [epoch: 10.6 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.231623552665843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.231623552665843 | validation: 7.404616812848232]
	TIME [epoch: 10.6 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.239724067848126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.239724067848126 | validation: 5.879981413173055]
	TIME [epoch: 10.6 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5479483644929815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5479483644929815 | validation: 5.71578358862964]
	TIME [epoch: 10.6 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.06530710015975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.06530710015975 | validation: 4.868401870774035]
	TIME [epoch: 10.6 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.71165961800243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.71165961800243 | validation: 4.790245595070927]
	TIME [epoch: 10.6 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776284949086981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.776284949086981 | validation: 5.857695168925946]
	TIME [epoch: 10.6 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.967856239601014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.967856239601014 | validation: 5.037186842460776]
	TIME [epoch: 10.6 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813258133490616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.813258133490616 | validation: 7.028963067417306]
	TIME [epoch: 10.6 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.487247830556415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.487247830556415 | validation: 4.314098695259409]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.374730718519304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.374730718519304 | validation: 4.433613109141375]
	TIME [epoch: 10.6 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.121745301855131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.121745301855131 | validation: 5.747377262279316]
	TIME [epoch: 10.6 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.346617961281237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.346617961281237 | validation: 4.071623368129826]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9383911431394614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9383911431394614 | validation: 5.390994588643471]
	TIME [epoch: 10.5 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.636430981778642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.636430981778642 | validation: 8.612741194286487]
	TIME [epoch: 10.6 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.077607885332313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.077607885332313 | validation: 8.151366772550178]
	TIME [epoch: 10.6 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.492765965809806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.492765965809806 | validation: 4.304227302818388]
	TIME [epoch: 10.6 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.008540337997073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008540337997073 | validation: 4.94850469298194]
	TIME [epoch: 10.5 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.600857035836455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.600857035836455 | validation: 5.682909536444763]
	TIME [epoch: 10.5 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.992173857267284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.992173857267284 | validation: 5.088253704338136]
	TIME [epoch: 10.6 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.445804090381947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.445804090381947 | validation: 5.1625588201985195]
	TIME [epoch: 10.6 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.877195189541982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.877195189541982 | validation: 6.295910694609299]
	TIME [epoch: 10.5 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.07498089104669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.07498089104669 | validation: 7.493254342039675]
	TIME [epoch: 10.5 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.225877478165755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.225877478165755 | validation: 5.904357386065113]
	TIME [epoch: 10.6 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.997449060802277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.997449060802277 | validation: 5.230953766786826]
	TIME [epoch: 10.6 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.661564197763094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.661564197763094 | validation: 4.491486901386422]
	TIME [epoch: 10.6 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.286130933590086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.286130933590086 | validation: 4.854496829612694]
	TIME [epoch: 10.5 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4741441261199215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4741441261199215 | validation: 4.692552806786939]
	TIME [epoch: 10.5 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.334615356605731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.334615356605731 | validation: 4.722750354712514]
	TIME [epoch: 10.5 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.248842954123941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.248842954123941 | validation: 9.210434704263301]
	TIME [epoch: 10.6 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.081789260236304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.081789260236304 | validation: 4.900184406354351]
	TIME [epoch: 10.6 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.488420295889492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.488420295889492 | validation: 6.638618601724556]
	TIME [epoch: 10.5 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.070416252175639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.070416252175639 | validation: 4.28338941047725]
	TIME [epoch: 10.5 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.273820678735197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.273820678735197 | validation: 4.13645327716783]
	TIME [epoch: 10.5 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.183053504151681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.183053504151681 | validation: 4.855652353160572]
	TIME [epoch: 10.6 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4976139330990685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4976139330990685 | validation: 4.411599780255631]
	TIME [epoch: 10.6 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.914299153731264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.914299153731264 | validation: 4.916365251187496]
	TIME [epoch: 10.5 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8472180015740713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8472180015740713 | validation: 3.7175229288719844]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.144653394781915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144653394781915 | validation: 4.083202364534741]
	TIME [epoch: 10.6 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.102398007592708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.102398007592708 | validation: 4.116594823834027]
	TIME [epoch: 10.6 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.760577397218203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.760577397218203 | validation: 4.563065785691428]
	TIME [epoch: 10.5 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.230922072900428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.230922072900428 | validation: 4.511542415275281]
	TIME [epoch: 10.5 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1921077701000415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1921077701000415 | validation: 3.332148846394897]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.42697834851735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.42697834851735 | validation: 5.599745852649842]
	TIME [epoch: 10.6 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1097892856088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1097892856088 | validation: 4.768283988313169]
	TIME [epoch: 10.5 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.144242887396472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144242887396472 | validation: 3.70748237336386]
	TIME [epoch: 10.6 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.832302370673477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.832302370673477 | validation: 8.190636494387117]
	TIME [epoch: 10.6 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.397636727660382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.397636727660382 | validation: 6.791701816796315]
	TIME [epoch: 10.6 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.192432960623167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.192432960623167 | validation: 5.084708429190443]
	TIME [epoch: 10.6 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.869660834390349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.869660834390349 | validation: 4.5861862480324955]
	TIME [epoch: 10.6 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.012082518103783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.012082518103783 | validation: 4.506088462030995]
	TIME [epoch: 10.6 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7071980554954584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7071980554954584 | validation: 3.8386402390585763]
	TIME [epoch: 10.6 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.621362160171936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.621362160171936 | validation: 5.184365229345974]
	TIME [epoch: 10.5 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.531699464322665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.531699464322665 | validation: 3.9120694193509777]
	TIME [epoch: 10.5 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6898142402010357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6898142402010357 | validation: 3.5743611662541013]
	TIME [epoch: 10.6 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7645611672992167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7645611672992167 | validation: 3.6640729719214744]
	TIME [epoch: 10.6 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7248693492494653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7248693492494653 | validation: 5.492978732259419]
	TIME [epoch: 10.6 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.549102608058262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.549102608058262 | validation: 3.9751300251643897]
	TIME [epoch: 10.6 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.565981060181054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.565981060181054 | validation: 4.18867872253647]
	TIME [epoch: 10.6 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.681327339295041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.681327339295041 | validation: 3.495322254950555]
	TIME [epoch: 10.6 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5088196993993983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5088196993993983 | validation: 3.2685752284553145]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4485140141916566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4485140141916566 | validation: 3.3306127730745483]
	TIME [epoch: 10.6 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9096254125592864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9096254125592864 | validation: 6.170011068591712]
	TIME [epoch: 10.6 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9227041419268978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9227041419268978 | validation: 3.0755529048737937]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60629613828402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.60629613828402 | validation: 3.6003791634745563]
	TIME [epoch: 10.6 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.047675674752857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.047675674752857 | validation: 4.37206352086504]
	TIME [epoch: 10.6 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.160646043934635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.160646043934635 | validation: 3.669546720343581]
	TIME [epoch: 10.6 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.138902609757991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.138902609757991 | validation: 4.635894686946373]
	TIME [epoch: 10.6 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9075481096972147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9075481096972147 | validation: 4.359746213516156]
	TIME [epoch: 10.6 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7545004064333076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7545004064333076 | validation: 3.1421700222597533]
	TIME [epoch: 10.6 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5819224850075115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5819224850075115 | validation: 3.866238738001842]
	TIME [epoch: 10.6 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4516353182045973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4516353182045973 | validation: 2.7031296585187254]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.337232716254151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.337232716254151 | validation: 4.107992374349127]
	TIME [epoch: 10.6 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.426276538747421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.426276538747421 | validation: 3.9574690304602305]
	TIME [epoch: 10.6 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5719977261937657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5719977261937657 | validation: 3.6157283855970865]
	TIME [epoch: 10.6 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.697151747734369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.697151747734369 | validation: 4.667437467448263]
	TIME [epoch: 10.6 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.98649020944932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.98649020944932 | validation: 4.176453687130887]
	TIME [epoch: 10.5 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8195776095972627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8195776095972627 | validation: 3.4339351352431184]
	TIME [epoch: 10.6 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.009654115744115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.009654115744115 | validation: 3.4048915267953057]
	TIME [epoch: 10.6 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.699170027411161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.699170027411161 | validation: 3.4737750175296176]
	TIME [epoch: 10.6 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6738663648520573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6738663648520573 | validation: 3.0802202575508004]
	TIME [epoch: 10.6 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.328939705275727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.328939705275727 | validation: 3.1484905847268156]
	TIME [epoch: 10.6 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.635163249291568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.635163249291568 | validation: 3.5091861089805114]
	TIME [epoch: 10.6 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4381260748440234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4381260748440234 | validation: 3.0796043654474]
	TIME [epoch: 10.5 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.278218184696004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.278218184696004 | validation: 3.2788070155204223]
	TIME [epoch: 10.5 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8576444807051367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8576444807051367 | validation: 2.913685754310973]
	TIME [epoch: 10.6 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4871482675068783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4871482675068783 | validation: 3.867135777055824]
	TIME [epoch: 10.6 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.337435625926522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.337435625926522 | validation: 3.430464265215083]
	TIME [epoch: 10.5 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.329155380072483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.329155380072483 | validation: 3.816368737355936]
	TIME [epoch: 10.6 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599113667946259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.599113667946259 | validation: 3.950010679359295]
	TIME [epoch: 10.6 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.945289182584416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.945289182584416 | validation: 3.41634278439139]
	TIME [epoch: 10.6 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1941305213468807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1941305213468807 | validation: 3.3492573552372824]
	TIME [epoch: 10.6 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96032446246435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.96032446246435 | validation: 2.947254922965415]
	TIME [epoch: 10.5 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0958332441021335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0958332441021335 | validation: 3.1448123007454796]
	TIME [epoch: 10.6 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9779314284255087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9779314284255087 | validation: 3.6028718697541424]
	TIME [epoch: 10.6 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9295463772475525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9295463772475525 | validation: 3.2806516020432697]
	TIME [epoch: 10.6 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.986968525062212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.986968525062212 | validation: 3.448479267213769]
	TIME [epoch: 10.5 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.745795188799013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.745795188799013 | validation: 3.0981846576223986]
	TIME [epoch: 10.6 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.894726054071376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.894726054071376 | validation: 3.8909555859900236]
	TIME [epoch: 10.6 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.743378170604687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.743378170604687 | validation: 2.9168284968158678]
	TIME [epoch: 10.6 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.476807517734523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.476807517734523 | validation: 4.299019240132217]
	TIME [epoch: 10.6 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2918679422057453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2918679422057453 | validation: 3.4151124807711386]
	TIME [epoch: 10.6 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.810080312545724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.810080312545724 | validation: 3.6503883712552887]
	TIME [epoch: 10.6 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6031469033412646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6031469033412646 | validation: 2.814298589969396]
	TIME [epoch: 10.6 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5668652498876696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5668652498876696 | validation: 4.285316147966904]
	TIME [epoch: 10.6 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.374248988079418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.374248988079418 | validation: 4.503967915280757]
	TIME [epoch: 10.6 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2452317897252874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2452317897252874 | validation: 3.2243505392432663]
	TIME [epoch: 10.6 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6685698910905558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6685698910905558 | validation: 2.618889100449385]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.302646167528087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.302646167528087 | validation: 3.0920148444359383]
	TIME [epoch: 10.5 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2180594562267912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2180594562267912 | validation: 5.2348352532221245]
	TIME [epoch: 10.6 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.751744036972935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.751744036972935 | validation: 5.447382692168982]
	TIME [epoch: 10.6 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.376666659115019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.376666659115019 | validation: 3.684762098013416]
	TIME [epoch: 10.5 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.052135667474853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.052135667474853 | validation: 3.559686461430882]
	TIME [epoch: 10.6 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8274978441613934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8274978441613934 | validation: 3.5378718773020994]
	TIME [epoch: 10.6 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.925783049297705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.925783049297705 | validation: 3.121326501575703]
	TIME [epoch: 10.6 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6511425569679945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6511425569679945 | validation: 4.081860977056486]
	TIME [epoch: 10.5 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.23424378784064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.23424378784064 | validation: 2.7023922960390423]
	TIME [epoch: 10.6 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7969936988101565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7969936988101565 | validation: 4.192396165996626]
	TIME [epoch: 10.5 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7965542244147215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7965542244147215 | validation: 2.9585178123190286]
	TIME [epoch: 10.6 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3489298114175257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3489298114175257 | validation: 2.4411609190891714]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1624693565641606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1624693565641606 | validation: 3.1727514818329507]
	TIME [epoch: 10.6 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.098646386453915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.098646386453915 | validation: 2.1057343576566954]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0042876457633407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0042876457633407 | validation: 2.440237399454022]
	TIME [epoch: 10.6 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6951151850921216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6951151850921216 | validation: 2.4038389106443883]
	TIME [epoch: 10.6 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8799315870448148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8799315870448148 | validation: 2.086132088454145]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.23629521143106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.23629521143106 | validation: 4.888891902350628]
	TIME [epoch: 10.6 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5879931239927148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5879931239927148 | validation: 2.2306012722123585]
	TIME [epoch: 10.6 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.046902207952207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.046902207952207 | validation: 2.265499179373171]
	TIME [epoch: 10.6 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5863104741733203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5863104741733203 | validation: 3.8805531694358995]
	TIME [epoch: 10.6 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.083823431780874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.083823431780874 | validation: 2.0870358459428577]
	TIME [epoch: 10.6 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.074496009319491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.074496009319491 | validation: 2.438278953013385]
	TIME [epoch: 10.6 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1681889619113965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1681889619113965 | validation: 1.899932799211359]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.821448358289781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.821448358289781 | validation: 2.9542181777049508]
	TIME [epoch: 10.5 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0741245484880393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0741245484880393 | validation: 2.676793047277002]
	TIME [epoch: 10.6 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.346432385210809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.346432385210809 | validation: 5.398881720643072]
	TIME [epoch: 10.6 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.822724783008294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.822724783008294 | validation: 2.146010748213949]
	TIME [epoch: 10.6 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.367717147788779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.367717147788779 | validation: 5.810186677756928]
	TIME [epoch: 10.6 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.376774454719153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.376774454719153 | validation: 2.9958396009565114]
	TIME [epoch: 10.6 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3472376935189376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3472376935189376 | validation: 2.7856505988127664]
	TIME [epoch: 10.6 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0649331553042463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0649331553042463 | validation: 1.6551815246408093]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8344538254274039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8344538254274039 | validation: 2.1757684705656515]
	TIME [epoch: 10.6 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9836920746580766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9836920746580766 | validation: 1.8353700296101465]
	TIME [epoch: 10.6 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6160373195090654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6160373195090654 | validation: 2.0910518423073383]
	TIME [epoch: 10.6 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4647489536457186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4647489536457186 | validation: 2.272762872103046]
	TIME [epoch: 10.6 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7513834309290413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7513834309290413 | validation: 1.6545279678570182]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7216029843565215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7216029843565215 | validation: 3.113918761973673]
	TIME [epoch: 10.6 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2913407432259745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2913407432259745 | validation: 2.0441783591277956]
	TIME [epoch: 10.5 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0911767814468374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0911767814468374 | validation: 2.0243539460069635]
	TIME [epoch: 10.5 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.138019537173778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.138019537173778 | validation: 2.425300917392117]
	TIME [epoch: 10.5 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0531849970963973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0531849970963973 | validation: 2.6253652719197365]
	TIME [epoch: 10.5 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9995606159277752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9995606159277752 | validation: 1.4095804088159627]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9814878699458234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9814878699458234 | validation: 2.5376371267882827]
	TIME [epoch: 10.6 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8909795952162756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8909795952162756 | validation: 1.390363657902447]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7233399276964263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7233399276964263 | validation: 2.0394071170329546]
	TIME [epoch: 10.6 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9219300515752953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9219300515752953 | validation: 2.031904434287489]
	TIME [epoch: 10.6 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.106328167548887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.106328167548887 | validation: 2.4170523587087978]
	TIME [epoch: 10.6 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.097260724934016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.097260724934016 | validation: 2.125915148787665]
	TIME [epoch: 10.6 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0849265819900804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0849265819900804 | validation: 3.343477228501905]
	TIME [epoch: 10.6 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9910721614091713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9910721614091713 | validation: 2.0579811681683013]
	TIME [epoch: 10.6 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8905729035141796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8905729035141796 | validation: 2.701374819876828]
	TIME [epoch: 10.6 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3334446948241165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3334446948241165 | validation: 2.0188355675876855]
	TIME [epoch: 10.6 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0433393129592403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0433393129592403 | validation: 3.069040246134773]
	TIME [epoch: 10.6 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.69545090813527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.69545090813527 | validation: 2.371856447571073]
	TIME [epoch: 10.6 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9462128275154673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9462128275154673 | validation: 2.2445939800135886]
	TIME [epoch: 10.6 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.233290618600747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.233290618600747 | validation: 2.0874170690726017]
	TIME [epoch: 10.6 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7527881083409738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7527881083409738 | validation: 2.1758778755983004]
	TIME [epoch: 10.5 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9294977544596117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9294977544596117 | validation: 1.8554863286418242]
	TIME [epoch: 10.6 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3624166448953288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3624166448953288 | validation: 1.9551318641041664]
	TIME [epoch: 10.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1198889358090733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1198889358090733 | validation: 2.635600180723482]
	TIME [epoch: 10.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.721737232876746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.721737232876746 | validation: 3.111893678225049]
	TIME [epoch: 10.6 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3571928698355675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3571928698355675 | validation: 2.6471569228433687]
	TIME [epoch: 10.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0796446388619776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0796446388619776 | validation: 1.6349781110191672]
	TIME [epoch: 10.6 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2707087952178044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2707087952178044 | validation: 2.390339833272548]
	TIME [epoch: 10.6 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245833995834889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.245833995834889 | validation: 2.103561404393089]
	TIME [epoch: 10.6 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270157708393664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.270157708393664 | validation: 2.7937268348718227]
	TIME [epoch: 10.6 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185165380227636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.185165380227636 | validation: 1.7620405567147162]
	TIME [epoch: 10.6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.877073204403119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.877073204403119 | validation: 2.563685407680317]
	TIME [epoch: 10.6 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.978561858654635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.978561858654635 | validation: 1.8686823705684532]
	TIME [epoch: 10.6 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9371722997428562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9371722997428562 | validation: 3.1655720493361477]
	TIME [epoch: 10.6 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1772570298079676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1772570298079676 | validation: 2.0237279719523786]
	TIME [epoch: 10.6 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.976000557846264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.976000557846264 | validation: 6.056702835752483]
	TIME [epoch: 10.6 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.376574957770088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.376574957770088 | validation: 2.037450329564671]
	TIME [epoch: 10.6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3437306010860164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3437306010860164 | validation: 2.8563780825209917]
	TIME [epoch: 10.6 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.565196774748626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.565196774748626 | validation: 4.563114835427931]
	TIME [epoch: 10.6 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.434164894278422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.434164894278422 | validation: 2.2702913377412712]
	TIME [epoch: 10.6 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1108505933321022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1108505933321022 | validation: 2.226974985373663]
	TIME [epoch: 10.6 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3530275758440857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3530275758440857 | validation: 2.8483002866564515]
	TIME [epoch: 10.6 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9718164061745185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9718164061745185 | validation: 2.380940142874562]
	TIME [epoch: 10.6 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182767453479734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.182767453479734 | validation: 2.325901055094449]
	TIME [epoch: 10.6 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198206442260527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.198206442260527 | validation: 2.0178418392163455]
	TIME [epoch: 10.6 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.399781247876178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.399781247876178 | validation: 2.503124794482312]
	TIME [epoch: 10.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.277468118960525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.277468118960525 | validation: 1.6362625159045332]
	TIME [epoch: 10.6 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9922603448071687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9922603448071687 | validation: 2.895652028901097]
	TIME [epoch: 10.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.057727066907197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.057727066907197 | validation: 3.5585608365530255]
	TIME [epoch: 10.6 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9227974844769427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9227974844769427 | validation: 3.2503884067671582]
	TIME [epoch: 10.6 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.105641494248503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.105641494248503 | validation: 1.6271383847122058]
	TIME [epoch: 10.6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5891021634121958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5891021634121958 | validation: 1.8954491720410533]
	TIME [epoch: 10.6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6275167034423574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6275167034423574 | validation: 1.5605854605154867]
	TIME [epoch: 10.6 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4918046420426585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4918046420426585 | validation: 1.7545511815391646]
	TIME [epoch: 10.6 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.68610966136192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.68610966136192 | validation: 1.9856014989340713]
	TIME [epoch: 10.6 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7661553157079173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7661553157079173 | validation: 1.9295015157588762]
	TIME [epoch: 10.6 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8625029232148997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8625029232148997 | validation: 2.0251343184216353]
	TIME [epoch: 10.6 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.882624966582115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.882624966582115 | validation: 2.8638187834887088]
	TIME [epoch: 10.6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8941409999618997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8941409999618997 | validation: 2.653780562104862]
	TIME [epoch: 10.6 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4243511698679674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4243511698679674 | validation: 1.8071677189013948]
	TIME [epoch: 10.6 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9010343172064306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9010343172064306 | validation: 2.2328438228767]
	TIME [epoch: 10.6 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.680264267059267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.680264267059267 | validation: 1.9884700032126121]
	TIME [epoch: 10.6 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.103384775975307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.103384775975307 | validation: 1.8706736780200126]
	TIME [epoch: 10.6 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1319140041218305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1319140041218305 | validation: 1.819777650363492]
	TIME [epoch: 10.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0237623059300685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0237623059300685 | validation: 2.192521194375727]
	TIME [epoch: 10.6 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3124683978981944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3124683978981944 | validation: 1.7301726794819758]
	TIME [epoch: 10.6 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1441472280054494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1441472280054494 | validation: 1.9091713287446128]
	TIME [epoch: 10.6 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.163872077819197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.163872077819197 | validation: 2.6561077349363904]
	TIME [epoch: 10.6 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5725591695478167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5725591695478167 | validation: 2.8094154934583457]
	TIME [epoch: 10.6 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.353202081972007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.353202081972007 | validation: 2.3038287552152927]
	TIME [epoch: 10.6 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8108952678499333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8108952678499333 | validation: 1.8233558809213413]
	TIME [epoch: 10.6 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4771237255761092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4771237255761092 | validation: 2.5016055194433497]
	TIME [epoch: 10.6 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0653113917475485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0653113917475485 | validation: 1.8141500308416758]
	TIME [epoch: 10.6 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8712282291309652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8712282291309652 | validation: 3.4516632124737225]
	TIME [epoch: 10.6 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265817582914356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.265817582914356 | validation: 1.8784850124941885]
	TIME [epoch: 10.6 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.156935753949064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.156935753949064 | validation: 1.7864449803436855]
	TIME [epoch: 10.6 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6493844377905909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6493844377905909 | validation: 1.6217330521569147]
	TIME [epoch: 10.6 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6883297686607608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6883297686607608 | validation: 1.8303080748356184]
	TIME [epoch: 10.6 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265058205824257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.265058205824257 | validation: 1.669698441933769]
	TIME [epoch: 10.6 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2480389412046056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2480389412046056 | validation: 1.9658418744607025]
	TIME [epoch: 10.6 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.504307673553227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.504307673553227 | validation: 1.8552527985448177]
	TIME [epoch: 10.6 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.824712938077631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.824712938077631 | validation: 3.398166806256011]
	TIME [epoch: 10.6 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.163856271760779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.163856271760779 | validation: 1.402892949314088]
	TIME [epoch: 10.6 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.487471772605669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.487471772605669 | validation: 2.408608452997453]
	TIME [epoch: 10.6 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9206536216841914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9206536216841914 | validation: 1.9849302351894436]
	TIME [epoch: 10.6 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.335954132831506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.335954132831506 | validation: 2.7845987092067337]
	TIME [epoch: 10.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6942737579987899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6942737579987899 | validation: 1.7307980679027057]
	TIME [epoch: 10.6 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7829603241113285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7829603241113285 | validation: 2.728269214394593]
	TIME [epoch: 10.6 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9544697895786853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9544697895786853 | validation: 5.647249690061706]
	TIME [epoch: 10.6 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4424823446476074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4424823446476074 | validation: 2.323144370479387]
	TIME [epoch: 10.6 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8410489190274721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8410489190274721 | validation: 2.2024634008425052]
	TIME [epoch: 10.6 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7556417216650577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7556417216650577 | validation: 2.0800823158961976]
	TIME [epoch: 10.6 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7897585714197866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7897585714197866 | validation: 1.4169921826497998]
	TIME [epoch: 10.6 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8147573005468716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8147573005468716 | validation: 2.376035877412466]
	TIME [epoch: 10.6 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.234885922221886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.234885922221886 | validation: 1.7090400822725007]
	TIME [epoch: 10.6 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7583490312455332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7583490312455332 | validation: 1.602400311691782]
	TIME [epoch: 10.6 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8358695540666772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8358695540666772 | validation: 2.023974607577253]
	TIME [epoch: 10.6 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.145896246785826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.145896246785826 | validation: 1.9015024705943022]
	TIME [epoch: 10.6 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8215358500628938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8215358500628938 | validation: 1.7790626820573687]
	TIME [epoch: 10.6 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8559155766791675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8559155766791675 | validation: 3.6174756916214585]
	TIME [epoch: 10.6 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.92749063529071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.92749063529071 | validation: 2.9714787550874213]
	TIME [epoch: 10.6 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8634020354807663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8634020354807663 | validation: 2.2426939419278544]
	TIME [epoch: 10.6 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9853603068120793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9853603068120793 | validation: 1.8945559149258488]
	TIME [epoch: 10.6 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0934806642522346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0934806642522346 | validation: 1.831444002090247]
	TIME [epoch: 10.6 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8828032479350714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8828032479350714 | validation: 2.593211805144361]
	TIME [epoch: 10.6 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9626945477045559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9626945477045559 | validation: 1.8876036116472825]
	TIME [epoch: 10.6 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6045489597252298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6045489597252298 | validation: 1.6824417480346672]
	TIME [epoch: 10.6 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6283767314551298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6283767314551298 | validation: 2.073927630497852]
	TIME [epoch: 10.6 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.867708937094745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.867708937094745 | validation: 2.3913526859168743]
	TIME [epoch: 10.6 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752594610689475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.752594610689475 | validation: 1.829826969711296]
	TIME [epoch: 10.6 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5953884395863063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5953884395863063 | validation: 2.917795395196955]
	TIME [epoch: 10.6 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8225981943618113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8225981943618113 | validation: 1.933749130505308]
	TIME [epoch: 10.6 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.205020974576579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.205020974576579 | validation: 1.5698370047083103]
	TIME [epoch: 10.6 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7294910776980008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7294910776980008 | validation: 1.9129346183819178]
	TIME [epoch: 10.6 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.720134936285275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.720134936285275 | validation: 2.2246552917901794]
	TIME [epoch: 10.6 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6266665339303832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6266665339303832 | validation: 1.5327693822301436]
	TIME [epoch: 10.6 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8263810648624268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8263810648624268 | validation: 2.3641148190736336]
	TIME [epoch: 10.6 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9168600019694935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9168600019694935 | validation: 1.6247424203475833]
	TIME [epoch: 10.6 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8501244396017533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8501244396017533 | validation: 2.400922680414387]
	TIME [epoch: 10.6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2334615653408925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2334615653408925 | validation: 2.5846168956882485]
	TIME [epoch: 10.6 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0219032609902676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0219032609902676 | validation: 2.2145178428726835]
	TIME [epoch: 10.6 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.939634076390711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.939634076390711 | validation: 1.9466614556236]
	TIME [epoch: 10.6 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0131590030986644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0131590030986644 | validation: 2.46748392633592]
	TIME [epoch: 10.6 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0410415115936695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0410415115936695 | validation: 1.6789344129491668]
	TIME [epoch: 10.6 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.065139291118516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.065139291118516 | validation: 2.6376155156391015]
	TIME [epoch: 10.6 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.050167051781762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.050167051781762 | validation: 2.1622060100714053]
	TIME [epoch: 10.6 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1505876877723735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1505876877723735 | validation: 3.0039609185135188]
	TIME [epoch: 10.6 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.644523725450201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.644523725450201 | validation: 6.70576293777997]
	TIME [epoch: 10.6 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.650077715376415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.650077715376415 | validation: 7.150382964707959]
	TIME [epoch: 10.6 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.743329131812544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.743329131812544 | validation: 6.629396425382701]
	TIME [epoch: 10.6 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.512736633533352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.512736633533352 | validation: 6.585953626227961]
	TIME [epoch: 10.6 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.81175555478851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.81175555478851 | validation: 2.12903124290922]
	TIME [epoch: 10.6 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.101972128156214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.101972128156214 | validation: 4.962569777297934]
	TIME [epoch: 10.6 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5246464618764533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5246464618764533 | validation: 2.510297307678886]
	TIME [epoch: 10.6 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.820061896561711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.820061896561711 | validation: 2.2103746059071083]
	TIME [epoch: 10.6 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0711942811648263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0711942811648263 | validation: 2.9243504110105683]
	TIME [epoch: 10.6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2357321308364027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2357321308364027 | validation: 2.367557730859738]
	TIME [epoch: 10.6 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1176126690707067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1176126690707067 | validation: 2.2150402973914374]
	TIME [epoch: 10.6 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9481102456113781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9481102456113781 | validation: 1.8813962508964057]
	TIME [epoch: 10.6 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.793104745563446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.793104745563446 | validation: 2.8615171850148373]
	TIME [epoch: 10.6 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3444492242564996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3444492242564996 | validation: 1.7237475815680265]
	TIME [epoch: 10.6 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5702441272392682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5702441272392682 | validation: 1.9744408317479138]
	TIME [epoch: 10.6 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9215895076086293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9215895076086293 | validation: 2.0933212593581145]
	TIME [epoch: 10.6 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1884431379350686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1884431379350686 | validation: 1.7144624419353311]
	TIME [epoch: 10.6 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8220969739930322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8220969739930322 | validation: 2.4964856955887376]
	TIME [epoch: 10.6 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0886241474115277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0886241474115277 | validation: 5.776566553371975]
	TIME [epoch: 10.6 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.395797842056024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.395797842056024 | validation: 2.1450673169421934]
	TIME [epoch: 10.6 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.370608628600075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.370608628600075 | validation: 1.9284784927023404]
	TIME [epoch: 10.6 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1459724299479754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1459724299479754 | validation: 2.7347919077829714]
	TIME [epoch: 10.6 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.396907853076352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.396907853076352 | validation: 2.4231935667135316]
	TIME [epoch: 10.6 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.578753717387185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.578753717387185 | validation: 2.520110520939407]
	TIME [epoch: 10.6 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9839300084703886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9839300084703886 | validation: 1.8050120332864623]
	TIME [epoch: 10.6 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5705119114486124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5705119114486124 | validation: 2.368737968387936]
	TIME [epoch: 10.6 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3006493243503834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3006493243503834 | validation: 1.751281335901592]
	TIME [epoch: 10.6 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.858508989435371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.858508989435371 | validation: 2.783245556202603]
	TIME [epoch: 10.6 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.053556579013344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.053556579013344 | validation: 4.522023956053341]
	TIME [epoch: 10.6 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.320434601434625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.320434601434625 | validation: 2.2104790571957955]
	TIME [epoch: 10.6 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.969183476359325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.969183476359325 | validation: 1.896763183014384]
	TIME [epoch: 10.6 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7579749654862358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7579749654862358 | validation: 2.9297292757140485]
	TIME [epoch: 10.6 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0496324543123987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0496324543123987 | validation: 1.6548493683455732]
	TIME [epoch: 10.6 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.128577564806463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.128577564806463 | validation: 2.14971371206261]
	TIME [epoch: 10.6 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9257554521322404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9257554521322404 | validation: 2.1262587318717108]
	TIME [epoch: 10.6 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7944019756638627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7944019756638627 | validation: 2.1892468422668396]
	TIME [epoch: 10.6 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8406112976343842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8406112976343842 | validation: 1.6400222638764883]
	TIME [epoch: 10.6 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1089956163658146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1089956163658146 | validation: 2.7825774732949276]
	TIME [epoch: 10.6 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.094649518719323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.094649518719323 | validation: 1.7469272986105666]
	TIME [epoch: 10.6 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1187501533752657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1187501533752657 | validation: 2.191413304513053]
	TIME [epoch: 10.6 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7862626542638995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7862626542638995 | validation: 1.5523399820971393]
	TIME [epoch: 10.6 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5876271760658296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5876271760658296 | validation: 2.417072841483718]
	TIME [epoch: 10.6 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8823088530003882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8823088530003882 | validation: 1.79769386439872]
	TIME [epoch: 10.6 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8681372948780492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8681372948780492 | validation: 3.2073969754492304]
	TIME [epoch: 10.6 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.359032056847437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.359032056847437 | validation: 2.3793467444777043]
	TIME [epoch: 10.6 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4389052013036636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4389052013036636 | validation: 2.6902772909286514]
	TIME [epoch: 10.6 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9475239888040985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9475239888040985 | validation: 1.9622763028586363]
	TIME [epoch: 10.6 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.738918070947875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.738918070947875 | validation: 2.88480636504239]
	TIME [epoch: 10.6 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9850552574942142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9850552574942142 | validation: 1.8919347563404971]
	TIME [epoch: 10.6 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9502437919329008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9502437919329008 | validation: 1.9102902768245682]
	TIME [epoch: 10.6 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195395002446138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.195395002446138 | validation: 1.9112682098067815]
	TIME [epoch: 10.6 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2157789255662506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2157789255662506 | validation: 2.6774011388994268]
	TIME [epoch: 10.6 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.215863008971754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.215863008971754 | validation: 2.1798764264624197]
	TIME [epoch: 10.6 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9159084921542764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9159084921542764 | validation: 2.746384575089347]
	TIME [epoch: 10.6 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0469329662692943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0469329662692943 | validation: 1.8115667899127135]
	TIME [epoch: 10.6 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.913382115853795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.913382115853795 | validation: 2.630874740254059]
	TIME [epoch: 10.6 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8655170626959037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8655170626959037 | validation: 1.7482323043622352]
	TIME [epoch: 10.6 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.837552366197837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.837552366197837 | validation: 2.5599713669226904]
	TIME [epoch: 10.6 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7111445407428185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7111445407428185 | validation: 2.007465936987914]
	TIME [epoch: 10.6 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2727419448102006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2727419448102006 | validation: 2.997662298458032]
	TIME [epoch: 10.6 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.672909865078312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.672909865078312 | validation: 3.2276723807836754]
	TIME [epoch: 10.6 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1665142827965482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1665142827965482 | validation: 2.256327072311249]
	TIME [epoch: 10.6 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3847902306746347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3847902306746347 | validation: 3.501682128653489]
	TIME [epoch: 10.6 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6288882676359364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6288882676359364 | validation: 2.655433460559924]
	TIME [epoch: 10.6 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.34598452144822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.34598452144822 | validation: 3.13104476064855]
	TIME [epoch: 10.6 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4947748028101735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4947748028101735 | validation: 2.434416279649264]
	TIME [epoch: 10.6 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3963360427147893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3963360427147893 | validation: 2.3406707912512923]
	TIME [epoch: 10.6 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1925744159278384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1925744159278384 | validation: 2.3652018104858654]
	TIME [epoch: 10.6 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6929618979919825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6929618979919825 | validation: 6.150609866385428]
	TIME [epoch: 10.6 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.896013697787241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.896013697787241 | validation: 4.7563363989099425]
	TIME [epoch: 10.6 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1312213406673526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1312213406673526 | validation: 2.3234430654454936]
	TIME [epoch: 10.6 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.863217117882931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.863217117882931 | validation: 3.299082295087682]
	TIME [epoch: 10.6 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.643936207124753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.643936207124753 | validation: 3.2158435586959513]
	TIME [epoch: 10.6 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.661007146692795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.661007146692795 | validation: 2.9878030929978894]
	TIME [epoch: 10.6 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.310596061883295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.310596061883295 | validation: 3.055698395047623]
	TIME [epoch: 10.6 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2921723158431004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2921723158431004 | validation: 1.575839965390474]
	TIME [epoch: 10.6 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.984898922816057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.984898922816057 | validation: 2.3491772937553144]
	TIME [epoch: 10.6 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9801120659299418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9801120659299418 | validation: 2.916644583678557]
	TIME [epoch: 10.6 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1103101024141653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1103101024141653 | validation: 2.3677844184623273]
	TIME [epoch: 10.6 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1820697339422184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1820697339422184 | validation: 2.5481662982667532]
	TIME [epoch: 10.6 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5756942519275183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5756942519275183 | validation: 2.3514544220962508]
	TIME [epoch: 10.6 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.046641820657611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.046641820657611 | validation: 2.4981092916065735]
	TIME [epoch: 10.6 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240490056925377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.240490056925377 | validation: 3.00404667294136]
	TIME [epoch: 10.6 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.681153273242148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.681153273242148 | validation: 2.689116840715116]
	TIME [epoch: 10.6 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4589966280597664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4589966280597664 | validation: 1.7664533328400212]
	TIME [epoch: 10.6 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9582155142015278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9582155142015278 | validation: 2.491532259297736]
	TIME [epoch: 10.6 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.686561366483477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.686561366483477 | validation: 4.2635765983369875]
	TIME [epoch: 10.6 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.734741161896708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.734741161896708 | validation: 2.371035433418831]
	TIME [epoch: 10.6 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3037397794989163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3037397794989163 | validation: 2.490346483854115]
	TIME [epoch: 10.6 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.09721275217878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.09721275217878 | validation: 2.387845671598781]
	TIME [epoch: 10.6 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9471598692648047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9471598692648047 | validation: 2.8374346526197884]
	TIME [epoch: 10.6 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.165932651939134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.165932651939134 | validation: 3.062602125520134]
	TIME [epoch: 10.6 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6126398534680972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6126398534680972 | validation: 2.563411391341092]
	TIME [epoch: 10.6 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.573206669213816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.573206669213816 | validation: 1.6997377388461934]
	TIME [epoch: 10.6 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9158005438506156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9158005438506156 | validation: 2.873037149575733]
	TIME [epoch: 10.6 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2514230760911587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2514230760911587 | validation: 2.7030301272466892]
	TIME [epoch: 10.6 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0351212726067587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0351212726067587 | validation: 2.1174212328840336]
	TIME [epoch: 10.6 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9729103165106263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9729103165106263 | validation: 2.900023640959387]
	TIME [epoch: 10.6 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.278734741105681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.278734741105681 | validation: 3.020382075143839]
	TIME [epoch: 10.6 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1690925508052095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1690925508052095 | validation: 1.8641479195376753]
	TIME [epoch: 10.6 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9548939182072005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9548939182072005 | validation: 2.2538777616837558]
	TIME [epoch: 10.6 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1964808933376947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1964808933376947 | validation: 2.2251570663153672]
	TIME [epoch: 10.6 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0992574083995774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0992574083995774 | validation: 1.9968874530635332]
	TIME [epoch: 10.6 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7771539742932763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7771539742932763 | validation: 1.6855980803304556]
	TIME [epoch: 10.6 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6060318678201653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6060318678201653 | validation: 1.9249668193773806]
	TIME [epoch: 10.6 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7308851844264144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7308851844264144 | validation: 2.8243195450346343]
	TIME [epoch: 10.6 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0694547161051737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0694547161051737 | validation: 2.3611000502226656]
	TIME [epoch: 10.6 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9875668688776205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9875668688776205 | validation: 2.215445306133797]
	TIME [epoch: 10.6 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.268999324195043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.268999324195043 | validation: 2.996230046034623]
	TIME [epoch: 10.6 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.335065437629102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.335065437629102 | validation: 2.045876165663236]
	TIME [epoch: 10.6 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.073727373052157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.073727373052157 | validation: 3.4670636625932207]
	TIME [epoch: 10.6 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5397609717793417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5397609717793417 | validation: 2.9088955240317365]
	TIME [epoch: 10.6 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0667673836126665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0667673836126665 | validation: 2.406233393423691]
	TIME [epoch: 10.6 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3005512317508257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3005512317508257 | validation: 2.721917322886124]
	TIME [epoch: 10.6 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8955177143374702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8955177143374702 | validation: 1.8166173086773751]
	TIME [epoch: 10.6 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8071082133731822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8071082133731822 | validation: 1.8734778611430545]
	TIME [epoch: 10.6 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8512456986929884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8512456986929884 | validation: 3.441336389986949]
	TIME [epoch: 10.6 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5109064247336144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5109064247336144 | validation: 3.4954368670795466]
	TIME [epoch: 10.6 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4422472890179385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4422472890179385 | validation: 2.048906665748662]
	TIME [epoch: 10.6 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.001862172753492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.001862172753492 | validation: 3.3642809474569653]
	TIME [epoch: 10.6 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.387530734719329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.387530734719329 | validation: 2.2915591618241815]
	TIME [epoch: 10.6 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.081900198313015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.081900198313015 | validation: 2.4120558097556306]
	TIME [epoch: 10.6 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2042043301052785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2042043301052785 | validation: 3.0965546023357113]
	TIME [epoch: 10.6 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.359051800851488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.359051800851488 | validation: 2.29141936079163]
	TIME [epoch: 10.6 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.17927095978962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.17927095978962 | validation: 2.4957547732605896]
	TIME [epoch: 10.6 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0590577503889045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0590577503889045 | validation: 2.0317144990623883]
	TIME [epoch: 10.6 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.930045040980779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.930045040980779 | validation: 2.468907390323534]
	TIME [epoch: 10.6 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0215996294913836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0215996294913836 | validation: 2.461551631604058]
	TIME [epoch: 10.6 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2124161033230356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2124161033230356 | validation: 2.510782758376255]
	TIME [epoch: 10.6 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261962875324224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.261962875324224 | validation: 2.4433146251670848]
	TIME [epoch: 10.6 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9581738691662802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9581738691662802 | validation: 2.4494524885356395]
	TIME [epoch: 10.6 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1187999513256743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1187999513256743 | validation: 2.4716030559323974]
	TIME [epoch: 10.6 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0742629618968804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0742629618968804 | validation: 2.2564638021848165]
	TIME [epoch: 10.6 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2271562768622495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2271562768622495 | validation: 2.199280834678528]
	TIME [epoch: 10.6 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1220403931456993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1220403931456993 | validation: 4.504117738494778]
	TIME [epoch: 10.6 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9251933806712933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9251933806712933 | validation: 2.368742231895555]
	TIME [epoch: 10.6 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1951167155686084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1951167155686084 | validation: 2.63692677139631]
	TIME [epoch: 10.6 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6194483724430024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6194483724430024 | validation: 3.885154145125605]
	TIME [epoch: 10.6 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1816386371384393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1816386371384393 | validation: 2.349245520895708]
	TIME [epoch: 10.6 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9250682581121363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9250682581121363 | validation: 1.8483896318078765]
	TIME [epoch: 10.6 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7041929351930072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7041929351930072 | validation: 1.7793396043065384]
	TIME [epoch: 10.6 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.930619266247009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.930619266247009 | validation: 2.1426373867841]
	TIME [epoch: 10.6 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.883685301892109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.883685301892109 | validation: 2.369514936483277]
	TIME [epoch: 10.6 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.799432450919637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.799432450919637 | validation: 2.1601358509154034]
	TIME [epoch: 10.6 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8888220523956183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8888220523956183 | validation: 1.7215597429885878]
	TIME [epoch: 10.6 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5071421915118863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5071421915118863 | validation: 2.3718419685868044]
	TIME [epoch: 10.6 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0538659454417862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0538659454417862 | validation: 2.4503812132685208]
	TIME [epoch: 10.6 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.122753747425269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.122753747425269 | validation: 2.535851417088357]
	TIME [epoch: 10.6 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1157654587704977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1157654587704977 | validation: 3.7605225949991676]
	TIME [epoch: 10.6 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.789589612142933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.789589612142933 | validation: 2.6178043105975037]
	TIME [epoch: 10.6 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0721846876082024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0721846876082024 | validation: 1.9576402381760891]
	TIME [epoch: 10.6 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.968958277917105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.968958277917105 | validation: 2.1747154200081638]
	TIME [epoch: 10.6 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.637920675918863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.637920675918863 | validation: 2.070204483950101]
	TIME [epoch: 10.6 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7807329458703358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7807329458703358 | validation: 3.7773414871868694]
	TIME [epoch: 10.6 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.225021314603916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.225021314603916 | validation: 2.586427149864116]
	TIME [epoch: 10.6 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9288501280110204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9288501280110204 | validation: 1.6828395474573035]
	TIME [epoch: 10.6 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6715294773612015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6715294773612015 | validation: 1.8028502125480663]
	TIME [epoch: 10.6 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6547613892836892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6547613892836892 | validation: 1.7830048506416154]
	TIME [epoch: 10.6 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.520270335239458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.520270335239458 | validation: 1.6015607662243398]
	TIME [epoch: 10.6 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7130536480735685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7130536480735685 | validation: 2.2195509510529314]
	TIME [epoch: 10.6 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.731066566193014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.731066566193014 | validation: 1.7241240486736846]
	TIME [epoch: 10.6 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.577834471908787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.577834471908787 | validation: 2.0399993688935725]
	TIME [epoch: 10.6 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7996971386301233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7996971386301233 | validation: 1.5795033209130245]
	TIME [epoch: 10.6 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5532484463258467		[learning rate: 0.0099755]
	Learning Rate: 0.00997547
	LOSS [training: 1.5532484463258467 | validation: 2.3895400040391577]
	TIME [epoch: 10.6 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8998672715217555		[learning rate: 0.0099449]
	Learning Rate: 0.00994489
	LOSS [training: 1.8998672715217555 | validation: 2.2631847512748524]
	TIME [epoch: 10.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.731220656265198		[learning rate: 0.0099144]
	Learning Rate: 0.0099144
	LOSS [training: 1.731220656265198 | validation: 1.7507071894650899]
	TIME [epoch: 10.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4671476388335607		[learning rate: 0.009884]
	Learning Rate: 0.00988401
	LOSS [training: 1.4671476388335607 | validation: 1.811636960216135]
	TIME [epoch: 10.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4719237961144924		[learning rate: 0.0098537]
	Learning Rate: 0.00985371
	LOSS [training: 1.4719237961144924 | validation: 1.5249081800284274]
	TIME [epoch: 10.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8038410685450728		[learning rate: 0.0098235]
	Learning Rate: 0.00982351
	LOSS [training: 1.8038410685450728 | validation: 3.2049483981333124]
	TIME [epoch: 10.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7455715828097556		[learning rate: 0.0097934]
	Learning Rate: 0.0097934
	LOSS [training: 1.7455715828097556 | validation: 2.0951761906228152]
	TIME [epoch: 10.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6066759683417842		[learning rate: 0.0097634]
	Learning Rate: 0.00976337
	LOSS [training: 1.6066759683417842 | validation: 1.7350830229946752]
	TIME [epoch: 10.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.574778002278628		[learning rate: 0.0097334]
	Learning Rate: 0.00973345
	LOSS [training: 1.574778002278628 | validation: 1.420993649335141]
	TIME [epoch: 10.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3839934196821333		[learning rate: 0.0097036]
	Learning Rate: 0.00970361
	LOSS [training: 2.3839934196821333 | validation: 1.29255324159094]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4379079425407257		[learning rate: 0.0096739]
	Learning Rate: 0.00967386
	LOSS [training: 1.4379079425407257 | validation: 1.70477234774968]
	TIME [epoch: 10.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6436834380221605		[learning rate: 0.0096442]
	Learning Rate: 0.00964421
	LOSS [training: 1.6436834380221605 | validation: 2.3519284348406364]
	TIME [epoch: 10.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.840018385380736		[learning rate: 0.0096146]
	Learning Rate: 0.00961465
	LOSS [training: 1.840018385380736 | validation: 1.748148273970568]
	TIME [epoch: 10.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5056996557479168		[learning rate: 0.0095852]
	Learning Rate: 0.00958517
	LOSS [training: 1.5056996557479168 | validation: 1.7477660920889637]
	TIME [epoch: 10.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0546257494570455		[learning rate: 0.0095558]
	Learning Rate: 0.00955579
	LOSS [training: 2.0546257494570455 | validation: 2.505668546107474]
	TIME [epoch: 10.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.846037483988012		[learning rate: 0.0095265]
	Learning Rate: 0.0095265
	LOSS [training: 1.846037483988012 | validation: 1.61474468528449]
	TIME [epoch: 10.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6657761582409711		[learning rate: 0.0094973]
	Learning Rate: 0.0094973
	LOSS [training: 1.6657761582409711 | validation: 1.9113488377198165]
	TIME [epoch: 10.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5136393909501833		[learning rate: 0.0094682]
	Learning Rate: 0.00946818
	LOSS [training: 1.5136393909501833 | validation: 2.201582396957229]
	TIME [epoch: 10.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.721896218396904		[learning rate: 0.0094392]
	Learning Rate: 0.00943916
	LOSS [training: 1.721896218396904 | validation: 1.5137054103105545]
	TIME [epoch: 10.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752593885578277		[learning rate: 0.0094102]
	Learning Rate: 0.00941022
	LOSS [training: 1.752593885578277 | validation: 2.164751872259294]
	TIME [epoch: 10.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.651624570880174		[learning rate: 0.0093814]
	Learning Rate: 0.00938138
	LOSS [training: 1.651624570880174 | validation: 1.580259180953973]
	TIME [epoch: 10.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.656576921586752		[learning rate: 0.0093526]
	Learning Rate: 0.00935262
	LOSS [training: 2.656576921586752 | validation: 2.681827545948504]
	TIME [epoch: 10.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.444970175117784		[learning rate: 0.009324]
	Learning Rate: 0.00932395
	LOSS [training: 2.444970175117784 | validation: 3.3450189781392465]
	TIME [epoch: 10.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.053408272048267		[learning rate: 0.0092954]
	Learning Rate: 0.00929537
	LOSS [training: 2.053408272048267 | validation: 1.9850704470318619]
	TIME [epoch: 10.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75370092523302		[learning rate: 0.0092669]
	Learning Rate: 0.00926687
	LOSS [training: 1.75370092523302 | validation: 1.7410270551763767]
	TIME [epoch: 10.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4541759241934127		[learning rate: 0.0092385]
	Learning Rate: 0.00923847
	LOSS [training: 1.4541759241934127 | validation: 2.1822443041838273]
	TIME [epoch: 10.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.556583383614266		[learning rate: 0.0092101]
	Learning Rate: 0.00921015
	LOSS [training: 1.556583383614266 | validation: 2.4016344021813874]
	TIME [epoch: 10.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9149005221709117		[learning rate: 0.0091819]
	Learning Rate: 0.00918192
	LOSS [training: 1.9149005221709117 | validation: 1.4426862828441398]
	TIME [epoch: 10.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7655607618542195		[learning rate: 0.0091538]
	Learning Rate: 0.00915377
	LOSS [training: 1.7655607618542195 | validation: 1.5417828643883058]
	TIME [epoch: 10.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.700464275174938		[learning rate: 0.0091257]
	Learning Rate: 0.00912571
	LOSS [training: 1.700464275174938 | validation: 1.400701528081438]
	TIME [epoch: 10.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5689447115664086		[learning rate: 0.0090977]
	Learning Rate: 0.00909774
	LOSS [training: 1.5689447115664086 | validation: 2.0267556354553893]
	TIME [epoch: 10.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.611371037179833		[learning rate: 0.0090698]
	Learning Rate: 0.00906985
	LOSS [training: 1.611371037179833 | validation: 1.6916857624143427]
	TIME [epoch: 10.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6756684596982254		[learning rate: 0.009042]
	Learning Rate: 0.00904205
	LOSS [training: 1.6756684596982254 | validation: 1.9949182687445852]
	TIME [epoch: 10.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.635290041319833		[learning rate: 0.0090143]
	Learning Rate: 0.00901433
	LOSS [training: 1.635290041319833 | validation: 1.5651012392430865]
	TIME [epoch: 10.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4201532149466114		[learning rate: 0.0089867]
	Learning Rate: 0.00898669
	LOSS [training: 1.4201532149466114 | validation: 2.1227042908583535]
	TIME [epoch: 10.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7503209284510024		[learning rate: 0.0089591]
	Learning Rate: 0.00895915
	LOSS [training: 1.7503209284510024 | validation: 1.6042564501925898]
	TIME [epoch: 10.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5164475565510867		[learning rate: 0.0089317]
	Learning Rate: 0.00893168
	LOSS [training: 1.5164475565510867 | validation: 2.0689081079614344]
	TIME [epoch: 10.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5655972954695005		[learning rate: 0.0089043]
	Learning Rate: 0.0089043
	LOSS [training: 1.5655972954695005 | validation: 1.6233160206475747]
	TIME [epoch: 10.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.413117025533046		[learning rate: 0.008877]
	Learning Rate: 0.00887701
	LOSS [training: 1.413117025533046 | validation: 1.5533516848427666]
	TIME [epoch: 10.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7100111552362798		[learning rate: 0.0088498]
	Learning Rate: 0.0088498
	LOSS [training: 1.7100111552362798 | validation: 2.1746930851299946]
	TIME [epoch: 10.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.679663146402378		[learning rate: 0.0088227]
	Learning Rate: 0.00882267
	LOSS [training: 1.679663146402378 | validation: 3.0084804814679074]
	TIME [epoch: 10.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7983437796952004		[learning rate: 0.0087956]
	Learning Rate: 0.00879562
	LOSS [training: 1.7983437796952004 | validation: 2.132250684359068]
	TIME [epoch: 10.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.453610765958187		[learning rate: 0.0087687]
	Learning Rate: 0.00876866
	LOSS [training: 1.453610765958187 | validation: 1.8394408525122203]
	TIME [epoch: 10.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2687672360302342		[learning rate: 0.0087418]
	Learning Rate: 0.00874178
	LOSS [training: 1.2687672360302342 | validation: 1.8572978624251881]
	TIME [epoch: 10.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.464103607295545		[learning rate: 0.008715]
	Learning Rate: 0.00871499
	LOSS [training: 1.464103607295545 | validation: 1.7233955402481191]
	TIME [epoch: 10.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.65335794769999		[learning rate: 0.0086883]
	Learning Rate: 0.00868827
	LOSS [training: 1.65335794769999 | validation: 2.356527949360849]
	TIME [epoch: 10.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6779418756485245		[learning rate: 0.0086616]
	Learning Rate: 0.00866164
	LOSS [training: 1.6779418756485245 | validation: 2.167817696959847]
	TIME [epoch: 10.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3133274288793493		[learning rate: 0.0086351]
	Learning Rate: 0.00863509
	LOSS [training: 1.3133274288793493 | validation: 1.7637311830227964]
	TIME [epoch: 10.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3075199647486322		[learning rate: 0.0086086]
	Learning Rate: 0.00860862
	LOSS [training: 1.3075199647486322 | validation: 1.4537766303975839]
	TIME [epoch: 10.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1879740527804095		[learning rate: 0.0085822]
	Learning Rate: 0.00858223
	LOSS [training: 1.1879740527804095 | validation: 1.241361271645331]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3597801364219462		[learning rate: 0.0085559]
	Learning Rate: 0.00855592
	LOSS [training: 1.3597801364219462 | validation: 1.6221778991985758]
	TIME [epoch: 10.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.783622607196665		[learning rate: 0.0085297]
	Learning Rate: 0.00852969
	LOSS [training: 1.783622607196665 | validation: 2.16187420253093]
	TIME [epoch: 10.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4474471693748125		[learning rate: 0.0085035]
	Learning Rate: 0.00850354
	LOSS [training: 1.4474471693748125 | validation: 1.6751937628365516]
	TIME [epoch: 10.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3098044278065892		[learning rate: 0.0084775]
	Learning Rate: 0.00847748
	LOSS [training: 1.3098044278065892 | validation: 1.3214059413187993]
	TIME [epoch: 10.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2474675799705353		[learning rate: 0.0084515]
	Learning Rate: 0.00845149
	LOSS [training: 1.2474675799705353 | validation: 1.311474475299084]
	TIME [epoch: 10.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1889400809363588		[learning rate: 0.0084256]
	Learning Rate: 0.00842558
	LOSS [training: 1.1889400809363588 | validation: 1.264438708717953]
	TIME [epoch: 10.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1367753391140574		[learning rate: 0.0083998]
	Learning Rate: 0.00839976
	LOSS [training: 1.1367753391140574 | validation: 2.0704783514682954]
	TIME [epoch: 10.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.429434558864095		[learning rate: 0.008374]
	Learning Rate: 0.00837401
	LOSS [training: 1.429434558864095 | validation: 1.408896649514512]
	TIME [epoch: 10.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2344392438123506		[learning rate: 0.0083483]
	Learning Rate: 0.00834834
	LOSS [training: 1.2344392438123506 | validation: 2.0083195530177624]
	TIME [epoch: 10.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298799492968098		[learning rate: 0.0083227]
	Learning Rate: 0.00832275
	LOSS [training: 1.298799492968098 | validation: 1.8080797851963764]
	TIME [epoch: 10.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.621678187913097		[learning rate: 0.0082972]
	Learning Rate: 0.00829723
	LOSS [training: 1.621678187913097 | validation: 2.1751786969860007]
	TIME [epoch: 10.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3132019178538268		[learning rate: 0.0082718]
	Learning Rate: 0.0082718
	LOSS [training: 1.3132019178538268 | validation: 1.8346436014199423]
	TIME [epoch: 10.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2386865354582883		[learning rate: 0.0082464]
	Learning Rate: 0.00824644
	LOSS [training: 1.2386865354582883 | validation: 1.1805501120867317]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1468432019152222		[learning rate: 0.0082212]
	Learning Rate: 0.00822116
	LOSS [training: 1.1468432019152222 | validation: 0.9747388741416185]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1328235415438268		[learning rate: 0.008196]
	Learning Rate: 0.00819596
	LOSS [training: 1.1328235415438268 | validation: 1.2870753094692384]
	TIME [epoch: 10.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1746858201903572		[learning rate: 0.0081708]
	Learning Rate: 0.00817084
	LOSS [training: 1.1746858201903572 | validation: 1.1293138496468527]
	TIME [epoch: 10.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9556168727516281		[learning rate: 0.0081458]
	Learning Rate: 0.00814579
	LOSS [training: 0.9556168727516281 | validation: 1.316777889543979]
	TIME [epoch: 10.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1100710484318443		[learning rate: 0.0081208]
	Learning Rate: 0.00812082
	LOSS [training: 1.1100710484318443 | validation: 1.0554633127173292]
	TIME [epoch: 10.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0882653804900224		[learning rate: 0.0080959]
	Learning Rate: 0.00809593
	LOSS [training: 1.0882653804900224 | validation: 1.4454247058048035]
	TIME [epoch: 10.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3949250578651164		[learning rate: 0.0080711]
	Learning Rate: 0.00807111
	LOSS [training: 1.3949250578651164 | validation: 1.9112462545142077]
	TIME [epoch: 10.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.193456612132581		[learning rate: 0.0080464]
	Learning Rate: 0.00804637
	LOSS [training: 1.193456612132581 | validation: 1.0925957436377816]
	TIME [epoch: 10.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2264751076314875		[learning rate: 0.0080217]
	Learning Rate: 0.0080217
	LOSS [training: 1.2264751076314875 | validation: 1.40538892928178]
	TIME [epoch: 10.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.164821367084047		[learning rate: 0.0079971]
	Learning Rate: 0.00799711
	LOSS [training: 1.164821367084047 | validation: 1.9399030300831692]
	TIME [epoch: 10.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3197347647263724		[learning rate: 0.0079726]
	Learning Rate: 0.0079726
	LOSS [training: 1.3197347647263724 | validation: 1.5956999346172498]
	TIME [epoch: 10.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0743455878397234		[learning rate: 0.0079482]
	Learning Rate: 0.00794816
	LOSS [training: 1.0743455878397234 | validation: 1.042134675767777]
	TIME [epoch: 10.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4746121074769223		[learning rate: 0.0079238]
	Learning Rate: 0.0079238
	LOSS [training: 1.4746121074769223 | validation: 1.1528801194885838]
	TIME [epoch: 10.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.01847731802191		[learning rate: 0.0078995]
	Learning Rate: 0.00789951
	LOSS [training: 1.01847731802191 | validation: 1.2648019846426994]
	TIME [epoch: 10.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.343591987584911		[learning rate: 0.0078753]
	Learning Rate: 0.00787529
	LOSS [training: 1.343591987584911 | validation: 1.21042770803611]
	TIME [epoch: 10.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.021143947886932		[learning rate: 0.0078512]
	Learning Rate: 0.00785115
	LOSS [training: 1.021143947886932 | validation: 1.0685799420837483]
	TIME [epoch: 10.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9381955367372221		[learning rate: 0.0078271]
	Learning Rate: 0.00782708
	LOSS [training: 0.9381955367372221 | validation: 1.6600357551221077]
	TIME [epoch: 10.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1194735663124138		[learning rate: 0.0078031]
	Learning Rate: 0.00780309
	LOSS [training: 1.1194735663124138 | validation: 1.7259695381518512]
	TIME [epoch: 10.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1438810341509158		[learning rate: 0.0077792]
	Learning Rate: 0.00777917
	LOSS [training: 1.1438810341509158 | validation: 1.1090690535161292]
	TIME [epoch: 10.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3761666055499944		[learning rate: 0.0077553]
	Learning Rate: 0.00775532
	LOSS [training: 1.3761666055499944 | validation: 2.069816882954831]
	TIME [epoch: 10.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2700450649557768		[learning rate: 0.0077316]
	Learning Rate: 0.00773155
	LOSS [training: 1.2700450649557768 | validation: 1.0571057102442234]
	TIME [epoch: 10.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9302873836105572		[learning rate: 0.0077079]
	Learning Rate: 0.00770785
	LOSS [training: 0.9302873836105572 | validation: 1.6305510479131022]
	TIME [epoch: 10.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1031257141197597		[learning rate: 0.0076842]
	Learning Rate: 0.00768422
	LOSS [training: 1.1031257141197597 | validation: 2.167919206696932]
	TIME [epoch: 10.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7755259605622569		[learning rate: 0.0076607]
	Learning Rate: 0.00766067
	LOSS [training: 1.7755259605622569 | validation: 1.9023093701120666]
	TIME [epoch: 10.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1005637806396362		[learning rate: 0.0076372]
	Learning Rate: 0.00763718
	LOSS [training: 1.1005637806396362 | validation: 1.013846379659908]
	TIME [epoch: 10.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9658417482912987		[learning rate: 0.0076138]
	Learning Rate: 0.00761377
	LOSS [training: 0.9658417482912987 | validation: 1.0356051171268033]
	TIME [epoch: 10.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0059467487874898		[learning rate: 0.0075904]
	Learning Rate: 0.00759043
	LOSS [training: 1.0059467487874898 | validation: 0.8429325606003053]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0033897184855083		[learning rate: 0.0075672]
	Learning Rate: 0.00756717
	LOSS [training: 1.0033897184855083 | validation: 1.8698275905584723]
	TIME [epoch: 10.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1569688132894078		[learning rate: 0.007544]
	Learning Rate: 0.00754397
	LOSS [training: 1.1569688132894078 | validation: 2.2596880079678257]
	TIME [epoch: 10.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3330868450535929		[learning rate: 0.0075208]
	Learning Rate: 0.00752085
	LOSS [training: 1.3330868450535929 | validation: 1.3617011048606864]
	TIME [epoch: 10.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1098136133669336		[learning rate: 0.0074978]
	Learning Rate: 0.00749779
	LOSS [training: 1.1098136133669336 | validation: 0.838993805575289]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9631342763574195		[learning rate: 0.0074748]
	Learning Rate: 0.00747481
	LOSS [training: 0.9631342763574195 | validation: 0.7935795149328594]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9333134618376823		[learning rate: 0.0074519]
	Learning Rate: 0.00745189
	LOSS [training: 0.9333134618376823 | validation: 0.8558678002074802]
	TIME [epoch: 10.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8882866328464332		[learning rate: 0.0074291]
	Learning Rate: 0.00742905
	LOSS [training: 0.8882866328464332 | validation: 1.1803939384367437]
	TIME [epoch: 10.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.057992712398099		[learning rate: 0.0074063]
	Learning Rate: 0.00740628
	LOSS [training: 1.057992712398099 | validation: 0.9790547740814597]
	TIME [epoch: 10.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2581488577799453		[learning rate: 0.0073836]
	Learning Rate: 0.00738357
	LOSS [training: 1.2581488577799453 | validation: 0.8959862004356032]
	TIME [epoch: 10.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9265100433364409		[learning rate: 0.0073609]
	Learning Rate: 0.00736094
	LOSS [training: 0.9265100433364409 | validation: 2.0709986776386846]
	TIME [epoch: 10.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.065799866709789		[learning rate: 0.0073384]
	Learning Rate: 0.00733838
	LOSS [training: 1.065799866709789 | validation: 1.2089050398124308]
	TIME [epoch: 10.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0012871868984705		[learning rate: 0.0073159]
	Learning Rate: 0.00731588
	LOSS [training: 1.0012871868984705 | validation: 0.8898894270962957]
	TIME [epoch: 10.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1220495503477195		[learning rate: 0.0072935]
	Learning Rate: 0.00729345
	LOSS [training: 2.1220495503477195 | validation: 1.01966215279876]
	TIME [epoch: 10.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2202023711856227		[learning rate: 0.0072711]
	Learning Rate: 0.0072711
	LOSS [training: 1.2202023711856227 | validation: 0.9599286512515601]
	TIME [epoch: 10.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.810282364377114		[learning rate: 0.0072488]
	Learning Rate: 0.00724881
	LOSS [training: 0.810282364377114 | validation: 1.7951447945748695]
	TIME [epoch: 10.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.183523907212525		[learning rate: 0.0072266]
	Learning Rate: 0.00722659
	LOSS [training: 1.183523907212525 | validation: 1.2081048345654533]
	TIME [epoch: 10.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.190403349090736		[learning rate: 0.0072044]
	Learning Rate: 0.00720444
	LOSS [training: 1.190403349090736 | validation: 1.5288543581982834]
	TIME [epoch: 10.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2018414774572634		[learning rate: 0.0071824]
	Learning Rate: 0.00718235
	LOSS [training: 1.2018414774572634 | validation: 1.1817873811461344]
	TIME [epoch: 10.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1510874534122528		[learning rate: 0.0071603]
	Learning Rate: 0.00716033
	LOSS [training: 1.1510874534122528 | validation: 1.8010763782519592]
	TIME [epoch: 10.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.225501087806312		[learning rate: 0.0071384]
	Learning Rate: 0.00713838
	LOSS [training: 1.225501087806312 | validation: 1.0305883949863308]
	TIME [epoch: 10.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0046607489833583		[learning rate: 0.0071165]
	Learning Rate: 0.0071165
	LOSS [training: 1.0046607489833583 | validation: 1.4054819847153723]
	TIME [epoch: 10.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1858755576879583		[learning rate: 0.0070947]
	Learning Rate: 0.00709469
	LOSS [training: 1.1858755576879583 | validation: 0.9118439892349269]
	TIME [epoch: 10.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8791490076011103		[learning rate: 0.0070729]
	Learning Rate: 0.00707294
	LOSS [training: 0.8791490076011103 | validation: 3.3169820489333666]
	TIME [epoch: 10.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9039443671919298		[learning rate: 0.0070513]
	Learning Rate: 0.00705126
	LOSS [training: 1.9039443671919298 | validation: 0.9502688782103834]
	TIME [epoch: 10.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0753962086316076		[learning rate: 0.0070296]
	Learning Rate: 0.00702964
	LOSS [training: 1.0753962086316076 | validation: 1.1741382382717849]
	TIME [epoch: 10.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.104218749688227		[learning rate: 0.0070081]
	Learning Rate: 0.00700809
	LOSS [training: 1.104218749688227 | validation: 1.5738822086288267]
	TIME [epoch: 10.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.274891843589672		[learning rate: 0.0069866]
	Learning Rate: 0.00698661
	LOSS [training: 1.274891843589672 | validation: 1.0581319279929864]
	TIME [epoch: 10.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1283843694550044		[learning rate: 0.0069652]
	Learning Rate: 0.0069652
	LOSS [training: 1.1283843694550044 | validation: 1.3203112334059726]
	TIME [epoch: 10.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3158088692818948		[learning rate: 0.0069438]
	Learning Rate: 0.00694384
	LOSS [training: 1.3158088692818948 | validation: 1.106617595948411]
	TIME [epoch: 10.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1929273545312		[learning rate: 0.0069226]
	Learning Rate: 0.00692256
	LOSS [training: 1.1929273545312 | validation: 1.5330618805078342]
	TIME [epoch: 10.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0372781066389265		[learning rate: 0.0069013]
	Learning Rate: 0.00690134
	LOSS [training: 1.0372781066389265 | validation: 0.9028470087822589]
	TIME [epoch: 10.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.155341293667742		[learning rate: 0.0068802]
	Learning Rate: 0.00688018
	LOSS [training: 1.155341293667742 | validation: 1.7570235339114681]
	TIME [epoch: 10.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2727318369741933		[learning rate: 0.0068591]
	Learning Rate: 0.00685909
	LOSS [training: 1.2727318369741933 | validation: 0.9089093796869829]
	TIME [epoch: 10.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.254893347837648		[learning rate: 0.0068381]
	Learning Rate: 0.00683807
	LOSS [training: 1.254893347837648 | validation: 1.7203970478407313]
	TIME [epoch: 10.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.231343218013813		[learning rate: 0.0068171]
	Learning Rate: 0.0068171
	LOSS [training: 1.231343218013813 | validation: 0.8389974574902592]
	TIME [epoch: 10.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1365711957204132		[learning rate: 0.0067962]
	Learning Rate: 0.00679621
	LOSS [training: 1.1365711957204132 | validation: 1.4492142731990116]
	TIME [epoch: 10.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2577301927580768		[learning rate: 0.0067754]
	Learning Rate: 0.00677537
	LOSS [training: 1.2577301927580768 | validation: 1.3966656331726077]
	TIME [epoch: 10.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2077639262269533		[learning rate: 0.0067546]
	Learning Rate: 0.00675461
	LOSS [training: 1.2077639262269533 | validation: 1.1350536144438799]
	TIME [epoch: 10.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1055422888865514		[learning rate: 0.0067339]
	Learning Rate: 0.0067339
	LOSS [training: 1.1055422888865514 | validation: 1.164304497477327]
	TIME [epoch: 10.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.704389393212658		[learning rate: 0.0067133]
	Learning Rate: 0.00671326
	LOSS [training: 3.704389393212658 | validation: 2.2691259748975363]
	TIME [epoch: 10.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7525485586271743		[learning rate: 0.0066927]
	Learning Rate: 0.00669268
	LOSS [training: 1.7525485586271743 | validation: 0.9405596605019045]
	TIME [epoch: 10.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6567098591272296		[learning rate: 0.0066722]
	Learning Rate: 0.00667216
	LOSS [training: 1.6567098591272296 | validation: 2.569500650462782]
	TIME [epoch: 10.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8575585507406167		[learning rate: 0.0066517]
	Learning Rate: 0.00665171
	LOSS [training: 1.8575585507406167 | validation: 1.5051336613373718]
	TIME [epoch: 10.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2581752930420165		[learning rate: 0.0066313]
	Learning Rate: 0.00663132
	LOSS [training: 1.2581752930420165 | validation: 2.589518454661188]
	TIME [epoch: 10.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8523245730000077		[learning rate: 0.006611]
	Learning Rate: 0.00661099
	LOSS [training: 1.8523245730000077 | validation: 2.212143463213367]
	TIME [epoch: 10.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2768351516596765		[learning rate: 0.0065907]
	Learning Rate: 0.00659073
	LOSS [training: 3.2768351516596765 | validation: 1.2599408708644213]
	TIME [epoch: 10.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6895350958676265		[learning rate: 0.0065705]
	Learning Rate: 0.00657052
	LOSS [training: 1.6895350958676265 | validation: 1.739520299146085]
	TIME [epoch: 10.5 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2193715074014886		[learning rate: 0.0065504]
	Learning Rate: 0.00655038
	LOSS [training: 2.2193715074014886 | validation: 3.544480777938821]
	TIME [epoch: 10.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.374338909752569		[learning rate: 0.0065303]
	Learning Rate: 0.0065303
	LOSS [training: 2.374338909752569 | validation: 1.3019841189474324]
	TIME [epoch: 10.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.489929805883143		[learning rate: 0.0065103]
	Learning Rate: 0.00651028
	LOSS [training: 1.489929805883143 | validation: 1.0725469288189564]
	TIME [epoch: 10.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9730802268756626		[learning rate: 0.0064903]
	Learning Rate: 0.00649033
	LOSS [training: 1.9730802268756626 | validation: 1.4925719394626302]
	TIME [epoch: 10.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.623755819681514		[learning rate: 0.0064704]
	Learning Rate: 0.00647043
	LOSS [training: 1.623755819681514 | validation: 1.305240086051803]
	TIME [epoch: 10.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3187064482193		[learning rate: 0.0064506]
	Learning Rate: 0.0064506
	LOSS [training: 1.3187064482193 | validation: 0.9552516559538651]
	TIME [epoch: 10.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3238793808874865		[learning rate: 0.0064308]
	Learning Rate: 0.00643082
	LOSS [training: 1.3238793808874865 | validation: 1.6693790405946238]
	TIME [epoch: 10.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.219109702345354		[learning rate: 0.0064111]
	Learning Rate: 0.00641111
	LOSS [training: 1.219109702345354 | validation: 0.9979623497440979]
	TIME [epoch: 10.5 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1383607220686731		[learning rate: 0.0063915]
	Learning Rate: 0.00639146
	LOSS [training: 1.1383607220686731 | validation: 0.8396226217342877]
	TIME [epoch: 10.5 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8822958795270847		[learning rate: 0.0063719]
	Learning Rate: 0.00637187
	LOSS [training: 0.8822958795270847 | validation: 0.6122047195144886]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2413712357951385		[learning rate: 0.0063523]
	Learning Rate: 0.00635233
	LOSS [training: 1.2413712357951385 | validation: 0.8926821724571934]
	TIME [epoch: 10.5 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9835877179745636		[learning rate: 0.0063329]
	Learning Rate: 0.00633286
	LOSS [training: 0.9835877179745636 | validation: 1.2160322544028483]
	TIME [epoch: 10.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0602134339907203		[learning rate: 0.0063134]
	Learning Rate: 0.00631345
	LOSS [training: 1.0602134339907203 | validation: 1.0916539056114432]
	TIME [epoch: 10.5 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9659335502633027		[learning rate: 0.0062941]
	Learning Rate: 0.0062941
	LOSS [training: 0.9659335502633027 | validation: 1.2004298914341627]
	TIME [epoch: 10.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8910018200186844		[learning rate: 0.0062748]
	Learning Rate: 0.0062748
	LOSS [training: 0.8910018200186844 | validation: 1.0093107795458218]
	TIME [epoch: 10.5 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8204617468738725		[learning rate: 0.0062556]
	Learning Rate: 0.00625557
	LOSS [training: 0.8204617468738725 | validation: 1.1219624737176739]
	TIME [epoch: 10.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2923841122996267		[learning rate: 0.0062364]
	Learning Rate: 0.00623639
	LOSS [training: 1.2923841122996267 | validation: 1.2115405127351133]
	TIME [epoch: 10.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1797076369888164		[learning rate: 0.0062173]
	Learning Rate: 0.00621727
	LOSS [training: 1.1797076369888164 | validation: 1.9497195158344314]
	TIME [epoch: 10.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.358797745880722		[learning rate: 0.0061982]
	Learning Rate: 0.00619822
	LOSS [training: 1.358797745880722 | validation: 1.239579080940037]
	TIME [epoch: 10.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2287264343803879		[learning rate: 0.0061792]
	Learning Rate: 0.00617922
	LOSS [training: 1.2287264343803879 | validation: 2.0273364859592893]
	TIME [epoch: 10.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1598812996179086		[learning rate: 0.0061603]
	Learning Rate: 0.00616027
	LOSS [training: 1.1598812996179086 | validation: 0.7157397895365276]
	TIME [epoch: 10.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7888877970022998		[learning rate: 0.0061414]
	Learning Rate: 0.00614139
	LOSS [training: 0.7888877970022998 | validation: 2.0958485007861922]
	TIME [epoch: 10.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1527563611084524		[learning rate: 0.0061226]
	Learning Rate: 0.00612256
	LOSS [training: 1.1527563611084524 | validation: 0.709296571251559]
	TIME [epoch: 10.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.88009265249555		[learning rate: 0.0061038]
	Learning Rate: 0.0061038
	LOSS [training: 0.88009265249555 | validation: 0.7069222650791922]
	TIME [epoch: 10.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2532716571267528		[learning rate: 0.0060851]
	Learning Rate: 0.00608508
	LOSS [training: 1.2532716571267528 | validation: 2.3104347900626614]
	TIME [epoch: 10.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0422463878688586		[learning rate: 0.0060664]
	Learning Rate: 0.00606643
	LOSS [training: 1.0422463878688586 | validation: 1.3439151940100755]
	TIME [epoch: 10.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0523997637010727		[learning rate: 0.0060478]
	Learning Rate: 0.00604784
	LOSS [training: 1.0523997637010727 | validation: 1.2035971829579415]
	TIME [epoch: 10.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8902910517344493		[learning rate: 0.0060293]
	Learning Rate: 0.0060293
	LOSS [training: 0.8902910517344493 | validation: 0.7050074369082404]
	TIME [epoch: 10.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0441300910559643		[learning rate: 0.0060108]
	Learning Rate: 0.00601081
	LOSS [training: 1.0441300910559643 | validation: 0.8323766878998017]
	TIME [epoch: 10.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8748112215208403		[learning rate: 0.0059924]
	Learning Rate: 0.00599239
	LOSS [training: 0.8748112215208403 | validation: 0.8748729193736078]
	TIME [epoch: 10.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8340214506072445		[learning rate: 0.005974]
	Learning Rate: 0.00597402
	LOSS [training: 0.8340214506072445 | validation: 1.132501434421416]
	TIME [epoch: 10.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0048860671544113		[learning rate: 0.0059557]
	Learning Rate: 0.00595571
	LOSS [training: 1.0048860671544113 | validation: 1.8042336620983246]
	TIME [epoch: 10.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0504357837936709		[learning rate: 0.0059375]
	Learning Rate: 0.00593745
	LOSS [training: 1.0504357837936709 | validation: 0.7618996314768377]
	TIME [epoch: 10.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9906968073990384		[learning rate: 0.0059192]
	Learning Rate: 0.00591925
	LOSS [training: 0.9906968073990384 | validation: 1.2883667387250852]
	TIME [epoch: 10.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1068625225929767		[learning rate: 0.0059011]
	Learning Rate: 0.0059011
	LOSS [training: 1.1068625225929767 | validation: 0.9676586199905629]
	TIME [epoch: 10.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0311067360940278		[learning rate: 0.005883]
	Learning Rate: 0.00588302
	LOSS [training: 1.0311067360940278 | validation: 1.212412747430314]
	TIME [epoch: 10.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9417691234411025		[learning rate: 0.005865]
	Learning Rate: 0.00586498
	LOSS [training: 0.9417691234411025 | validation: 2.192015534144657]
	TIME [epoch: 10.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.431667111583778		[learning rate: 0.005847]
	Learning Rate: 0.005847
	LOSS [training: 1.431667111583778 | validation: 0.8201258220799852]
	TIME [epoch: 10.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7555471919128642		[learning rate: 0.0058291]
	Learning Rate: 0.00582908
	LOSS [training: 0.7555471919128642 | validation: 0.8681507805217733]
	TIME [epoch: 10.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7365890932855064		[learning rate: 0.0058112]
	Learning Rate: 0.00581121
	LOSS [training: 0.7365890932855064 | validation: 1.0705983286665528]
	TIME [epoch: 10.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8070420933969198		[learning rate: 0.0057934]
	Learning Rate: 0.0057934
	LOSS [training: 0.8070420933969198 | validation: 0.8128535602505409]
	TIME [epoch: 10.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7610250068963431		[learning rate: 0.0057756]
	Learning Rate: 0.00577564
	LOSS [training: 0.7610250068963431 | validation: 1.2663743306508584]
	TIME [epoch: 10.5 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.778979739575734		[learning rate: 0.0057579]
	Learning Rate: 0.00575793
	LOSS [training: 0.778979739575734 | validation: 0.6666015770022299]
	TIME [epoch: 10.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6666454571139463		[learning rate: 0.0057403]
	Learning Rate: 0.00574028
	LOSS [training: 0.6666454571139463 | validation: 0.7799333510990266]
	TIME [epoch: 10.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7455983966539682		[learning rate: 0.0057227]
	Learning Rate: 0.00572269
	LOSS [training: 0.7455983966539682 | validation: 0.7114073781562994]
	TIME [epoch: 10.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7282983425305171		[learning rate: 0.0057051]
	Learning Rate: 0.00570514
	LOSS [training: 0.7282983425305171 | validation: 0.6413950036857894]
	TIME [epoch: 10.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7100284494158677		[learning rate: 0.0056877]
	Learning Rate: 0.00568766
	LOSS [training: 0.7100284494158677 | validation: 0.5848599327827384]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9180030084957214		[learning rate: 0.0056702]
	Learning Rate: 0.00567022
	LOSS [training: 0.9180030084957214 | validation: 0.6370806620945605]
	TIME [epoch: 10.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9651920339804582		[learning rate: 0.0056528]
	Learning Rate: 0.00565284
	LOSS [training: 0.9651920339804582 | validation: 0.9339186864427669]
	TIME [epoch: 10.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.097502645117626		[learning rate: 0.0056355]
	Learning Rate: 0.00563551
	LOSS [training: 1.097502645117626 | validation: 1.1740666232575694]
	TIME [epoch: 10.5 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7657941711381948		[learning rate: 0.0056182]
	Learning Rate: 0.00561824
	LOSS [training: 0.7657941711381948 | validation: 0.819223630822635]
	TIME [epoch: 10.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6540209566216035		[learning rate: 0.005601]
	Learning Rate: 0.00560101
	LOSS [training: 0.6540209566216035 | validation: 1.1886542414888923]
	TIME [epoch: 10.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9272608441558651		[learning rate: 0.0055838]
	Learning Rate: 0.00558384
	LOSS [training: 0.9272608441558651 | validation: 0.8153059497177131]
	TIME [epoch: 10.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6127023387077728		[learning rate: 0.0055667]
	Learning Rate: 0.00556673
	LOSS [training: 0.6127023387077728 | validation: 0.649544000624775]
	TIME [epoch: 10.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8578280498882522		[learning rate: 0.0055497]
	Learning Rate: 0.00554966
	LOSS [training: 0.8578280498882522 | validation: 2.034183256858007]
	TIME [epoch: 10.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.362622976573602		[learning rate: 0.0055327]
	Learning Rate: 0.00553265
	LOSS [training: 1.362622976573602 | validation: 0.9018803942862249]
	TIME [epoch: 10.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8150598564773578		[learning rate: 0.0055157]
	Learning Rate: 0.00551569
	LOSS [training: 0.8150598564773578 | validation: 1.1651360484875917]
	TIME [epoch: 10.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9612918021499153		[learning rate: 0.0054988]
	Learning Rate: 0.00549878
	LOSS [training: 0.9612918021499153 | validation: 0.9337518773228383]
	TIME [epoch: 10.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8048287557238878		[learning rate: 0.0054819]
	Learning Rate: 0.00548193
	LOSS [training: 0.8048287557238878 | validation: 1.100937928224879]
	TIME [epoch: 10.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7068706014506155		[learning rate: 0.0054651]
	Learning Rate: 0.00546512
	LOSS [training: 0.7068706014506155 | validation: 1.462187133752527]
	TIME [epoch: 10.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9001749614880593		[learning rate: 0.0054484]
	Learning Rate: 0.00544837
	LOSS [training: 0.9001749614880593 | validation: 0.6561190161882954]
	TIME [epoch: 10.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7656418140856758		[learning rate: 0.0054317]
	Learning Rate: 0.00543167
	LOSS [training: 0.7656418140856758 | validation: 1.0744305735984936]
	TIME [epoch: 10.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7696847377519075		[learning rate: 0.005415]
	Learning Rate: 0.00541502
	LOSS [training: 0.7696847377519075 | validation: 0.9933364657278404]
	TIME [epoch: 10.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7658912010259947		[learning rate: 0.0053984]
	Learning Rate: 0.00539842
	LOSS [training: 0.7658912010259947 | validation: 1.6326847059628438]
	TIME [epoch: 10.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0098199926982532		[learning rate: 0.0053819]
	Learning Rate: 0.00538187
	LOSS [training: 1.0098199926982532 | validation: 0.8423717336280315]
	TIME [epoch: 10.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8633262690539307		[learning rate: 0.0053654]
	Learning Rate: 0.00536537
	LOSS [training: 0.8633262690539307 | validation: 0.9217890086235707]
	TIME [epoch: 10.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.184406961677837		[learning rate: 0.0053489]
	Learning Rate: 0.00534893
	LOSS [training: 1.184406961677837 | validation: 0.9008759939118894]
	TIME [epoch: 10.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8240416232831818		[learning rate: 0.0053325]
	Learning Rate: 0.00533253
	LOSS [training: 0.8240416232831818 | validation: 1.2218404420270745]
	TIME [epoch: 10.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8620755738815818		[learning rate: 0.0053162]
	Learning Rate: 0.00531618
	LOSS [training: 0.8620755738815818 | validation: 0.7888355842922454]
	TIME [epoch: 10.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7655428556065881		[learning rate: 0.0052999]
	Learning Rate: 0.00529989
	LOSS [training: 0.7655428556065881 | validation: 0.7426929809484539]
	TIME [epoch: 10.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8199365683997273		[learning rate: 0.0052836]
	Learning Rate: 0.00528364
	LOSS [training: 0.8199365683997273 | validation: 1.5176509362164865]
	TIME [epoch: 10.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0967858715361842		[learning rate: 0.0052674]
	Learning Rate: 0.00526744
	LOSS [training: 1.0967858715361842 | validation: 0.9385593224247117]
	TIME [epoch: 10.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8947049789822694		[learning rate: 0.0052513]
	Learning Rate: 0.0052513
	LOSS [training: 0.8947049789822694 | validation: 1.1448277230429893]
	TIME [epoch: 10.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8764517380064655		[learning rate: 0.0052352]
	Learning Rate: 0.0052352
	LOSS [training: 0.8764517380064655 | validation: 0.8127006827996044]
	TIME [epoch: 10.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7843263646339866		[learning rate: 0.0052192]
	Learning Rate: 0.00521915
	LOSS [training: 0.7843263646339866 | validation: 0.8084669788019445]
	TIME [epoch: 10.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7197732744380094		[learning rate: 0.0052032]
	Learning Rate: 0.00520315
	LOSS [training: 0.7197732744380094 | validation: 0.830101453618901]
	TIME [epoch: 10.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0463519976254636		[learning rate: 0.0051872]
	Learning Rate: 0.0051872
	LOSS [training: 1.0463519976254636 | validation: 1.12540490843348]
	TIME [epoch: 10.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0083877118289972		[learning rate: 0.0051713]
	Learning Rate: 0.0051713
	LOSS [training: 1.0083877118289972 | validation: 0.8964573482744459]
	TIME [epoch: 10.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8394336364549811		[learning rate: 0.0051555]
	Learning Rate: 0.00515545
	LOSS [training: 0.8394336364549811 | validation: 1.0548522689733895]
	TIME [epoch: 10.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.973173264867159		[learning rate: 0.0051396]
	Learning Rate: 0.00513965
	LOSS [training: 0.973173264867159 | validation: 1.4327826856901098]
	TIME [epoch: 10.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8305253050997905		[learning rate: 0.0051239]
	Learning Rate: 0.00512389
	LOSS [training: 0.8305253050997905 | validation: 0.8587270252305524]
	TIME [epoch: 10.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7436912649254033		[learning rate: 0.0051082]
	Learning Rate: 0.00510819
	LOSS [training: 0.7436912649254033 | validation: 1.460610530316085]
	TIME [epoch: 10.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8982628441386276		[learning rate: 0.0050925]
	Learning Rate: 0.00509253
	LOSS [training: 0.8982628441386276 | validation: 0.7470199731556377]
	TIME [epoch: 10.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7214335190208369		[learning rate: 0.0050769]
	Learning Rate: 0.00507692
	LOSS [training: 0.7214335190208369 | validation: 0.5741927626713076]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5412634699057929		[learning rate: 0.0050614]
	Learning Rate: 0.00506135
	LOSS [training: 0.5412634699057929 | validation: 0.9076062256302706]
	TIME [epoch: 10.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8304492124201289		[learning rate: 0.0050458]
	Learning Rate: 0.00504584
	LOSS [training: 0.8304492124201289 | validation: 0.9278137111648178]
	TIME [epoch: 10.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.759406592485593		[learning rate: 0.0050304]
	Learning Rate: 0.00503037
	LOSS [training: 0.759406592485593 | validation: 0.8243279426763599]
	TIME [epoch: 10.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7111732508421964		[learning rate: 0.005015]
	Learning Rate: 0.00501495
	LOSS [training: 0.7111732508421964 | validation: 0.9644372109621068]
	TIME [epoch: 10.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976948987716642		[learning rate: 0.0049996]
	Learning Rate: 0.00499958
	LOSS [training: 0.6976948987716642 | validation: 0.7207192553282704]
	TIME [epoch: 10.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6758261287609872		[learning rate: 0.0049843]
	Learning Rate: 0.00498425
	LOSS [training: 0.6758261287609872 | validation: 0.9171334528933487]
	TIME [epoch: 10.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8463587779008627		[learning rate: 0.004969]
	Learning Rate: 0.00496897
	LOSS [training: 0.8463587779008627 | validation: 1.0434865054011997]
	TIME [epoch: 10.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1888355088918285		[learning rate: 0.0049537]
	Learning Rate: 0.00495374
	LOSS [training: 1.1888355088918285 | validation: 1.304547837850139]
	TIME [epoch: 10.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9234160281257295		[learning rate: 0.0049386]
	Learning Rate: 0.00493856
	LOSS [training: 0.9234160281257295 | validation: 0.5579925943727596]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7128011649707396		[learning rate: 0.0049234]
	Learning Rate: 0.00492342
	LOSS [training: 0.7128011649707396 | validation: 0.627764599620603]
	TIME [epoch: 10.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7999282628556179		[learning rate: 0.0049083]
	Learning Rate: 0.00490832
	LOSS [training: 0.7999282628556179 | validation: 0.6081180607005467]
	TIME [epoch: 10.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5818095744900269		[learning rate: 0.0048933]
	Learning Rate: 0.00489328
	LOSS [training: 0.5818095744900269 | validation: 0.7084657893571696]
	TIME [epoch: 10.6 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7554200540059777		[learning rate: 0.0048783]
	Learning Rate: 0.00487828
	LOSS [training: 0.7554200540059777 | validation: 0.8853559102881512]
	TIME [epoch: 10.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8471763152106122		[learning rate: 0.0048633]
	Learning Rate: 0.00486333
	LOSS [training: 0.8471763152106122 | validation: 0.7703925926611512]
	TIME [epoch: 10.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0599024146136227		[learning rate: 0.0048484]
	Learning Rate: 0.00484842
	LOSS [training: 1.0599024146136227 | validation: 0.49314741208535096]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7037581851640526		[learning rate: 0.0048336]
	Learning Rate: 0.00483355
	LOSS [training: 0.7037581851640526 | validation: 1.22953067422287]
	TIME [epoch: 10.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9699552016323828		[learning rate: 0.0048187]
	Learning Rate: 0.00481874
	LOSS [training: 0.9699552016323828 | validation: 1.0345699468829161]
	TIME [epoch: 10.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9394731245303701		[learning rate: 0.004804]
	Learning Rate: 0.00480397
	LOSS [training: 0.9394731245303701 | validation: 0.6313186802012534]
	TIME [epoch: 10.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7998822144864712		[learning rate: 0.0047892]
	Learning Rate: 0.00478924
	LOSS [training: 0.7998822144864712 | validation: 0.6410175494544678]
	TIME [epoch: 10.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7725276389566732		[learning rate: 0.0047746]
	Learning Rate: 0.00477456
	LOSS [training: 0.7725276389566732 | validation: 0.987596894866326]
	TIME [epoch: 10.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9433824802967206		[learning rate: 0.0047599]
	Learning Rate: 0.00475992
	LOSS [training: 0.9433824802967206 | validation: 0.7984668648190412]
	TIME [epoch: 10.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7391431771737769		[learning rate: 0.0047453]
	Learning Rate: 0.00474533
	LOSS [training: 0.7391431771737769 | validation: 1.019825489225851]
	TIME [epoch: 10.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.846169163616673		[learning rate: 0.0047308]
	Learning Rate: 0.00473079
	LOSS [training: 0.846169163616673 | validation: 0.6835709830317112]
	TIME [epoch: 10.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5725773101929617		[learning rate: 0.0047163]
	Learning Rate: 0.00471628
	LOSS [training: 0.5725773101929617 | validation: 0.49076557302249996]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6737051722633292		[learning rate: 0.0047018]
	Learning Rate: 0.00470183
	LOSS [training: 0.6737051722633292 | validation: 0.6035109758404791]
	TIME [epoch: 10.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5474090603481537		[learning rate: 0.0046874]
	Learning Rate: 0.00468741
	LOSS [training: 0.5474090603481537 | validation: 0.9775378920890418]
	TIME [epoch: 10.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7598999854050185		[learning rate: 0.004673]
	Learning Rate: 0.00467305
	LOSS [training: 0.7598999854050185 | validation: 1.0098434806781655]
	TIME [epoch: 10.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0972351548529231		[learning rate: 0.0046587]
	Learning Rate: 0.00465872
	LOSS [training: 1.0972351548529231 | validation: 2.8428824750531554]
	TIME [epoch: 10.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2709468820851517		[learning rate: 0.0046444]
	Learning Rate: 0.00464444
	LOSS [training: 1.2709468820851517 | validation: 0.8330439792086008]
	TIME [epoch: 10.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7358912260301588		[learning rate: 0.0046302]
	Learning Rate: 0.0046302
	LOSS [training: 0.7358912260301588 | validation: 0.9011323590082836]
	TIME [epoch: 10.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6132432027505648		[learning rate: 0.004616]
	Learning Rate: 0.00461601
	LOSS [training: 0.6132432027505648 | validation: 1.3059133511342378]
	TIME [epoch: 10.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8485723079499469		[learning rate: 0.0046019]
	Learning Rate: 0.00460186
	LOSS [training: 0.8485723079499469 | validation: 0.8357011388561466]
	TIME [epoch: 10.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5679547533736343		[learning rate: 0.0045878]
	Learning Rate: 0.00458775
	LOSS [training: 0.5679547533736343 | validation: 0.9810457793279082]
	TIME [epoch: 10.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6689458527570998		[learning rate: 0.0045737]
	Learning Rate: 0.00457369
	LOSS [training: 0.6689458527570998 | validation: 0.5692362955595586]
	TIME [epoch: 10.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9019800158557754		[learning rate: 0.0045597]
	Learning Rate: 0.00455967
	LOSS [training: 0.9019800158557754 | validation: 0.9056815731327547]
	TIME [epoch: 10.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6551219446456658		[learning rate: 0.0045457]
	Learning Rate: 0.00454569
	LOSS [training: 0.6551219446456658 | validation: 0.5813301132106143]
	TIME [epoch: 10.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6400590198913912		[learning rate: 0.0045318]
	Learning Rate: 0.00453176
	LOSS [training: 0.6400590198913912 | validation: 0.6852045705968507]
	TIME [epoch: 10.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6552492461875105		[learning rate: 0.0045179]
	Learning Rate: 0.00451787
	LOSS [training: 0.6552492461875105 | validation: 0.7743872310116982]
	TIME [epoch: 10.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.823079987020089		[learning rate: 0.004504]
	Learning Rate: 0.00450402
	LOSS [training: 0.823079987020089 | validation: 0.758824662691917]
	TIME [epoch: 10.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.991015372990101		[learning rate: 0.0044902]
	Learning Rate: 0.00449021
	LOSS [training: 0.991015372990101 | validation: 1.1681819881660358]
	TIME [epoch: 10.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0432980974503554		[learning rate: 0.0044764]
	Learning Rate: 0.00447645
	LOSS [training: 1.0432980974503554 | validation: 0.8267893184996322]
	TIME [epoch: 10.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0839903775607724		[learning rate: 0.0044627]
	Learning Rate: 0.00446272
	LOSS [training: 1.0839903775607724 | validation: 0.9828813795488752]
	TIME [epoch: 10.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.788904706250073		[learning rate: 0.004449]
	Learning Rate: 0.00444904
	LOSS [training: 0.788904706250073 | validation: 0.7282818130844774]
	TIME [epoch: 10.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.647218674403925		[learning rate: 0.0044354]
	Learning Rate: 0.0044354
	LOSS [training: 0.647218674403925 | validation: 0.6135990952985041]
	TIME [epoch: 10.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7160273605879854		[learning rate: 0.0044218]
	Learning Rate: 0.00442181
	LOSS [training: 0.7160273605879854 | validation: 0.755941546717115]
	TIME [epoch: 10.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7760326230669576		[learning rate: 0.0044083]
	Learning Rate: 0.00440825
	LOSS [training: 0.7760326230669576 | validation: 1.8665317513346316]
	TIME [epoch: 10.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0476712452149093		[learning rate: 0.0043947]
	Learning Rate: 0.00439474
	LOSS [training: 1.0476712452149093 | validation: 0.7076903781022718]
	TIME [epoch: 10.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.678055832601543		[learning rate: 0.0043813]
	Learning Rate: 0.00438127
	LOSS [training: 0.678055832601543 | validation: 0.7125236823056781]
	TIME [epoch: 10.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7102780044799888		[learning rate: 0.0043678]
	Learning Rate: 0.00436784
	LOSS [training: 0.7102780044799888 | validation: 0.6797396901341398]
	TIME [epoch: 10.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7051675127794798		[learning rate: 0.0043544]
	Learning Rate: 0.00435445
	LOSS [training: 0.7051675127794798 | validation: 0.7668447211809338]
	TIME [epoch: 10.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8226303205875724		[learning rate: 0.0043411]
	Learning Rate: 0.0043411
	LOSS [training: 0.8226303205875724 | validation: 0.6722370787247127]
	TIME [epoch: 10.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7792693161529393		[learning rate: 0.0043278]
	Learning Rate: 0.0043278
	LOSS [training: 0.7792693161529393 | validation: 0.9970119458353923]
	TIME [epoch: 10.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7329434635051264		[learning rate: 0.0043145]
	Learning Rate: 0.00431453
	LOSS [training: 0.7329434635051264 | validation: 0.6084117208158287]
	TIME [epoch: 10.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7420522854179838		[learning rate: 0.0043013]
	Learning Rate: 0.0043013
	LOSS [training: 0.7420522854179838 | validation: 0.9702704243282543]
	TIME [epoch: 10.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0624977622622065		[learning rate: 0.0042881]
	Learning Rate: 0.00428812
	LOSS [training: 1.0624977622622065 | validation: 2.1988870613955127]
	TIME [epoch: 10.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.325393329496397		[learning rate: 0.004275]
	Learning Rate: 0.00427497
	LOSS [training: 1.325393329496397 | validation: 0.7617224238376626]
	TIME [epoch: 10.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6659122614813919		[learning rate: 0.0042619]
	Learning Rate: 0.00426187
	LOSS [training: 0.6659122614813919 | validation: 0.5766205723458112]
	TIME [epoch: 10.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6967569271640732		[learning rate: 0.0042488]
	Learning Rate: 0.0042488
	LOSS [training: 0.6967569271640732 | validation: 1.1053846576926338]
	TIME [epoch: 10.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8988960622729291		[learning rate: 0.0042358]
	Learning Rate: 0.00423578
	LOSS [training: 0.8988960622729291 | validation: 1.2154518726186498]
	TIME [epoch: 10.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908009356867757		[learning rate: 0.0042228]
	Learning Rate: 0.00422279
	LOSS [training: 0.6908009356867757 | validation: 1.62158737200832]
	TIME [epoch: 10.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8355612604346417		[learning rate: 0.0042098]
	Learning Rate: 0.00420985
	LOSS [training: 0.8355612604346417 | validation: 0.7890553480428532]
	TIME [epoch: 10.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4561126696963872		[learning rate: 0.0041969]
	Learning Rate: 0.00419695
	LOSS [training: 0.4561126696963872 | validation: 0.9626276252249182]
	TIME [epoch: 10.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6903443330180784		[learning rate: 0.0041841]
	Learning Rate: 0.00418408
	LOSS [training: 0.6903443330180784 | validation: 0.7916515650018123]
	TIME [epoch: 10.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.879367394097472		[learning rate: 0.0041713]
	Learning Rate: 0.00417125
	LOSS [training: 0.879367394097472 | validation: 0.8854500010569794]
	TIME [epoch: 10.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9872774503546564		[learning rate: 0.0041585]
	Learning Rate: 0.00415847
	LOSS [training: 0.9872774503546564 | validation: 1.8054564905647719]
	TIME [epoch: 10.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8362414168250035		[learning rate: 0.0041457]
	Learning Rate: 0.00414572
	LOSS [training: 0.8362414168250035 | validation: 0.9216570715380769]
	TIME [epoch: 10.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6353343439525376		[learning rate: 0.004133]
	Learning Rate: 0.00413301
	LOSS [training: 0.6353343439525376 | validation: 0.5522290881031248]
	TIME [epoch: 10.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7130597689547115		[learning rate: 0.0041203]
	Learning Rate: 0.00412034
	LOSS [training: 0.7130597689547115 | validation: 1.0913935659413365]
	TIME [epoch: 10.6 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8480725111946606		[learning rate: 0.0041077]
	Learning Rate: 0.00410771
	LOSS [training: 0.8480725111946606 | validation: 0.9845559448495541]
	TIME [epoch: 10.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6373259174929276		[learning rate: 0.0040951]
	Learning Rate: 0.00409512
	LOSS [training: 0.6373259174929276 | validation: 0.6382681372719494]
	TIME [epoch: 10.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7209089341294405		[learning rate: 0.0040826]
	Learning Rate: 0.00408257
	LOSS [training: 0.7209089341294405 | validation: 0.5744715735667523]
	TIME [epoch: 10.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.714355417934151		[learning rate: 0.0040701]
	Learning Rate: 0.00407005
	LOSS [training: 0.714355417934151 | validation: 0.7746018448764612]
	TIME [epoch: 10.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5592041056362616		[learning rate: 0.0040576]
	Learning Rate: 0.00405758
	LOSS [training: 0.5592041056362616 | validation: 0.7494539784359262]
	TIME [epoch: 10.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5638599084855302		[learning rate: 0.0040451]
	Learning Rate: 0.00404514
	LOSS [training: 0.5638599084855302 | validation: 1.6942824392757105]
	TIME [epoch: 10.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7283963991689242		[learning rate: 0.0040327]
	Learning Rate: 0.00403274
	LOSS [training: 1.7283963991689242 | validation: 1.0148588167522437]
	TIME [epoch: 10.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8927027588030064		[learning rate: 0.0040204]
	Learning Rate: 0.00402038
	LOSS [training: 0.8927027588030064 | validation: 0.9121872106756449]
	TIME [epoch: 10.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6835408774411073		[learning rate: 0.0040081]
	Learning Rate: 0.00400805
	LOSS [training: 0.6835408774411073 | validation: 0.6398537604994913]
	TIME [epoch: 10.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3232563529420436		[learning rate: 0.0039958]
	Learning Rate: 0.00399577
	LOSS [training: 1.3232563529420436 | validation: 0.5728803474892269]
	TIME [epoch: 10.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8478956199625868		[learning rate: 0.0039835]
	Learning Rate: 0.00398352
	LOSS [training: 0.8478956199625868 | validation: 0.6282484480867913]
	TIME [epoch: 10.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7599923469027183		[learning rate: 0.0039713]
	Learning Rate: 0.00397131
	LOSS [training: 0.7599923469027183 | validation: 0.6589410109630534]
	TIME [epoch: 10.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5941271890218707		[learning rate: 0.0039591]
	Learning Rate: 0.00395913
	LOSS [training: 0.5941271890218707 | validation: 0.7387918144028512]
	TIME [epoch: 10.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691917114777541		[learning rate: 0.003947]
	Learning Rate: 0.003947
	LOSS [training: 0.691917114777541 | validation: 0.5399663869351263]
	TIME [epoch: 10.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5953930334429789		[learning rate: 0.0039349]
	Learning Rate: 0.0039349
	LOSS [training: 0.5953930334429789 | validation: 0.552694161402339]
	TIME [epoch: 10.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5276920981870797		[learning rate: 0.0039228]
	Learning Rate: 0.00392283
	LOSS [training: 0.5276920981870797 | validation: 0.7851318160446272]
	TIME [epoch: 10.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7838648140776708		[learning rate: 0.0039108]
	Learning Rate: 0.00391081
	LOSS [training: 0.7838648140776708 | validation: 1.3146533220992385]
	TIME [epoch: 10.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8419478573103663		[learning rate: 0.0038988]
	Learning Rate: 0.00389882
	LOSS [training: 0.8419478573103663 | validation: 0.8248052091780252]
	TIME [epoch: 10.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6102462163182693		[learning rate: 0.0038869]
	Learning Rate: 0.00388687
	LOSS [training: 0.6102462163182693 | validation: 0.523004983009749]
	TIME [epoch: 10.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6643934892836874		[learning rate: 0.003875]
	Learning Rate: 0.00387495
	LOSS [training: 0.6643934892836874 | validation: 0.6742612308508728]
	TIME [epoch: 10.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6149326336771997		[learning rate: 0.0038631]
	Learning Rate: 0.00386308
	LOSS [training: 0.6149326336771997 | validation: 0.7434332199769206]
	TIME [epoch: 10.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.620252364557479		[learning rate: 0.0038512]
	Learning Rate: 0.00385123
	LOSS [training: 0.620252364557479 | validation: 0.6825935825616203]
	TIME [epoch: 10.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.591121891636605		[learning rate: 0.0038394]
	Learning Rate: 0.00383943
	LOSS [training: 0.591121891636605 | validation: 0.5003134456483413]
	TIME [epoch: 10.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7111544881114253		[learning rate: 0.0038277]
	Learning Rate: 0.00382766
	LOSS [training: 0.7111544881114253 | validation: 0.46125477802551196]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6037660028091448		[learning rate: 0.0038159]
	Learning Rate: 0.00381593
	LOSS [training: 0.6037660028091448 | validation: 0.6340140559613456]
	TIME [epoch: 10.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6410021670904438		[learning rate: 0.0038042]
	Learning Rate: 0.00380423
	LOSS [training: 0.6410021670904438 | validation: 1.2601335428000389]
	TIME [epoch: 10.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7215961230411858		[learning rate: 0.0037926]
	Learning Rate: 0.00379257
	LOSS [training: 0.7215961230411858 | validation: 2.4276191684513]
	TIME [epoch: 10.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5787835083559285		[learning rate: 0.0037809]
	Learning Rate: 0.00378094
	LOSS [training: 1.5787835083559285 | validation: 0.45702287743581366]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5762085684571604		[learning rate: 0.0037694]
	Learning Rate: 0.00376935
	LOSS [training: 0.5762085684571604 | validation: 1.0918326038348833]
	TIME [epoch: 10.5 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8120275155404159		[learning rate: 0.0037578]
	Learning Rate: 0.0037578
	LOSS [training: 0.8120275155404159 | validation: 0.4235745063954072]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_819.pth
	Model improved!!!
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6021313659021275		[learning rate: 0.0037463]
	Learning Rate: 0.00374628
	LOSS [training: 0.6021313659021275 | validation: 0.573780015574356]
	TIME [epoch: 10.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5838575792200981		[learning rate: 0.0037348]
	Learning Rate: 0.00373479
	LOSS [training: 0.5838575792200981 | validation: 0.9445774919001149]
	TIME [epoch: 10.5 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942069690892867		[learning rate: 0.0037233]
	Learning Rate: 0.00372335
	LOSS [training: 0.6942069690892867 | validation: 1.1062372348327827]
	TIME [epoch: 10.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6264480181347154		[learning rate: 0.0037119]
	Learning Rate: 0.00371193
	LOSS [training: 0.6264480181347154 | validation: 1.0487710789472042]
	TIME [epoch: 10.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7356919846490453		[learning rate: 0.0037006]
	Learning Rate: 0.00370055
	LOSS [training: 0.7356919846490453 | validation: 0.7804752920857638]
	TIME [epoch: 10.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.548993776996167		[learning rate: 0.0036892]
	Learning Rate: 0.00368921
	LOSS [training: 0.548993776996167 | validation: 0.8720032276795084]
	TIME [epoch: 10.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49855061234832554		[learning rate: 0.0036779]
	Learning Rate: 0.0036779
	LOSS [training: 0.49855061234832554 | validation: 0.5090772486834004]
	TIME [epoch: 10.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5518645203327757		[learning rate: 0.0036666]
	Learning Rate: 0.00366663
	LOSS [training: 0.5518645203327757 | validation: 0.4813556452625617]
	TIME [epoch: 10.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4841229706740536		[learning rate: 0.0036554]
	Learning Rate: 0.00365539
	LOSS [training: 0.4841229706740536 | validation: 0.4944986942727069]
	TIME [epoch: 10.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7049245979055874		[learning rate: 0.0036442]
	Learning Rate: 0.00364418
	LOSS [training: 0.7049245979055874 | validation: 0.7350539243050835]
	TIME [epoch: 10.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7008236677080418		[learning rate: 0.003633]
	Learning Rate: 0.00363301
	LOSS [training: 0.7008236677080418 | validation: 0.4967922844851718]
	TIME [epoch: 10.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5341243515323659		[learning rate: 0.0036219]
	Learning Rate: 0.00362187
	LOSS [training: 0.5341243515323659 | validation: 0.40625877700281393]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.451333802578449		[learning rate: 0.0036108]
	Learning Rate: 0.00361077
	LOSS [training: 0.451333802578449 | validation: 0.8219893206577706]
	TIME [epoch: 10.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5074724532805563		[learning rate: 0.0035997]
	Learning Rate: 0.0035997
	LOSS [training: 0.5074724532805563 | validation: 0.3638269559444531]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_833.pth
	Model improved!!!
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4150384375947061		[learning rate: 0.0035887]
	Learning Rate: 0.00358867
	LOSS [training: 0.4150384375947061 | validation: 0.6754893860257022]
	TIME [epoch: 10.6 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7581502843032397		[learning rate: 0.0035777]
	Learning Rate: 0.00357767
	LOSS [training: 0.7581502843032397 | validation: 0.39410333145453535]
	TIME [epoch: 10.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5747971477573646		[learning rate: 0.0035667]
	Learning Rate: 0.0035667
	LOSS [training: 0.5747971477573646 | validation: 1.1084296881077629]
	TIME [epoch: 10.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7145456859661916		[learning rate: 0.0035558]
	Learning Rate: 0.00355577
	LOSS [training: 0.7145456859661916 | validation: 0.8342475533520001]
	TIME [epoch: 10.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7416413562503056		[learning rate: 0.0035449]
	Learning Rate: 0.00354487
	LOSS [training: 0.7416413562503056 | validation: 0.7603016307724844]
	TIME [epoch: 10.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6078394507067089		[learning rate: 0.003534]
	Learning Rate: 0.003534
	LOSS [training: 0.6078394507067089 | validation: 0.7017667408086963]
	TIME [epoch: 10.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7507582566039426		[learning rate: 0.0035232]
	Learning Rate: 0.00352317
	LOSS [training: 0.7507582566039426 | validation: 1.0416082122058061]
	TIME [epoch: 10.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8708507336705346		[learning rate: 0.0035124]
	Learning Rate: 0.00351237
	LOSS [training: 0.8708507336705346 | validation: 0.4765471359763289]
	TIME [epoch: 10.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.066138748926793		[learning rate: 0.0035016]
	Learning Rate: 0.0035016
	LOSS [training: 1.066138748926793 | validation: 0.7039024428856229]
	TIME [epoch: 10.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.257791312262639		[learning rate: 0.0034909]
	Learning Rate: 0.00349087
	LOSS [training: 1.257791312262639 | validation: 0.7755659652000756]
	TIME [epoch: 10.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.21839729141034		[learning rate: 0.0034802]
	Learning Rate: 0.00348017
	LOSS [training: 1.21839729141034 | validation: 0.6702383754576983]
	TIME [epoch: 10.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7043041033455153		[learning rate: 0.0034695]
	Learning Rate: 0.0034695
	LOSS [training: 0.7043041033455153 | validation: 0.782585852044148]
	TIME [epoch: 10.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.67516302569835		[learning rate: 0.0034589]
	Learning Rate: 0.00345886
	LOSS [training: 0.67516302569835 | validation: 0.5125891357414583]
	TIME [epoch: 10.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6659587717092367		[learning rate: 0.0034483]
	Learning Rate: 0.00344826
	LOSS [training: 0.6659587717092367 | validation: 1.2668354943138802]
	TIME [epoch: 10.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8112297976477072		[learning rate: 0.0034377]
	Learning Rate: 0.00343769
	LOSS [training: 0.8112297976477072 | validation: 0.6314914404872408]
	TIME [epoch: 10.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5302006931344106		[learning rate: 0.0034272]
	Learning Rate: 0.00342715
	LOSS [training: 0.5302006931344106 | validation: 0.5216994874689089]
	TIME [epoch: 10.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5782410814995648		[learning rate: 0.0034166]
	Learning Rate: 0.00341665
	LOSS [training: 0.5782410814995648 | validation: 0.5033794073553687]
	TIME [epoch: 10.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.570220610887615		[learning rate: 0.0034062]
	Learning Rate: 0.00340617
	LOSS [training: 0.570220610887615 | validation: 1.7454263502103868]
	TIME [epoch: 10.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.022746956570863		[learning rate: 0.0033957]
	Learning Rate: 0.00339573
	LOSS [training: 1.022746956570863 | validation: 0.48447175112878]
	TIME [epoch: 10.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.632023709513744		[learning rate: 0.0033853]
	Learning Rate: 0.00338532
	LOSS [training: 0.632023709513744 | validation: 0.5163037529012668]
	TIME [epoch: 10.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6081580542562924		[learning rate: 0.0033749]
	Learning Rate: 0.00337494
	LOSS [training: 0.6081580542562924 | validation: 0.46240219223992257]
	TIME [epoch: 10.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8214939935850714		[learning rate: 0.0033646]
	Learning Rate: 0.0033646
	LOSS [training: 0.8214939935850714 | validation: 0.6428126316749682]
	TIME [epoch: 10.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6146575427997233		[learning rate: 0.0033543]
	Learning Rate: 0.00335428
	LOSS [training: 0.6146575427997233 | validation: 0.6647366870355715]
	TIME [epoch: 10.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6431700948815564		[learning rate: 0.003344]
	Learning Rate: 0.003344
	LOSS [training: 0.6431700948815564 | validation: 0.5543596774256463]
	TIME [epoch: 10.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4145393052641566		[learning rate: 0.0033338]
	Learning Rate: 0.00333375
	LOSS [training: 0.4145393052641566 | validation: 0.48406553799796626]
	TIME [epoch: 10.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5014533001252739		[learning rate: 0.0033235]
	Learning Rate: 0.00332353
	LOSS [training: 0.5014533001252739 | validation: 0.5924006074160569]
	TIME [epoch: 10.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.507459996614054		[learning rate: 0.0033133]
	Learning Rate: 0.00331334
	LOSS [training: 0.507459996614054 | validation: 0.8134211843747845]
	TIME [epoch: 10.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5412748921198045		[learning rate: 0.0033032]
	Learning Rate: 0.00330319
	LOSS [training: 0.5412748921198045 | validation: 0.6922043138487711]
	TIME [epoch: 10.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5401239229621237		[learning rate: 0.0032931]
	Learning Rate: 0.00329306
	LOSS [training: 0.5401239229621237 | validation: 1.3836315522335025]
	TIME [epoch: 10.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8680063644943161		[learning rate: 0.003283]
	Learning Rate: 0.00328297
	LOSS [training: 0.8680063644943161 | validation: 0.4785280774191177]
	TIME [epoch: 10.6 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7266900471287385		[learning rate: 0.0032729]
	Learning Rate: 0.0032729
	LOSS [training: 0.7266900471287385 | validation: 0.7841680534870977]
	TIME [epoch: 10.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7422479119822729		[learning rate: 0.0032629]
	Learning Rate: 0.00326287
	LOSS [training: 0.7422479119822729 | validation: 0.7320225683728397]
	TIME [epoch: 10.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49079927759129066		[learning rate: 0.0032529]
	Learning Rate: 0.00325287
	LOSS [training: 0.49079927759129066 | validation: 1.1412691606404262]
	TIME [epoch: 10.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9092818184025507		[learning rate: 0.0032429]
	Learning Rate: 0.0032429
	LOSS [training: 0.9092818184025507 | validation: 0.8657924760801353]
	TIME [epoch: 10.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9829103466261353		[learning rate: 0.003233]
	Learning Rate: 0.00323296
	LOSS [training: 0.9829103466261353 | validation: 1.0169177551008834]
	TIME [epoch: 10.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.146930431947601		[learning rate: 0.003223]
	Learning Rate: 0.00322305
	LOSS [training: 1.146930431947601 | validation: 0.899738051052901]
	TIME [epoch: 10.5 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8253324570443658		[learning rate: 0.0032132]
	Learning Rate: 0.00321317
	LOSS [training: 0.8253324570443658 | validation: 0.632733299160231]
	TIME [epoch: 10.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6527984952359788		[learning rate: 0.0032033]
	Learning Rate: 0.00320332
	LOSS [training: 0.6527984952359788 | validation: 1.1000034630825508]
	TIME [epoch: 10.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.599833714297181		[learning rate: 0.0031935]
	Learning Rate: 0.0031935
	LOSS [training: 0.599833714297181 | validation: 0.5868386642501215]
	TIME [epoch: 10.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6484158883406617		[learning rate: 0.0031837]
	Learning Rate: 0.00318371
	LOSS [training: 0.6484158883406617 | validation: 0.764273691285693]
	TIME [epoch: 10.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5448498690136867		[learning rate: 0.0031739]
	Learning Rate: 0.00317395
	LOSS [training: 0.5448498690136867 | validation: 0.5950194543902352]
	TIME [epoch: 10.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6648151500497975		[learning rate: 0.0031642]
	Learning Rate: 0.00316422
	LOSS [training: 0.6648151500497975 | validation: 0.7401205064312746]
	TIME [epoch: 10.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.595668638888133		[learning rate: 0.0031545]
	Learning Rate: 0.00315452
	LOSS [training: 0.595668638888133 | validation: 0.6656463588672601]
	TIME [epoch: 10.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5500288038896871		[learning rate: 0.0031449]
	Learning Rate: 0.00314485
	LOSS [training: 0.5500288038896871 | validation: 1.517652455724394]
	TIME [epoch: 10.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.794478111149617		[learning rate: 0.0031352]
	Learning Rate: 0.00313521
	LOSS [training: 0.794478111149617 | validation: 0.7138310938050708]
	TIME [epoch: 10.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7135771188284294		[learning rate: 0.0031256]
	Learning Rate: 0.0031256
	LOSS [training: 0.7135771188284294 | validation: 0.5796892665153826]
	TIME [epoch: 10.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6059779376853983		[learning rate: 0.003116]
	Learning Rate: 0.00311602
	LOSS [training: 0.6059779376853983 | validation: 0.7518712477513567]
	TIME [epoch: 10.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8628337114790572		[learning rate: 0.0031065]
	Learning Rate: 0.00310647
	LOSS [training: 0.8628337114790572 | validation: 1.2871203603609416]
	TIME [epoch: 10.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6152044517460442		[learning rate: 0.0030969]
	Learning Rate: 0.00309694
	LOSS [training: 0.6152044517460442 | validation: 1.1165659291568413]
	TIME [epoch: 10.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6338414696778536		[learning rate: 0.0030875]
	Learning Rate: 0.00308745
	LOSS [training: 0.6338414696778536 | validation: 1.1544195066936787]
	TIME [epoch: 10.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7177226433096277		[learning rate: 0.003078]
	Learning Rate: 0.00307799
	LOSS [training: 0.7177226433096277 | validation: 0.7294565537470805]
	TIME [epoch: 10.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5841592442183035		[learning rate: 0.0030686]
	Learning Rate: 0.00306855
	LOSS [training: 0.5841592442183035 | validation: 0.5774652007570721]
	TIME [epoch: 10.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6785655040340031		[learning rate: 0.0030591]
	Learning Rate: 0.00305914
	LOSS [training: 0.6785655040340031 | validation: 0.5584073833342856]
	TIME [epoch: 10.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5344434149050517		[learning rate: 0.0030498]
	Learning Rate: 0.00304977
	LOSS [training: 0.5344434149050517 | validation: 0.5476945713273352]
	TIME [epoch: 10.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5745267681516664		[learning rate: 0.0030404]
	Learning Rate: 0.00304042
	LOSS [training: 0.5745267681516664 | validation: 0.843643108415479]
	TIME [epoch: 10.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8141155823026951		[learning rate: 0.0030311]
	Learning Rate: 0.0030311
	LOSS [training: 0.8141155823026951 | validation: 0.6981261589205286]
	TIME [epoch: 10.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8815732403106284		[learning rate: 0.0030218]
	Learning Rate: 0.00302181
	LOSS [training: 0.8815732403106284 | validation: 0.5047701821039101]
	TIME [epoch: 10.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6694344283407752		[learning rate: 0.0030125]
	Learning Rate: 0.00301254
	LOSS [training: 0.6694344283407752 | validation: 0.47998502449062813]
	TIME [epoch: 10.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6735310883849749		[learning rate: 0.0030033]
	Learning Rate: 0.00300331
	LOSS [training: 0.6735310883849749 | validation: 0.5433927815700725]
	TIME [epoch: 10.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6157249117756629		[learning rate: 0.0029941]
	Learning Rate: 0.0029941
	LOSS [training: 0.6157249117756629 | validation: 0.4730483965778436]
	TIME [epoch: 10.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7445084220509023		[learning rate: 0.0029849]
	Learning Rate: 0.00298492
	LOSS [training: 0.7445084220509023 | validation: 0.8883241812307989]
	TIME [epoch: 10.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5858950874154985		[learning rate: 0.0029758]
	Learning Rate: 0.00297577
	LOSS [training: 0.5858950874154985 | validation: 0.5847913342612653]
	TIME [epoch: 10.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6685021019451407		[learning rate: 0.0029667]
	Learning Rate: 0.00296665
	LOSS [training: 0.6685021019451407 | validation: 0.8390002560606488]
	TIME [epoch: 10.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8876630421735149		[learning rate: 0.0029576]
	Learning Rate: 0.00295756
	LOSS [training: 0.8876630421735149 | validation: 1.628010352758372]
	TIME [epoch: 10.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3129268850703852		[learning rate: 0.0029485]
	Learning Rate: 0.00294849
	LOSS [training: 1.3129268850703852 | validation: 0.6736861194785294]
	TIME [epoch: 10.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7055073078198436		[learning rate: 0.0029395]
	Learning Rate: 0.00293945
	LOSS [training: 0.7055073078198436 | validation: 0.7384214429371712]
	TIME [epoch: 10.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6562586928702853		[learning rate: 0.0029304]
	Learning Rate: 0.00293044
	LOSS [training: 0.6562586928702853 | validation: 0.4137651338221005]
	TIME [epoch: 10.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47567796757611813		[learning rate: 0.0029215]
	Learning Rate: 0.00292146
	LOSS [training: 0.47567796757611813 | validation: 0.9431499482138008]
	TIME [epoch: 10.5 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5633581981386546		[learning rate: 0.0029125]
	Learning Rate: 0.0029125
	LOSS [training: 0.5633581981386546 | validation: 0.8712271059180771]
	TIME [epoch: 10.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49640969832649995		[learning rate: 0.0029036]
	Learning Rate: 0.00290358
	LOSS [training: 0.49640969832649995 | validation: 0.6458972676339946]
	TIME [epoch: 10.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5264696282771542		[learning rate: 0.0028947]
	Learning Rate: 0.00289468
	LOSS [training: 0.5264696282771542 | validation: 0.42261450911329645]
	TIME [epoch: 10.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48753417184715486		[learning rate: 0.0028858]
	Learning Rate: 0.0028858
	LOSS [training: 0.48753417184715486 | validation: 0.692109523620654]
	TIME [epoch: 10.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40448388497990917		[learning rate: 0.002877]
	Learning Rate: 0.00287696
	LOSS [training: 0.40448388497990917 | validation: 0.556911740099665]
	TIME [epoch: 10.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6243901442970785		[learning rate: 0.0028681]
	Learning Rate: 0.00286814
	LOSS [training: 0.6243901442970785 | validation: 0.8527225577984565]
	TIME [epoch: 10.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954775414204271		[learning rate: 0.0028593]
	Learning Rate: 0.00285935
	LOSS [training: 0.6954775414204271 | validation: 0.49937127557214167]
	TIME [epoch: 10.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4989471078461348		[learning rate: 0.0028506]
	Learning Rate: 0.00285058
	LOSS [training: 0.4989471078461348 | validation: 0.6913300257674451]
	TIME [epoch: 10.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46215522572906237		[learning rate: 0.0028418]
	Learning Rate: 0.00284184
	LOSS [training: 0.46215522572906237 | validation: 0.7798654899522888]
	TIME [epoch: 10.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.554493380139113		[learning rate: 0.0028331]
	Learning Rate: 0.00283313
	LOSS [training: 0.554493380139113 | validation: 1.1214247402406328]
	TIME [epoch: 10.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9506787548133329		[learning rate: 0.0028244]
	Learning Rate: 0.00282445
	LOSS [training: 0.9506787548133329 | validation: 1.092228534920119]
	TIME [epoch: 10.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8264527900811233		[learning rate: 0.0028158]
	Learning Rate: 0.00281579
	LOSS [training: 0.8264527900811233 | validation: 0.6663147281973244]
	TIME [epoch: 10.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5328160220254318		[learning rate: 0.0028072]
	Learning Rate: 0.00280716
	LOSS [training: 0.5328160220254318 | validation: 0.75118549590402]
	TIME [epoch: 10.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6075046898759539		[learning rate: 0.0027986]
	Learning Rate: 0.00279855
	LOSS [training: 0.6075046898759539 | validation: 1.2328806580799694]
	TIME [epoch: 10.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.989303952600101		[learning rate: 0.00279]
	Learning Rate: 0.00278997
	LOSS [training: 0.989303952600101 | validation: 1.3949242849840457]
	TIME [epoch: 10.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.97014966414187		[learning rate: 0.0027814]
	Learning Rate: 0.00278142
	LOSS [training: 0.97014966414187 | validation: 1.4064562799399374]
	TIME [epoch: 10.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2478709419276013		[learning rate: 0.0027729]
	Learning Rate: 0.00277289
	LOSS [training: 1.2478709419276013 | validation: 1.691057100358351]
	TIME [epoch: 10.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3057534558255186		[learning rate: 0.0027644]
	Learning Rate: 0.00276439
	LOSS [training: 1.3057534558255186 | validation: 1.4495010712618979]
	TIME [epoch: 10.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9947753518348936		[learning rate: 0.0027559]
	Learning Rate: 0.00275592
	LOSS [training: 0.9947753518348936 | validation: 1.12318719840157]
	TIME [epoch: 10.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9075766460971924		[learning rate: 0.0027475]
	Learning Rate: 0.00274747
	LOSS [training: 0.9075766460971924 | validation: 1.3113760221518558]
	TIME [epoch: 10.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.492524930207518		[learning rate: 0.002739]
	Learning Rate: 0.00273905
	LOSS [training: 1.492524930207518 | validation: 0.8042865507534049]
	TIME [epoch: 10.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.615123751283672		[learning rate: 0.0027307]
	Learning Rate: 0.00273065
	LOSS [training: 0.615123751283672 | validation: 0.638435618018253]
	TIME [epoch: 10.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6172044292154193		[learning rate: 0.0027223]
	Learning Rate: 0.00272228
	LOSS [training: 0.6172044292154193 | validation: 0.8338253813367064]
	TIME [epoch: 10.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.549362160049355		[learning rate: 0.0027139]
	Learning Rate: 0.00271394
	LOSS [training: 0.549362160049355 | validation: 0.6404250316519731]
	TIME [epoch: 10.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6257827266347205		[learning rate: 0.0027056]
	Learning Rate: 0.00270562
	LOSS [training: 0.6257827266347205 | validation: 1.0767648530372917]
	TIME [epoch: 10.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6291672072026689		[learning rate: 0.0026973]
	Learning Rate: 0.00269733
	LOSS [training: 0.6291672072026689 | validation: 0.6033576948346171]
	TIME [epoch: 10.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5129217590697758		[learning rate: 0.0026891]
	Learning Rate: 0.00268906
	LOSS [training: 0.5129217590697758 | validation: 0.7411447081442887]
	TIME [epoch: 10.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4802874391188863		[learning rate: 0.0026808]
	Learning Rate: 0.00268081
	LOSS [training: 0.4802874391188863 | validation: 0.7393926295592016]
	TIME [epoch: 10.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9121369608710859		[learning rate: 0.0026726]
	Learning Rate: 0.0026726
	LOSS [training: 0.9121369608710859 | validation: 1.1563454892636937]
	TIME [epoch: 10.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7682833991905964		[learning rate: 0.0026644]
	Learning Rate: 0.0026644
	LOSS [training: 0.7682833991905964 | validation: 0.6352795300801733]
	TIME [epoch: 10.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6377897732666321		[learning rate: 0.0026562]
	Learning Rate: 0.00265624
	LOSS [training: 0.6377897732666321 | validation: 0.8956748011464624]
	TIME [epoch: 10.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7242472676708414		[learning rate: 0.0026481]
	Learning Rate: 0.00264809
	LOSS [training: 0.7242472676708414 | validation: 0.5630361663493618]
	TIME [epoch: 10.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.524617122980015		[learning rate: 0.00264]
	Learning Rate: 0.00263998
	LOSS [training: 0.524617122980015 | validation: 1.0978330755425376]
	TIME [epoch: 10.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6237836310867719		[learning rate: 0.0026319]
	Learning Rate: 0.00263188
	LOSS [training: 0.6237836310867719 | validation: 0.4991281616766209]
	TIME [epoch: 10.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41878109442337264		[learning rate: 0.0026238]
	Learning Rate: 0.00262382
	LOSS [training: 0.41878109442337264 | validation: 0.5743491345824782]
	TIME [epoch: 10.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40296396433658044		[learning rate: 0.0026158]
	Learning Rate: 0.00261577
	LOSS [training: 0.40296396433658044 | validation: 0.4862260283408354]
	TIME [epoch: 10.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4906346186480044		[learning rate: 0.0026078]
	Learning Rate: 0.00260775
	LOSS [training: 0.4906346186480044 | validation: 0.4077697370018194]
	TIME [epoch: 10.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3633096315418147		[learning rate: 0.0025998]
	Learning Rate: 0.00259976
	LOSS [training: 0.3633096315418147 | validation: 0.4721027140617309]
	TIME [epoch: 10.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3672926977155626		[learning rate: 0.0025918]
	Learning Rate: 0.00259179
	LOSS [training: 0.3672926977155626 | validation: 0.24742790140644844]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_940.pth
	Model improved!!!
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.412263037442023		[learning rate: 0.0025838]
	Learning Rate: 0.00258385
	LOSS [training: 0.412263037442023 | validation: 0.3092994647241799]
	TIME [epoch: 10.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4504545227954523		[learning rate: 0.0025759]
	Learning Rate: 0.00257593
	LOSS [training: 0.4504545227954523 | validation: 0.6542872880657677]
	TIME [epoch: 10.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.457101710841468		[learning rate: 0.002568]
	Learning Rate: 0.00256803
	LOSS [training: 0.457101710841468 | validation: 0.6409831495130144]
	TIME [epoch: 10.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5426155341369728		[learning rate: 0.0025602]
	Learning Rate: 0.00256016
	LOSS [training: 0.5426155341369728 | validation: 0.8823944026783076]
	TIME [epoch: 10.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5106031486665024		[learning rate: 0.0025523]
	Learning Rate: 0.00255231
	LOSS [training: 0.5106031486665024 | validation: 0.4482101865693683]
	TIME [epoch: 10.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47854577449609514		[learning rate: 0.0025445]
	Learning Rate: 0.00254449
	LOSS [training: 0.47854577449609514 | validation: 0.6213433873349132]
	TIME [epoch: 10.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3876094762485887		[learning rate: 0.0025367]
	Learning Rate: 0.00253669
	LOSS [training: 0.3876094762485887 | validation: 0.37572003354256495]
	TIME [epoch: 10.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40852514047996796		[learning rate: 0.0025289]
	Learning Rate: 0.00252891
	LOSS [training: 0.40852514047996796 | validation: 0.39486275180794594]
	TIME [epoch: 10.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34503976170823647		[learning rate: 0.0025212]
	Learning Rate: 0.00252116
	LOSS [training: 0.34503976170823647 | validation: 0.4655600170929401]
	TIME [epoch: 10.5 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6562543699093739		[learning rate: 0.0025134]
	Learning Rate: 0.00251343
	LOSS [training: 0.6562543699093739 | validation: 0.4908731784741899]
	TIME [epoch: 10.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8252204930440271		[learning rate: 0.0025057]
	Learning Rate: 0.00250572
	LOSS [training: 0.8252204930440271 | validation: 0.8813061198480416]
	TIME [epoch: 10.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4546502619391298		[learning rate: 0.002498]
	Learning Rate: 0.00249804
	LOSS [training: 0.4546502619391298 | validation: 0.42609179784121154]
	TIME [epoch: 10.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.457497331124957		[learning rate: 0.0024904]
	Learning Rate: 0.00249039
	LOSS [training: 0.457497331124957 | validation: 0.5199793061628623]
	TIME [epoch: 10.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4069491479482771		[learning rate: 0.0024828]
	Learning Rate: 0.00248275
	LOSS [training: 0.4069491479482771 | validation: 0.49777890605959896]
	TIME [epoch: 10.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5505940828305652		[learning rate: 0.0024751]
	Learning Rate: 0.00247514
	LOSS [training: 0.5505940828305652 | validation: 0.42415625427165254]
	TIME [epoch: 10.5 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.370647385691714		[learning rate: 0.0024676]
	Learning Rate: 0.00246755
	LOSS [training: 0.370647385691714 | validation: 0.4526791898501087]
	TIME [epoch: 10.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37877657682839516		[learning rate: 0.00246]
	Learning Rate: 0.00245999
	LOSS [training: 0.37877657682839516 | validation: 0.35113327838383085]
	TIME [epoch: 10.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6667400423981097		[learning rate: 0.0024524]
	Learning Rate: 0.00245245
	LOSS [training: 0.6667400423981097 | validation: 0.4761926124166355]
	TIME [epoch: 10.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47863635494500156		[learning rate: 0.0024449]
	Learning Rate: 0.00244493
	LOSS [training: 0.47863635494500156 | validation: 0.5817146312709782]
	TIME [epoch: 10.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.456176098863602		[learning rate: 0.0024374]
	Learning Rate: 0.00243744
	LOSS [training: 0.456176098863602 | validation: 0.5437460538189803]
	TIME [epoch: 10.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6094979124152948		[learning rate: 0.00243]
	Learning Rate: 0.00242996
	LOSS [training: 0.6094979124152948 | validation: 0.26973160505423915]
	TIME [epoch: 10.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.378576529846885		[learning rate: 0.0024225]
	Learning Rate: 0.00242252
	LOSS [training: 0.378576529846885 | validation: 0.7144290985187987]
	TIME [epoch: 10.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42918612413178436		[learning rate: 0.0024151]
	Learning Rate: 0.00241509
	LOSS [training: 0.42918612413178436 | validation: 0.36742817936261246]
	TIME [epoch: 10.6 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3866879113607361		[learning rate: 0.0024077]
	Learning Rate: 0.00240769
	LOSS [training: 0.3866879113607361 | validation: 0.48430062869929574]
	TIME [epoch: 10.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39182252038443943		[learning rate: 0.0024003]
	Learning Rate: 0.00240031
	LOSS [training: 0.39182252038443943 | validation: 0.32085810161590317]
	TIME [epoch: 10.5 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3981626697686278		[learning rate: 0.0023929]
	Learning Rate: 0.00239295
	LOSS [training: 0.3981626697686278 | validation: 0.6720040525914206]
	TIME [epoch: 10.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3951648318699258		[learning rate: 0.0023856]
	Learning Rate: 0.00238561
	LOSS [training: 0.3951648318699258 | validation: 0.3702328192026541]
	TIME [epoch: 10.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39695483565534795		[learning rate: 0.0023783]
	Learning Rate: 0.0023783
	LOSS [training: 0.39695483565534795 | validation: 0.4416757071315945]
	TIME [epoch: 10.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4278138656694825		[learning rate: 0.002371]
	Learning Rate: 0.00237101
	LOSS [training: 0.4278138656694825 | validation: 0.587570008960618]
	TIME [epoch: 10.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42538801825829103		[learning rate: 0.0023637]
	Learning Rate: 0.00236374
	LOSS [training: 0.42538801825829103 | validation: 0.456071747171713]
	TIME [epoch: 10.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8158672407978654		[learning rate: 0.0023565]
	Learning Rate: 0.0023565
	LOSS [training: 0.8158672407978654 | validation: 0.3577549501423475]
	TIME [epoch: 10.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4140996804529218		[learning rate: 0.0023493]
	Learning Rate: 0.00234927
	LOSS [training: 0.4140996804529218 | validation: 0.9273034348556765]
	TIME [epoch: 10.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5161043544099997		[learning rate: 0.0023421]
	Learning Rate: 0.00234207
	LOSS [training: 0.5161043544099997 | validation: 0.3287164191521933]
	TIME [epoch: 10.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5340285878627393		[learning rate: 0.0023349]
	Learning Rate: 0.00233489
	LOSS [training: 0.5340285878627393 | validation: 0.41380631516675936]
	TIME [epoch: 10.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6115264857469774		[learning rate: 0.0023277]
	Learning Rate: 0.00232773
	LOSS [training: 0.6115264857469774 | validation: 0.49811661694298226]
	TIME [epoch: 10.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6669116514237234		[learning rate: 0.0023206]
	Learning Rate: 0.0023206
	LOSS [training: 0.6669116514237234 | validation: 0.42785140834364443]
	TIME [epoch: 10.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7189029421587505		[learning rate: 0.0023135]
	Learning Rate: 0.00231348
	LOSS [training: 0.7189029421587505 | validation: 0.5820611960037907]
	TIME [epoch: 10.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5646328705127801		[learning rate: 0.0023064]
	Learning Rate: 0.00230639
	LOSS [training: 0.5646328705127801 | validation: 1.1324630784047804]
	TIME [epoch: 10.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5156523327611625		[learning rate: 0.0022993]
	Learning Rate: 0.00229932
	LOSS [training: 0.5156523327611625 | validation: 0.4507944533960068]
	TIME [epoch: 10.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41960346185050545		[learning rate: 0.0022923]
	Learning Rate: 0.00229227
	LOSS [training: 0.41960346185050545 | validation: 0.5005378797920881]
	TIME [epoch: 10.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3725943020754416		[learning rate: 0.0022852]
	Learning Rate: 0.00228525
	LOSS [training: 0.3725943020754416 | validation: 0.637894141013541]
	TIME [epoch: 10.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4808937130211709		[learning rate: 0.0022782]
	Learning Rate: 0.00227824
	LOSS [training: 0.4808937130211709 | validation: 0.9892338710525419]
	TIME [epoch: 10.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6223995344030401		[learning rate: 0.0022713]
	Learning Rate: 0.00227126
	LOSS [training: 0.6223995344030401 | validation: 0.5026578105168935]
	TIME [epoch: 10.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4219049954731856		[learning rate: 0.0022643]
	Learning Rate: 0.0022643
	LOSS [training: 0.4219049954731856 | validation: 0.5490839831186972]
	TIME [epoch: 10.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6182518364344579		[learning rate: 0.0022574]
	Learning Rate: 0.00225736
	LOSS [training: 0.6182518364344579 | validation: 0.44764845825238725]
	TIME [epoch: 10.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40452347059936217		[learning rate: 0.0022504]
	Learning Rate: 0.00225044
	LOSS [training: 0.40452347059936217 | validation: 0.6809290064857182]
	TIME [epoch: 10.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7648880859646294		[learning rate: 0.0022435]
	Learning Rate: 0.00224354
	LOSS [training: 0.7648880859646294 | validation: 0.4589854123690751]
	TIME [epoch: 10.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4521493951016898		[learning rate: 0.0022367]
	Learning Rate: 0.00223666
	LOSS [training: 0.4521493951016898 | validation: 0.6045933274168969]
	TIME [epoch: 10.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.353727035478622		[learning rate: 0.0022298]
	Learning Rate: 0.0022298
	LOSS [training: 0.353727035478622 | validation: 0.41041828738716746]
	TIME [epoch: 10.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4585862661777999		[learning rate: 0.002223]
	Learning Rate: 0.00222297
	LOSS [training: 0.4585862661777999 | validation: 0.4367251436025484]
	TIME [epoch: 10.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4269768865039233		[learning rate: 0.0022162]
	Learning Rate: 0.00221615
	LOSS [training: 0.4269768865039233 | validation: 0.5204642716528877]
	TIME [epoch: 10.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3948436101679516		[learning rate: 0.0022094]
	Learning Rate: 0.00220936
	LOSS [training: 0.3948436101679516 | validation: 1.0474408410483051]
	TIME [epoch: 10.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5218823568245804		[learning rate: 0.0022026]
	Learning Rate: 0.00220259
	LOSS [training: 0.5218823568245804 | validation: 0.38352577271851174]
	TIME [epoch: 10.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29984292921796746		[learning rate: 0.0021958]
	Learning Rate: 0.00219584
	LOSS [training: 0.29984292921796746 | validation: 0.6054106785515008]
	TIME [epoch: 10.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43389289442693774		[learning rate: 0.0021891]
	Learning Rate: 0.00218911
	LOSS [training: 0.43389289442693774 | validation: 0.42711877720830954]
	TIME [epoch: 10.5 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4709601633929169		[learning rate: 0.0021824]
	Learning Rate: 0.00218239
	LOSS [training: 0.4709601633929169 | validation: 0.5147308445898935]
	TIME [epoch: 10.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46706357992771697		[learning rate: 0.0021757]
	Learning Rate: 0.00217571
	LOSS [training: 0.46706357992771697 | validation: 0.6308669372992101]
	TIME [epoch: 10.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4566617917267191		[learning rate: 0.002169]
	Learning Rate: 0.00216904
	LOSS [training: 0.4566617917267191 | validation: 0.46055950659129685]
	TIME [epoch: 10.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4413490614465857		[learning rate: 0.0021624]
	Learning Rate: 0.00216239
	LOSS [training: 0.4413490614465857 | validation: 0.38624933212472207]
	TIME [epoch: 10.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34903484904519827		[learning rate: 0.0021558]
	Learning Rate: 0.00215576
	LOSS [training: 0.34903484904519827 | validation: 0.442521850283567]
	TIME [epoch: 10.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3485940360606107		[learning rate: 0.0021491]
	Learning Rate: 0.00214915
	LOSS [training: 0.3485940360606107 | validation: 0.5998872418381618]
	TIME [epoch: 10.6 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4155534485036366		[learning rate: 0.0021426]
	Learning Rate: 0.00214256
	LOSS [training: 0.4155534485036366 | validation: 0.7029758613948951]
	TIME [epoch: 10.6 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3493207305210044		[learning rate: 0.002136]
	Learning Rate: 0.00213599
	LOSS [training: 0.3493207305210044 | validation: 0.4622014127775559]
	TIME [epoch: 10.6 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3827037183302338		[learning rate: 0.0021294]
	Learning Rate: 0.00212945
	LOSS [training: 0.3827037183302338 | validation: 0.7311934509342616]
	TIME [epoch: 10.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4416326974387762		[learning rate: 0.0021229]
	Learning Rate: 0.00212292
	LOSS [training: 0.4416326974387762 | validation: 0.8026187066193791]
	TIME [epoch: 10.6 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5225480847047256		[learning rate: 0.0021164]
	Learning Rate: 0.00211641
	LOSS [training: 0.5225480847047256 | validation: 0.4465107339503108]
	TIME [epoch: 10.6 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3187201077923417		[learning rate: 0.0021099]
	Learning Rate: 0.00210992
	LOSS [training: 0.3187201077923417 | validation: 0.5128767191027728]
	TIME [epoch: 10.6 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4205939814001445		[learning rate: 0.0021035]
	Learning Rate: 0.00210346
	LOSS [training: 0.4205939814001445 | validation: 0.5972142429219299]
	TIME [epoch: 10.6 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3745465679136698		[learning rate: 0.002097]
	Learning Rate: 0.00209701
	LOSS [training: 0.3745465679136698 | validation: 0.39915324806677827]
	TIME [epoch: 10.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29492821362549854		[learning rate: 0.0020906]
	Learning Rate: 0.00209058
	LOSS [training: 0.29492821362549854 | validation: 0.4167226441018032]
	TIME [epoch: 10.6 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4004623227420874		[learning rate: 0.0020842]
	Learning Rate: 0.00208417
	LOSS [training: 0.4004623227420874 | validation: 0.5940214264141735]
	TIME [epoch: 10.6 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31002129847691584		[learning rate: 0.0020778]
	Learning Rate: 0.00207778
	LOSS [training: 0.31002129847691584 | validation: 0.3541372226625198]
	TIME [epoch: 10.6 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24173336755654376		[learning rate: 0.0020714]
	Learning Rate: 0.00207141
	LOSS [training: 0.24173336755654376 | validation: 0.3101182066722626]
	TIME [epoch: 10.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23673755694576865		[learning rate: 0.0020651]
	Learning Rate: 0.00206506
	LOSS [training: 0.23673755694576865 | validation: 0.270428266612668]
	TIME [epoch: 10.6 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47053405214293065		[learning rate: 0.0020587]
	Learning Rate: 0.00205873
	LOSS [training: 0.47053405214293065 | validation: 0.7984788493557424]
	TIME [epoch: 10.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6974276222858734		[learning rate: 0.0020524]
	Learning Rate: 0.00205242
	LOSS [training: 0.6974276222858734 | validation: 0.326391804409412]
	TIME [epoch: 10.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3378779323492684		[learning rate: 0.0020461]
	Learning Rate: 0.00204613
	LOSS [training: 0.3378779323492684 | validation: 0.4302783604181996]
	TIME [epoch: 10.6 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4663923593180339		[learning rate: 0.0020399]
	Learning Rate: 0.00203986
	LOSS [training: 0.4663923593180339 | validation: 0.4995313607660708]
	TIME [epoch: 10.6 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5734918014991839		[learning rate: 0.0020336]
	Learning Rate: 0.00203361
	LOSS [training: 0.5734918014991839 | validation: 0.29602426900137874]
	TIME [epoch: 10.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4183677943746891		[learning rate: 0.0020274]
	Learning Rate: 0.00202737
	LOSS [training: 0.4183677943746891 | validation: 0.40129172898796056]
	TIME [epoch: 10.6 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5314905358285218		[learning rate: 0.0020212]
	Learning Rate: 0.00202116
	LOSS [training: 0.5314905358285218 | validation: 0.4270551667818866]
	TIME [epoch: 10.6 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5209706955623192		[learning rate: 0.002015]
	Learning Rate: 0.00201496
	LOSS [training: 0.5209706955623192 | validation: 0.45940238228363717]
	TIME [epoch: 10.6 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5521418029085038		[learning rate: 0.0020088]
	Learning Rate: 0.00200878
	LOSS [training: 0.5521418029085038 | validation: 0.3915329565508968]
	TIME [epoch: 10.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5421423823134657		[learning rate: 0.0020026]
	Learning Rate: 0.00200263
	LOSS [training: 0.5421423823134657 | validation: 0.41315362244142606]
	TIME [epoch: 10.6 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5300944205637109		[learning rate: 0.0019965]
	Learning Rate: 0.00199649
	LOSS [training: 0.5300944205637109 | validation: 0.4803301295155451]
	TIME [epoch: 10.6 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5711339377203737		[learning rate: 0.0019904]
	Learning Rate: 0.00199037
	LOSS [training: 0.5711339377203737 | validation: 0.4186217458184773]
	TIME [epoch: 10.6 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4511810012063334		[learning rate: 0.0019843]
	Learning Rate: 0.00198427
	LOSS [training: 0.4511810012063334 | validation: 0.7208822580214743]
	TIME [epoch: 10.6 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4555419333945333		[learning rate: 0.0019782]
	Learning Rate: 0.00197818
	LOSS [training: 0.4555419333945333 | validation: 0.565983557896378]
	TIME [epoch: 10.6 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5214659176719546		[learning rate: 0.0019721]
	Learning Rate: 0.00197212
	LOSS [training: 0.5214659176719546 | validation: 0.4582227359099379]
	TIME [epoch: 10.6 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40079140891245923		[learning rate: 0.0019661]
	Learning Rate: 0.00196607
	LOSS [training: 0.40079140891245923 | validation: 0.6246909365576846]
	TIME [epoch: 10.6 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4173085750798139		[learning rate: 0.00196]
	Learning Rate: 0.00196005
	LOSS [training: 0.4173085750798139 | validation: 0.4175258567225219]
	TIME [epoch: 10.6 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3312642661927129		[learning rate: 0.001954]
	Learning Rate: 0.00195404
	LOSS [training: 0.3312642661927129 | validation: 0.35176749369513083]
	TIME [epoch: 10.6 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.332264788748513		[learning rate: 0.001948]
	Learning Rate: 0.00194805
	LOSS [training: 0.332264788748513 | validation: 0.7266763379144274]
	TIME [epoch: 10.6 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5200006140807136		[learning rate: 0.0019421]
	Learning Rate: 0.00194208
	LOSS [training: 0.5200006140807136 | validation: 0.3185860063901299]
	TIME [epoch: 10.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28884588593621174		[learning rate: 0.0019361]
	Learning Rate: 0.00193612
	LOSS [training: 0.28884588593621174 | validation: 0.4375715570915503]
	TIME [epoch: 10.6 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39339322565696877		[learning rate: 0.0019302]
	Learning Rate: 0.00193019
	LOSS [training: 0.39339322565696877 | validation: 0.3638398762832172]
	TIME [epoch: 10.6 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3520380822492875		[learning rate: 0.0019243]
	Learning Rate: 0.00192427
	LOSS [training: 0.3520380822492875 | validation: 0.31932184079473797]
	TIME [epoch: 10.6 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3446498639472977		[learning rate: 0.0019184]
	Learning Rate: 0.00191837
	LOSS [training: 0.3446498639472977 | validation: 0.3781267787511311]
	TIME [epoch: 10.6 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41054032989055916		[learning rate: 0.0019125]
	Learning Rate: 0.00191249
	LOSS [training: 0.41054032989055916 | validation: 0.3108359897768842]
	TIME [epoch: 10.6 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35299697351801607		[learning rate: 0.0019066]
	Learning Rate: 0.00190663
	LOSS [training: 0.35299697351801607 | validation: 0.25216858752055016]
	TIME [epoch: 10.6 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3412595205336945		[learning rate: 0.0019008]
	Learning Rate: 0.00190079
	LOSS [training: 0.3412595205336945 | validation: 0.30621770198223947]
	TIME [epoch: 10.6 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29147042755787367		[learning rate: 0.001895]
	Learning Rate: 0.00189496
	LOSS [training: 0.29147042755787367 | validation: 0.44068709778901044]
	TIME [epoch: 10.6 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42408311688887534		[learning rate: 0.0018892]
	Learning Rate: 0.00188915
	LOSS [training: 0.42408311688887534 | validation: 0.36951602228174885]
	TIME [epoch: 10.6 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.346685336953197		[learning rate: 0.0018834]
	Learning Rate: 0.00188336
	LOSS [training: 0.346685336953197 | validation: 0.4090653670884503]
	TIME [epoch: 10.6 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3034587416170968		[learning rate: 0.0018776]
	Learning Rate: 0.00187759
	LOSS [training: 0.3034587416170968 | validation: 0.4883389534851042]
	TIME [epoch: 10.6 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2983691829068319		[learning rate: 0.0018718]
	Learning Rate: 0.00187183
	LOSS [training: 0.2983691829068319 | validation: 0.5066048917271234]
	TIME [epoch: 10.6 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3213447307219658		[learning rate: 0.0018661]
	Learning Rate: 0.00186609
	LOSS [training: 0.3213447307219658 | validation: 0.3428816792182879]
	TIME [epoch: 10.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3686750425294046		[learning rate: 0.0018604]
	Learning Rate: 0.00186037
	LOSS [training: 0.3686750425294046 | validation: 0.34121775457910486]
	TIME [epoch: 10.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4591115625558636		[learning rate: 0.0018547]
	Learning Rate: 0.00185467
	LOSS [training: 0.4591115625558636 | validation: 0.3494769460713664]
	TIME [epoch: 10.6 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4136758902429699		[learning rate: 0.001849]
	Learning Rate: 0.00184898
	LOSS [training: 0.4136758902429699 | validation: 0.3188524414499689]
	TIME [epoch: 10.6 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2699405902066889		[learning rate: 0.0018433]
	Learning Rate: 0.00184332
	LOSS [training: 0.2699405902066889 | validation: 0.32766403344356954]
	TIME [epoch: 10.6 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32652974655309563		[learning rate: 0.0018377]
	Learning Rate: 0.00183767
	LOSS [training: 0.32652974655309563 | validation: 0.3491667005971222]
	TIME [epoch: 10.6 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30997177751250204		[learning rate: 0.001832]
	Learning Rate: 0.00183203
	LOSS [training: 0.30997177751250204 | validation: 0.3577787543359488]
	TIME [epoch: 10.6 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43462804457342585		[learning rate: 0.0018264]
	Learning Rate: 0.00182642
	LOSS [training: 0.43462804457342585 | validation: 0.586917232087941]
	TIME [epoch: 10.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37775299279231633		[learning rate: 0.0018208]
	Learning Rate: 0.00182082
	LOSS [training: 0.37775299279231633 | validation: 0.49579436716103603]
	TIME [epoch: 10.6 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38743767919321515		[learning rate: 0.0018152]
	Learning Rate: 0.00181524
	LOSS [training: 0.38743767919321515 | validation: 0.5753235143509029]
	TIME [epoch: 10.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34688675192347357		[learning rate: 0.0018097]
	Learning Rate: 0.00180967
	LOSS [training: 0.34688675192347357 | validation: 0.45552258085874003]
	TIME [epoch: 10.6 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4759261364592862		[learning rate: 0.0018041]
	Learning Rate: 0.00180412
	LOSS [training: 0.4759261364592862 | validation: 0.732777562181307]
	TIME [epoch: 10.6 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36975338573311645		[learning rate: 0.0017986]
	Learning Rate: 0.00179859
	LOSS [training: 0.36975338573311645 | validation: 0.7406352726543622]
	TIME [epoch: 10.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35999489832620823		[learning rate: 0.0017931]
	Learning Rate: 0.00179308
	LOSS [training: 0.35999489832620823 | validation: 0.39083627370374935]
	TIME [epoch: 10.6 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3010616106572171		[learning rate: 0.0017876]
	Learning Rate: 0.00178758
	LOSS [training: 0.3010616106572171 | validation: 0.5204589751571106]
	TIME [epoch: 10.6 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3616308754661106		[learning rate: 0.0017821]
	Learning Rate: 0.00178211
	LOSS [training: 0.3616308754661106 | validation: 0.23425656944523726]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1062.pth
	Model improved!!!
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3650564022579096		[learning rate: 0.0017766]
	Learning Rate: 0.00177664
	LOSS [training: 0.3650564022579096 | validation: 0.37132107640745743]
	TIME [epoch: 10.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3516189007484887		[learning rate: 0.0017712]
	Learning Rate: 0.0017712
	LOSS [training: 0.3516189007484887 | validation: 0.480146258403229]
	TIME [epoch: 10.6 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5363238887521438		[learning rate: 0.0017658]
	Learning Rate: 0.00176577
	LOSS [training: 0.5363238887521438 | validation: 0.2784861122047973]
	TIME [epoch: 10.6 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31319039416828465		[learning rate: 0.0017604]
	Learning Rate: 0.00176035
	LOSS [training: 0.31319039416828465 | validation: 0.3289790072230936]
	TIME [epoch: 10.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3370032338328464		[learning rate: 0.001755]
	Learning Rate: 0.00175496
	LOSS [training: 0.3370032338328464 | validation: 0.36435082602355073]
	TIME [epoch: 10.6 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.338590380079555		[learning rate: 0.0017496]
	Learning Rate: 0.00174958
	LOSS [training: 0.338590380079555 | validation: 0.3902614709444231]
	TIME [epoch: 10.6 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3348586950465082		[learning rate: 0.0017442]
	Learning Rate: 0.00174421
	LOSS [training: 0.3348586950465082 | validation: 0.6759761401214217]
	TIME [epoch: 10.6 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36664414085401525		[learning rate: 0.0017389]
	Learning Rate: 0.00173887
	LOSS [training: 0.36664414085401525 | validation: 0.4068992224949925]
	TIME [epoch: 10.6 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5353564575967374		[learning rate: 0.0017335]
	Learning Rate: 0.00173354
	LOSS [training: 0.5353564575967374 | validation: 0.4479976733978888]
	TIME [epoch: 10.6 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3516501315849605		[learning rate: 0.0017282]
	Learning Rate: 0.00172822
	LOSS [training: 0.3516501315849605 | validation: 0.3929998432975602]
	TIME [epoch: 10.6 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30882672837010017		[learning rate: 0.0017229]
	Learning Rate: 0.00172293
	LOSS [training: 0.30882672837010017 | validation: 0.5817783629085457]
	TIME [epoch: 10.6 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40754646084268753		[learning rate: 0.0017176]
	Learning Rate: 0.00171764
	LOSS [training: 0.40754646084268753 | validation: 0.5715537196382957]
	TIME [epoch: 10.6 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5541423340124656		[learning rate: 0.0017124]
	Learning Rate: 0.00171238
	LOSS [training: 0.5541423340124656 | validation: 0.40583472381381597]
	TIME [epoch: 10.6 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3904445063477563		[learning rate: 0.0017071]
	Learning Rate: 0.00170713
	LOSS [training: 0.3904445063477563 | validation: 0.5005288808042906]
	TIME [epoch: 10.6 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34946707222388157		[learning rate: 0.0017019]
	Learning Rate: 0.0017019
	LOSS [training: 0.34946707222388157 | validation: 0.3363414447561368]
	TIME [epoch: 10.6 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27827305563529003		[learning rate: 0.0016967]
	Learning Rate: 0.00169668
	LOSS [training: 0.27827305563529003 | validation: 0.5771398434700994]
	TIME [epoch: 10.6 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32811491997355324		[learning rate: 0.0016915]
	Learning Rate: 0.00169148
	LOSS [training: 0.32811491997355324 | validation: 0.38359476670090203]
	TIME [epoch: 10.6 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4281338881032487		[learning rate: 0.0016863]
	Learning Rate: 0.00168629
	LOSS [training: 0.4281338881032487 | validation: 0.8400069370078274]
	TIME [epoch: 10.6 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35745413485666416		[learning rate: 0.0016811]
	Learning Rate: 0.00168113
	LOSS [training: 0.35745413485666416 | validation: 0.41958560544730433]
	TIME [epoch: 10.6 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48282838291497054		[learning rate: 0.001676]
	Learning Rate: 0.00167597
	LOSS [training: 0.48282838291497054 | validation: 0.6881839933484813]
	TIME [epoch: 10.6 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3916188620445067		[learning rate: 0.0016708]
	Learning Rate: 0.00167083
	LOSS [training: 0.3916188620445067 | validation: 0.4959897308097554]
	TIME [epoch: 10.6 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4106105658784804		[learning rate: 0.0016657]
	Learning Rate: 0.00166571
	LOSS [training: 0.4106105658784804 | validation: 0.9322420887333758]
	TIME [epoch: 10.6 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5363995398499464		[learning rate: 0.0016606]
	Learning Rate: 0.00166061
	LOSS [training: 0.5363995398499464 | validation: 0.3632841446509781]
	TIME [epoch: 10.6 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36643770334465725		[learning rate: 0.0016555]
	Learning Rate: 0.00165552
	LOSS [training: 0.36643770334465725 | validation: 0.33698329292432244]
	TIME [epoch: 10.6 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3055624334957293		[learning rate: 0.0016504]
	Learning Rate: 0.00165044
	LOSS [training: 0.3055624334957293 | validation: 0.3695582713175593]
	TIME [epoch: 10.6 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3259718881916107		[learning rate: 0.0016454]
	Learning Rate: 0.00164538
	LOSS [training: 0.3259718881916107 | validation: 0.6260435001437651]
	TIME [epoch: 10.6 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6156526010383769		[learning rate: 0.0016403]
	Learning Rate: 0.00164034
	LOSS [training: 0.6156526010383769 | validation: 0.49090042545775]
	TIME [epoch: 10.6 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27440542312764105		[learning rate: 0.0016353]
	Learning Rate: 0.00163531
	LOSS [training: 0.27440542312764105 | validation: 0.2919878977405366]
	TIME [epoch: 10.6 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26954227039255574		[learning rate: 0.0016303]
	Learning Rate: 0.0016303
	LOSS [training: 0.26954227039255574 | validation: 0.3188664988767717]
	TIME [epoch: 10.6 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3340235420416001		[learning rate: 0.0016253]
	Learning Rate: 0.0016253
	LOSS [training: 0.3340235420416001 | validation: 0.5268284772101104]
	TIME [epoch: 10.6 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38368520117024685		[learning rate: 0.0016203]
	Learning Rate: 0.00162032
	LOSS [training: 0.38368520117024685 | validation: 0.4456675717428095]
	TIME [epoch: 10.6 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25252897719220857		[learning rate: 0.0016154]
	Learning Rate: 0.00161535
	LOSS [training: 0.25252897719220857 | validation: 0.44449310637992917]
	TIME [epoch: 10.6 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44174807844539166		[learning rate: 0.0016104]
	Learning Rate: 0.0016104
	LOSS [training: 0.44174807844539166 | validation: 1.0932768429083788]
	TIME [epoch: 10.6 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42174043697797464		[learning rate: 0.0016055]
	Learning Rate: 0.00160546
	LOSS [training: 0.42174043697797464 | validation: 0.28322242038450585]
	TIME [epoch: 10.6 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37496558827738913		[learning rate: 0.0016005]
	Learning Rate: 0.00160054
	LOSS [training: 0.37496558827738913 | validation: 0.4243500786658404]
	TIME [epoch: 10.6 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2652567175667949		[learning rate: 0.0015956]
	Learning Rate: 0.00159563
	LOSS [training: 0.2652567175667949 | validation: 0.627682739458186]
	TIME [epoch: 10.6 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32399445535620547		[learning rate: 0.0015907]
	Learning Rate: 0.00159074
	LOSS [training: 0.32399445535620547 | validation: 0.24176472589500386]
	TIME [epoch: 10.6 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2737156141558096		[learning rate: 0.0015859]
	Learning Rate: 0.00158587
	LOSS [training: 0.2737156141558096 | validation: 0.403331053725946]
	TIME [epoch: 10.6 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2712190249069092		[learning rate: 0.001581]
	Learning Rate: 0.00158101
	LOSS [training: 0.2712190249069092 | validation: 0.9168490284894594]
	TIME [epoch: 10.6 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5188503030743405		[learning rate: 0.0015762]
	Learning Rate: 0.00157616
	LOSS [training: 0.5188503030743405 | validation: 0.796935771764309]
	TIME [epoch: 10.6 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43800616005725246		[learning rate: 0.0015713]
	Learning Rate: 0.00157133
	LOSS [training: 0.43800616005725246 | validation: 0.7124926351485705]
	TIME [epoch: 10.6 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34827927620116717		[learning rate: 0.0015665]
	Learning Rate: 0.00156651
	LOSS [training: 0.34827927620116717 | validation: 0.6226532174731826]
	TIME [epoch: 10.6 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2709720592350192		[learning rate: 0.0015617]
	Learning Rate: 0.00156171
	LOSS [training: 0.2709720592350192 | validation: 0.7295368939346548]
	TIME [epoch: 10.6 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4817809944455922		[learning rate: 0.0015569]
	Learning Rate: 0.00155692
	LOSS [training: 0.4817809944455922 | validation: 0.5639862471416146]
	TIME [epoch: 10.6 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46249696225205417		[learning rate: 0.0015521]
	Learning Rate: 0.00155215
	LOSS [training: 0.46249696225205417 | validation: 0.8311624365867434]
	TIME [epoch: 10.6 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5467596400586301		[learning rate: 0.0015474]
	Learning Rate: 0.00154739
	LOSS [training: 0.5467596400586301 | validation: 0.7130815918125544]
	TIME [epoch: 10.6 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34453835386230064		[learning rate: 0.0015426]
	Learning Rate: 0.00154265
	LOSS [training: 0.34453835386230064 | validation: 0.7968743420065133]
	TIME [epoch: 10.6 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48941648169773355		[learning rate: 0.0015379]
	Learning Rate: 0.00153792
	LOSS [training: 0.48941648169773355 | validation: 0.9952801747278707]
	TIME [epoch: 10.6 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40920803317834464		[learning rate: 0.0015332]
	Learning Rate: 0.0015332
	LOSS [training: 0.40920803317834464 | validation: 0.7397019314920905]
	TIME [epoch: 10.6 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.370953677884666		[learning rate: 0.0015285]
	Learning Rate: 0.0015285
	LOSS [training: 0.370953677884666 | validation: 1.0430286761681797]
	TIME [epoch: 10.6 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48648671427750034		[learning rate: 0.0015238]
	Learning Rate: 0.00152382
	LOSS [training: 0.48648671427750034 | validation: 0.6654676297231142]
	TIME [epoch: 10.6 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5721443422323051		[learning rate: 0.0015191]
	Learning Rate: 0.00151915
	LOSS [training: 0.5721443422323051 | validation: 0.5600680848806661]
	TIME [epoch: 10.6 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40191916194621885		[learning rate: 0.0015145]
	Learning Rate: 0.00151449
	LOSS [training: 0.40191916194621885 | validation: 0.9838717679306841]
	TIME [epoch: 10.6 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38727723113622037		[learning rate: 0.0015098]
	Learning Rate: 0.00150985
	LOSS [training: 0.38727723113622037 | validation: 0.39893359173046505]
	TIME [epoch: 10.6 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46364840634665005		[learning rate: 0.0015052]
	Learning Rate: 0.00150522
	LOSS [training: 0.46364840634665005 | validation: 0.45580421249165126]
	TIME [epoch: 10.6 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3606112400173176		[learning rate: 0.0015006]
	Learning Rate: 0.00150061
	LOSS [training: 0.3606112400173176 | validation: 0.7659805599723809]
	TIME [epoch: 10.6 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4557760646518775		[learning rate: 0.001496]
	Learning Rate: 0.00149601
	LOSS [training: 0.4557760646518775 | validation: 0.8791926502624466]
	TIME [epoch: 10.6 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3904283926041856		[learning rate: 0.0014914]
	Learning Rate: 0.00149142
	LOSS [training: 0.3904283926041856 | validation: 0.5159989826343518]
	TIME [epoch: 10.6 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30472117293129697		[learning rate: 0.0014868]
	Learning Rate: 0.00148685
	LOSS [training: 0.30472117293129697 | validation: 0.7045305998480805]
	TIME [epoch: 10.6 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3629243983205887		[learning rate: 0.0014823]
	Learning Rate: 0.00148229
	LOSS [training: 0.3629243983205887 | validation: 0.7504786637989542]
	TIME [epoch: 10.6 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31857022503533095		[learning rate: 0.0014777]
	Learning Rate: 0.00147775
	LOSS [training: 0.31857022503533095 | validation: 0.5829422272040933]
	TIME [epoch: 10.6 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27332046721042647		[learning rate: 0.0014732]
	Learning Rate: 0.00147322
	LOSS [training: 0.27332046721042647 | validation: 0.8946560739478983]
	TIME [epoch: 10.6 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2782259266803074		[learning rate: 0.0014687]
	Learning Rate: 0.0014687
	LOSS [training: 0.2782259266803074 | validation: 0.8016687211910981]
	TIME [epoch: 10.6 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2900565060779653		[learning rate: 0.0014642]
	Learning Rate: 0.0014642
	LOSS [training: 0.2900565060779653 | validation: 0.9719587806674425]
	TIME [epoch: 10.6 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3820402920804176		[learning rate: 0.0014597]
	Learning Rate: 0.00145971
	LOSS [training: 0.3820402920804176 | validation: 0.4571295469494767]
	TIME [epoch: 10.6 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30560883145193224		[learning rate: 0.0014552]
	Learning Rate: 0.00145524
	LOSS [training: 0.30560883145193224 | validation: 0.4289759353458378]
	TIME [epoch: 10.6 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3700206244815049		[learning rate: 0.0014508]
	Learning Rate: 0.00145077
	LOSS [training: 0.3700206244815049 | validation: 0.3802747214449575]
	TIME [epoch: 10.5 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2677986965271463		[learning rate: 0.0014463]
	Learning Rate: 0.00144633
	LOSS [training: 0.2677986965271463 | validation: 0.4038409600820117]
	TIME [epoch: 10.6 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3119491283095195		[learning rate: 0.0014419]
	Learning Rate: 0.00144189
	LOSS [training: 0.3119491283095195 | validation: 0.44751561266106143]
	TIME [epoch: 10.6 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3459094469056901		[learning rate: 0.0014375]
	Learning Rate: 0.00143747
	LOSS [training: 0.3459094469056901 | validation: 0.34558143368380245]
	TIME [epoch: 10.6 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24838675556314774		[learning rate: 0.0014331]
	Learning Rate: 0.00143307
	LOSS [training: 0.24838675556314774 | validation: 0.23599830386423404]
	TIME [epoch: 10.6 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3425501031318864		[learning rate: 0.0014287]
	Learning Rate: 0.00142867
	LOSS [training: 0.3425501031318864 | validation: 0.30108632739548763]
	TIME [epoch: 10.6 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2958808116087087		[learning rate: 0.0014243]
	Learning Rate: 0.0014243
	LOSS [training: 0.2958808116087087 | validation: 0.3139207915941967]
	TIME [epoch: 10.6 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20479624794699153		[learning rate: 0.0014199]
	Learning Rate: 0.00141993
	LOSS [training: 0.20479624794699153 | validation: 0.3893839189630528]
	TIME [epoch: 10.6 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3314382236791752		[learning rate: 0.0014156]
	Learning Rate: 0.00141558
	LOSS [training: 0.3314382236791752 | validation: 0.29410501294779073]
	TIME [epoch: 10.6 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24707173379729305		[learning rate: 0.0014112]
	Learning Rate: 0.00141124
	LOSS [training: 0.24707173379729305 | validation: 0.30167473200183287]
	TIME [epoch: 10.6 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2749447388700613		[learning rate: 0.0014069]
	Learning Rate: 0.00140691
	LOSS [training: 0.2749447388700613 | validation: 0.21935309915764614]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1139.pth
	Model improved!!!
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2937026252816618		[learning rate: 0.0014026]
	Learning Rate: 0.0014026
	LOSS [training: 0.2937026252816618 | validation: 0.37140740719895143]
	TIME [epoch: 10.6 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23551542429889802		[learning rate: 0.0013983]
	Learning Rate: 0.0013983
	LOSS [training: 0.23551542429889802 | validation: 0.3241079077256221]
	TIME [epoch: 10.6 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26647309144631315		[learning rate: 0.001394]
	Learning Rate: 0.00139401
	LOSS [training: 0.26647309144631315 | validation: 0.33036207989545957]
	TIME [epoch: 10.6 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2573192231516749		[learning rate: 0.0013897]
	Learning Rate: 0.00138974
	LOSS [training: 0.2573192231516749 | validation: 0.266359007795912]
	TIME [epoch: 10.6 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21830847036084472		[learning rate: 0.0013855]
	Learning Rate: 0.00138548
	LOSS [training: 0.21830847036084472 | validation: 0.26460906359651537]
	TIME [epoch: 10.6 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24856733055419927		[learning rate: 0.0013812]
	Learning Rate: 0.00138123
	LOSS [training: 0.24856733055419927 | validation: 0.32677904669779817]
	TIME [epoch: 10.6 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3007314763737123		[learning rate: 0.001377]
	Learning Rate: 0.001377
	LOSS [training: 0.3007314763737123 | validation: 0.36282905468022836]
	TIME [epoch: 10.6 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23063419364120802		[learning rate: 0.0013728]
	Learning Rate: 0.00137278
	LOSS [training: 0.23063419364120802 | validation: 0.4435182294268614]
	TIME [epoch: 10.6 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.291437669533832		[learning rate: 0.0013686]
	Learning Rate: 0.00136857
	LOSS [training: 0.291437669533832 | validation: 0.4050295210193208]
	TIME [epoch: 10.6 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.226102120779143		[learning rate: 0.0013644]
	Learning Rate: 0.00136437
	LOSS [training: 0.226102120779143 | validation: 0.3799438214683605]
	TIME [epoch: 10.6 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24062678400722853		[learning rate: 0.0013602]
	Learning Rate: 0.00136019
	LOSS [training: 0.24062678400722853 | validation: 0.39027856916769793]
	TIME [epoch: 10.6 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26969867855270985		[learning rate: 0.001356]
	Learning Rate: 0.00135602
	LOSS [training: 0.26969867855270985 | validation: 0.33406137677622155]
	TIME [epoch: 10.6 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20166146505520105		[learning rate: 0.0013519]
	Learning Rate: 0.00135187
	LOSS [training: 0.20166146505520105 | validation: 0.32862730730106293]
	TIME [epoch: 10.5 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2231557659743933		[learning rate: 0.0013477]
	Learning Rate: 0.00134772
	LOSS [training: 0.2231557659743933 | validation: 0.29567532236172667]
	TIME [epoch: 10.6 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2073852837566644		[learning rate: 0.0013436]
	Learning Rate: 0.00134359
	LOSS [training: 0.2073852837566644 | validation: 0.4020373867789638]
	TIME [epoch: 10.6 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26553693821746666		[learning rate: 0.0013395]
	Learning Rate: 0.00133947
	LOSS [training: 0.26553693821746666 | validation: 0.27347049531086337]
	TIME [epoch: 10.6 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2095681219381933		[learning rate: 0.0013354]
	Learning Rate: 0.00133536
	LOSS [training: 0.2095681219381933 | validation: 0.31346154784702257]
	TIME [epoch: 10.6 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21785335915647833		[learning rate: 0.0013313]
	Learning Rate: 0.00133127
	LOSS [training: 0.21785335915647833 | validation: 0.2641214005971782]
	TIME [epoch: 10.6 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19903407584604627		[learning rate: 0.0013272]
	Learning Rate: 0.00132719
	LOSS [training: 0.19903407584604627 | validation: 0.44075318046012124]
	TIME [epoch: 10.6 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2742869247093559		[learning rate: 0.0013231]
	Learning Rate: 0.00132312
	LOSS [training: 0.2742869247093559 | validation: 0.24691871878355245]
	TIME [epoch: 10.6 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22159434248812268		[learning rate: 0.0013191]
	Learning Rate: 0.00131907
	LOSS [training: 0.22159434248812268 | validation: 0.30076727202939346]
	TIME [epoch: 10.6 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21487101975369421		[learning rate: 0.001315]
	Learning Rate: 0.00131502
	LOSS [training: 0.21487101975369421 | validation: 0.2751483287306018]
	TIME [epoch: 10.6 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1821719457182072		[learning rate: 0.001311]
	Learning Rate: 0.00131099
	LOSS [training: 0.1821719457182072 | validation: 0.2211099369161822]
	TIME [epoch: 10.6 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18176468583367975		[learning rate: 0.001307]
	Learning Rate: 0.00130697
	LOSS [training: 0.18176468583367975 | validation: 0.20826093711043117]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1163.pth
	Model improved!!!
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24101532835896333		[learning rate: 0.001303]
	Learning Rate: 0.00130297
	LOSS [training: 0.24101532835896333 | validation: 0.3530279349364747]
	TIME [epoch: 10.6 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.218977068522822		[learning rate: 0.001299]
	Learning Rate: 0.00129897
	LOSS [training: 0.218977068522822 | validation: 0.22527484063162745]
	TIME [epoch: 10.6 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2515195612116678		[learning rate: 0.001295]
	Learning Rate: 0.00129499
	LOSS [training: 0.2515195612116678 | validation: 0.49291178900852334]
	TIME [epoch: 10.6 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3593928883063987		[learning rate: 0.001291]
	Learning Rate: 0.00129102
	LOSS [training: 0.3593928883063987 | validation: 0.24598052058407913]
	TIME [epoch: 10.6 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19438438447590747		[learning rate: 0.0012871]
	Learning Rate: 0.00128706
	LOSS [training: 0.19438438447590747 | validation: 0.3666078289184892]
	TIME [epoch: 10.6 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28202297177971564		[learning rate: 0.0012831]
	Learning Rate: 0.00128312
	LOSS [training: 0.28202297177971564 | validation: 0.24921850024040118]
	TIME [epoch: 10.6 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2385854386013638		[learning rate: 0.0012792]
	Learning Rate: 0.00127918
	LOSS [training: 0.2385854386013638 | validation: 0.3023412472919186]
	TIME [epoch: 10.6 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33755244749315744		[learning rate: 0.0012753]
	Learning Rate: 0.00127526
	LOSS [training: 0.33755244749315744 | validation: 0.31811288310809177]
	TIME [epoch: 10.6 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2487842980359593		[learning rate: 0.0012714]
	Learning Rate: 0.00127135
	LOSS [training: 0.2487842980359593 | validation: 0.34042107633673085]
	TIME [epoch: 10.6 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1907037115731677		[learning rate: 0.0012675]
	Learning Rate: 0.00126746
	LOSS [training: 0.1907037115731677 | validation: 0.2772791629364609]
	TIME [epoch: 10.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21161827711238385		[learning rate: 0.0012636]
	Learning Rate: 0.00126357
	LOSS [training: 0.21161827711238385 | validation: 0.4678388041896671]
	TIME [epoch: 10.6 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29898940272435215		[learning rate: 0.0012597]
	Learning Rate: 0.0012597
	LOSS [training: 0.29898940272435215 | validation: 0.38920183139606507]
	TIME [epoch: 10.6 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22602183104036108		[learning rate: 0.0012558]
	Learning Rate: 0.00125584
	LOSS [training: 0.22602183104036108 | validation: 0.3886495876660973]
	TIME [epoch: 10.6 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25725846201053704		[learning rate: 0.001252]
	Learning Rate: 0.00125199
	LOSS [training: 0.25725846201053704 | validation: 0.3659999351314343]
	TIME [epoch: 10.6 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2531680835749904		[learning rate: 0.0012481]
	Learning Rate: 0.00124815
	LOSS [training: 0.2531680835749904 | validation: 0.2760036933180559]
	TIME [epoch: 10.6 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20459532456062451		[learning rate: 0.0012443]
	Learning Rate: 0.00124432
	LOSS [training: 0.20459532456062451 | validation: 0.4352250257652739]
	TIME [epoch: 10.6 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.282547194962888		[learning rate: 0.0012405]
	Learning Rate: 0.00124051
	LOSS [training: 0.282547194962888 | validation: 0.4083700732198601]
	TIME [epoch: 10.6 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23866704415923903		[learning rate: 0.0012367]
	Learning Rate: 0.00123671
	LOSS [training: 0.23866704415923903 | validation: 0.2643313513370052]
	TIME [epoch: 10.6 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2672219849138787		[learning rate: 0.0012329]
	Learning Rate: 0.00123292
	LOSS [training: 0.2672219849138787 | validation: 0.5580482380674318]
	TIME [epoch: 10.6 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26585477945600056		[learning rate: 0.0012291]
	Learning Rate: 0.00122914
	LOSS [training: 0.26585477945600056 | validation: 0.28367804729769186]
	TIME [epoch: 10.6 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26080436834058957		[learning rate: 0.0012254]
	Learning Rate: 0.00122537
	LOSS [training: 0.26080436834058957 | validation: 0.3104980807806371]
	TIME [epoch: 10.6 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21880917431546593		[learning rate: 0.0012216]
	Learning Rate: 0.00122161
	LOSS [training: 0.21880917431546593 | validation: 0.36287248757873747]
	TIME [epoch: 10.6 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24696091912161564		[learning rate: 0.0012179]
	Learning Rate: 0.00121787
	LOSS [training: 0.24696091912161564 | validation: 0.31512354655182245]
	TIME [epoch: 10.6 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25698292217753776		[learning rate: 0.0012141]
	Learning Rate: 0.00121413
	LOSS [training: 0.25698292217753776 | validation: 0.29351873115981336]
	TIME [epoch: 10.6 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21858654396326269		[learning rate: 0.0012104]
	Learning Rate: 0.00121041
	LOSS [training: 0.21858654396326269 | validation: 0.22179008546099824]
	TIME [epoch: 10.6 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20951967052292453		[learning rate: 0.0012067]
	Learning Rate: 0.0012067
	LOSS [training: 0.20951967052292453 | validation: 0.34556208845203484]
	TIME [epoch: 10.6 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3540085756948603		[learning rate: 0.001203]
	Learning Rate: 0.001203
	LOSS [training: 0.3540085756948603 | validation: 0.3962541346830387]
	TIME [epoch: 10.9 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24156259151444814		[learning rate: 0.0011993]
	Learning Rate: 0.00119932
	LOSS [training: 0.24156259151444814 | validation: 0.20703182445885948]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1191.pth
	Model improved!!!
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23655811864182663		[learning rate: 0.0011956]
	Learning Rate: 0.00119564
	LOSS [training: 0.23655811864182663 | validation: 0.21484040575324279]
	TIME [epoch: 10.6 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21840332320268585		[learning rate: 0.001192]
	Learning Rate: 0.00119197
	LOSS [training: 0.21840332320268585 | validation: 0.26534073773712175]
	TIME [epoch: 10.6 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21544308312757027		[learning rate: 0.0011883]
	Learning Rate: 0.00118832
	LOSS [training: 0.21544308312757027 | validation: 0.31644598274979596]
	TIME [epoch: 10.6 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2393036486630325		[learning rate: 0.0011847]
	Learning Rate: 0.00118468
	LOSS [training: 0.2393036486630325 | validation: 0.41631842098835975]
	TIME [epoch: 10.6 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24483050362900222		[learning rate: 0.001181]
	Learning Rate: 0.00118105
	LOSS [training: 0.24483050362900222 | validation: 0.34301449312886206]
	TIME [epoch: 10.6 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24221003391066936		[learning rate: 0.0011774]
	Learning Rate: 0.00117743
	LOSS [training: 0.24221003391066936 | validation: 0.3610082795162518]
	TIME [epoch: 10.6 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2913566922666807		[learning rate: 0.0011738]
	Learning Rate: 0.00117382
	LOSS [training: 0.2913566922666807 | validation: 0.7774037623953842]
	TIME [epoch: 10.6 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37891698895449355		[learning rate: 0.0011702]
	Learning Rate: 0.00117022
	LOSS [training: 0.37891698895449355 | validation: 0.3098977489973879]
	TIME [epoch: 10.6 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23778742443406714		[learning rate: 0.0011666]
	Learning Rate: 0.00116663
	LOSS [training: 0.23778742443406714 | validation: 0.3317266942259711]
	TIME [epoch: 10.6 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24596139905522824		[learning rate: 0.0011631]
	Learning Rate: 0.00116305
	LOSS [training: 0.24596139905522824 | validation: 0.42764942542138407]
	TIME [epoch: 10.6 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2349261347301952		[learning rate: 0.0011595]
	Learning Rate: 0.00115949
	LOSS [training: 0.2349261347301952 | validation: 0.35882781397052144]
	TIME [epoch: 10.6 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2261565045717484		[learning rate: 0.0011559]
	Learning Rate: 0.00115593
	LOSS [training: 0.2261565045717484 | validation: 0.32521132344656545]
	TIME [epoch: 10.6 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1869154694726153		[learning rate: 0.0011524]
	Learning Rate: 0.00115239
	LOSS [training: 0.1869154694726153 | validation: 0.3044494267724398]
	TIME [epoch: 10.6 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18496602964252898		[learning rate: 0.0011489]
	Learning Rate: 0.00114886
	LOSS [training: 0.18496602964252898 | validation: 0.3642573752632343]
	TIME [epoch: 10.6 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25191469376790265		[learning rate: 0.0011453]
	Learning Rate: 0.00114534
	LOSS [training: 0.25191469376790265 | validation: 0.29773113651900196]
	TIME [epoch: 10.6 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20852700337356317		[learning rate: 0.0011418]
	Learning Rate: 0.00114183
	LOSS [training: 0.20852700337356317 | validation: 0.22141119555784472]
	TIME [epoch: 10.6 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2695625351237778		[learning rate: 0.0011383]
	Learning Rate: 0.00113833
	LOSS [training: 0.2695625351237778 | validation: 0.2161669341338053]
	TIME [epoch: 10.6 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2339898184064803		[learning rate: 0.0011348]
	Learning Rate: 0.00113484
	LOSS [training: 0.2339898184064803 | validation: 0.3087376733609434]
	TIME [epoch: 10.6 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21881011899977584		[learning rate: 0.0011314]
	Learning Rate: 0.00113136
	LOSS [training: 0.21881011899977584 | validation: 0.21595264927373584]
	TIME [epoch: 10.6 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25938462574679916		[learning rate: 0.0011279]
	Learning Rate: 0.00112789
	LOSS [training: 0.25938462574679916 | validation: 0.32584493272161097]
	TIME [epoch: 10.6 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21139249003620045		[learning rate: 0.0011244]
	Learning Rate: 0.00112443
	LOSS [training: 0.21139249003620045 | validation: 0.4660576266430965]
	TIME [epoch: 10.6 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2397281947571877		[learning rate: 0.001121]
	Learning Rate: 0.00112099
	LOSS [training: 0.2397281947571877 | validation: 0.21976132283506367]
	TIME [epoch: 10.6 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17615369967399438		[learning rate: 0.0011175]
	Learning Rate: 0.00111755
	LOSS [training: 0.17615369967399438 | validation: 0.19490690055549034]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1214.pth
	Model improved!!!
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26202612901026345		[learning rate: 0.0011141]
	Learning Rate: 0.00111412
	LOSS [training: 0.26202612901026345 | validation: 0.32615315897441216]
	TIME [epoch: 10.6 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2629257690406158		[learning rate: 0.0011107]
	Learning Rate: 0.00111071
	LOSS [training: 0.2629257690406158 | validation: 0.3138032816029185]
	TIME [epoch: 10.6 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22598044914120705		[learning rate: 0.0011073]
	Learning Rate: 0.0011073
	LOSS [training: 0.22598044914120705 | validation: 0.30813837726958204]
	TIME [epoch: 10.6 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20164754421432973		[learning rate: 0.0011039]
	Learning Rate: 0.00110391
	LOSS [training: 0.20164754421432973 | validation: 0.24227708478341015]
	TIME [epoch: 10.6 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30205714742146095		[learning rate: 0.0011005]
	Learning Rate: 0.00110053
	LOSS [training: 0.30205714742146095 | validation: 0.2455829876100127]
	TIME [epoch: 10.6 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2984793001892228		[learning rate: 0.0010972]
	Learning Rate: 0.00109715
	LOSS [training: 0.2984793001892228 | validation: 0.3319934269369814]
	TIME [epoch: 10.6 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21870931672076915		[learning rate: 0.0010938]
	Learning Rate: 0.00109379
	LOSS [training: 0.21870931672076915 | validation: 0.207052722986179]
	TIME [epoch: 10.6 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1583916184811176		[learning rate: 0.0010904]
	Learning Rate: 0.00109044
	LOSS [training: 0.1583916184811176 | validation: 0.20729766251861734]
	TIME [epoch: 10.6 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19875161456916796		[learning rate: 0.0010871]
	Learning Rate: 0.00108709
	LOSS [training: 0.19875161456916796 | validation: 0.20186076477486212]
	TIME [epoch: 10.6 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21455339218658337		[learning rate: 0.0010838]
	Learning Rate: 0.00108376
	LOSS [training: 0.21455339218658337 | validation: 0.18888050881451968]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1224.pth
	Model improved!!!
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1900666935286973		[learning rate: 0.0010804]
	Learning Rate: 0.00108044
	LOSS [training: 0.1900666935286973 | validation: 0.21952698360590586]
	TIME [epoch: 10.6 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2506684123279589		[learning rate: 0.0010771]
	Learning Rate: 0.00107713
	LOSS [training: 0.2506684123279589 | validation: 0.26649025827373163]
	TIME [epoch: 10.6 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1953353015300065		[learning rate: 0.0010738]
	Learning Rate: 0.00107382
	LOSS [training: 0.1953353015300065 | validation: 0.19598298826291477]
	TIME [epoch: 10.6 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1880606875166006		[learning rate: 0.0010705]
	Learning Rate: 0.00107053
	LOSS [training: 0.1880606875166006 | validation: 0.23901681173941783]
	TIME [epoch: 10.6 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17031070916517127		[learning rate: 0.0010673]
	Learning Rate: 0.00106725
	LOSS [training: 0.17031070916517127 | validation: 0.2289742767748774]
	TIME [epoch: 10.6 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2017095698310186		[learning rate: 0.001064]
	Learning Rate: 0.00106398
	LOSS [training: 0.2017095698310186 | validation: 0.32717949108823385]
	TIME [epoch: 10.6 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18378364922363027		[learning rate: 0.0010607]
	Learning Rate: 0.00106072
	LOSS [training: 0.18378364922363027 | validation: 0.2195946372924274]
	TIME [epoch: 10.6 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2094149419589924		[learning rate: 0.0010575]
	Learning Rate: 0.00105747
	LOSS [training: 0.2094149419589924 | validation: 0.30017825500265743]
	TIME [epoch: 10.6 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19496890143709272		[learning rate: 0.0010542]
	Learning Rate: 0.00105422
	LOSS [training: 0.19496890143709272 | validation: 0.22121823053670883]
	TIME [epoch: 10.6 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36953055653919903		[learning rate: 0.001051]
	Learning Rate: 0.00105099
	LOSS [training: 0.36953055653919903 | validation: 0.3346545445330467]
	TIME [epoch: 10.6 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32644160239933473		[learning rate: 0.0010478]
	Learning Rate: 0.00104777
	LOSS [training: 0.32644160239933473 | validation: 0.3289433531961011]
	TIME [epoch: 10.6 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3156174428002556		[learning rate: 0.0010446]
	Learning Rate: 0.00104456
	LOSS [training: 0.3156174428002556 | validation: 0.2520557820378512]
	TIME [epoch: 10.6 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2179723784664188		[learning rate: 0.0010414]
	Learning Rate: 0.00104136
	LOSS [training: 0.2179723784664188 | validation: 0.2123515103106783]
	TIME [epoch: 10.6 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24476700587487432		[learning rate: 0.0010382]
	Learning Rate: 0.00103817
	LOSS [training: 0.24476700587487432 | validation: 0.2989290046381661]
	TIME [epoch: 10.6 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1798183095549448		[learning rate: 0.001035]
	Learning Rate: 0.00103498
	LOSS [training: 0.1798183095549448 | validation: 0.24349911474108418]
	TIME [epoch: 10.6 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16221198033500725		[learning rate: 0.0010318]
	Learning Rate: 0.00103181
	LOSS [training: 0.16221198033500725 | validation: 0.23113211572105935]
	TIME [epoch: 10.6 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2006649533966777		[learning rate: 0.0010286]
	Learning Rate: 0.00102865
	LOSS [training: 0.2006649533966777 | validation: 0.3476332684360608]
	TIME [epoch: 10.6 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1978715068372316		[learning rate: 0.0010255]
	Learning Rate: 0.00102549
	LOSS [training: 0.1978715068372316 | validation: 0.24799752044884343]
	TIME [epoch: 10.6 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19432177036543372		[learning rate: 0.0010224]
	Learning Rate: 0.00102235
	LOSS [training: 0.19432177036543372 | validation: 0.2282968496300475]
	TIME [epoch: 10.6 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20415075552035772		[learning rate: 0.0010192]
	Learning Rate: 0.00101922
	LOSS [training: 0.20415075552035772 | validation: 0.23329054545454375]
	TIME [epoch: 10.6 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22352924906598398		[learning rate: 0.0010161]
	Learning Rate: 0.00101609
	LOSS [training: 0.22352924906598398 | validation: 0.2921665438254717]
	TIME [epoch: 10.6 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24149451031392855		[learning rate: 0.001013]
	Learning Rate: 0.00101298
	LOSS [training: 0.24149451031392855 | validation: 0.21101502992964732]
	TIME [epoch: 10.6 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15067429144189526		[learning rate: 0.0010099]
	Learning Rate: 0.00100987
	LOSS [training: 0.15067429144189526 | validation: 0.25200853921844885]
	TIME [epoch: 10.6 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17529290254879926		[learning rate: 0.0010068]
	Learning Rate: 0.00100678
	LOSS [training: 0.17529290254879926 | validation: 0.2763441429120912]
	TIME [epoch: 10.6 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2135400218778703		[learning rate: 0.0010037]
	Learning Rate: 0.00100369
	LOSS [training: 0.2135400218778703 | validation: 0.2522039644699623]
	TIME [epoch: 10.6 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27593907662420597		[learning rate: 0.0010006]
	Learning Rate: 0.00100061
	LOSS [training: 0.27593907662420597 | validation: 0.21435268380385972]
	TIME [epoch: 10.6 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19992958263309696		[learning rate: 0.00099755]
	Learning Rate: 0.000997547
	LOSS [training: 0.19992958263309696 | validation: 0.2622252767837073]
	TIME [epoch: 10.6 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1975847642396718		[learning rate: 0.00099449]
	Learning Rate: 0.000994489
	LOSS [training: 0.1975847642396718 | validation: 0.31247151292660985]
	TIME [epoch: 10.6 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18382878210004996		[learning rate: 0.00099144]
	Learning Rate: 0.00099144
	LOSS [training: 0.18382878210004996 | validation: 0.31888426994059094]
	TIME [epoch: 10.6 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2777932730778279		[learning rate: 0.0009884]
	Learning Rate: 0.000988401
	LOSS [training: 0.2777932730778279 | validation: 0.30145380333638017]
	TIME [epoch: 10.6 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2109527666030903		[learning rate: 0.00098537]
	Learning Rate: 0.000985371
	LOSS [training: 0.2109527666030903 | validation: 0.5432086990955948]
	TIME [epoch: 10.6 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23643454362721336		[learning rate: 0.00098235]
	Learning Rate: 0.000982351
	LOSS [training: 0.23643454362721336 | validation: 0.3222828995615529]
	TIME [epoch: 10.6 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2804338830353199		[learning rate: 0.00097934]
	Learning Rate: 0.000979339
	LOSS [training: 0.2804338830353199 | validation: 0.4571691211481141]
	TIME [epoch: 10.6 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.340856743071542		[learning rate: 0.00097634]
	Learning Rate: 0.000976337
	LOSS [training: 0.340856743071542 | validation: 0.4776086454858609]
	TIME [epoch: 10.6 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2634828494339607		[learning rate: 0.00097334]
	Learning Rate: 0.000973345
	LOSS [training: 0.2634828494339607 | validation: 0.3608837372032287]
	TIME [epoch: 10.6 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21323892673677855		[learning rate: 0.00097036]
	Learning Rate: 0.000970361
	LOSS [training: 0.21323892673677855 | validation: 0.2933414472543432]
	TIME [epoch: 10.6 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24714987380737422		[learning rate: 0.00096739]
	Learning Rate: 0.000967386
	LOSS [training: 0.24714987380737422 | validation: 0.42101898212869104]
	TIME [epoch: 10.6 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2714252688046349		[learning rate: 0.00096442]
	Learning Rate: 0.000964421
	LOSS [training: 0.2714252688046349 | validation: 0.3866187942147171]
	TIME [epoch: 10.6 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2469682459441509		[learning rate: 0.00096146]
	Learning Rate: 0.000961464
	LOSS [training: 0.2469682459441509 | validation: 0.3007529602974535]
	TIME [epoch: 10.6 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2077485639282241		[learning rate: 0.00095852]
	Learning Rate: 0.000958517
	LOSS [training: 0.2077485639282241 | validation: 0.4552268470809993]
	TIME [epoch: 10.6 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3372998253909416		[learning rate: 0.00095558]
	Learning Rate: 0.000955579
	LOSS [training: 0.3372998253909416 | validation: 0.8569496212251992]
	TIME [epoch: 10.6 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4412889381370701		[learning rate: 0.00095265]
	Learning Rate: 0.00095265
	LOSS [training: 0.4412889381370701 | validation: 0.26089543104752694]
	TIME [epoch: 10.6 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19270030632614388		[learning rate: 0.00094973]
	Learning Rate: 0.00094973
	LOSS [training: 0.19270030632614388 | validation: 0.33716282986242524]
	TIME [epoch: 10.6 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22023384638505292		[learning rate: 0.00094682]
	Learning Rate: 0.000946818
	LOSS [training: 0.22023384638505292 | validation: 0.27300748961484045]
	TIME [epoch: 10.6 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20533545232339634		[learning rate: 0.00094392]
	Learning Rate: 0.000943916
	LOSS [training: 0.20533545232339634 | validation: 0.31182129330312397]
	TIME [epoch: 10.6 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18001406497700717		[learning rate: 0.00094102]
	Learning Rate: 0.000941023
	LOSS [training: 0.18001406497700717 | validation: 0.19573045541487077]
	TIME [epoch: 10.6 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22165929913701166		[learning rate: 0.00093814]
	Learning Rate: 0.000938138
	LOSS [training: 0.22165929913701166 | validation: 0.28806648193029655]
	TIME [epoch: 10.6 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15511269557196836		[learning rate: 0.00093526]
	Learning Rate: 0.000935262
	LOSS [training: 0.15511269557196836 | validation: 0.25348868313604667]
	TIME [epoch: 10.6 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23420838171655758		[learning rate: 0.0009324]
	Learning Rate: 0.000932395
	LOSS [training: 0.23420838171655758 | validation: 0.3362098535068843]
	TIME [epoch: 10.6 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2356940480845373		[learning rate: 0.00092954]
	Learning Rate: 0.000929537
	LOSS [training: 0.2356940480845373 | validation: 0.2910251375259019]
	TIME [epoch: 10.6 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18694742053163582		[learning rate: 0.00092669]
	Learning Rate: 0.000926688
	LOSS [training: 0.18694742053163582 | validation: 0.30064690856217036]
	TIME [epoch: 10.6 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21062394378497684		[learning rate: 0.00092385]
	Learning Rate: 0.000923847
	LOSS [training: 0.21062394378497684 | validation: 0.33217605048182647]
	TIME [epoch: 10.6 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21913530825170674		[learning rate: 0.00092101]
	Learning Rate: 0.000921015
	LOSS [training: 0.21913530825170674 | validation: 0.38276075756450495]
	TIME [epoch: 10.6 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2177895520935559		[learning rate: 0.00091819]
	Learning Rate: 0.000918192
	LOSS [training: 0.2177895520935559 | validation: 0.2972503654177555]
	TIME [epoch: 10.6 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1778308920684778		[learning rate: 0.00091538]
	Learning Rate: 0.000915377
	LOSS [training: 0.1778308920684778 | validation: 0.3856054316835386]
	TIME [epoch: 10.6 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25056137849222154		[learning rate: 0.00091257]
	Learning Rate: 0.000912571
	LOSS [training: 0.25056137849222154 | validation: 0.3432837062913529]
	TIME [epoch: 10.6 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18926891369603846		[learning rate: 0.00090977]
	Learning Rate: 0.000909774
	LOSS [training: 0.18926891369603846 | validation: 0.3650494679974007]
	TIME [epoch: 10.6 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19944740594001123		[learning rate: 0.00090698]
	Learning Rate: 0.000906985
	LOSS [training: 0.19944740594001123 | validation: 0.2987646745845711]
	TIME [epoch: 10.6 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17256849000352562		[learning rate: 0.0009042]
	Learning Rate: 0.000904204
	LOSS [training: 0.17256849000352562 | validation: 0.31247717542609144]
	TIME [epoch: 10.6 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18927536783895893		[learning rate: 0.00090143]
	Learning Rate: 0.000901433
	LOSS [training: 0.18927536783895893 | validation: 0.3196095198360281]
	TIME [epoch: 10.6 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17726636795444123		[learning rate: 0.00089867]
	Learning Rate: 0.000898669
	LOSS [training: 0.17726636795444123 | validation: 0.3217116812191299]
	TIME [epoch: 10.6 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.201274370994122		[learning rate: 0.00089591]
	Learning Rate: 0.000895915
	LOSS [training: 0.201274370994122 | validation: 0.24812142773376417]
	TIME [epoch: 10.6 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16931665541634838		[learning rate: 0.00089317]
	Learning Rate: 0.000893168
	LOSS [training: 0.16931665541634838 | validation: 0.30404657318448713]
	TIME [epoch: 10.6 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2038186359702367		[learning rate: 0.00089043]
	Learning Rate: 0.00089043
	LOSS [training: 0.2038186359702367 | validation: 0.24530056517687857]
	TIME [epoch: 10.6 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.177708301165031		[learning rate: 0.0008877]
	Learning Rate: 0.000887701
	LOSS [training: 0.177708301165031 | validation: 0.26013774122834493]
	TIME [epoch: 10.6 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18218951104340078		[learning rate: 0.00088498]
	Learning Rate: 0.00088498
	LOSS [training: 0.18218951104340078 | validation: 0.3218921863890759]
	TIME [epoch: 10.6 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17691385042838062		[learning rate: 0.00088227]
	Learning Rate: 0.000882267
	LOSS [training: 0.17691385042838062 | validation: 0.32841659247859056]
	TIME [epoch: 10.6 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15433289944888046		[learning rate: 0.00087956]
	Learning Rate: 0.000879562
	LOSS [training: 0.15433289944888046 | validation: 0.28299355902493134]
	TIME [epoch: 10.6 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17997654650117886		[learning rate: 0.00087687]
	Learning Rate: 0.000876866
	LOSS [training: 0.17997654650117886 | validation: 0.5288278776140656]
	TIME [epoch: 10.6 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28598618916282936		[learning rate: 0.00087418]
	Learning Rate: 0.000874178
	LOSS [training: 0.28598618916282936 | validation: 0.25267336731175954]
	TIME [epoch: 10.6 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15924125778170115		[learning rate: 0.0008715]
	Learning Rate: 0.000871498
	LOSS [training: 0.15924125778170115 | validation: 0.27425220192420774]
	TIME [epoch: 10.6 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19398110025637377		[learning rate: 0.00086883]
	Learning Rate: 0.000868827
	LOSS [training: 0.19398110025637377 | validation: 0.3011342912580108]
	TIME [epoch: 10.6 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21592813902098257		[learning rate: 0.00086616]
	Learning Rate: 0.000866164
	LOSS [training: 0.21592813902098257 | validation: 0.44019152937250805]
	TIME [epoch: 10.6 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3490475729059071		[learning rate: 0.00086351]
	Learning Rate: 0.000863509
	LOSS [training: 0.3490475729059071 | validation: 0.8328849466218116]
	TIME [epoch: 10.6 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.267143559231081		[learning rate: 0.00086086]
	Learning Rate: 0.000860861
	LOSS [training: 0.267143559231081 | validation: 0.31188123276884566]
	TIME [epoch: 10.6 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18996552902077476		[learning rate: 0.00085822]
	Learning Rate: 0.000858223
	LOSS [training: 0.18996552902077476 | validation: 0.31453301099559045]
	TIME [epoch: 10.6 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19831549300672896		[learning rate: 0.00085559]
	Learning Rate: 0.000855592
	LOSS [training: 0.19831549300672896 | validation: 0.3227998868783593]
	TIME [epoch: 10.6 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18481022497734872		[learning rate: 0.00085297]
	Learning Rate: 0.000852969
	LOSS [training: 0.18481022497734872 | validation: 0.28582406753945866]
	TIME [epoch: 10.6 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18112254771159278		[learning rate: 0.00085035]
	Learning Rate: 0.000850354
	LOSS [training: 0.18112254771159278 | validation: 0.3437163485617934]
	TIME [epoch: 10.6 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1946560379527627		[learning rate: 0.00084775]
	Learning Rate: 0.000847748
	LOSS [training: 0.1946560379527627 | validation: 0.2897439831223518]
	TIME [epoch: 10.6 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15906256120340842		[learning rate: 0.00084515]
	Learning Rate: 0.000845149
	LOSS [training: 0.15906256120340842 | validation: 0.20461663083052395]
	TIME [epoch: 10.6 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14967955116279374		[learning rate: 0.00084256]
	Learning Rate: 0.000842558
	LOSS [training: 0.14967955116279374 | validation: 0.23315388082586339]
	TIME [epoch: 10.6 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1689656337029953		[learning rate: 0.00083998]
	Learning Rate: 0.000839976
	LOSS [training: 0.1689656337029953 | validation: 0.30316231206766664]
	TIME [epoch: 10.6 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15017374655699842		[learning rate: 0.0008374]
	Learning Rate: 0.000837401
	LOSS [training: 0.15017374655699842 | validation: 0.2857123829432159]
	TIME [epoch: 10.6 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14936518667220325		[learning rate: 0.00083483]
	Learning Rate: 0.000834834
	LOSS [training: 0.14936518667220325 | validation: 0.3008482234631706]
	TIME [epoch: 10.6 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17600150390287178		[learning rate: 0.00083227]
	Learning Rate: 0.000832274
	LOSS [training: 0.17600150390287178 | validation: 0.3407439380269268]
	TIME [epoch: 10.6 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17626557782227795		[learning rate: 0.00082972]
	Learning Rate: 0.000829723
	LOSS [training: 0.17626557782227795 | validation: 0.2554053471526236]
	TIME [epoch: 10.6 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16291012940533367		[learning rate: 0.00082718]
	Learning Rate: 0.00082718
	LOSS [training: 0.16291012940533367 | validation: 0.3890166667050602]
	TIME [epoch: 10.6 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2789688142201219		[learning rate: 0.00082464]
	Learning Rate: 0.000824644
	LOSS [training: 0.2789688142201219 | validation: 0.3432681535507793]
	TIME [epoch: 10.6 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32591009384757713		[learning rate: 0.00082212]
	Learning Rate: 0.000822116
	LOSS [training: 0.32591009384757713 | validation: 0.4037383218243767]
	TIME [epoch: 10.6 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18526379665962245		[learning rate: 0.0008196]
	Learning Rate: 0.000819596
	LOSS [training: 0.18526379665962245 | validation: 0.31978162825358336]
	TIME [epoch: 10.6 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1935306839598923		[learning rate: 0.00081708]
	Learning Rate: 0.000817084
	LOSS [training: 0.1935306839598923 | validation: 0.2805211977815287]
	TIME [epoch: 10.6 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1882819767910509		[learning rate: 0.00081458]
	Learning Rate: 0.000814579
	LOSS [training: 0.1882819767910509 | validation: 0.3328901975077016]
	TIME [epoch: 10.6 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23367569454273976		[learning rate: 0.00081208]
	Learning Rate: 0.000812082
	LOSS [training: 0.23367569454273976 | validation: 0.36626419011915157]
	TIME [epoch: 10.6 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26379671072301036		[learning rate: 0.00080959]
	Learning Rate: 0.000809593
	LOSS [training: 0.26379671072301036 | validation: 0.5038582379165486]
	TIME [epoch: 10.6 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31381804696141435		[learning rate: 0.00080711]
	Learning Rate: 0.000807111
	LOSS [training: 0.31381804696141435 | validation: 0.373548146104569]
	TIME [epoch: 10.6 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22656332561947568		[learning rate: 0.00080464]
	Learning Rate: 0.000804637
	LOSS [training: 0.22656332561947568 | validation: 0.29878763429744454]
	TIME [epoch: 10.6 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22139560821231888		[learning rate: 0.00080217]
	Learning Rate: 0.00080217
	LOSS [training: 0.22139560821231888 | validation: 0.26132658446305007]
	TIME [epoch: 10.6 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1720191711550124		[learning rate: 0.00079971]
	Learning Rate: 0.000799712
	LOSS [training: 0.1720191711550124 | validation: 0.29955786643766613]
	TIME [epoch: 10.6 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15806539164032204		[learning rate: 0.00079726]
	Learning Rate: 0.00079726
	LOSS [training: 0.15806539164032204 | validation: 0.28655461435403984]
	TIME [epoch: 10.6 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16530547788344457		[learning rate: 0.00079482]
	Learning Rate: 0.000794816
	LOSS [training: 0.16530547788344457 | validation: 0.3625019589865665]
	TIME [epoch: 10.6 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18459888936790209		[learning rate: 0.00079238]
	Learning Rate: 0.00079238
	LOSS [training: 0.18459888936790209 | validation: 0.29757835434349406]
	TIME [epoch: 10.6 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18141120287162163		[learning rate: 0.00078995]
	Learning Rate: 0.000789951
	LOSS [training: 0.18141120287162163 | validation: 0.36226407205617195]
	TIME [epoch: 10.6 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17251239572280525		[learning rate: 0.00078753]
	Learning Rate: 0.000787529
	LOSS [training: 0.17251239572280525 | validation: 0.24952521991870313]
	TIME [epoch: 10.6 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21843268431232796		[learning rate: 0.00078512]
	Learning Rate: 0.000785115
	LOSS [training: 0.21843268431232796 | validation: 0.34257661537774964]
	TIME [epoch: 10.6 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22665224653650742		[learning rate: 0.00078271]
	Learning Rate: 0.000782708
	LOSS [training: 0.22665224653650742 | validation: 0.33439946822361577]
	TIME [epoch: 10.6 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2920102327332632		[learning rate: 0.00078031]
	Learning Rate: 0.000780309
	LOSS [training: 0.2920102327332632 | validation: 0.2670706608677251]
	TIME [epoch: 10.6 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1666797377034028		[learning rate: 0.00077792]
	Learning Rate: 0.000777917
	LOSS [training: 0.1666797377034028 | validation: 0.21774526841739922]
	TIME [epoch: 10.6 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3123987525234796		[learning rate: 0.00077553]
	Learning Rate: 0.000775533
	LOSS [training: 0.3123987525234796 | validation: 0.23869351380554613]
	TIME [epoch: 10.6 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20336612024395384		[learning rate: 0.00077316]
	Learning Rate: 0.000773155
	LOSS [training: 0.20336612024395384 | validation: 0.2308536346534873]
	TIME [epoch: 10.6 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1493707185600352		[learning rate: 0.00077079]
	Learning Rate: 0.000770785
	LOSS [training: 0.1493707185600352 | validation: 0.25602045179550864]
	TIME [epoch: 10.6 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1834190359896866		[learning rate: 0.00076842]
	Learning Rate: 0.000768422
	LOSS [training: 0.1834190359896866 | validation: 0.27426217296023625]
	TIME [epoch: 10.6 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18981456114480225		[learning rate: 0.00076607]
	Learning Rate: 0.000766067
	LOSS [training: 0.18981456114480225 | validation: 0.3291225719518272]
	TIME [epoch: 10.6 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23843869415129876		[learning rate: 0.00076372]
	Learning Rate: 0.000763719
	LOSS [training: 0.23843869415129876 | validation: 0.4423576837089949]
	TIME [epoch: 10.6 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24644939800076404		[learning rate: 0.00076138]
	Learning Rate: 0.000761377
	LOSS [training: 0.24644939800076404 | validation: 0.29922092721063925]
	TIME [epoch: 10.6 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1834433079555646		[learning rate: 0.00075904]
	Learning Rate: 0.000759043
	LOSS [training: 0.1834433079555646 | validation: 0.314865165788089]
	TIME [epoch: 10.6 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14169150226065658		[learning rate: 0.00075672]
	Learning Rate: 0.000756717
	LOSS [training: 0.14169150226065658 | validation: 0.30673407852620305]
	TIME [epoch: 10.6 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17471380676342257		[learning rate: 0.0007544]
	Learning Rate: 0.000754397
	LOSS [training: 0.17471380676342257 | validation: 0.402074563996684]
	TIME [epoch: 10.6 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17135023342041447		[learning rate: 0.00075208]
	Learning Rate: 0.000752084
	LOSS [training: 0.17135023342041447 | validation: 0.29402994036161917]
	TIME [epoch: 10.6 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16573088791762836		[learning rate: 0.00074978]
	Learning Rate: 0.000749779
	LOSS [training: 0.16573088791762836 | validation: 0.2527930826817235]
	TIME [epoch: 10.6 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1619697106375996		[learning rate: 0.00074748]
	Learning Rate: 0.000747481
	LOSS [training: 0.1619697106375996 | validation: 0.1771466827842259]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1345.pth
	Model improved!!!
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18073242013685045		[learning rate: 0.00074519]
	Learning Rate: 0.000745189
	LOSS [training: 0.18073242013685045 | validation: 0.2032883056389925]
	TIME [epoch: 10.6 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16310372205919782		[learning rate: 0.0007429]
	Learning Rate: 0.000742905
	LOSS [training: 0.16310372205919782 | validation: 0.217381429169881]
	TIME [epoch: 10.6 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14838546453197107		[learning rate: 0.00074063]
	Learning Rate: 0.000740628
	LOSS [training: 0.14838546453197107 | validation: 0.18253882483426778]
	TIME [epoch: 10.6 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18317872138526967		[learning rate: 0.00073836]
	Learning Rate: 0.000738357
	LOSS [training: 0.18317872138526967 | validation: 0.2162099171193249]
	TIME [epoch: 10.6 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16239518053562607		[learning rate: 0.00073609]
	Learning Rate: 0.000736094
	LOSS [training: 0.16239518053562607 | validation: 0.21196384983925956]
	TIME [epoch: 10.6 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23831853405005204		[learning rate: 0.00073384]
	Learning Rate: 0.000733838
	LOSS [training: 0.23831853405005204 | validation: 0.17607299398859041]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1351.pth
	Model improved!!!
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18736659195673738		[learning rate: 0.00073159]
	Learning Rate: 0.000731588
	LOSS [training: 0.18736659195673738 | validation: 0.19156558280714805]
	TIME [epoch: 10.6 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19357373003971634		[learning rate: 0.00072935]
	Learning Rate: 0.000729345
	LOSS [training: 0.19357373003971634 | validation: 0.18947865524776875]
	TIME [epoch: 10.6 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17750493288005734		[learning rate: 0.00072711]
	Learning Rate: 0.00072711
	LOSS [training: 0.17750493288005734 | validation: 0.2067138121465384]
	TIME [epoch: 10.6 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30620053814774073		[learning rate: 0.00072488]
	Learning Rate: 0.000724881
	LOSS [training: 0.30620053814774073 | validation: 0.3191049596029985]
	TIME [epoch: 10.6 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28563743567322525		[learning rate: 0.00072266]
	Learning Rate: 0.000722659
	LOSS [training: 0.28563743567322525 | validation: 0.2788339319350931]
	TIME [epoch: 10.6 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2264694408601887		[learning rate: 0.00072044]
	Learning Rate: 0.000720444
	LOSS [training: 0.2264694408601887 | validation: 0.19279845908161433]
	TIME [epoch: 10.6 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18315076687401194		[learning rate: 0.00071824]
	Learning Rate: 0.000718235
	LOSS [training: 0.18315076687401194 | validation: 0.1780947855825464]
	TIME [epoch: 10.6 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13933363022685263		[learning rate: 0.00071603]
	Learning Rate: 0.000716033
	LOSS [training: 0.13933363022685263 | validation: 0.21047238246475475]
	TIME [epoch: 10.6 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14167461604248413		[learning rate: 0.00071384]
	Learning Rate: 0.000713839
	LOSS [training: 0.14167461604248413 | validation: 0.2016648417735761]
	TIME [epoch: 10.6 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14081087925915875		[learning rate: 0.00071165]
	Learning Rate: 0.00071165
	LOSS [training: 0.14081087925915875 | validation: 0.2027243318727323]
	TIME [epoch: 10.6 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.139641460594995		[learning rate: 0.00070947]
	Learning Rate: 0.000709469
	LOSS [training: 0.139641460594995 | validation: 0.2557488483998176]
	TIME [epoch: 10.6 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.191803320200492		[learning rate: 0.00070729]
	Learning Rate: 0.000707294
	LOSS [training: 0.191803320200492 | validation: 0.27454092892337467]
	TIME [epoch: 10.6 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13019700097068987		[learning rate: 0.00070513]
	Learning Rate: 0.000705126
	LOSS [training: 0.13019700097068987 | validation: 0.230386593825979]
	TIME [epoch: 10.6 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15033215904403058		[learning rate: 0.00070296]
	Learning Rate: 0.000702964
	LOSS [training: 0.15033215904403058 | validation: 0.29950579739671573]
	TIME [epoch: 10.6 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15940328422240796		[learning rate: 0.00070081]
	Learning Rate: 0.00070081
	LOSS [training: 0.15940328422240796 | validation: 0.20660588044046777]
	TIME [epoch: 10.6 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17176426110026555		[learning rate: 0.00069866]
	Learning Rate: 0.000698661
	LOSS [training: 0.17176426110026555 | validation: 0.2844231844281863]
	TIME [epoch: 10.6 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16112174675594634		[learning rate: 0.00069652]
	Learning Rate: 0.000696519
	LOSS [training: 0.16112174675594634 | validation: 0.2617112876664642]
	TIME [epoch: 10.6 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14131307783644587		[learning rate: 0.00069438]
	Learning Rate: 0.000694384
	LOSS [training: 0.14131307783644587 | validation: 0.189822195459115]
	TIME [epoch: 10.6 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19705276411169392		[learning rate: 0.00069226]
	Learning Rate: 0.000692256
	LOSS [training: 0.19705276411169392 | validation: 0.20114643819404657]
	TIME [epoch: 10.6 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15105128012361257		[learning rate: 0.00069013]
	Learning Rate: 0.000690134
	LOSS [training: 0.15105128012361257 | validation: 0.17269618540694542]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1371.pth
	Model improved!!!
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13528149878685422		[learning rate: 0.00068802]
	Learning Rate: 0.000688018
	LOSS [training: 0.13528149878685422 | validation: 0.20897350928166483]
	TIME [epoch: 10.6 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22933483616065478		[learning rate: 0.00068591]
	Learning Rate: 0.000685909
	LOSS [training: 0.22933483616065478 | validation: 0.2386007125321628]
	TIME [epoch: 10.6 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19296441799581032		[learning rate: 0.00068381]
	Learning Rate: 0.000683807
	LOSS [training: 0.19296441799581032 | validation: 0.19419677123791393]
	TIME [epoch: 10.6 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1852288865078009		[learning rate: 0.00068171]
	Learning Rate: 0.000681711
	LOSS [training: 0.1852288865078009 | validation: 0.5097578774925505]
	TIME [epoch: 10.6 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21536544441083358		[learning rate: 0.00067962]
	Learning Rate: 0.000679621
	LOSS [training: 0.21536544441083358 | validation: 0.20585448722104077]
	TIME [epoch: 10.6 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.120869744204016		[learning rate: 0.00067754]
	Learning Rate: 0.000677538
	LOSS [training: 0.120869744204016 | validation: 0.16825496801889425]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1377.pth
	Model improved!!!
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14866498353040425		[learning rate: 0.00067546]
	Learning Rate: 0.000675461
	LOSS [training: 0.14866498353040425 | validation: 0.23988663795496673]
	TIME [epoch: 10.6 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15603807729632796		[learning rate: 0.00067339]
	Learning Rate: 0.00067339
	LOSS [training: 0.15603807729632796 | validation: 0.18054762153030626]
	TIME [epoch: 10.6 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15762192154393598		[learning rate: 0.00067133]
	Learning Rate: 0.000671326
	LOSS [training: 0.15762192154393598 | validation: 0.2747370942992965]
	TIME [epoch: 10.6 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16453408654662297		[learning rate: 0.00066927]
	Learning Rate: 0.000669268
	LOSS [training: 0.16453408654662297 | validation: 0.21733788445034868]
	TIME [epoch: 10.6 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1277751471772502		[learning rate: 0.00066722]
	Learning Rate: 0.000667216
	LOSS [training: 0.1277751471772502 | validation: 0.2752278426226048]
	TIME [epoch: 10.6 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23644883721773216		[learning rate: 0.00066517]
	Learning Rate: 0.000665171
	LOSS [training: 0.23644883721773216 | validation: 0.23693070480463213]
	TIME [epoch: 10.6 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13764487281622		[learning rate: 0.00066313]
	Learning Rate: 0.000663132
	LOSS [training: 0.13764487281622 | validation: 0.2594386280359046]
	TIME [epoch: 10.6 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18530978136845147		[learning rate: 0.0006611]
	Learning Rate: 0.000661099
	LOSS [training: 0.18530978136845147 | validation: 0.41489610672967703]
	TIME [epoch: 10.6 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14439983366582432		[learning rate: 0.00065907]
	Learning Rate: 0.000659073
	LOSS [training: 0.14439983366582432 | validation: 0.20519802424406913]
	TIME [epoch: 10.6 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1278123580918213		[learning rate: 0.00065705]
	Learning Rate: 0.000657052
	LOSS [training: 0.1278123580918213 | validation: 0.19228020248793193]
	TIME [epoch: 10.6 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12229565355544443		[learning rate: 0.00065504]
	Learning Rate: 0.000655038
	LOSS [training: 0.12229565355544443 | validation: 0.23213590802978368]
	TIME [epoch: 10.6 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22982695264257763		[learning rate: 0.00065303]
	Learning Rate: 0.00065303
	LOSS [training: 0.22982695264257763 | validation: 0.2939963258442521]
	TIME [epoch: 10.6 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15135626757316617		[learning rate: 0.00065103]
	Learning Rate: 0.000651028
	LOSS [training: 0.15135626757316617 | validation: 0.2278190531635262]
	TIME [epoch: 10.6 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12803603263749813		[learning rate: 0.00064903]
	Learning Rate: 0.000649033
	LOSS [training: 0.12803603263749813 | validation: 0.1953143558751516]
	TIME [epoch: 10.6 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23252748436877732		[learning rate: 0.00064704]
	Learning Rate: 0.000647043
	LOSS [training: 0.23252748436877732 | validation: 0.2327032472173969]
	TIME [epoch: 10.6 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1518962598594465		[learning rate: 0.00064506]
	Learning Rate: 0.00064506
	LOSS [training: 0.1518962598594465 | validation: 0.1605098964784347]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1393.pth
	Model improved!!!
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1392835569331586		[learning rate: 0.00064308]
	Learning Rate: 0.000643082
	LOSS [training: 0.1392835569331586 | validation: 0.19335691331939864]
	TIME [epoch: 10.6 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18439363786863422		[learning rate: 0.00064111]
	Learning Rate: 0.000641111
	LOSS [training: 0.18439363786863422 | validation: 0.20925078058690452]
	TIME [epoch: 10.6 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12795914217869947		[learning rate: 0.00063915]
	Learning Rate: 0.000639146
	LOSS [training: 0.12795914217869947 | validation: 0.17556120317680535]
	TIME [epoch: 10.6 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12299795067361965		[learning rate: 0.00063719]
	Learning Rate: 0.000637187
	LOSS [training: 0.12299795067361965 | validation: 0.32187065000625176]
	TIME [epoch: 10.6 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14998388930479872		[learning rate: 0.00063523]
	Learning Rate: 0.000635233
	LOSS [training: 0.14998388930479872 | validation: 0.2248648394042934]
	TIME [epoch: 10.6 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13115023672391424		[learning rate: 0.00063329]
	Learning Rate: 0.000633286
	LOSS [training: 0.13115023672391424 | validation: 0.2268736707359059]
	TIME [epoch: 10.6 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23468439816267325		[learning rate: 0.00063134]
	Learning Rate: 0.000631345
	LOSS [training: 0.23468439816267325 | validation: 0.34097697249317566]
	TIME [epoch: 10.6 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21795557344965175		[learning rate: 0.00062941]
	Learning Rate: 0.000629409
	LOSS [training: 0.21795557344965175 | validation: 0.27305986173372526]
	TIME [epoch: 10.6 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15374237823860185		[learning rate: 0.00062748]
	Learning Rate: 0.00062748
	LOSS [training: 0.15374237823860185 | validation: 0.2538463104047618]
	TIME [epoch: 10.6 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2045481313776354		[learning rate: 0.00062556]
	Learning Rate: 0.000625557
	LOSS [training: 0.2045481313776354 | validation: 0.2341685204086872]
	TIME [epoch: 10.6 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24795934149737353		[learning rate: 0.00062364]
	Learning Rate: 0.000623639
	LOSS [training: 0.24795934149737353 | validation: 0.20027146172911817]
	TIME [epoch: 10.6 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1803688902301601		[learning rate: 0.00062173]
	Learning Rate: 0.000621727
	LOSS [training: 0.1803688902301601 | validation: 0.2349747650637431]
	TIME [epoch: 10.6 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19111639795870347		[learning rate: 0.00061982]
	Learning Rate: 0.000619821
	LOSS [training: 0.19111639795870347 | validation: 0.24544661168868437]
	TIME [epoch: 10.6 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19896275199731162		[learning rate: 0.00061792]
	Learning Rate: 0.000617922
	LOSS [training: 0.19896275199731162 | validation: 0.21419086554183564]
	TIME [epoch: 10.6 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1790693749983389		[learning rate: 0.00061603]
	Learning Rate: 0.000616027
	LOSS [training: 0.1790693749983389 | validation: 0.2458542212861477]
	TIME [epoch: 10.6 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1958716350300438		[learning rate: 0.00061414]
	Learning Rate: 0.000614139
	LOSS [training: 0.1958716350300438 | validation: 0.17459185184945952]
	TIME [epoch: 10.6 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1452080036769491		[learning rate: 0.00061226]
	Learning Rate: 0.000612256
	LOSS [training: 0.1452080036769491 | validation: 0.18235114732689375]
	TIME [epoch: 10.6 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21328819269553523		[learning rate: 0.00061038]
	Learning Rate: 0.00061038
	LOSS [training: 0.21328819269553523 | validation: 0.1664750414564808]
	TIME [epoch: 10.6 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20063178602521425		[learning rate: 0.00060851]
	Learning Rate: 0.000608509
	LOSS [training: 0.20063178602521425 | validation: 0.24200809124093886]
	TIME [epoch: 10.6 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1961754573612048		[learning rate: 0.00060664]
	Learning Rate: 0.000606643
	LOSS [training: 0.1961754573612048 | validation: 0.2825352461685238]
	TIME [epoch: 10.6 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1825390097254535		[learning rate: 0.00060478]
	Learning Rate: 0.000604784
	LOSS [training: 0.1825390097254535 | validation: 0.23559593086451036]
	TIME [epoch: 10.6 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16417122088385297		[learning rate: 0.00060293]
	Learning Rate: 0.00060293
	LOSS [training: 0.16417122088385297 | validation: 0.21334769082916055]
	TIME [epoch: 10.6 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17883868659844018		[learning rate: 0.00060108]
	Learning Rate: 0.000601081
	LOSS [training: 0.17883868659844018 | validation: 0.29328251373188013]
	TIME [epoch: 10.6 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2598145441157187		[learning rate: 0.00059924]
	Learning Rate: 0.000599239
	LOSS [training: 0.2598145441157187 | validation: 0.3306421153722714]
	TIME [epoch: 10.6 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21396766243673185		[learning rate: 0.0005974]
	Learning Rate: 0.000597402
	LOSS [training: 0.21396766243673185 | validation: 0.2586343040799602]
	TIME [epoch: 10.6 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15804234749430196		[learning rate: 0.00059557]
	Learning Rate: 0.000595571
	LOSS [training: 0.15804234749430196 | validation: 0.2195269943148982]
	TIME [epoch: 10.6 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19471413674335375		[learning rate: 0.00059375]
	Learning Rate: 0.000593745
	LOSS [training: 0.19471413674335375 | validation: 0.4034453953273912]
	TIME [epoch: 10.6 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22183220115759314		[learning rate: 0.00059192]
	Learning Rate: 0.000591925
	LOSS [training: 0.22183220115759314 | validation: 0.1809791361533073]
	TIME [epoch: 10.6 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16994606095986414		[learning rate: 0.00059011]
	Learning Rate: 0.00059011
	LOSS [training: 0.16994606095986414 | validation: 0.20183968932808136]
	TIME [epoch: 10.6 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17444173600377894		[learning rate: 0.0005883]
	Learning Rate: 0.000588302
	LOSS [training: 0.17444173600377894 | validation: 0.2027351074367595]
	TIME [epoch: 10.6 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15680222810570116		[learning rate: 0.0005865]
	Learning Rate: 0.000586498
	LOSS [training: 0.15680222810570116 | validation: 0.19651056239080597]
	TIME [epoch: 10.6 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18334456272992766		[learning rate: 0.0005847]
	Learning Rate: 0.0005847
	LOSS [training: 0.18334456272992766 | validation: 0.1955965023754937]
	TIME [epoch: 10.6 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15286524106190594		[learning rate: 0.00058291]
	Learning Rate: 0.000582908
	LOSS [training: 0.15286524106190594 | validation: 0.16902741613362765]
	TIME [epoch: 10.6 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.168019789738789		[learning rate: 0.00058112]
	Learning Rate: 0.000581121
	LOSS [training: 0.168019789738789 | validation: 0.19363668279032487]
	TIME [epoch: 10.6 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1492368135075968		[learning rate: 0.00057934]
	Learning Rate: 0.00057934
	LOSS [training: 0.1492368135075968 | validation: 0.3005923960919938]
	TIME [epoch: 10.6 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16961359644644303		[learning rate: 0.00057756]
	Learning Rate: 0.000577564
	LOSS [training: 0.16961359644644303 | validation: 0.31126118087799176]
	TIME [epoch: 10.6 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1912990475192518		[learning rate: 0.00057579]
	Learning Rate: 0.000575793
	LOSS [training: 0.1912990475192518 | validation: 0.3714672298822046]
	TIME [epoch: 10.6 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17038525876607974		[learning rate: 0.00057403]
	Learning Rate: 0.000574028
	LOSS [training: 0.17038525876607974 | validation: 0.21866481685971542]
	TIME [epoch: 10.6 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1335723269934682		[learning rate: 0.00057227]
	Learning Rate: 0.000572269
	LOSS [training: 0.1335723269934682 | validation: 0.32731707456037845]
	TIME [epoch: 10.6 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25236062340617704		[learning rate: 0.00057051]
	Learning Rate: 0.000570514
	LOSS [training: 0.25236062340617704 | validation: 0.2925849103381651]
	TIME [epoch: 10.6 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19175836768443896		[learning rate: 0.00056877]
	Learning Rate: 0.000568766
	LOSS [training: 0.19175836768443896 | validation: 0.26756327148759945]
	TIME [epoch: 10.6 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1623168761134794		[learning rate: 0.00056702]
	Learning Rate: 0.000567022
	LOSS [training: 0.1623168761134794 | validation: 0.2942566015286943]
	TIME [epoch: 10.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18254023017926452		[learning rate: 0.00056528]
	Learning Rate: 0.000565284
	LOSS [training: 0.18254023017926452 | validation: 0.25434722500141344]
	TIME [epoch: 10.6 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17590388843125698		[learning rate: 0.00056355]
	Learning Rate: 0.000563551
	LOSS [training: 0.17590388843125698 | validation: 0.3154113158775171]
	TIME [epoch: 10.6 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1869967745963351		[learning rate: 0.00056182]
	Learning Rate: 0.000561824
	LOSS [training: 0.1869967745963351 | validation: 0.26047375422917085]
	TIME [epoch: 10.6 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13949055979627834		[learning rate: 0.0005601]
	Learning Rate: 0.000560101
	LOSS [training: 0.13949055979627834 | validation: 0.25574111884210865]
	TIME [epoch: 10.6 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1589816295645216		[learning rate: 0.00055838]
	Learning Rate: 0.000558385
	LOSS [training: 0.1589816295645216 | validation: 0.2730878314815557]
	TIME [epoch: 10.6 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1680632104084677		[learning rate: 0.00055667]
	Learning Rate: 0.000556673
	LOSS [training: 0.1680632104084677 | validation: 0.25261270744876513]
	TIME [epoch: 10.6 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20573832448524074		[learning rate: 0.00055497]
	Learning Rate: 0.000554966
	LOSS [training: 0.20573832448524074 | validation: 0.3074106137660458]
	TIME [epoch: 10.6 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2159282903055284		[learning rate: 0.00055327]
	Learning Rate: 0.000553265
	LOSS [training: 0.2159282903055284 | validation: 0.29664530050803545]
	TIME [epoch: 10.6 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20968492585171764		[learning rate: 0.00055157]
	Learning Rate: 0.000551569
	LOSS [training: 0.20968492585171764 | validation: 0.2452489546770545]
	TIME [epoch: 10.6 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15056835039889047		[learning rate: 0.00054988]
	Learning Rate: 0.000549878
	LOSS [training: 0.15056835039889047 | validation: 0.2703380825793778]
	TIME [epoch: 10.6 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16681861642813348		[learning rate: 0.00054819]
	Learning Rate: 0.000548193
	LOSS [training: 0.16681861642813348 | validation: 0.19184710111749678]
	TIME [epoch: 10.6 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16958059231743142		[learning rate: 0.00054651]
	Learning Rate: 0.000546512
	LOSS [training: 0.16958059231743142 | validation: 0.31180637768979436]
	TIME [epoch: 10.6 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.150715495328007		[learning rate: 0.00054484]
	Learning Rate: 0.000544837
	LOSS [training: 0.150715495328007 | validation: 0.1684295443550489]
	TIME [epoch: 10.6 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16795660207017546		[learning rate: 0.00054317]
	Learning Rate: 0.000543167
	LOSS [training: 0.16795660207017546 | validation: 0.21611708856627462]
	TIME [epoch: 10.6 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1771253801671105		[learning rate: 0.0005415]
	Learning Rate: 0.000541502
	LOSS [training: 0.1771253801671105 | validation: 0.16499781012021977]
	TIME [epoch: 10.6 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1692465289944646		[learning rate: 0.00053984]
	Learning Rate: 0.000539842
	LOSS [training: 0.1692465289944646 | validation: 0.17004963205443918]
	TIME [epoch: 10.6 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21916274658803214		[learning rate: 0.00053819]
	Learning Rate: 0.000538187
	LOSS [training: 0.21916274658803214 | validation: 0.2442206677238952]
	TIME [epoch: 10.6 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16039346329937731		[learning rate: 0.00053654]
	Learning Rate: 0.000536537
	LOSS [training: 0.16039346329937731 | validation: 0.1748196492331688]
	TIME [epoch: 10.6 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11535095409509621		[learning rate: 0.00053489]
	Learning Rate: 0.000534893
	LOSS [training: 0.11535095409509621 | validation: 0.16664042678961621]
	TIME [epoch: 10.6 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11576755478833523		[learning rate: 0.00053325]
	Learning Rate: 0.000533253
	LOSS [training: 0.11576755478833523 | validation: 0.20205621095274937]
	TIME [epoch: 10.6 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1303941958173099		[learning rate: 0.00053162]
	Learning Rate: 0.000531618
	LOSS [training: 0.1303941958173099 | validation: 0.3255973420340149]
	TIME [epoch: 10.6 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17945962512700336		[learning rate: 0.00052999]
	Learning Rate: 0.000529989
	LOSS [training: 0.17945962512700336 | validation: 0.18624537474248612]
	TIME [epoch: 10.6 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16073607479474883		[learning rate: 0.00052836]
	Learning Rate: 0.000528364
	LOSS [training: 0.16073607479474883 | validation: 0.28554466116658506]
	TIME [epoch: 10.6 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12977757618933014		[learning rate: 0.00052674]
	Learning Rate: 0.000526744
	LOSS [training: 0.12977757618933014 | validation: 0.26446513023150586]
	TIME [epoch: 10.6 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13471682239127597		[learning rate: 0.00052513]
	Learning Rate: 0.00052513
	LOSS [training: 0.13471682239127597 | validation: 0.23885353778930246]
	TIME [epoch: 10.6 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36295190655685217		[learning rate: 0.00052352]
	Learning Rate: 0.00052352
	LOSS [training: 0.36295190655685217 | validation: 0.3071022706696156]
	TIME [epoch: 10.6 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16052035883202484		[learning rate: 0.00052192]
	Learning Rate: 0.000521915
	LOSS [training: 0.16052035883202484 | validation: 0.18170170732201044]
	TIME [epoch: 10.6 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14288720316924183		[learning rate: 0.00052032]
	Learning Rate: 0.000520315
	LOSS [training: 0.14288720316924183 | validation: 0.2600099498452341]
	TIME [epoch: 10.6 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15226817321193836		[learning rate: 0.00051872]
	Learning Rate: 0.00051872
	LOSS [training: 0.15226817321193836 | validation: 0.25695203996668964]
	TIME [epoch: 10.6 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16070885445301802		[learning rate: 0.00051713]
	Learning Rate: 0.00051713
	LOSS [training: 0.16070885445301802 | validation: 0.21269124853257132]
	TIME [epoch: 10.6 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15335922493990758		[learning rate: 0.00051555]
	Learning Rate: 0.000515545
	LOSS [training: 0.15335922493990758 | validation: 0.2548075480650488]
	TIME [epoch: 10.6 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16874260195532154		[learning rate: 0.00051396]
	Learning Rate: 0.000513965
	LOSS [training: 0.16874260195532154 | validation: 0.37746058888469314]
	TIME [epoch: 10.6 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20113358663318545		[learning rate: 0.00051239]
	Learning Rate: 0.000512389
	LOSS [training: 0.20113358663318545 | validation: 0.2555273337290353]
	TIME [epoch: 10.6 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1277818831755997		[learning rate: 0.00051082]
	Learning Rate: 0.000510818
	LOSS [training: 0.1277818831755997 | validation: 0.2161845849344072]
	TIME [epoch: 10.6 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15012820439749347		[learning rate: 0.00050925]
	Learning Rate: 0.000509253
	LOSS [training: 0.15012820439749347 | validation: 0.17396543707286624]
	TIME [epoch: 10.6 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12281342600772523		[learning rate: 0.00050769]
	Learning Rate: 0.000507692
	LOSS [training: 0.12281342600772523 | validation: 0.20094876050667637]
	TIME [epoch: 10.6 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1677770667123811		[learning rate: 0.00050614]
	Learning Rate: 0.000506135
	LOSS [training: 0.1677770667123811 | validation: 0.23812914011970507]
	TIME [epoch: 10.6 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13428881185310929		[learning rate: 0.00050458]
	Learning Rate: 0.000504584
	LOSS [training: 0.13428881185310929 | validation: 0.16884895994346075]
	TIME [epoch: 10.6 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1298457063132191		[learning rate: 0.00050304]
	Learning Rate: 0.000503037
	LOSS [training: 0.1298457063132191 | validation: 0.18430046053063212]
	TIME [epoch: 10.6 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1318459742297569		[learning rate: 0.00050149]
	Learning Rate: 0.000501495
	LOSS [training: 0.1318459742297569 | validation: 0.208811707746742]
	TIME [epoch: 10.6 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12791047807971373		[learning rate: 0.00049996]
	Learning Rate: 0.000499958
	LOSS [training: 0.12791047807971373 | validation: 0.1570508712799106]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1476.pth
	Model improved!!!
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1462130232168533		[learning rate: 0.00049843]
	Learning Rate: 0.000498425
	LOSS [training: 0.1462130232168533 | validation: 0.2057363099078037]
	TIME [epoch: 10.6 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20206671027758136		[learning rate: 0.0004969]
	Learning Rate: 0.000496897
	LOSS [training: 0.20206671027758136 | validation: 0.19131789322515722]
	TIME [epoch: 10.6 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15582168125863202		[learning rate: 0.00049537]
	Learning Rate: 0.000495374
	LOSS [training: 0.15582168125863202 | validation: 0.14878181950132408]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1479.pth
	Model improved!!!
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13825240913115636		[learning rate: 0.00049386]
	Learning Rate: 0.000493856
	LOSS [training: 0.13825240913115636 | validation: 0.19426452057678398]
	TIME [epoch: 10.6 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18813004271215095		[learning rate: 0.00049234]
	Learning Rate: 0.000492342
	LOSS [training: 0.18813004271215095 | validation: 0.15718223198912692]
	TIME [epoch: 10.6 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15646661889341035		[learning rate: 0.00049083]
	Learning Rate: 0.000490832
	LOSS [training: 0.15646661889341035 | validation: 0.17818724566178418]
	TIME [epoch: 10.6 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17695819186038125		[learning rate: 0.00048933]
	Learning Rate: 0.000489328
	LOSS [training: 0.17695819186038125 | validation: 0.23207285317459345]
	TIME [epoch: 10.6 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13281680810250845		[learning rate: 0.00048783]
	Learning Rate: 0.000487828
	LOSS [training: 0.13281680810250845 | validation: 0.19501830469361628]
	TIME [epoch: 10.6 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14880626744873476		[learning rate: 0.00048633]
	Learning Rate: 0.000486333
	LOSS [training: 0.14880626744873476 | validation: 0.23204553416728024]
	TIME [epoch: 10.6 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13400223314863166		[learning rate: 0.00048484]
	Learning Rate: 0.000484842
	LOSS [training: 0.13400223314863166 | validation: 0.1846792769981856]
	TIME [epoch: 10.6 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1326150132537686		[learning rate: 0.00048336]
	Learning Rate: 0.000483356
	LOSS [training: 0.1326150132537686 | validation: 0.18845908674621772]
	TIME [epoch: 10.6 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1350472997023857		[learning rate: 0.00048187]
	Learning Rate: 0.000481874
	LOSS [training: 0.1350472997023857 | validation: 0.1833938849415034]
	TIME [epoch: 10.6 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12824214587599223		[learning rate: 0.0004804]
	Learning Rate: 0.000480397
	LOSS [training: 0.12824214587599223 | validation: 0.26609942759883176]
	TIME [epoch: 10.6 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14109876619143685		[learning rate: 0.00047892]
	Learning Rate: 0.000478924
	LOSS [training: 0.14109876619143685 | validation: 0.1740798585055839]
	TIME [epoch: 10.6 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14502052524734513		[learning rate: 0.00047746]
	Learning Rate: 0.000477456
	LOSS [training: 0.14502052524734513 | validation: 0.18805206215653855]
	TIME [epoch: 10.6 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16934864372690148		[learning rate: 0.00047599]
	Learning Rate: 0.000475992
	LOSS [training: 0.16934864372690148 | validation: 0.27870998016748194]
	TIME [epoch: 10.6 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.184493280946666		[learning rate: 0.00047453]
	Learning Rate: 0.000474533
	LOSS [training: 0.184493280946666 | validation: 0.176552659067175]
	TIME [epoch: 10.6 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14430513141303689		[learning rate: 0.00047308]
	Learning Rate: 0.000473079
	LOSS [training: 0.14430513141303689 | validation: 0.12107943496493664]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1494.pth
	Model improved!!!
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16087127624543093		[learning rate: 0.00047163]
	Learning Rate: 0.000471628
	LOSS [training: 0.16087127624543093 | validation: 0.18323283808930796]
	TIME [epoch: 10.6 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16005640383047218		[learning rate: 0.00047018]
	Learning Rate: 0.000470183
	LOSS [training: 0.16005640383047218 | validation: 0.12826022984344898]
	TIME [epoch: 10.6 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11832664352914295		[learning rate: 0.00046874]
	Learning Rate: 0.000468741
	LOSS [training: 0.11832664352914295 | validation: 0.14065919693568446]
	TIME [epoch: 10.6 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12296288185603528		[learning rate: 0.0004673]
	Learning Rate: 0.000467304
	LOSS [training: 0.12296288185603528 | validation: 0.16755870042689133]
	TIME [epoch: 10.6 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1465798145103131		[learning rate: 0.00046587]
	Learning Rate: 0.000465872
	LOSS [training: 0.1465798145103131 | validation: 0.1679603012482179]
	TIME [epoch: 10.6 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15722167498898293		[learning rate: 0.00046444]
	Learning Rate: 0.000464444
	LOSS [training: 0.15722167498898293 | validation: 0.31456198592063833]
	TIME [epoch: 10.6 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17487601678608583		[learning rate: 0.00046302]
	Learning Rate: 0.00046302
	LOSS [training: 0.17487601678608583 | validation: 0.19087516914634126]
	TIME [epoch: 10.6 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1310205959831076		[learning rate: 0.0004616]
	Learning Rate: 0.000461601
	LOSS [training: 0.1310205959831076 | validation: 0.2528939268578441]
	TIME [epoch: 10.6 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16860506144340087		[learning rate: 0.00046019]
	Learning Rate: 0.000460186
	LOSS [training: 0.16860506144340087 | validation: 0.1392564962747536]
	TIME [epoch: 10.6 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12373865633715071		[learning rate: 0.00045878]
	Learning Rate: 0.000458775
	LOSS [training: 0.12373865633715071 | validation: 0.20405712579270124]
	TIME [epoch: 10.6 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1325350849986998		[learning rate: 0.00045737]
	Learning Rate: 0.000457369
	LOSS [training: 0.1325350849986998 | validation: 0.17183001376136858]
	TIME [epoch: 10.6 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14026790201388595		[learning rate: 0.00045597]
	Learning Rate: 0.000455967
	LOSS [training: 0.14026790201388595 | validation: 0.18073072811720947]
	TIME [epoch: 10.6 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1995895250935396		[learning rate: 0.00045457]
	Learning Rate: 0.000454569
	LOSS [training: 0.1995895250935396 | validation: 0.1570712340236065]
	TIME [epoch: 10.6 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10993228185572716		[learning rate: 0.00045318]
	Learning Rate: 0.000453176
	LOSS [training: 0.10993228185572716 | validation: 0.18330099688482557]
	TIME [epoch: 10.6 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13626786911420813		[learning rate: 0.00045179]
	Learning Rate: 0.000451787
	LOSS [training: 0.13626786911420813 | validation: 0.14601669827389305]
	TIME [epoch: 10.6 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14354744459897778		[learning rate: 0.0004504]
	Learning Rate: 0.000450402
	LOSS [training: 0.14354744459897778 | validation: 0.17001338450791534]
	TIME [epoch: 10.6 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15025782750809538		[learning rate: 0.00044902]
	Learning Rate: 0.000449021
	LOSS [training: 0.15025782750809538 | validation: 0.12559450876590397]
	TIME [epoch: 10.6 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20517077450471985		[learning rate: 0.00044764]
	Learning Rate: 0.000447645
	LOSS [training: 0.20517077450471985 | validation: 0.1532210484414554]
	TIME [epoch: 10.6 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13590432304783648		[learning rate: 0.00044627]
	Learning Rate: 0.000446272
	LOSS [training: 0.13590432304783648 | validation: 0.15606308937821814]
	TIME [epoch: 10.6 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12997575569641798		[learning rate: 0.0004449]
	Learning Rate: 0.000444904
	LOSS [training: 0.12997575569641798 | validation: 0.11725085412279844]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1514.pth
	Model improved!!!
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1305293076669832		[learning rate: 0.00044354]
	Learning Rate: 0.000443541
	LOSS [training: 0.1305293076669832 | validation: 0.14282633940402623]
	TIME [epoch: 10.6 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12506686302636494		[learning rate: 0.00044218]
	Learning Rate: 0.000442181
	LOSS [training: 0.12506686302636494 | validation: 0.16065048196355022]
	TIME [epoch: 10.6 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12417532412711067		[learning rate: 0.00044083]
	Learning Rate: 0.000440825
	LOSS [training: 0.12417532412711067 | validation: 0.1563634481444618]
	TIME [epoch: 10.6 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1427159742508804		[learning rate: 0.00043947]
	Learning Rate: 0.000439474
	LOSS [training: 0.1427159742508804 | validation: 0.15293812930834175]
	TIME [epoch: 10.6 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14877626096400223		[learning rate: 0.00043813]
	Learning Rate: 0.000438127
	LOSS [training: 0.14877626096400223 | validation: 0.21809972889529733]
	TIME [epoch: 10.6 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16558045130106203		[learning rate: 0.00043678]
	Learning Rate: 0.000436784
	LOSS [training: 0.16558045130106203 | validation: 0.15045810696032788]
	TIME [epoch: 10.6 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1303851896009953		[learning rate: 0.00043544]
	Learning Rate: 0.000435445
	LOSS [training: 0.1303851896009953 | validation: 0.12314282478908975]
	TIME [epoch: 10.6 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15436419430076614		[learning rate: 0.00043411]
	Learning Rate: 0.00043411
	LOSS [training: 0.15436419430076614 | validation: 0.20721523143316845]
	TIME [epoch: 10.6 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16025099781993857		[learning rate: 0.00043278]
	Learning Rate: 0.00043278
	LOSS [training: 0.16025099781993857 | validation: 0.14962246629625614]
	TIME [epoch: 10.6 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15248612617711718		[learning rate: 0.00043145]
	Learning Rate: 0.000431453
	LOSS [training: 0.15248612617711718 | validation: 0.1580568077943459]
	TIME [epoch: 10.6 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14871866310308937		[learning rate: 0.00043013]
	Learning Rate: 0.00043013
	LOSS [training: 0.14871866310308937 | validation: 0.18373899592356024]
	TIME [epoch: 10.6 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15364884274702245		[learning rate: 0.00042881]
	Learning Rate: 0.000428812
	LOSS [training: 0.15364884274702245 | validation: 0.13677152812375556]
	TIME [epoch: 10.6 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12648835053118773		[learning rate: 0.0004275]
	Learning Rate: 0.000427497
	LOSS [training: 0.12648835053118773 | validation: 0.18383002113795194]
	TIME [epoch: 10.6 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1441816983795537		[learning rate: 0.00042619]
	Learning Rate: 0.000426187
	LOSS [training: 0.1441816983795537 | validation: 0.11380971915619538]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1528.pth
	Model improved!!!
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11111478754343942		[learning rate: 0.00042488]
	Learning Rate: 0.00042488
	LOSS [training: 0.11111478754343942 | validation: 0.14861877700501425]
	TIME [epoch: 10.6 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10906622860398481		[learning rate: 0.00042358]
	Learning Rate: 0.000423578
	LOSS [training: 0.10906622860398481 | validation: 0.16490279353388113]
	TIME [epoch: 10.6 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11232203021579572		[learning rate: 0.00042228]
	Learning Rate: 0.000422279
	LOSS [training: 0.11232203021579572 | validation: 0.18484657294952844]
	TIME [epoch: 10.6 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12029728063156955		[learning rate: 0.00042098]
	Learning Rate: 0.000420985
	LOSS [training: 0.12029728063156955 | validation: 0.22916321095074885]
	TIME [epoch: 10.6 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12347436728057012		[learning rate: 0.00041969]
	Learning Rate: 0.000419694
	LOSS [training: 0.12347436728057012 | validation: 0.1724920107137342]
	TIME [epoch: 10.6 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12737798640718068		[learning rate: 0.00041841]
	Learning Rate: 0.000418408
	LOSS [training: 0.12737798640718068 | validation: 0.19049793963417447]
	TIME [epoch: 10.6 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13589434418103252		[learning rate: 0.00041713]
	Learning Rate: 0.000417125
	LOSS [training: 0.13589434418103252 | validation: 0.20221405899944686]
	TIME [epoch: 10.6 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13884048852322467		[learning rate: 0.00041585]
	Learning Rate: 0.000415847
	LOSS [training: 0.13884048852322467 | validation: 0.2182991334707289]
	TIME [epoch: 10.6 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12709706712934152		[learning rate: 0.00041457]
	Learning Rate: 0.000414572
	LOSS [training: 0.12709706712934152 | validation: 0.13559576227896966]
	TIME [epoch: 10.5 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11452514249191355		[learning rate: 0.0004133]
	Learning Rate: 0.000413301
	LOSS [training: 0.11452514249191355 | validation: 0.175051156193428]
	TIME [epoch: 10.6 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14627299934039845		[learning rate: 0.00041203]
	Learning Rate: 0.000412034
	LOSS [training: 0.14627299934039845 | validation: 0.1849784190966422]
	TIME [epoch: 10.6 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21079151922435174		[learning rate: 0.00041077]
	Learning Rate: 0.000410771
	LOSS [training: 0.21079151922435174 | validation: 0.4424900306819458]
	TIME [epoch: 10.6 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19175846335185792		[learning rate: 0.00040951]
	Learning Rate: 0.000409512
	LOSS [training: 0.19175846335185792 | validation: 0.13493766224551698]
	TIME [epoch: 10.6 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12318680638904075		[learning rate: 0.00040826]
	Learning Rate: 0.000408257
	LOSS [training: 0.12318680638904075 | validation: 0.17207867624181739]
	TIME [epoch: 10.6 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12083829541213975		[learning rate: 0.00040701]
	Learning Rate: 0.000407005
	LOSS [training: 0.12083829541213975 | validation: 0.16691322961917374]
	TIME [epoch: 10.6 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12839708233723188		[learning rate: 0.00040576]
	Learning Rate: 0.000405758
	LOSS [training: 0.12839708233723188 | validation: 0.2941993706983749]
	TIME [epoch: 10.6 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18811660705206448		[learning rate: 0.00040451]
	Learning Rate: 0.000404514
	LOSS [training: 0.18811660705206448 | validation: 0.1410241471473463]
	TIME [epoch: 10.6 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13183777338114372		[learning rate: 0.00040327]
	Learning Rate: 0.000403274
	LOSS [training: 0.13183777338114372 | validation: 0.1964896332292387]
	TIME [epoch: 10.6 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12358790227068575		[learning rate: 0.00040204]
	Learning Rate: 0.000402038
	LOSS [training: 0.12358790227068575 | validation: 0.19031489767773505]
	TIME [epoch: 10.6 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1303262388082894		[learning rate: 0.00040081]
	Learning Rate: 0.000400805
	LOSS [training: 0.1303262388082894 | validation: 0.20218340158848871]
	TIME [epoch: 10.6 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16521162947889187		[learning rate: 0.00039958]
	Learning Rate: 0.000399577
	LOSS [training: 0.16521162947889187 | validation: 0.17741897836023462]
	TIME [epoch: 10.6 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11955419377378074		[learning rate: 0.00039835]
	Learning Rate: 0.000398352
	LOSS [training: 0.11955419377378074 | validation: 0.22760354164460922]
	TIME [epoch: 10.6 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12636598657245543		[learning rate: 0.00039713]
	Learning Rate: 0.000397131
	LOSS [training: 0.12636598657245543 | validation: 0.13712538461990398]
	TIME [epoch: 10.6 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11020568589343434		[learning rate: 0.00039591]
	Learning Rate: 0.000395913
	LOSS [training: 0.11020568589343434 | validation: 0.23701262210603274]
	TIME [epoch: 10.6 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1184686351395138		[learning rate: 0.0003947]
	Learning Rate: 0.0003947
	LOSS [training: 0.1184686351395138 | validation: 0.16883426446022717]
	TIME [epoch: 10.6 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12265629929884159		[learning rate: 0.00039349]
	Learning Rate: 0.00039349
	LOSS [training: 0.12265629929884159 | validation: 0.1763964149298564]
	TIME [epoch: 10.6 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1602290472987868		[learning rate: 0.00039228]
	Learning Rate: 0.000392283
	LOSS [training: 0.1602290472987868 | validation: 0.11955870144682784]
	TIME [epoch: 10.5 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11677863141522374		[learning rate: 0.00039108]
	Learning Rate: 0.000391081
	LOSS [training: 0.11677863141522374 | validation: 0.13322762134729]
	TIME [epoch: 10.6 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10407679892179393		[learning rate: 0.00038988]
	Learning Rate: 0.000389882
	LOSS [training: 0.10407679892179393 | validation: 0.132341670651228]
	TIME [epoch: 10.6 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12670868650709563		[learning rate: 0.00038869]
	Learning Rate: 0.000388687
	LOSS [training: 0.12670868650709563 | validation: 0.1737400499085225]
	TIME [epoch: 10.6 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13383407327212898		[learning rate: 0.0003875]
	Learning Rate: 0.000387495
	LOSS [training: 0.13383407327212898 | validation: 0.1389385222161947]
	TIME [epoch: 10.5 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13088046265257153		[learning rate: 0.00038631]
	Learning Rate: 0.000386308
	LOSS [training: 0.13088046265257153 | validation: 0.1276102695023942]
	TIME [epoch: 10.6 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11155321874625428		[learning rate: 0.00038512]
	Learning Rate: 0.000385123
	LOSS [training: 0.11155321874625428 | validation: 0.15100437505479336]
	TIME [epoch: 10.6 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12176094019430558		[learning rate: 0.00038394]
	Learning Rate: 0.000383943
	LOSS [training: 0.12176094019430558 | validation: 0.14889571237702673]
	TIME [epoch: 10.6 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12007170125667968		[learning rate: 0.00038277]
	Learning Rate: 0.000382766
	LOSS [training: 0.12007170125667968 | validation: 0.1435059460411543]
	TIME [epoch: 10.5 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12315433901565213		[learning rate: 0.00038159]
	Learning Rate: 0.000381593
	LOSS [training: 0.12315433901565213 | validation: 0.27951225918736505]
	TIME [epoch: 10.6 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17454969692279693		[learning rate: 0.00038042]
	Learning Rate: 0.000380423
	LOSS [training: 0.17454969692279693 | validation: 0.16871684418019547]
	TIME [epoch: 10.6 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14476062704005493		[learning rate: 0.00037926]
	Learning Rate: 0.000379257
	LOSS [training: 0.14476062704005493 | validation: 0.3210502745446894]
	TIME [epoch: 10.6 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15854457026956448		[learning rate: 0.00037809]
	Learning Rate: 0.000378094
	LOSS [training: 0.15854457026956448 | validation: 0.20439410776912895]
	TIME [epoch: 10.6 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13885837822708283		[learning rate: 0.00037694]
	Learning Rate: 0.000376935
	LOSS [training: 0.13885837822708283 | validation: 0.18486438895909338]
	TIME [epoch: 10.6 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12235337431185525		[learning rate: 0.00037578]
	Learning Rate: 0.00037578
	LOSS [training: 0.12235337431185525 | validation: 0.18407472941040914]
	TIME [epoch: 10.6 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11898525134091731		[learning rate: 0.00037463]
	Learning Rate: 0.000374628
	LOSS [training: 0.11898525134091731 | validation: 0.19583488638793284]
	TIME [epoch: 10.6 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13201054168102697		[learning rate: 0.00037348]
	Learning Rate: 0.000373479
	LOSS [training: 0.13201054168102697 | validation: 0.212112764071084]
	TIME [epoch: 10.5 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13693253659909374		[learning rate: 0.00037233]
	Learning Rate: 0.000372335
	LOSS [training: 0.13693253659909374 | validation: 0.1884589421589037]
	TIME [epoch: 10.6 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1299655445583653		[learning rate: 0.00037119]
	Learning Rate: 0.000371193
	LOSS [training: 0.1299655445583653 | validation: 0.16388128305718813]
	TIME [epoch: 10.6 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10453072120212821		[learning rate: 0.00037006]
	Learning Rate: 0.000370055
	LOSS [training: 0.10453072120212821 | validation: 0.2092339312141734]
	TIME [epoch: 10.6 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11198858542278323		[learning rate: 0.00036892]
	Learning Rate: 0.000368921
	LOSS [training: 0.11198858542278323 | validation: 0.15442627143210655]
	TIME [epoch: 10.6 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11057709052909306		[learning rate: 0.00036779]
	Learning Rate: 0.00036779
	LOSS [training: 0.11057709052909306 | validation: 0.15176870570251855]
	TIME [epoch: 10.5 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10353640495748964		[learning rate: 0.00036666]
	Learning Rate: 0.000366663
	LOSS [training: 0.10353640495748964 | validation: 0.14165096189447832]
	TIME [epoch: 10.6 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11130984548506659		[learning rate: 0.00036554]
	Learning Rate: 0.000365539
	LOSS [training: 0.11130984548506659 | validation: 0.16198170332572545]
	TIME [epoch: 10.6 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12916309603737566		[learning rate: 0.00036442]
	Learning Rate: 0.000364418
	LOSS [training: 0.12916309603737566 | validation: 0.16151017093861122]
	TIME [epoch: 10.6 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1146638197747969		[learning rate: 0.0003633]
	Learning Rate: 0.000363301
	LOSS [training: 0.1146638197747969 | validation: 0.2048547555196421]
	TIME [epoch: 10.6 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11090627917127216		[learning rate: 0.00036219]
	Learning Rate: 0.000362187
	LOSS [training: 0.11090627917127216 | validation: 0.16299494074250295]
	TIME [epoch: 10.6 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12252724860951433		[learning rate: 0.00036108]
	Learning Rate: 0.000361077
	LOSS [training: 0.12252724860951433 | validation: 0.15663007240399848]
	TIME [epoch: 10.6 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11145928992849488		[learning rate: 0.00035997]
	Learning Rate: 0.00035997
	LOSS [training: 0.11145928992849488 | validation: 0.1676033250171744]
	TIME [epoch: 10.6 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16357067377130308		[learning rate: 0.00035887]
	Learning Rate: 0.000358867
	LOSS [training: 0.16357067377130308 | validation: 0.18358866798000698]
	TIME [epoch: 10.6 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11468554733858582		[learning rate: 0.00035777]
	Learning Rate: 0.000357767
	LOSS [training: 0.11468554733858582 | validation: 0.161460828118704]
	TIME [epoch: 10.6 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12637322837191717		[learning rate: 0.00035667]
	Learning Rate: 0.00035667
	LOSS [training: 0.12637322837191717 | validation: 0.19012036561124623]
	TIME [epoch: 10.6 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12884703509542555		[learning rate: 0.00035558]
	Learning Rate: 0.000355577
	LOSS [training: 0.12884703509542555 | validation: 0.17667775732699756]
	TIME [epoch: 10.6 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09725519841520497		[learning rate: 0.00035449]
	Learning Rate: 0.000354487
	LOSS [training: 0.09725519841520497 | validation: 0.17228179976146504]
	TIME [epoch: 10.5 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12230123326699241		[learning rate: 0.0003534]
	Learning Rate: 0.0003534
	LOSS [training: 0.12230123326699241 | validation: 0.17279666241581484]
	TIME [epoch: 10.6 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11192251067823608		[learning rate: 0.00035232]
	Learning Rate: 0.000352317
	LOSS [training: 0.11192251067823608 | validation: 0.21659226219397823]
	TIME [epoch: 10.6 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1098326622286031		[learning rate: 0.00035124]
	Learning Rate: 0.000351237
	LOSS [training: 0.1098326622286031 | validation: 0.2434306108908338]
	TIME [epoch: 10.6 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12907139894021652		[learning rate: 0.00035016]
	Learning Rate: 0.00035016
	LOSS [training: 0.12907139894021652 | validation: 0.18703020859799593]
	TIME [epoch: 10.6 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10676497965854208		[learning rate: 0.00034909]
	Learning Rate: 0.000349087
	LOSS [training: 0.10676497965854208 | validation: 0.18526265552938717]
	TIME [epoch: 10.6 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11045996928818466		[learning rate: 0.00034802]
	Learning Rate: 0.000348017
	LOSS [training: 0.11045996928818466 | validation: 0.17521579251466673]
	TIME [epoch: 10.6 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10071256988559059		[learning rate: 0.00034695]
	Learning Rate: 0.00034695
	LOSS [training: 0.10071256988559059 | validation: 0.14515749175784015]
	TIME [epoch: 10.6 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09926878179671947		[learning rate: 0.00034589]
	Learning Rate: 0.000345886
	LOSS [training: 0.09926878179671947 | validation: 0.1729259809322379]
	TIME [epoch: 10.6 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10020560804478304		[learning rate: 0.00034483]
	Learning Rate: 0.000344826
	LOSS [training: 0.10020560804478304 | validation: 0.209468351014108]
	TIME [epoch: 10.6 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1155329353160964		[learning rate: 0.00034377]
	Learning Rate: 0.000343769
	LOSS [training: 0.1155329353160964 | validation: 0.289166102615461]
	TIME [epoch: 10.6 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15180498330239395		[learning rate: 0.00034272]
	Learning Rate: 0.000342715
	LOSS [training: 0.15180498330239395 | validation: 0.15621123171940599]
	TIME [epoch: 10.6 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09734714506026412		[learning rate: 0.00034166]
	Learning Rate: 0.000341665
	LOSS [training: 0.09734714506026412 | validation: 0.1313374974306096]
	TIME [epoch: 10.6 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10638863694213592		[learning rate: 0.00034062]
	Learning Rate: 0.000340617
	LOSS [training: 0.10638863694213592 | validation: 0.25727026300501005]
	TIME [epoch: 10.6 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14320503905196935		[learning rate: 0.00033957]
	Learning Rate: 0.000339573
	LOSS [training: 0.14320503905196935 | validation: 0.13911461297559258]
	TIME [epoch: 10.6 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1411789267518127		[learning rate: 0.00033853]
	Learning Rate: 0.000338532
	LOSS [training: 0.1411789267518127 | validation: 0.17238413069155806]
	TIME [epoch: 10.6 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14010153282064647		[learning rate: 0.00033749]
	Learning Rate: 0.000337494
	LOSS [training: 0.14010153282064647 | validation: 0.1679982275728593]
	TIME [epoch: 10.6 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11853528222793415		[learning rate: 0.00033646]
	Learning Rate: 0.00033646
	LOSS [training: 0.11853528222793415 | validation: 0.18007518086516186]
	TIME [epoch: 10.6 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14505174471219334		[learning rate: 0.00033543]
	Learning Rate: 0.000335428
	LOSS [training: 0.14505174471219334 | validation: 0.18084039310746342]
	TIME [epoch: 10.6 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1479469739310448		[learning rate: 0.0003344]
	Learning Rate: 0.0003344
	LOSS [training: 0.1479469739310448 | validation: 0.16770341163649533]
	TIME [epoch: 10.6 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1303341186101071		[learning rate: 0.00033338]
	Learning Rate: 0.000333375
	LOSS [training: 0.1303341186101071 | validation: 0.1948512698643782]
	TIME [epoch: 10.6 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10968808134194678		[learning rate: 0.00033235]
	Learning Rate: 0.000332353
	LOSS [training: 0.10968808134194678 | validation: 0.15136846682494753]
	TIME [epoch: 10.6 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12075560169963359		[learning rate: 0.00033133]
	Learning Rate: 0.000331334
	LOSS [training: 0.12075560169963359 | validation: 0.13485352715913151]
	TIME [epoch: 10.6 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11105669595314771		[learning rate: 0.00033032]
	Learning Rate: 0.000330319
	LOSS [training: 0.11105669595314771 | validation: 0.13417146857401419]
	TIME [epoch: 10.6 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11207243756412189		[learning rate: 0.00032931]
	Learning Rate: 0.000329306
	LOSS [training: 0.11207243756412189 | validation: 0.17819842619592702]
	TIME [epoch: 10.6 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15330413030125611		[learning rate: 0.0003283]
	Learning Rate: 0.000328297
	LOSS [training: 0.15330413030125611 | validation: 0.24010749830421066]
	TIME [epoch: 10.5 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12750286859926685		[learning rate: 0.00032729]
	Learning Rate: 0.00032729
	LOSS [training: 0.12750286859926685 | validation: 0.2697361401806139]
	TIME [epoch: 10.5 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2905765990620365		[learning rate: 0.00032629]
	Learning Rate: 0.000326287
	LOSS [training: 0.2905765990620365 | validation: 0.24731472643649277]
	TIME [epoch: 10.6 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13378014767947796		[learning rate: 0.00032529]
	Learning Rate: 0.000325287
	LOSS [training: 0.13378014767947796 | validation: 0.15474439525299857]
	TIME [epoch: 10.6 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.101829031327519		[learning rate: 0.00032429]
	Learning Rate: 0.00032429
	LOSS [training: 0.101829031327519 | validation: 0.18206912411148332]
	TIME [epoch: 10.6 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12100720842217179		[learning rate: 0.0003233]
	Learning Rate: 0.000323296
	LOSS [training: 0.12100720842217179 | validation: 0.21565805563524362]
	TIME [epoch: 10.6 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12473958747081024		[learning rate: 0.0003223]
	Learning Rate: 0.000322305
	LOSS [training: 0.12473958747081024 | validation: 0.17755480710290505]
	TIME [epoch: 10.6 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13466505902247372		[learning rate: 0.00032132]
	Learning Rate: 0.000321317
	LOSS [training: 0.13466505902247372 | validation: 0.21468760154399558]
	TIME [epoch: 10.6 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1298811189978154		[learning rate: 0.00032033]
	Learning Rate: 0.000320332
	LOSS [training: 0.1298811189978154 | validation: 0.250704499834096]
	TIME [epoch: 10.6 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17308885431972673		[learning rate: 0.00031935]
	Learning Rate: 0.00031935
	LOSS [training: 0.17308885431972673 | validation: 0.21598520257656575]
	TIME [epoch: 10.6 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11993543584321545		[learning rate: 0.00031837]
	Learning Rate: 0.000318371
	LOSS [training: 0.11993543584321545 | validation: 0.1947729672074919]
	TIME [epoch: 10.6 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16117210468563506		[learning rate: 0.00031739]
	Learning Rate: 0.000317395
	LOSS [training: 0.16117210468563506 | validation: 0.1753421778387439]
	TIME [epoch: 10.6 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12599705086445873		[learning rate: 0.00031642]
	Learning Rate: 0.000316422
	LOSS [training: 0.12599705086445873 | validation: 0.15065682018747914]
	TIME [epoch: 10.6 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10678329526131118		[learning rate: 0.00031545]
	Learning Rate: 0.000315452
	LOSS [training: 0.10678329526131118 | validation: 0.18669111659913967]
	TIME [epoch: 10.6 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15125093352592553		[learning rate: 0.00031449]
	Learning Rate: 0.000314485
	LOSS [training: 0.15125093352592553 | validation: 0.1257520858754037]
	TIME [epoch: 10.6 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10771901825019256		[learning rate: 0.00031352]
	Learning Rate: 0.000313521
	LOSS [training: 0.10771901825019256 | validation: 0.1470905119599778]
	TIME [epoch: 10.6 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11387852830086312		[learning rate: 0.00031256]
	Learning Rate: 0.00031256
	LOSS [training: 0.11387852830086312 | validation: 0.16516315797172992]
	TIME [epoch: 10.6 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1258555186399623		[learning rate: 0.0003116]
	Learning Rate: 0.000311602
	LOSS [training: 0.1258555186399623 | validation: 0.11895609862167682]
	TIME [epoch: 10.6 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12663102272096313		[learning rate: 0.00031065]
	Learning Rate: 0.000310647
	LOSS [training: 0.12663102272096313 | validation: 0.16989002747488072]
	TIME [epoch: 10.6 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16501636303605224		[learning rate: 0.00030969]
	Learning Rate: 0.000309694
	LOSS [training: 0.16501636303605224 | validation: 0.14761361610945126]
	TIME [epoch: 10.6 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12806622892524963		[learning rate: 0.00030874]
	Learning Rate: 0.000308745
	LOSS [training: 0.12806622892524963 | validation: 0.13227601993104682]
	TIME [epoch: 10.6 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10706125675715639		[learning rate: 0.0003078]
	Learning Rate: 0.000307799
	LOSS [training: 0.10706125675715639 | validation: 0.1971509683044415]
	TIME [epoch: 10.6 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14856222449034756		[learning rate: 0.00030686]
	Learning Rate: 0.000306855
	LOSS [training: 0.14856222449034756 | validation: 0.25663252447446616]
	TIME [epoch: 10.6 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12171463031337815		[learning rate: 0.00030591]
	Learning Rate: 0.000305914
	LOSS [training: 0.12171463031337815 | validation: 0.15062895950624175]
	TIME [epoch: 10.6 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12432834030986836		[learning rate: 0.00030498]
	Learning Rate: 0.000304977
	LOSS [training: 0.12432834030986836 | validation: 0.17972089047820242]
	TIME [epoch: 10.6 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13293659655242823		[learning rate: 0.00030404]
	Learning Rate: 0.000304042
	LOSS [training: 0.13293659655242823 | validation: 0.16082005996997137]
	TIME [epoch: 10.6 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10240094200800262		[learning rate: 0.00030311]
	Learning Rate: 0.00030311
	LOSS [training: 0.10240094200800262 | validation: 0.19736524378728681]
	TIME [epoch: 10.6 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12007590423548845		[learning rate: 0.00030218]
	Learning Rate: 0.000302181
	LOSS [training: 0.12007590423548845 | validation: 0.15652346430403138]
	TIME [epoch: 10.6 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11377961648049495		[learning rate: 0.00030125]
	Learning Rate: 0.000301254
	LOSS [training: 0.11377961648049495 | validation: 0.17500472462797823]
	TIME [epoch: 10.6 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12781473532368895		[learning rate: 0.00030033]
	Learning Rate: 0.000300331
	LOSS [training: 0.12781473532368895 | validation: 0.1859042531318034]
	TIME [epoch: 10.6 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12446014831993064		[learning rate: 0.00029941]
	Learning Rate: 0.00029941
	LOSS [training: 0.12446014831993064 | validation: 0.1470124536361519]
	TIME [epoch: 10.6 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12871375404302812		[learning rate: 0.00029849]
	Learning Rate: 0.000298492
	LOSS [training: 0.12871375404302812 | validation: 0.13870388152466073]
	TIME [epoch: 10.6 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11804521559004451		[learning rate: 0.00029758]
	Learning Rate: 0.000297577
	LOSS [training: 0.11804521559004451 | validation: 0.18649567623212682]
	TIME [epoch: 10.6 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11693179086637288		[learning rate: 0.00029667]
	Learning Rate: 0.000296665
	LOSS [training: 0.11693179086637288 | validation: 0.1360566156061573]
	TIME [epoch: 10.6 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10999681492848261		[learning rate: 0.00029576]
	Learning Rate: 0.000295756
	LOSS [training: 0.10999681492848261 | validation: 0.24356029924478464]
	TIME [epoch: 10.6 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1415033632037076		[learning rate: 0.00029485]
	Learning Rate: 0.000294849
	LOSS [training: 0.1415033632037076 | validation: 0.1436094386831617]
	TIME [epoch: 10.6 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10962518498259093		[learning rate: 0.00029395]
	Learning Rate: 0.000293945
	LOSS [training: 0.10962518498259093 | validation: 0.13283991437759612]
	TIME [epoch: 10.6 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10495655893185232		[learning rate: 0.00029304]
	Learning Rate: 0.000293044
	LOSS [training: 0.10495655893185232 | validation: 0.15510072607902683]
	TIME [epoch: 10.6 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11884244639851893		[learning rate: 0.00029215]
	Learning Rate: 0.000292146
	LOSS [training: 0.11884244639851893 | validation: 0.19397340020457926]
	TIME [epoch: 10.6 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12075644851427429		[learning rate: 0.00029125]
	Learning Rate: 0.00029125
	LOSS [training: 0.12075644851427429 | validation: 0.17063346729890547]
	TIME [epoch: 10.6 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1402888899887253		[learning rate: 0.00029036]
	Learning Rate: 0.000290358
	LOSS [training: 0.1402888899887253 | validation: 0.26453448266118196]
	TIME [epoch: 10.6 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14337285021407112		[learning rate: 0.00028947]
	Learning Rate: 0.000289468
	LOSS [training: 0.14337285021407112 | validation: 0.1764949174144935]
	TIME [epoch: 10.6 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11191894831386484		[learning rate: 0.00028858]
	Learning Rate: 0.00028858
	LOSS [training: 0.11191894831386484 | validation: 0.1411735415056947]
	TIME [epoch: 10.6 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12029474406180189		[learning rate: 0.0002877]
	Learning Rate: 0.000287696
	LOSS [training: 0.12029474406180189 | validation: 0.15586326749137489]
	TIME [epoch: 10.6 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13466142635828832		[learning rate: 0.00028681]
	Learning Rate: 0.000286814
	LOSS [training: 0.13466142635828832 | validation: 0.14344488311666181]
	TIME [epoch: 10.6 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13274370576191563		[learning rate: 0.00028593]
	Learning Rate: 0.000285935
	LOSS [training: 0.13274370576191563 | validation: 0.198748474381202]
	TIME [epoch: 10.6 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12874602115864248		[learning rate: 0.00028506]
	Learning Rate: 0.000285058
	LOSS [training: 0.12874602115864248 | validation: 0.13338475489462176]
	TIME [epoch: 10.6 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11630818547241957		[learning rate: 0.00028418]
	Learning Rate: 0.000284184
	LOSS [training: 0.11630818547241957 | validation: 0.1533211659607735]
	TIME [epoch: 10.6 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13238846818590516		[learning rate: 0.00028331]
	Learning Rate: 0.000283313
	LOSS [training: 0.13238846818590516 | validation: 0.12657029511096432]
	TIME [epoch: 10.6 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.118784260697872		[learning rate: 0.00028244]
	Learning Rate: 0.000282445
	LOSS [training: 0.118784260697872 | validation: 0.1512553078317287]
	TIME [epoch: 10.6 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1271824711331103		[learning rate: 0.00028158]
	Learning Rate: 0.000281579
	LOSS [training: 0.1271824711331103 | validation: 0.15631579744097507]
	TIME [epoch: 10.6 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1389163161876779		[learning rate: 0.00028072]
	Learning Rate: 0.000280716
	LOSS [training: 0.1389163161876779 | validation: 0.20449482895650214]
	TIME [epoch: 10.6 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1410886583485153		[learning rate: 0.00027986]
	Learning Rate: 0.000279855
	LOSS [training: 0.1410886583485153 | validation: 0.16338477353183872]
	TIME [epoch: 10.6 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12192401039425023		[learning rate: 0.000279]
	Learning Rate: 0.000278997
	LOSS [training: 0.12192401039425023 | validation: 0.13190215596687682]
	TIME [epoch: 10.6 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10801258132045066		[learning rate: 0.00027814]
	Learning Rate: 0.000278142
	LOSS [training: 0.10801258132045066 | validation: 0.1514888420728375]
	TIME [epoch: 10.6 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12686799782509298		[learning rate: 0.00027729]
	Learning Rate: 0.000277289
	LOSS [training: 0.12686799782509298 | validation: 0.15491485815064107]
	TIME [epoch: 10.6 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1138630865604223		[learning rate: 0.00027644]
	Learning Rate: 0.000276439
	LOSS [training: 0.1138630865604223 | validation: 0.1388927585056685]
	TIME [epoch: 10.6 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11008436955751363		[learning rate: 0.00027559]
	Learning Rate: 0.000275592
	LOSS [training: 0.11008436955751363 | validation: 0.14761848347555856]
	TIME [epoch: 10.6 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1183815323583006		[learning rate: 0.00027475]
	Learning Rate: 0.000274747
	LOSS [training: 0.1183815323583006 | validation: 0.1548976175356484]
	TIME [epoch: 10.6 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12019279674788268		[learning rate: 0.00027391]
	Learning Rate: 0.000273905
	LOSS [training: 0.12019279674788268 | validation: 0.2337920593582473]
	TIME [epoch: 10.6 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12681138831664407		[learning rate: 0.00027307]
	Learning Rate: 0.000273065
	LOSS [training: 0.12681138831664407 | validation: 0.21242491316648904]
	TIME [epoch: 10.6 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1261764194514151		[learning rate: 0.00027223]
	Learning Rate: 0.000272228
	LOSS [training: 0.1261764194514151 | validation: 0.14159009670501005]
	TIME [epoch: 10.6 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09537025603622704		[learning rate: 0.00027139]
	Learning Rate: 0.000271394
	LOSS [training: 0.09537025603622704 | validation: 0.16586696025387349]
	TIME [epoch: 10.6 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10381838029984428		[learning rate: 0.00027056]
	Learning Rate: 0.000270562
	LOSS [training: 0.10381838029984428 | validation: 0.15531244550499165]
	TIME [epoch: 10.6 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09927035729416471		[learning rate: 0.00026973]
	Learning Rate: 0.000269733
	LOSS [training: 0.09927035729416471 | validation: 0.16387450389764424]
	TIME [epoch: 10.6 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09513748869791994		[learning rate: 0.00026891]
	Learning Rate: 0.000268906
	LOSS [training: 0.09513748869791994 | validation: 0.1746128019423881]
	TIME [epoch: 10.5 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10075463732931625		[learning rate: 0.00026808]
	Learning Rate: 0.000268081
	LOSS [training: 0.10075463732931625 | validation: 0.1652229658994875]
	TIME [epoch: 10.6 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09635800563489602		[learning rate: 0.00026726]
	Learning Rate: 0.00026726
	LOSS [training: 0.09635800563489602 | validation: 0.1570407264077988]
	TIME [epoch: 10.6 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11225475665374059		[learning rate: 0.00026644]
	Learning Rate: 0.00026644
	LOSS [training: 0.11225475665374059 | validation: 0.2295379246705286]
	TIME [epoch: 10.6 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15631151746730992		[learning rate: 0.00026562]
	Learning Rate: 0.000265624
	LOSS [training: 0.15631151746730992 | validation: 0.19965038635491816]
	TIME [epoch: 10.6 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10187743835369192		[learning rate: 0.00026481]
	Learning Rate: 0.000264809
	LOSS [training: 0.10187743835369192 | validation: 0.17534886305361522]
	TIME [epoch: 10.6 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09375302375325184		[learning rate: 0.000264]
	Learning Rate: 0.000263998
	LOSS [training: 0.09375302375325184 | validation: 0.1928458610302053]
	TIME [epoch: 10.6 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1369293307026916		[learning rate: 0.00026319]
	Learning Rate: 0.000263188
	LOSS [training: 0.1369293307026916 | validation: 0.1987041016886568]
	TIME [epoch: 10.6 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11181913263754739		[learning rate: 0.00026238]
	Learning Rate: 0.000262382
	LOSS [training: 0.11181913263754739 | validation: 0.16091187949610425]
	TIME [epoch: 10.6 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09936979837512219		[learning rate: 0.00026158]
	Learning Rate: 0.000261577
	LOSS [training: 0.09936979837512219 | validation: 0.13714483662622542]
	TIME [epoch: 10.6 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09404525420472268		[learning rate: 0.00026078]
	Learning Rate: 0.000260775
	LOSS [training: 0.09404525420472268 | validation: 0.15915676856407132]
	TIME [epoch: 10.6 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11490008185604364		[learning rate: 0.00025998]
	Learning Rate: 0.000259976
	LOSS [training: 0.11490008185604364 | validation: 0.1624148653888041]
	TIME [epoch: 10.6 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09676939493629413		[learning rate: 0.00025918]
	Learning Rate: 0.000259179
	LOSS [training: 0.09676939493629413 | validation: 0.1489454007556948]
	TIME [epoch: 10.6 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10387023545522638		[learning rate: 0.00025838]
	Learning Rate: 0.000258385
	LOSS [training: 0.10387023545522638 | validation: 0.16515756928922287]
	TIME [epoch: 10.6 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11684530045513022		[learning rate: 0.00025759]
	Learning Rate: 0.000257593
	LOSS [training: 0.11684530045513022 | validation: 0.14044373760974954]
	TIME [epoch: 10.6 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12879753598564328		[learning rate: 0.0002568]
	Learning Rate: 0.000256803
	LOSS [training: 0.12879753598564328 | validation: 0.1742545117688052]
	TIME [epoch: 10.6 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1464227501562118		[learning rate: 0.00025602]
	Learning Rate: 0.000256016
	LOSS [training: 0.1464227501562118 | validation: 0.14064891281184286]
	TIME [epoch: 10.6 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.097200532965207		[learning rate: 0.00025523]
	Learning Rate: 0.000255231
	LOSS [training: 0.097200532965207 | validation: 0.1198097921914681]
	TIME [epoch: 10.6 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09079578054772794		[learning rate: 0.00025445]
	Learning Rate: 0.000254449
	LOSS [training: 0.09079578054772794 | validation: 0.1418815348094161]
	TIME [epoch: 10.6 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1125692704606815		[learning rate: 0.00025367]
	Learning Rate: 0.000253669
	LOSS [training: 0.1125692704606815 | validation: 0.1332587132513407]
	TIME [epoch: 10.6 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1076063065953605		[learning rate: 0.00025289]
	Learning Rate: 0.000252891
	LOSS [training: 0.1076063065953605 | validation: 0.1176431499791487]
	TIME [epoch: 10.6 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09906496445743693		[learning rate: 0.00025212]
	Learning Rate: 0.000252116
	LOSS [training: 0.09906496445743693 | validation: 0.13916475349437904]
	TIME [epoch: 10.6 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09988374813073353		[learning rate: 0.00025134]
	Learning Rate: 0.000251343
	LOSS [training: 0.09988374813073353 | validation: 0.14030013459305313]
	TIME [epoch: 10.6 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13041849951242981		[learning rate: 0.00025057]
	Learning Rate: 0.000250572
	LOSS [training: 0.13041849951242981 | validation: 0.21696587884994728]
	TIME [epoch: 10.6 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1122131188552444		[learning rate: 0.0002498]
	Learning Rate: 0.000249804
	LOSS [training: 0.1122131188552444 | validation: 0.12332842762273905]
	TIME [epoch: 10.6 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07942715953864712		[learning rate: 0.00024904]
	Learning Rate: 0.000249039
	LOSS [training: 0.07942715953864712 | validation: 0.1737627276196271]
	TIME [epoch: 10.6 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11139415922374121		[learning rate: 0.00024828]
	Learning Rate: 0.000248275
	LOSS [training: 0.11139415922374121 | validation: 0.14139135479740625]
	TIME [epoch: 10.6 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09274868596492819		[learning rate: 0.00024751]
	Learning Rate: 0.000247514
	LOSS [training: 0.09274868596492819 | validation: 0.16972586383477306]
	TIME [epoch: 10.6 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09878290314326092		[learning rate: 0.00024676]
	Learning Rate: 0.000246755
	LOSS [training: 0.09878290314326092 | validation: 0.11817730861212229]
	TIME [epoch: 10.6 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09050461226743667		[learning rate: 0.000246]
	Learning Rate: 0.000245999
	LOSS [training: 0.09050461226743667 | validation: 0.11548426994399012]
	TIME [epoch: 10.6 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08904839649759515		[learning rate: 0.00024524]
	Learning Rate: 0.000245245
	LOSS [training: 0.08904839649759515 | validation: 0.1296892311455234]
	TIME [epoch: 10.6 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09478601967627882		[learning rate: 0.00024449]
	Learning Rate: 0.000244493
	LOSS [training: 0.09478601967627882 | validation: 0.18533597121256526]
	TIME [epoch: 10.6 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1193558164218329		[learning rate: 0.00024374]
	Learning Rate: 0.000243744
	LOSS [training: 0.1193558164218329 | validation: 0.1382907917463893]
	TIME [epoch: 10.6 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08566208343358436		[learning rate: 0.000243]
	Learning Rate: 0.000242996
	LOSS [training: 0.08566208343358436 | validation: 0.12413622415241025]
	TIME [epoch: 10.6 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10077406274098474		[learning rate: 0.00024225]
	Learning Rate: 0.000242252
	LOSS [training: 0.10077406274098474 | validation: 0.1533454817275539]
	TIME [epoch: 10.6 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10850395901626267		[learning rate: 0.00024151]
	Learning Rate: 0.000241509
	LOSS [training: 0.10850395901626267 | validation: 0.11721636577724968]
	TIME [epoch: 10.6 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08629100375504235		[learning rate: 0.00024077]
	Learning Rate: 0.000240769
	LOSS [training: 0.08629100375504235 | validation: 0.14769017431119313]
	TIME [epoch: 10.6 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11948907188573958		[learning rate: 0.00024003]
	Learning Rate: 0.000240031
	LOSS [training: 0.11948907188573958 | validation: 0.16219901074165363]
	TIME [epoch: 10.6 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10031291646991933		[learning rate: 0.00023929]
	Learning Rate: 0.000239295
	LOSS [training: 0.10031291646991933 | validation: 0.15977345940423893]
	TIME [epoch: 10.6 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11490798013970609		[learning rate: 0.00023856]
	Learning Rate: 0.000238561
	LOSS [training: 0.11490798013970609 | validation: 0.17622296294031153]
	TIME [epoch: 10.6 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09391136557778035		[learning rate: 0.00023783]
	Learning Rate: 0.00023783
	LOSS [training: 0.09391136557778035 | validation: 0.13449113858786102]
	TIME [epoch: 10.6 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07826785397912081		[learning rate: 0.0002371]
	Learning Rate: 0.000237101
	LOSS [training: 0.07826785397912081 | validation: 0.15167352487143712]
	TIME [epoch: 10.6 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.084821832504803		[learning rate: 0.00023637]
	Learning Rate: 0.000236374
	LOSS [training: 0.084821832504803 | validation: 0.15466206489330728]
	TIME [epoch: 10.6 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1341201183710435		[learning rate: 0.00023565]
	Learning Rate: 0.00023565
	LOSS [training: 0.1341201183710435 | validation: 0.22311641059291823]
	TIME [epoch: 10.6 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11684761950095217		[learning rate: 0.00023493]
	Learning Rate: 0.000234927
	LOSS [training: 0.11684761950095217 | validation: 0.15736554757064083]
	TIME [epoch: 10.6 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09642515631992661		[learning rate: 0.00023421]
	Learning Rate: 0.000234207
	LOSS [training: 0.09642515631992661 | validation: 0.16766015019285285]
	TIME [epoch: 10.6 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10307037871514665		[learning rate: 0.00023349]
	Learning Rate: 0.000233489
	LOSS [training: 0.10307037871514665 | validation: 0.14476537257008404]
	TIME [epoch: 10.6 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09423135146461296		[learning rate: 0.00023277]
	Learning Rate: 0.000232773
	LOSS [training: 0.09423135146461296 | validation: 0.21787833355429598]
	TIME [epoch: 10.6 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14069035453265738		[learning rate: 0.00023206]
	Learning Rate: 0.00023206
	LOSS [training: 0.14069035453265738 | validation: 0.18265221963787237]
	TIME [epoch: 10.6 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10454741384860118		[learning rate: 0.00023135]
	Learning Rate: 0.000231348
	LOSS [training: 0.10454741384860118 | validation: 0.14958789169561015]
	TIME [epoch: 10.6 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09581711496843963		[learning rate: 0.00023064]
	Learning Rate: 0.000230639
	LOSS [training: 0.09581711496843963 | validation: 0.15542745414296374]
	TIME [epoch: 10.5 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08301256931030537		[learning rate: 0.00022993]
	Learning Rate: 0.000229932
	LOSS [training: 0.08301256931030537 | validation: 0.12988132707333383]
	TIME [epoch: 10.6 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09836115012578911		[learning rate: 0.00022923]
	Learning Rate: 0.000229227
	LOSS [training: 0.09836115012578911 | validation: 0.1879362756988964]
	TIME [epoch: 10.6 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10044207483575573		[learning rate: 0.00022852]
	Learning Rate: 0.000228525
	LOSS [training: 0.10044207483575573 | validation: 0.15507072085843177]
	TIME [epoch: 10.6 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10415444981162851		[learning rate: 0.00022782]
	Learning Rate: 0.000227824
	LOSS [training: 0.10415444981162851 | validation: 0.18251543983418814]
	TIME [epoch: 10.6 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09110257544252771		[learning rate: 0.00022713]
	Learning Rate: 0.000227126
	LOSS [training: 0.09110257544252771 | validation: 0.15176145197148688]
	TIME [epoch: 10.6 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09236069004375319		[learning rate: 0.00022643]
	Learning Rate: 0.00022643
	LOSS [training: 0.09236069004375319 | validation: 0.15350619963343395]
	TIME [epoch: 10.6 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08191846273680141		[learning rate: 0.00022574]
	Learning Rate: 0.000225736
	LOSS [training: 0.08191846273680141 | validation: 0.1470464438582373]
	TIME [epoch: 10.5 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08971217016251293		[learning rate: 0.00022504]
	Learning Rate: 0.000225044
	LOSS [training: 0.08971217016251293 | validation: 0.16633891403384887]
	TIME [epoch: 10.6 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09673197442334555		[learning rate: 0.00022435]
	Learning Rate: 0.000224354
	LOSS [training: 0.09673197442334555 | validation: 0.22586581964813732]
	TIME [epoch: 10.6 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15866140364978298		[learning rate: 0.00022367]
	Learning Rate: 0.000223666
	LOSS [training: 0.15866140364978298 | validation: 0.15018424481403364]
	TIME [epoch: 10.6 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09524344126291977		[learning rate: 0.00022298]
	Learning Rate: 0.00022298
	LOSS [training: 0.09524344126291977 | validation: 0.22760437303767733]
	TIME [epoch: 10.6 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13303570257069236		[learning rate: 0.0002223]
	Learning Rate: 0.000222297
	LOSS [training: 0.13303570257069236 | validation: 0.23059629520925629]
	TIME [epoch: 10.6 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10415605822238116		[learning rate: 0.00022162]
	Learning Rate: 0.000221615
	LOSS [training: 0.10415605822238116 | validation: 0.16949477952703618]
	TIME [epoch: 10.6 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08748796349525156		[learning rate: 0.00022094]
	Learning Rate: 0.000220936
	LOSS [training: 0.08748796349525156 | validation: 0.17084361678355628]
	TIME [epoch: 10.6 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09447794973840754		[learning rate: 0.00022026]
	Learning Rate: 0.000220259
	LOSS [training: 0.09447794973840754 | validation: 0.1627808468465301]
	TIME [epoch: 10.6 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08771263952174461		[learning rate: 0.00021958]
	Learning Rate: 0.000219584
	LOSS [training: 0.08771263952174461 | validation: 0.20861614983309235]
	TIME [epoch: 10.6 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11057708365695484		[learning rate: 0.00021891]
	Learning Rate: 0.000218911
	LOSS [training: 0.11057708365695484 | validation: 0.1539691977078799]
	TIME [epoch: 10.6 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08599292988114979		[learning rate: 0.00021824]
	Learning Rate: 0.000218239
	LOSS [training: 0.08599292988114979 | validation: 0.17268258862849925]
	TIME [epoch: 10.6 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0907417179435849		[learning rate: 0.00021757]
	Learning Rate: 0.00021757
	LOSS [training: 0.0907417179435849 | validation: 0.14811612303049063]
	TIME [epoch: 10.6 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08883626090630362		[learning rate: 0.0002169]
	Learning Rate: 0.000216904
	LOSS [training: 0.08883626090630362 | validation: 0.17129602123308993]
	TIME [epoch: 10.6 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12317653500466878		[learning rate: 0.00021624]
	Learning Rate: 0.000216239
	LOSS [training: 0.12317653500466878 | validation: 0.15548213043503176]
	TIME [epoch: 10.6 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13206047351209688		[learning rate: 0.00021558]
	Learning Rate: 0.000215576
	LOSS [training: 0.13206047351209688 | validation: 0.1699117296778477]
	TIME [epoch: 10.6 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.099640940818233		[learning rate: 0.00021491]
	Learning Rate: 0.000214915
	LOSS [training: 0.099640940818233 | validation: 0.13493121787363638]
	TIME [epoch: 10.6 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09812412879291588		[learning rate: 0.00021426]
	Learning Rate: 0.000214256
	LOSS [training: 0.09812412879291588 | validation: 0.12930524704818475]
	TIME [epoch: 10.6 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09752342947374122		[learning rate: 0.0002136]
	Learning Rate: 0.000213599
	LOSS [training: 0.09752342947374122 | validation: 0.22219382015133357]
	TIME [epoch: 10.6 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11083583600836448		[learning rate: 0.00021294]
	Learning Rate: 0.000212945
	LOSS [training: 0.11083583600836448 | validation: 0.1269820007373022]
	TIME [epoch: 10.6 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09002283801366603		[learning rate: 0.00021229]
	Learning Rate: 0.000212292
	LOSS [training: 0.09002283801366603 | validation: 0.12943986582285888]
	TIME [epoch: 10.6 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08835516072193102		[learning rate: 0.00021164]
	Learning Rate: 0.000211641
	LOSS [training: 0.08835516072193102 | validation: 0.1347061916882527]
	TIME [epoch: 10.6 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10040616282006938		[learning rate: 0.00021099]
	Learning Rate: 0.000210992
	LOSS [training: 0.10040616282006938 | validation: 0.1366293903254236]
	TIME [epoch: 10.5 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10348016207555674		[learning rate: 0.00021035]
	Learning Rate: 0.000210346
	LOSS [training: 0.10348016207555674 | validation: 0.12188074123753641]
	TIME [epoch: 10.6 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11375152469853655		[learning rate: 0.0002097]
	Learning Rate: 0.000209701
	LOSS [training: 0.11375152469853655 | validation: 0.15075427842029165]
	TIME [epoch: 10.6 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08949976831223472		[learning rate: 0.00020906]
	Learning Rate: 0.000209058
	LOSS [training: 0.08949976831223472 | validation: 0.13216220023561553]
	TIME [epoch: 10.6 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10147665839171675		[learning rate: 0.00020842]
	Learning Rate: 0.000208417
	LOSS [training: 0.10147665839171675 | validation: 0.15224888537179498]
	TIME [epoch: 10.6 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11901260231813811		[learning rate: 0.00020778]
	Learning Rate: 0.000207778
	LOSS [training: 0.11901260231813811 | validation: 0.1350006083314033]
	TIME [epoch: 10.6 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1046026322425535		[learning rate: 0.00020714]
	Learning Rate: 0.000207141
	LOSS [training: 0.1046026322425535 | validation: 0.14188692903552366]
	TIME [epoch: 10.6 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11581869750203928		[learning rate: 0.00020651]
	Learning Rate: 0.000206506
	LOSS [training: 0.11581869750203928 | validation: 0.14983540154702202]
	TIME [epoch: 10.6 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12352521253526168		[learning rate: 0.00020587]
	Learning Rate: 0.000205873
	LOSS [training: 0.12352521253526168 | validation: 0.14245822559562446]
	TIME [epoch: 10.6 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11815480858549945		[learning rate: 0.00020524]
	Learning Rate: 0.000205242
	LOSS [training: 0.11815480858549945 | validation: 0.12377084682986439]
	TIME [epoch: 10.6 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12233763437449618		[learning rate: 0.00020461]
	Learning Rate: 0.000204613
	LOSS [training: 0.12233763437449618 | validation: 0.19751657256773034]
	TIME [epoch: 10.6 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13790489111666665		[learning rate: 0.00020399]
	Learning Rate: 0.000203986
	LOSS [training: 0.13790489111666665 | validation: 0.1945460392746925]
	TIME [epoch: 10.6 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11727949329661488		[learning rate: 0.00020336]
	Learning Rate: 0.00020336
	LOSS [training: 0.11727949329661488 | validation: 0.15635030788041476]
	TIME [epoch: 10.6 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10018391493800383		[learning rate: 0.00020274]
	Learning Rate: 0.000202737
	LOSS [training: 0.10018391493800383 | validation: 0.15376850293159275]
	TIME [epoch: 10.6 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11169073083551088		[learning rate: 0.00020212]
	Learning Rate: 0.000202116
	LOSS [training: 0.11169073083551088 | validation: 0.1486808864664645]
	TIME [epoch: 10.6 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11067309323031534		[learning rate: 0.0002015]
	Learning Rate: 0.000201496
	LOSS [training: 0.11067309323031534 | validation: 0.13938376747347295]
	TIME [epoch: 10.6 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11254449469091425		[learning rate: 0.00020088]
	Learning Rate: 0.000200878
	LOSS [training: 0.11254449469091425 | validation: 0.15480259247547587]
	TIME [epoch: 10.6 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1186426357801573		[learning rate: 0.00020026]
	Learning Rate: 0.000200263
	LOSS [training: 0.1186426357801573 | validation: 0.15641722657445215]
	TIME [epoch: 10.6 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09712735133753456		[learning rate: 0.00019965]
	Learning Rate: 0.000199649
	LOSS [training: 0.09712735133753456 | validation: 0.1486235770751084]
	TIME [epoch: 10.6 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10530264629537492		[learning rate: 0.00019904]
	Learning Rate: 0.000199037
	LOSS [training: 0.10530264629537492 | validation: 0.16504701531182014]
	TIME [epoch: 10.6 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09949077268922546		[learning rate: 0.00019843]
	Learning Rate: 0.000198427
	LOSS [training: 0.09949077268922546 | validation: 0.17098462249105248]
	TIME [epoch: 10.6 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08632131841167108		[learning rate: 0.00019782]
	Learning Rate: 0.000197818
	LOSS [training: 0.08632131841167108 | validation: 0.1465898869174426]
	TIME [epoch: 10.6 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07744659086898813		[learning rate: 0.00019721]
	Learning Rate: 0.000197212
	LOSS [training: 0.07744659086898813 | validation: 0.14339447524503185]
	TIME [epoch: 10.6 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08129503020915207		[learning rate: 0.00019661]
	Learning Rate: 0.000196607
	LOSS [training: 0.08129503020915207 | validation: 0.14422951837520528]
	TIME [epoch: 10.6 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0801018086828226		[learning rate: 0.000196]
	Learning Rate: 0.000196005
	LOSS [training: 0.0801018086828226 | validation: 0.15579333135834805]
	TIME [epoch: 10.6 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08641550318333804		[learning rate: 0.0001954]
	Learning Rate: 0.000195404
	LOSS [training: 0.08641550318333804 | validation: 0.13691420545834243]
	TIME [epoch: 10.6 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08969195854312298		[learning rate: 0.0001948]
	Learning Rate: 0.000194805
	LOSS [training: 0.08969195854312298 | validation: 0.128228260688723]
	TIME [epoch: 10.6 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08081674006269499		[learning rate: 0.00019421]
	Learning Rate: 0.000194208
	LOSS [training: 0.08081674006269499 | validation: 0.14934474485076923]
	TIME [epoch: 10.6 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10735027480240837		[learning rate: 0.00019361]
	Learning Rate: 0.000193612
	LOSS [training: 0.10735027480240837 | validation: 0.15835850623804212]
	TIME [epoch: 10.6 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10050271665346563		[learning rate: 0.00019302]
	Learning Rate: 0.000193019
	LOSS [training: 0.10050271665346563 | validation: 0.21223049005143652]
	TIME [epoch: 10.6 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12027959401507568		[learning rate: 0.00019243]
	Learning Rate: 0.000192427
	LOSS [training: 0.12027959401507568 | validation: 0.15148216887199709]
	TIME [epoch: 10.6 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08642558010119429		[learning rate: 0.00019184]
	Learning Rate: 0.000191837
	LOSS [training: 0.08642558010119429 | validation: 0.12801367519242698]
	TIME [epoch: 10.6 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10545381578258162		[learning rate: 0.00019125]
	Learning Rate: 0.000191249
	LOSS [training: 0.10545381578258162 | validation: 0.11285414483783879]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1789.pth
	Model improved!!!
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09849256179862337		[learning rate: 0.00019066]
	Learning Rate: 0.000190663
	LOSS [training: 0.09849256179862337 | validation: 0.12297216198524213]
	TIME [epoch: 10.6 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09805645013596674		[learning rate: 0.00019008]
	Learning Rate: 0.000190079
	LOSS [training: 0.09805645013596674 | validation: 0.1422271201152783]
	TIME [epoch: 10.6 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12843896917982772		[learning rate: 0.0001895]
	Learning Rate: 0.000189496
	LOSS [training: 0.12843896917982772 | validation: 0.13095987642393284]
	TIME [epoch: 10.6 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11536168439175835		[learning rate: 0.00018892]
	Learning Rate: 0.000188915
	LOSS [training: 0.11536168439175835 | validation: 0.1581913843008891]
	TIME [epoch: 10.6 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12593170692566694		[learning rate: 0.00018834]
	Learning Rate: 0.000188336
	LOSS [training: 0.12593170692566694 | validation: 0.16068867368578352]
	TIME [epoch: 10.6 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09929638121358571		[learning rate: 0.00018776]
	Learning Rate: 0.000187759
	LOSS [training: 0.09929638121358571 | validation: 0.11587072831532721]
	TIME [epoch: 10.6 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09337120860203656		[learning rate: 0.00018718]
	Learning Rate: 0.000187183
	LOSS [training: 0.09337120860203656 | validation: 0.12536524038129282]
	TIME [epoch: 10.6 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09378444022374746		[learning rate: 0.00018661]
	Learning Rate: 0.000186609
	LOSS [training: 0.09378444022374746 | validation: 0.1432573664935307]
	TIME [epoch: 10.6 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10629805122270422		[learning rate: 0.00018604]
	Learning Rate: 0.000186037
	LOSS [training: 0.10629805122270422 | validation: 0.1287493729450272]
	TIME [epoch: 10.6 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08659728395977809		[learning rate: 0.00018547]
	Learning Rate: 0.000185467
	LOSS [training: 0.08659728395977809 | validation: 0.1594006058375653]
	TIME [epoch: 10.6 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10094609043138195		[learning rate: 0.0001849]
	Learning Rate: 0.000184898
	LOSS [training: 0.10094609043138195 | validation: 0.16366970706162717]
	TIME [epoch: 10.6 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08645569340547796		[learning rate: 0.00018433]
	Learning Rate: 0.000184332
	LOSS [training: 0.08645569340547796 | validation: 0.13899860953115895]
	TIME [epoch: 10.6 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07522997488698045		[learning rate: 0.00018377]
	Learning Rate: 0.000183767
	LOSS [training: 0.07522997488698045 | validation: 0.15137991738725629]
	TIME [epoch: 10.6 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08052517767369294		[learning rate: 0.0001832]
	Learning Rate: 0.000183203
	LOSS [training: 0.08052517767369294 | validation: 0.14601610749266308]
	TIME [epoch: 10.6 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08212128573512449		[learning rate: 0.00018264]
	Learning Rate: 0.000182642
	LOSS [training: 0.08212128573512449 | validation: 0.1607912454389368]
	TIME [epoch: 10.6 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08303790134515544		[learning rate: 0.00018208]
	Learning Rate: 0.000182082
	LOSS [training: 0.08303790134515544 | validation: 0.15771421229299468]
	TIME [epoch: 10.6 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09015861608604146		[learning rate: 0.00018152]
	Learning Rate: 0.000181524
	LOSS [training: 0.09015861608604146 | validation: 0.1243848718212948]
	TIME [epoch: 10.6 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08906673580798671		[learning rate: 0.00018097]
	Learning Rate: 0.000180967
	LOSS [training: 0.08906673580798671 | validation: 0.13934990850220869]
	TIME [epoch: 10.6 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10782212864796299		[learning rate: 0.00018041]
	Learning Rate: 0.000180412
	LOSS [training: 0.10782212864796299 | validation: 0.18891281326357276]
	TIME [epoch: 10.6 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08774008219159761		[learning rate: 0.00017986]
	Learning Rate: 0.000179859
	LOSS [training: 0.08774008219159761 | validation: 0.12836585256033076]
	TIME [epoch: 10.6 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09948103796103591		[learning rate: 0.00017931]
	Learning Rate: 0.000179308
	LOSS [training: 0.09948103796103591 | validation: 0.13711522030818782]
	TIME [epoch: 10.6 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09189250252217515		[learning rate: 0.00017876]
	Learning Rate: 0.000178758
	LOSS [training: 0.09189250252217515 | validation: 0.13056953310256156]
	TIME [epoch: 10.6 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07979824143087805		[learning rate: 0.00017821]
	Learning Rate: 0.00017821
	LOSS [training: 0.07979824143087805 | validation: 0.10298565459215446]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1812.pth
	Model improved!!!
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09242402707863213		[learning rate: 0.00017766]
	Learning Rate: 0.000177664
	LOSS [training: 0.09242402707863213 | validation: 0.17465099206252305]
	TIME [epoch: 10.6 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09466260612491764		[learning rate: 0.00017712]
	Learning Rate: 0.00017712
	LOSS [training: 0.09466260612491764 | validation: 0.17515262367157242]
	TIME [epoch: 10.6 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11346156603357849		[learning rate: 0.00017658]
	Learning Rate: 0.000176577
	LOSS [training: 0.11346156603357849 | validation: 0.16226911468531782]
	TIME [epoch: 10.5 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09775663297594689		[learning rate: 0.00017604]
	Learning Rate: 0.000176035
	LOSS [training: 0.09775663297594689 | validation: 0.1281995265953817]
	TIME [epoch: 10.6 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0891157406387139		[learning rate: 0.0001755]
	Learning Rate: 0.000175496
	LOSS [training: 0.0891157406387139 | validation: 0.14358832442955005]
	TIME [epoch: 10.6 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0879532677118546		[learning rate: 0.00017496]
	Learning Rate: 0.000174958
	LOSS [training: 0.0879532677118546 | validation: 0.11390724351086615]
	TIME [epoch: 10.6 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09011824662869798		[learning rate: 0.00017442]
	Learning Rate: 0.000174421
	LOSS [training: 0.09011824662869798 | validation: 0.12117417593044734]
	TIME [epoch: 10.6 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0917612559704138		[learning rate: 0.00017389]
	Learning Rate: 0.000173887
	LOSS [training: 0.0917612559704138 | validation: 0.10974156004153247]
	TIME [epoch: 10.6 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09301762016656354		[learning rate: 0.00017335]
	Learning Rate: 0.000173354
	LOSS [training: 0.09301762016656354 | validation: 0.14030627405763227]
	TIME [epoch: 10.6 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0983914614647868		[learning rate: 0.00017282]
	Learning Rate: 0.000172822
	LOSS [training: 0.0983914614647868 | validation: 0.11148368479673203]
	TIME [epoch: 10.6 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09965169989010034		[learning rate: 0.00017229]
	Learning Rate: 0.000172293
	LOSS [training: 0.09965169989010034 | validation: 0.17123541027195407]
	TIME [epoch: 10.6 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1023292515247203		[learning rate: 0.00017176]
	Learning Rate: 0.000171764
	LOSS [training: 0.1023292515247203 | validation: 0.1444834403563936]
	TIME [epoch: 10.6 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10644892083313029		[learning rate: 0.00017124]
	Learning Rate: 0.000171238
	LOSS [training: 0.10644892083313029 | validation: 0.10751110517748433]
	TIME [epoch: 10.6 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09266437236586038		[learning rate: 0.00017071]
	Learning Rate: 0.000170713
	LOSS [training: 0.09266437236586038 | validation: 0.10708459134069244]
	TIME [epoch: 10.6 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09775801941619489		[learning rate: 0.00017019]
	Learning Rate: 0.00017019
	LOSS [training: 0.09775801941619489 | validation: 0.14623311724018373]
	TIME [epoch: 10.6 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09422007993477574		[learning rate: 0.00016967]
	Learning Rate: 0.000169668
	LOSS [training: 0.09422007993477574 | validation: 0.10911216439247898]
	TIME [epoch: 10.6 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13867715858231255		[learning rate: 0.00016915]
	Learning Rate: 0.000169148
	LOSS [training: 0.13867715858231255 | validation: 0.18028121295648258]
	TIME [epoch: 10.6 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1202276183536916		[learning rate: 0.00016863]
	Learning Rate: 0.000168629
	LOSS [training: 0.1202276183536916 | validation: 0.1178518892054472]
	TIME [epoch: 10.6 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08408361706569827		[learning rate: 0.00016811]
	Learning Rate: 0.000168112
	LOSS [training: 0.08408361706569827 | validation: 0.1295491153797437]
	TIME [epoch: 10.6 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08320612574155625		[learning rate: 0.0001676]
	Learning Rate: 0.000167597
	LOSS [training: 0.08320612574155625 | validation: 0.1181953555632628]
	TIME [epoch: 10.6 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08944778209784468		[learning rate: 0.00016708]
	Learning Rate: 0.000167083
	LOSS [training: 0.08944778209784468 | validation: 0.11945954551406837]
	TIME [epoch: 10.6 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10118775936137454		[learning rate: 0.00016657]
	Learning Rate: 0.000166571
	LOSS [training: 0.10118775936137454 | validation: 0.1647257004241162]
	TIME [epoch: 10.6 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13251096751409677		[learning rate: 0.00016606]
	Learning Rate: 0.000166061
	LOSS [training: 0.13251096751409677 | validation: 0.12010378456806407]
	TIME [epoch: 10.6 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08862172394314896		[learning rate: 0.00016555]
	Learning Rate: 0.000165552
	LOSS [training: 0.08862172394314896 | validation: 0.1332457714022892]
	TIME [epoch: 10.6 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09861580675868012		[learning rate: 0.00016504]
	Learning Rate: 0.000165044
	LOSS [training: 0.09861580675868012 | validation: 0.11556705795862632]
	TIME [epoch: 10.6 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0896529687869188		[learning rate: 0.00016454]
	Learning Rate: 0.000164538
	LOSS [training: 0.0896529687869188 | validation: 0.11604229609601838]
	TIME [epoch: 10.6 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07574006168300068		[learning rate: 0.00016403]
	Learning Rate: 0.000164034
	LOSS [training: 0.07574006168300068 | validation: 0.12856396085289087]
	TIME [epoch: 10.6 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10072596393812669		[learning rate: 0.00016353]
	Learning Rate: 0.000163531
	LOSS [training: 0.10072596393812669 | validation: 0.13386239448168122]
	TIME [epoch: 10.6 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0870706894259808		[learning rate: 0.00016303]
	Learning Rate: 0.00016303
	LOSS [training: 0.0870706894259808 | validation: 0.125104813118399]
	TIME [epoch: 10.6 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09601529363378412		[learning rate: 0.00016253]
	Learning Rate: 0.00016253
	LOSS [training: 0.09601529363378412 | validation: 0.10376028057423207]
	TIME [epoch: 10.6 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08022706002874769		[learning rate: 0.00016203]
	Learning Rate: 0.000162032
	LOSS [training: 0.08022706002874769 | validation: 0.15223072324004863]
	TIME [epoch: 10.6 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0838724634418525		[learning rate: 0.00016153]
	Learning Rate: 0.000161535
	LOSS [training: 0.0838724634418525 | validation: 0.1306219585992788]
	TIME [epoch: 10.6 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0801997876260459		[learning rate: 0.00016104]
	Learning Rate: 0.00016104
	LOSS [training: 0.0801997876260459 | validation: 0.13512786943245939]
	TIME [epoch: 10.6 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09567355909823685		[learning rate: 0.00016055]
	Learning Rate: 0.000160546
	LOSS [training: 0.09567355909823685 | validation: 0.14302995219111686]
	TIME [epoch: 10.6 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0865986735653446		[learning rate: 0.00016005]
	Learning Rate: 0.000160054
	LOSS [training: 0.0865986735653446 | validation: 0.15404301346457]
	TIME [epoch: 10.6 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1088056183899947		[learning rate: 0.00015956]
	Learning Rate: 0.000159563
	LOSS [training: 0.1088056183899947 | validation: 0.14920448894526836]
	TIME [epoch: 10.6 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09173649715016408		[learning rate: 0.00015907]
	Learning Rate: 0.000159074
	LOSS [training: 0.09173649715016408 | validation: 0.13424852475276222]
	TIME [epoch: 10.6 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07523670884952263		[learning rate: 0.00015859]
	Learning Rate: 0.000158587
	LOSS [training: 0.07523670884952263 | validation: 0.11294254201366584]
	TIME [epoch: 10.6 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0877500277342709		[learning rate: 0.0001581]
	Learning Rate: 0.000158101
	LOSS [training: 0.0877500277342709 | validation: 0.1030171615744067]
	TIME [epoch: 10.6 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08471572251768991		[learning rate: 0.00015762]
	Learning Rate: 0.000157616
	LOSS [training: 0.08471572251768991 | validation: 0.11680915496687203]
	TIME [epoch: 10.6 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08004464254362517		[learning rate: 0.00015713]
	Learning Rate: 0.000157133
	LOSS [training: 0.08004464254362517 | validation: 0.1070010679806456]
	TIME [epoch: 10.6 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07683049903346677		[learning rate: 0.00015665]
	Learning Rate: 0.000156651
	LOSS [training: 0.07683049903346677 | validation: 0.09869974460846023]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r5_20240217_093241/states/model_tr_study5_1854.pth
	Model improved!!!
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08146911778429855		[learning rate: 0.00015617]
	Learning Rate: 0.000156171
	LOSS [training: 0.08146911778429855 | validation: 0.12093340019801442]
	TIME [epoch: 10.6 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09612152897622579		[learning rate: 0.00015569]
	Learning Rate: 0.000155692
	LOSS [training: 0.09612152897622579 | validation: 0.10420506623371029]
	TIME [epoch: 10.6 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08157999388912215		[learning rate: 0.00015521]
	Learning Rate: 0.000155215
	LOSS [training: 0.08157999388912215 | validation: 0.11884684944268054]
	TIME [epoch: 10.6 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10317807896779096		[learning rate: 0.00015474]
	Learning Rate: 0.000154739
	LOSS [training: 0.10317807896779096 | validation: 0.164824303877462]
	TIME [epoch: 10.6 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11948075642021198		[learning rate: 0.00015426]
	Learning Rate: 0.000154265
	LOSS [training: 0.11948075642021198 | validation: 0.14838873608083952]
	TIME [epoch: 10.6 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12326375949217036		[learning rate: 0.00015379]
	Learning Rate: 0.000153792
	LOSS [training: 0.12326375949217036 | validation: 0.12951478782965178]
	TIME [epoch: 10.6 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10923011947376807		[learning rate: 0.00015332]
	Learning Rate: 0.00015332
	LOSS [training: 0.10923011947376807 | validation: 0.13746927977023152]
	TIME [epoch: 10.6 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09532283368248383		[learning rate: 0.00015285]
	Learning Rate: 0.00015285
	LOSS [training: 0.09532283368248383 | validation: 0.12323736710064552]
	TIME [epoch: 10.6 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09547682384700784		[learning rate: 0.00015238]
	Learning Rate: 0.000152382
	LOSS [training: 0.09547682384700784 | validation: 0.11450084057675301]
	TIME [epoch: 10.6 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09758610274308856		[learning rate: 0.00015191]
	Learning Rate: 0.000151915
	LOSS [training: 0.09758610274308856 | validation: 0.13717420083231693]
	TIME [epoch: 10.6 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10380658420469198		[learning rate: 0.00015145]
	Learning Rate: 0.000151449
	LOSS [training: 0.10380658420469198 | validation: 0.1232748561670649]
	TIME [epoch: 10.6 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09886054810066024		[learning rate: 0.00015098]
	Learning Rate: 0.000150985
	LOSS [training: 0.09886054810066024 | validation: 0.11552834246746403]
	TIME [epoch: 10.6 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08393996993066896		[learning rate: 0.00015052]
	Learning Rate: 0.000150522
	LOSS [training: 0.08393996993066896 | validation: 0.1401083302717997]
	TIME [epoch: 10.6 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10256090249151828		[learning rate: 0.00015006]
	Learning Rate: 0.000150061
	LOSS [training: 0.10256090249151828 | validation: 0.1628294083863944]
	TIME [epoch: 10.6 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09844163543116113		[learning rate: 0.0001496]
	Learning Rate: 0.000149601
	LOSS [training: 0.09844163543116113 | validation: 0.11579363211433513]
	TIME [epoch: 10.6 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.084069544007314		[learning rate: 0.00014914]
	Learning Rate: 0.000149142
	LOSS [training: 0.084069544007314 | validation: 0.10877228569694394]
	TIME [epoch: 10.6 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07873376824844933		[learning rate: 0.00014868]
	Learning Rate: 0.000148685
	LOSS [training: 0.07873376824844933 | validation: 0.12177724923611692]
	TIME [epoch: 10.6 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08657271441233563		[learning rate: 0.00014823]
	Learning Rate: 0.000148229
	LOSS [training: 0.08657271441233563 | validation: 0.15869936423577774]
	TIME [epoch: 10.6 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09164801660061273		[learning rate: 0.00014777]
	Learning Rate: 0.000147775
	LOSS [training: 0.09164801660061273 | validation: 0.1389909642426571]
	TIME [epoch: 10.6 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09330053681032194		[learning rate: 0.00014732]
	Learning Rate: 0.000147322
	LOSS [training: 0.09330053681032194 | validation: 0.1290659148522088]
	TIME [epoch: 10.6 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0802016891463424		[learning rate: 0.00014687]
	Learning Rate: 0.00014687
	LOSS [training: 0.0802016891463424 | validation: 0.11294786646834892]
	TIME [epoch: 10.6 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07358745682944694		[learning rate: 0.00014642]
	Learning Rate: 0.00014642
	LOSS [training: 0.07358745682944694 | validation: 0.1328099616322247]
	TIME [epoch: 10.6 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07879399413432756		[learning rate: 0.00014597]
	Learning Rate: 0.000145971
	LOSS [training: 0.07879399413432756 | validation: 0.1183391115254808]
	TIME [epoch: 10.6 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07512434811045825		[learning rate: 0.00014552]
	Learning Rate: 0.000145524
	LOSS [training: 0.07512434811045825 | validation: 0.1368818068235934]
	TIME [epoch: 10.6 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07545106951530238		[learning rate: 0.00014508]
	Learning Rate: 0.000145077
	LOSS [training: 0.07545106951530238 | validation: 0.15288526959085075]
	TIME [epoch: 10.6 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07917052586101515		[learning rate: 0.00014463]
	Learning Rate: 0.000144633
	LOSS [training: 0.07917052586101515 | validation: 0.11369593396563112]
	TIME [epoch: 10.6 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.070806317571519		[learning rate: 0.00014419]
	Learning Rate: 0.000144189
	LOSS [training: 0.070806317571519 | validation: 0.12937680099089108]
	TIME [epoch: 10.6 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0827637698972716		[learning rate: 0.00014375]
	Learning Rate: 0.000143747
	LOSS [training: 0.0827637698972716 | validation: 0.1261112877371948]
	TIME [epoch: 10.6 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07797061528469315		[learning rate: 0.00014331]
	Learning Rate: 0.000143307
	LOSS [training: 0.07797061528469315 | validation: 0.1197034788464035]
	TIME [epoch: 10.6 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1021728784784511		[learning rate: 0.00014287]
	Learning Rate: 0.000142867
	LOSS [training: 0.1021728784784511 | validation: 0.12451878413564518]
	TIME [epoch: 10.6 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08145467254765248		[learning rate: 0.00014243]
	Learning Rate: 0.00014243
	LOSS [training: 0.08145467254765248 | validation: 0.13745561378335366]
	TIME [epoch: 10.6 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08902349622390325		[learning rate: 0.00014199]
	Learning Rate: 0.000141993
	LOSS [training: 0.08902349622390325 | validation: 0.10292378645147064]
	TIME [epoch: 10.6 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07939345001138827		[learning rate: 0.00014156]
	Learning Rate: 0.000141558
	LOSS [training: 0.07939345001138827 | validation: 0.11140412273922982]
	TIME [epoch: 10.6 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08133839758119925		[learning rate: 0.00014112]
	Learning Rate: 0.000141124
	LOSS [training: 0.08133839758119925 | validation: 0.13174055289759104]
	TIME [epoch: 10.6 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07845692800548351		[learning rate: 0.00014069]
	Learning Rate: 0.000140691
	LOSS [training: 0.07845692800548351 | validation: 0.12438776221177116]
	TIME [epoch: 10.6 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.090970317207802		[learning rate: 0.00014026]
	Learning Rate: 0.00014026
	LOSS [training: 0.090970317207802 | validation: 0.13592126835410062]
	TIME [epoch: 10.6 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09633294849094245		[learning rate: 0.00013983]
	Learning Rate: 0.00013983
	LOSS [training: 0.09633294849094245 | validation: 0.14966878748206403]
	TIME [epoch: 10.6 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1080393200907035		[learning rate: 0.0001394]
	Learning Rate: 0.000139401
	LOSS [training: 0.1080393200907035 | validation: 0.1377418161570851]
	TIME [epoch: 10.6 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08798572693831827		[learning rate: 0.00013897]
	Learning Rate: 0.000138974
	LOSS [training: 0.08798572693831827 | validation: 0.1324530851031802]
	TIME [epoch: 10.6 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0967275852842906		[learning rate: 0.00013855]
	Learning Rate: 0.000138548
	LOSS [training: 0.0967275852842906 | validation: 0.13627130279965896]
	TIME [epoch: 10.6 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07979956134029083		[learning rate: 0.00013812]
	Learning Rate: 0.000138123
	LOSS [training: 0.07979956134029083 | validation: 0.12678095559528535]
	TIME [epoch: 10.6 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08656878593369918		[learning rate: 0.0001377]
	Learning Rate: 0.0001377
	LOSS [training: 0.08656878593369918 | validation: 0.12312860455924508]
	TIME [epoch: 10.6 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09156496582988893		[learning rate: 0.00013728]
	Learning Rate: 0.000137278
	LOSS [training: 0.09156496582988893 | validation: 0.1344454734196911]
	TIME [epoch: 10.6 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08524524896914923		[learning rate: 0.00013686]
	Learning Rate: 0.000136857
	LOSS [training: 0.08524524896914923 | validation: 0.1267521649079527]
	TIME [epoch: 10.6 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09505957672138439		[learning rate: 0.00013644]
	Learning Rate: 0.000136437
	LOSS [training: 0.09505957672138439 | validation: 0.13898118803344486]
	TIME [epoch: 10.6 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09517249110250639		[learning rate: 0.00013602]
	Learning Rate: 0.000136019
	LOSS [training: 0.09517249110250639 | validation: 0.12210763387231321]
	TIME [epoch: 10.6 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08721098689299536		[learning rate: 0.0001356]
	Learning Rate: 0.000135602
	LOSS [training: 0.08721098689299536 | validation: 0.15865882782547938]
	TIME [epoch: 10.6 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0884157794407021		[learning rate: 0.00013519]
	Learning Rate: 0.000135186
	LOSS [training: 0.0884157794407021 | validation: 0.12307946288074828]
	TIME [epoch: 10.6 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0849803320064567		[learning rate: 0.00013477]
	Learning Rate: 0.000134772
	LOSS [training: 0.0849803320064567 | validation: 0.15297547848097148]
	TIME [epoch: 10.6 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10237114369545867		[learning rate: 0.00013436]
	Learning Rate: 0.000134359
	LOSS [training: 0.10237114369545867 | validation: 0.13154154555648476]
	TIME [epoch: 10.6 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08213640601125707		[learning rate: 0.00013395]
	Learning Rate: 0.000133947
	LOSS [training: 0.08213640601125707 | validation: 0.13614174474148105]
	TIME [epoch: 10.6 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08459927482773123		[learning rate: 0.00013354]
	Learning Rate: 0.000133536
	LOSS [training: 0.08459927482773123 | validation: 0.1215798453697914]
	TIME [epoch: 10.6 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08975265584231074		[learning rate: 0.00013313]
	Learning Rate: 0.000133127
	LOSS [training: 0.08975265584231074 | validation: 0.15619509550961483]
	TIME [epoch: 10.6 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08806927823414903		[learning rate: 0.00013272]
	Learning Rate: 0.000132719
	LOSS [training: 0.08806927823414903 | validation: 0.13443259880489264]
	TIME [epoch: 10.6 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.083161646252494		[learning rate: 0.00013231]
	Learning Rate: 0.000132312
	LOSS [training: 0.083161646252494 | validation: 0.14434114661764502]
	TIME [epoch: 10.6 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08803605612112089		[learning rate: 0.00013191]
	Learning Rate: 0.000131907
	LOSS [training: 0.08803605612112089 | validation: 0.12162752054206165]
	TIME [epoch: 10.6 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0995250202963114		[learning rate: 0.0001315]
	Learning Rate: 0.000131502
	LOSS [training: 0.0995250202963114 | validation: 0.1286867252554631]
	TIME [epoch: 10.6 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07494122975149833		[learning rate: 0.0001311]
	Learning Rate: 0.000131099
	LOSS [training: 0.07494122975149833 | validation: 0.1265055177779306]
	TIME [epoch: 10.6 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08734035513389854		[learning rate: 0.0001307]
	Learning Rate: 0.000130697
	LOSS [training: 0.08734035513389854 | validation: 0.16038215110456436]
	TIME [epoch: 10.6 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09419201555193381		[learning rate: 0.0001303]
	Learning Rate: 0.000130297
	LOSS [training: 0.09419201555193381 | validation: 0.13788655059835198]
	TIME [epoch: 10.6 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08215513962373323		[learning rate: 0.0001299]
	Learning Rate: 0.000129897
	LOSS [training: 0.08215513962373323 | validation: 0.13219901726818825]
	TIME [epoch: 10.6 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09163311321542224		[learning rate: 0.0001295]
	Learning Rate: 0.000129499
	LOSS [training: 0.09163311321542224 | validation: 0.1331572311217269]
	TIME [epoch: 10.6 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11052474667284588		[learning rate: 0.0001291]
	Learning Rate: 0.000129102
	LOSS [training: 0.11052474667284588 | validation: 0.19544553847323684]
	TIME [epoch: 10.6 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12504262426156498		[learning rate: 0.00012871]
	Learning Rate: 0.000128706
	LOSS [training: 0.12504262426156498 | validation: 0.15619533669493113]
	TIME [epoch: 10.6 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10778651595122282		[learning rate: 0.00012831]
	Learning Rate: 0.000128312
	LOSS [training: 0.10778651595122282 | validation: 0.15104007862333177]
	TIME [epoch: 10.6 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13168584542534506		[learning rate: 0.00012792]
	Learning Rate: 0.000127918
	LOSS [training: 0.13168584542534506 | validation: 0.16699176281245137]
	TIME [epoch: 10.6 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1297819863900004		[learning rate: 0.00012753]
	Learning Rate: 0.000127526
	LOSS [training: 0.1297819863900004 | validation: 0.1414862540611941]
	TIME [epoch: 10.6 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10487076726855833		[learning rate: 0.00012714]
	Learning Rate: 0.000127135
	LOSS [training: 0.10487076726855833 | validation: 0.13870120181115855]
	TIME [epoch: 10.6 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09731819281238804		[learning rate: 0.00012675]
	Learning Rate: 0.000126746
	LOSS [training: 0.09731819281238804 | validation: 0.15394143387089468]
	TIME [epoch: 10.6 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09051687397736606		[learning rate: 0.00012636]
	Learning Rate: 0.000126357
	LOSS [training: 0.09051687397736606 | validation: 0.17796392095231678]
	TIME [epoch: 10.6 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09903813877206355		[learning rate: 0.00012597]
	Learning Rate: 0.00012597
	LOSS [training: 0.09903813877206355 | validation: 0.1651828528729565]
	TIME [epoch: 10.6 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10273291774660351		[learning rate: 0.00012558]
	Learning Rate: 0.000125584
	LOSS [training: 0.10273291774660351 | validation: 0.16935135396457163]
	TIME [epoch: 10.6 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09258041059485042		[learning rate: 0.0001252]
	Learning Rate: 0.000125199
	LOSS [training: 0.09258041059485042 | validation: 0.1473521032092896]
	TIME [epoch: 10.6 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08818000507311034		[learning rate: 0.00012481]
	Learning Rate: 0.000124815
	LOSS [training: 0.08818000507311034 | validation: 0.13013708446266184]
	TIME [epoch: 10.6 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10704220228477293		[learning rate: 0.00012443]
	Learning Rate: 0.000124432
	LOSS [training: 0.10704220228477293 | validation: 0.18005715001327288]
	TIME [epoch: 10.6 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1297109617475265		[learning rate: 0.00012405]
	Learning Rate: 0.000124051
	LOSS [training: 0.1297109617475265 | validation: 0.20705788959486784]
	TIME [epoch: 10.6 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1183324369719132		[learning rate: 0.00012367]
	Learning Rate: 0.000123671
	LOSS [training: 0.1183324369719132 | validation: 0.1273624280410828]
	TIME [epoch: 10.6 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09608299335634178		[learning rate: 0.00012329]
	Learning Rate: 0.000123292
	LOSS [training: 0.09608299335634178 | validation: 0.16526293418419877]
	TIME [epoch: 10.6 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09117623754021774		[learning rate: 0.00012291]
	Learning Rate: 0.000122914
	LOSS [training: 0.09117623754021774 | validation: 0.14833604561682454]
	TIME [epoch: 10.6 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08631370356274988		[learning rate: 0.00012254]
	Learning Rate: 0.000122537
	LOSS [training: 0.08631370356274988 | validation: 0.1390344506236448]
	TIME [epoch: 10.6 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08992415088814902		[learning rate: 0.00012216]
	Learning Rate: 0.000122161
	LOSS [training: 0.08992415088814902 | validation: 0.1367271563380314]
	TIME [epoch: 10.6 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08490338910058445		[learning rate: 0.00012179]
	Learning Rate: 0.000121787
	LOSS [training: 0.08490338910058445 | validation: 0.1279371237288239]
	TIME [epoch: 10.6 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07442258459820097		[learning rate: 0.00012141]
	Learning Rate: 0.000121413
	LOSS [training: 0.07442258459820097 | validation: 0.13404624759779396]
	TIME [epoch: 10.6 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07805705313481917		[learning rate: 0.00012104]
	Learning Rate: 0.000121041
	LOSS [training: 0.07805705313481917 | validation: 0.12120267681313979]
	TIME [epoch: 10.6 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08176224295096143		[learning rate: 0.00012067]
	Learning Rate: 0.00012067
	LOSS [training: 0.08176224295096143 | validation: 0.12718268048877285]
	TIME [epoch: 10.6 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08354247901137277		[learning rate: 0.0001203]
	Learning Rate: 0.0001203
	LOSS [training: 0.08354247901137277 | validation: 0.12246984935351286]
	TIME [epoch: 10.6 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08299635377636863		[learning rate: 0.00011993]
	Learning Rate: 0.000119932
	LOSS [training: 0.08299635377636863 | validation: 0.13346903334636526]
	TIME [epoch: 10.6 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0823833075291214		[learning rate: 0.00011956]
	Learning Rate: 0.000119564
	LOSS [training: 0.0823833075291214 | validation: 0.12059827413001653]
	TIME [epoch: 10.6 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07531453487087496		[learning rate: 0.0001192]
	Learning Rate: 0.000119197
	LOSS [training: 0.07531453487087496 | validation: 0.13009270547123014]
	TIME [epoch: 10.6 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08407926259783657		[learning rate: 0.00011883]
	Learning Rate: 0.000118832
	LOSS [training: 0.08407926259783657 | validation: 0.12992136188610595]
	TIME [epoch: 10.6 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08025780785989692		[learning rate: 0.00011847]
	Learning Rate: 0.000118468
	LOSS [training: 0.08025780785989692 | validation: 0.12590732335232135]
	TIME [epoch: 10.6 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08639940959391654		[learning rate: 0.0001181]
	Learning Rate: 0.000118105
	LOSS [training: 0.08639940959391654 | validation: 0.11355535518559223]
	TIME [epoch: 10.6 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0844740033018568		[learning rate: 0.00011774]
	Learning Rate: 0.000117743
	LOSS [training: 0.0844740033018568 | validation: 0.11932346519351499]
	TIME [epoch: 10.6 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09270170431928117		[learning rate: 0.00011738]
	Learning Rate: 0.000117382
	LOSS [training: 0.09270170431928117 | validation: 0.1280130997098691]
	TIME [epoch: 10.6 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09477495776017966		[learning rate: 0.00011702]
	Learning Rate: 0.000117022
	LOSS [training: 0.09477495776017966 | validation: 0.14776299946586105]
	TIME [epoch: 10.6 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09075809396652858		[learning rate: 0.00011666]
	Learning Rate: 0.000116663
	LOSS [training: 0.09075809396652858 | validation: 0.11320250155562303]
	TIME [epoch: 10.6 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08112618684792397		[learning rate: 0.00011631]
	Learning Rate: 0.000116305
	LOSS [training: 0.08112618684792397 | validation: 0.10565880145467109]
	TIME [epoch: 10.6 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09362460925946181		[learning rate: 0.00011595]
	Learning Rate: 0.000115949
	LOSS [training: 0.09362460925946181 | validation: 0.1138375695319016]
	TIME [epoch: 10.6 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09460422488578626		[learning rate: 0.00011559]
	Learning Rate: 0.000115593
	LOSS [training: 0.09460422488578626 | validation: 0.144026888291755]
	TIME [epoch: 10.6 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11548268990379666		[learning rate: 0.00011524]
	Learning Rate: 0.000115239
	LOSS [training: 0.11548268990379666 | validation: 0.14007889983507804]
	TIME [epoch: 10.6 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08970736430398443		[learning rate: 0.00011489]
	Learning Rate: 0.000114886
	LOSS [training: 0.08970736430398443 | validation: 0.1265133595900205]
	TIME [epoch: 10.6 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08052067238938937		[learning rate: 0.00011453]
	Learning Rate: 0.000114534
	LOSS [training: 0.08052067238938937 | validation: 0.1275884936567963]
	TIME [epoch: 10.6 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08431120911412407		[learning rate: 0.00011418]
	Learning Rate: 0.000114183
	LOSS [training: 0.08431120911412407 | validation: 0.1288901799777673]
	TIME [epoch: 10.6 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08707256398564878		[learning rate: 0.00011383]
	Learning Rate: 0.000113833
	LOSS [training: 0.08707256398564878 | validation: 0.127204070658533]
	TIME [epoch: 10.6 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07293570142340527		[learning rate: 0.00011348]
	Learning Rate: 0.000113484
	LOSS [training: 0.07293570142340527 | validation: 0.11247596347578893]
	TIME [epoch: 10.6 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08208221721764117		[learning rate: 0.00011314]
	Learning Rate: 0.000113136
	LOSS [training: 0.08208221721764117 | validation: 0.15452091029021722]
	TIME [epoch: 10.6 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09191265183565132		[learning rate: 0.00011279]
	Learning Rate: 0.000112789
	LOSS [training: 0.09191265183565132 | validation: 0.12105487069166766]
	TIME [epoch: 10.6 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08056713125844724		[learning rate: 0.00011244]
	Learning Rate: 0.000112443
	LOSS [training: 0.08056713125844724 | validation: 0.10887723414967025]
	TIME [epoch: 10.6 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09699249533084556		[learning rate: 0.0001121]
	Learning Rate: 0.000112099
	LOSS [training: 0.09699249533084556 | validation: 0.14165481297180274]
	TIME [epoch: 10.6 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10850394581124082		[learning rate: 0.00011175]
	Learning Rate: 0.000111755
	LOSS [training: 0.10850394581124082 | validation: 0.136936181545582]
	TIME [epoch: 10.6 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08039824449982493		[learning rate: 0.00011141]
	Learning Rate: 0.000111412
	LOSS [training: 0.08039824449982493 | validation: 0.12932541847118437]
	TIME [epoch: 10.6 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08595501283571566		[learning rate: 0.00011107]
	Learning Rate: 0.000111071
	LOSS [training: 0.08595501283571566 | validation: 0.11841507430478519]
	TIME [epoch: 10.6 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0920954512975288		[learning rate: 0.00011073]
	Learning Rate: 0.00011073
	LOSS [training: 0.0920954512975288 | validation: 0.15761904026936271]
	TIME [epoch: 10.6 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0874790980007525		[learning rate: 0.00011039]
	Learning Rate: 0.000110391
	LOSS [training: 0.0874790980007525 | validation: 0.13590803524669046]
	TIME [epoch: 10.6 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08552908220233332		[learning rate: 0.00011005]
	Learning Rate: 0.000110053
	LOSS [training: 0.08552908220233332 | validation: 0.1292039515225939]
	TIME [epoch: 10.6 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09122524096773074		[learning rate: 0.00010972]
	Learning Rate: 0.000109715
	LOSS [training: 0.09122524096773074 | validation: 0.1297415239079985]
	TIME [epoch: 10.6 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08595891083196396		[learning rate: 0.00010938]
	Learning Rate: 0.000109379
	LOSS [training: 0.08595891083196396 | validation: 0.11331117384430239]
	TIME [epoch: 10.6 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0793175194183475		[learning rate: 0.00010904]
	Learning Rate: 0.000109044
	LOSS [training: 0.0793175194183475 | validation: 0.11876726814381179]
	TIME [epoch: 10.6 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09040212718680699		[learning rate: 0.00010871]
	Learning Rate: 0.000108709
	LOSS [training: 0.09040212718680699 | validation: 0.10930849615423067]
	TIME [epoch: 10.6 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0833648418763839		[learning rate: 0.00010838]
	Learning Rate: 0.000108376
	LOSS [training: 0.0833648418763839 | validation: 0.11250685568915668]
	TIME [epoch: 10.6 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07957140541321935		[learning rate: 0.00010804]
	Learning Rate: 0.000108044
	LOSS [training: 0.07957140541321935 | validation: 0.11072425852585786]
	TIME [epoch: 10.6 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09228190105495844		[learning rate: 0.00010771]
	Learning Rate: 0.000107713
	LOSS [training: 0.09228190105495844 | validation: 0.12505333357175633]
	TIME [epoch: 10.6 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08248839545632872		[learning rate: 0.00010738]
	Learning Rate: 0.000107382
	LOSS [training: 0.08248839545632872 | validation: 0.1301949433775424]
	TIME [epoch: 10.6 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07863592269131973		[learning rate: 0.00010705]
	Learning Rate: 0.000107053
	LOSS [training: 0.07863592269131973 | validation: 0.11766121936431823]
	TIME [epoch: 10.6 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08683442072299817		[learning rate: 0.00010673]
	Learning Rate: 0.000106725
	LOSS [training: 0.08683442072299817 | validation: 0.13177406325616428]
	TIME [epoch: 10.6 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0836655116596358		[learning rate: 0.0001064]
	Learning Rate: 0.000106398
	LOSS [training: 0.0836655116596358 | validation: 0.10457374874926928]
	TIME [epoch: 10.6 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08722572435633069		[learning rate: 0.00010607]
	Learning Rate: 0.000106072
	LOSS [training: 0.08722572435633069 | validation: 0.10631133260821521]
	TIME [epoch: 10.6 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09476964473802738		[learning rate: 0.00010575]
	Learning Rate: 0.000105747
	LOSS [training: 0.09476964473802738 | validation: 0.11538164925829911]
	TIME [epoch: 10.6 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0800730575061378		[learning rate: 0.00010542]
	Learning Rate: 0.000105423
	LOSS [training: 0.0800730575061378 | validation: 0.12003720757356061]
	TIME [epoch: 10.5 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08186070364303702		[learning rate: 0.0001051]
	Learning Rate: 0.000105099
	LOSS [training: 0.08186070364303702 | validation: 0.11177985733591678]
	TIME [epoch: 10.6 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08704390789692307		[learning rate: 0.00010478]
	Learning Rate: 0.000104777
	LOSS [training: 0.08704390789692307 | validation: 0.11412485286042237]
	TIME [epoch: 10.6 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08839556446914844		[learning rate: 0.00010446]
	Learning Rate: 0.000104456
	LOSS [training: 0.08839556446914844 | validation: 0.13369792844208533]
	TIME [epoch: 10.6 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08980591513897898		[learning rate: 0.00010414]
	Learning Rate: 0.000104136
	LOSS [training: 0.08980591513897898 | validation: 0.11663735604984186]
	TIME [epoch: 10.6 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.07698403571822623		[learning rate: 0.00010382]
	Learning Rate: 0.000103817
	LOSS [training: 0.07698403571822623 | validation: 0.11793284873842785]
	TIME [epoch: 10.6 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08647054951844164		[learning rate: 0.0001035]
	Learning Rate: 0.000103498
	LOSS [training: 0.08647054951844164 | validation: 0.14678146243442264]
	TIME [epoch: 10.6 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08813070879183693		[learning rate: 0.00010318]
	Learning Rate: 0.000103181
	LOSS [training: 0.08813070879183693 | validation: 0.12402331244125073]
	TIME [epoch: 10.6 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08477190033439586		[learning rate: 0.00010286]
	Learning Rate: 0.000102865
	LOSS [training: 0.08477190033439586 | validation: 0.11303313164086426]
	TIME [epoch: 10.6 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08824081466773775		[learning rate: 0.00010255]
	Learning Rate: 0.000102549
	LOSS [training: 0.08824081466773775 | validation: 0.14303418173188065]
	TIME [epoch: 10.6 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08808314762844495		[learning rate: 0.00010224]
	Learning Rate: 0.000102235
	LOSS [training: 0.08808314762844495 | validation: 0.12065526515024225]
	TIME [epoch: 10.6 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08412557818097113		[learning rate: 0.00010192]
	Learning Rate: 0.000101922
	LOSS [training: 0.08412557818097113 | validation: 0.11928634717610781]
	TIME [epoch: 10.6 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08231673768071571		[learning rate: 0.00010161]
	Learning Rate: 0.000101609
	LOSS [training: 0.08231673768071571 | validation: 0.1047890017989614]
	TIME [epoch: 10.6 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08778772772648437		[learning rate: 0.0001013]
	Learning Rate: 0.000101298
	LOSS [training: 0.08778772772648437 | validation: 0.15264381479088027]
	TIME [epoch: 10.5 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08891607552205734		[learning rate: 0.00010099]
	Learning Rate: 0.000100987
	LOSS [training: 0.08891607552205734 | validation: 0.1494494607507989]
	TIME [epoch: 10.6 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08435716096885067		[learning rate: 0.00010068]
	Learning Rate: 0.000100678
	LOSS [training: 0.08435716096885067 | validation: 0.14250463001491262]
	TIME [epoch: 10.6 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08407439858650025		[learning rate: 0.00010037]
	Learning Rate: 0.000100369
	LOSS [training: 0.08407439858650025 | validation: 0.13954129620098094]
	TIME [epoch: 10.6 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09387680288904783		[learning rate: 0.00010006]
	Learning Rate: 0.000100061
	LOSS [training: 0.09387680288904783 | validation: 0.13292367030292931]
	TIME [epoch: 10.5 sec]
Finished training in 21273.206 seconds.
