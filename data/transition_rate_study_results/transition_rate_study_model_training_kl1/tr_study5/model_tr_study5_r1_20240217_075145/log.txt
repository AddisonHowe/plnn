Args:
Namespace(name='model_tr_study5', outdir='out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1', training_data='data/transition_rate_studies/tr_study5/tr_study5_training/r1', validation_data='data/transition_rate_studies/tr_study5/tr_study5_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4019853607

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.958810184636054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.958810184636054 | validation: 11.90151360969599]
	TIME [epoch: 49.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.000450333781458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.000450333781458 | validation: 11.739070848344792]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.418377894507593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.418377894507593 | validation: 11.122131958072277]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.861422864297978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.861422864297978 | validation: 11.404685189181668]
	TIME [epoch: 10.4 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.949622010558349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.949622010558349 | validation: 10.020628641436415]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.263616643156261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.263616643156261 | validation: 10.274217342192507]
	TIME [epoch: 10.4 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.214309648229268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.214309648229268 | validation: 9.807267116250733]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.044293552735558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.044293552735558 | validation: 9.895639870832051]
	TIME [epoch: 10.4 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.979628906987116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.979628906987116 | validation: 10.008221844009622]
	TIME [epoch: 10.4 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.914328593808294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.914328593808294 | validation: 9.47544473672179]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.821281799010837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.821281799010837 | validation: 9.915061447317902]
	TIME [epoch: 10.4 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.684860178149256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.684860178149256 | validation: 9.369023080687882]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.291116533699357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.291116533699357 | validation: 10.023883630235948]
	TIME [epoch: 10.4 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.5038027145699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.5038027145699 | validation: 7.911971252826939]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.9410937323040915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.9410937323040915 | validation: 8.798912785854332]
	TIME [epoch: 10.4 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.266446681371395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.266446681371395 | validation: 8.240446667681207]
	TIME [epoch: 10.4 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.890834223444554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.890834223444554 | validation: 7.190605190570109]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.934850568461485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.934850568461485 | validation: 7.1026947113979775]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.755062348377071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.755062348377071 | validation: 7.32688793512468]
	TIME [epoch: 10.4 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.3596390484568825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.3596390484568825 | validation: 6.970605632527468]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.5761812484131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.5761812484131 | validation: 6.891511790717621]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.126399247752528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.126399247752528 | validation: 6.841810315090579]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.95108882753708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.95108882753708 | validation: 6.6383970596109]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.152485416927249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.152485416927249 | validation: 6.064778404566066]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.376805133833949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.376805133833949 | validation: 6.09209119206102]
	TIME [epoch: 10.4 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.225737329343028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.225737329343028 | validation: 5.963668129251423]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0873163446911835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0873163446911835 | validation: 5.909402891030734]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1296514396909165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1296514396909165 | validation: 5.797184340120541]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.797688579806375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.797688579806375 | validation: 5.417652060615721]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.911028748262812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.911028748262812 | validation: 5.898225021779894]
	TIME [epoch: 10.4 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.962578030389265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.962578030389265 | validation: 5.618997393913185]
	TIME [epoch: 10.4 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.857057511977355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.857057511977355 | validation: 5.725905604711792]
	TIME [epoch: 10.4 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.862171168406247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.862171168406247 | validation: 5.432965283202472]
	TIME [epoch: 10.4 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.740920153485668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.740920153485668 | validation: 5.572855445619996]
	TIME [epoch: 10.4 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.753749167242987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.753749167242987 | validation: 5.543345452116321]
	TIME [epoch: 10.4 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.830110012556945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.830110012556945 | validation: 5.669982337637196]
	TIME [epoch: 10.4 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.774638720540728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.774638720540728 | validation: 5.859353392743904]
	TIME [epoch: 10.4 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.779847577390328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.779847577390328 | validation: 6.065447657125078]
	TIME [epoch: 10.4 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.840218710243026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.840218710243026 | validation: 5.33989301876453]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.574486239020778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.574486239020778 | validation: 5.445854576190491]
	TIME [epoch: 10.4 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.596284268990468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.596284268990468 | validation: 5.230752691115148]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.619853349708615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.619853349708615 | validation: 5.288196239267072]
	TIME [epoch: 10.4 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.599144431249343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.599144431249343 | validation: 5.136980914444311]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.6551111163945915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6551111163945915 | validation: 8.62865113842013]
	TIME [epoch: 10.4 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.736972423843578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.736972423843578 | validation: 5.371547960436782]
	TIME [epoch: 10.4 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.471492235626054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.471492235626054 | validation: 5.106339714916665]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.385836464445275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.385836464445275 | validation: 4.992928906894819]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.142851795226414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.142851795226414 | validation: 5.037521313187411]
	TIME [epoch: 10.4 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.912602418269586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.912602418269586 | validation: 5.471269503139633]
	TIME [epoch: 10.4 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.840462517369467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.840462517369467 | validation: 4.454979524840367]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.563625962869242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.563625962869242 | validation: 4.476613651494438]
	TIME [epoch: 10.4 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.993652207514748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.993652207514748 | validation: 6.3585410005594625]
	TIME [epoch: 10.4 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6259692975854625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6259692975854625 | validation: 4.9419460341827754]
	TIME [epoch: 10.4 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.005987420478169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.005987420478169 | validation: 4.521037310926033]
	TIME [epoch: 10.4 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.535566732814286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.535566732814286 | validation: 5.510979831524836]
	TIME [epoch: 10.4 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.828140786216325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.828140786216325 | validation: 5.256569496264665]
	TIME [epoch: 10.4 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796619116509612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.796619116509612 | validation: 4.649996853227911]
	TIME [epoch: 10.4 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.547292957593401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.547292957593401 | validation: 4.357103827294627]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.271824286339838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.271824286339838 | validation: 4.158486396050351]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3156967912659825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3156967912659825 | validation: 4.199473048821927]
	TIME [epoch: 10.4 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.328488980347068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.328488980347068 | validation: 5.1910755751898625]
	TIME [epoch: 10.4 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.274701255001272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.274701255001272 | validation: 4.656547957112817]
	TIME [epoch: 10.4 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.447364824310031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.447364824310031 | validation: 4.18918791104292]
	TIME [epoch: 10.5 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.163263753414894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.163263753414894 | validation: 5.15054277857379]
	TIME [epoch: 10.4 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5887966369375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5887966369375 | validation: 5.384944434256932]
	TIME [epoch: 10.4 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802489638059354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.802489638059354 | validation: 6.776278625803707]
	TIME [epoch: 10.4 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.437205874896304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.437205874896304 | validation: 4.453973396810041]
	TIME [epoch: 10.4 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.452912168333527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.452912168333527 | validation: 5.032659241418826]
	TIME [epoch: 10.4 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.329294308032837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.329294308032837 | validation: 4.317086011852208]
	TIME [epoch: 10.4 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.203085814007914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.203085814007914 | validation: 3.992308820285002]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.100491033735571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.100491033735571 | validation: 4.505514120324554]
	TIME [epoch: 10.4 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.995283636255455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.995283636255455 | validation: 3.868186867787624]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.902761898074901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.902761898074901 | validation: 3.835957200033091]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.133635277782025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.133635277782025 | validation: 4.619334045762442]
	TIME [epoch: 10.4 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.333398667902707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.333398667902707 | validation: 4.089421793715598]
	TIME [epoch: 10.4 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827531479186173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.827531479186173 | validation: 3.64914682729891]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5347736159332754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5347736159332754 | validation: 3.59384256830775]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8324984497055063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8324984497055063 | validation: 4.1146360177411605]
	TIME [epoch: 10.5 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6977277894783307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6977277894783307 | validation: 3.620085140152621]
	TIME [epoch: 10.5 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.84494479800991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.84494479800991 | validation: 7.138956564677102]
	TIME [epoch: 10.5 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.441546186335776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.441546186335776 | validation: 3.7143719020282915]
	TIME [epoch: 10.4 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8497118977120808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8497118977120808 | validation: 4.275459407560983]
	TIME [epoch: 10.4 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.415561221701251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.415561221701251 | validation: 4.079693980352666]
	TIME [epoch: 10.4 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4011492930927645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4011492930927645 | validation: 5.51658447461526]
	TIME [epoch: 10.4 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.956493150140895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.956493150140895 | validation: 4.415982928584858]
	TIME [epoch: 10.4 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.212967175087234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.212967175087234 | validation: 4.354846592210435]
	TIME [epoch: 10.4 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.044252329841489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.044252329841489 | validation: 3.685096182177271]
	TIME [epoch: 10.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612304019565717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.612304019565717 | validation: 3.800768200379284]
	TIME [epoch: 10.4 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8981064951236406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8981064951236406 | validation: 3.8313092482285858]
	TIME [epoch: 10.4 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.622385072004397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.622385072004397 | validation: 3.8155759313269777]
	TIME [epoch: 10.4 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.140752892274206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.140752892274206 | validation: 3.7326499036391367]
	TIME [epoch: 10.4 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.769227409978176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.769227409978176 | validation: 4.761673572029605]
	TIME [epoch: 10.4 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.314844796246594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.314844796246594 | validation: 4.736701208715005]
	TIME [epoch: 10.4 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9100488573628374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9100488573628374 | validation: 3.7498897684917813]
	TIME [epoch: 10.4 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.690977484594336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.690977484594336 | validation: 3.5044528254068497]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.297798391952788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.297798391952788 | validation: 4.088460007887059]
	TIME [epoch: 10.4 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.977020606573144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.977020606573144 | validation: 6.687713266860146]
	TIME [epoch: 10.4 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.109521810042233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.109521810042233 | validation: 3.624302247372166]
	TIME [epoch: 10.4 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.637384603500417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.637384603500417 | validation: 5.0864080367304725]
	TIME [epoch: 10.4 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.586202852690147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.586202852690147 | validation: 4.4416746707388]
	TIME [epoch: 10.4 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.624931997680075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.624931997680075 | validation: 4.35766019004816]
	TIME [epoch: 10.4 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.93591331517782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.93591331517782 | validation: 3.5822473013578766]
	TIME [epoch: 10.4 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7309108178758352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7309108178758352 | validation: 3.5586906155790228]
	TIME [epoch: 10.5 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.481798420186852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.481798420186852 | validation: 3.5911833508778805]
	TIME [epoch: 10.4 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.065173854933304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.065173854933304 | validation: 4.936055887062436]
	TIME [epoch: 10.5 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.173876540897969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.173876540897969 | validation: 3.832419903024729]
	TIME [epoch: 10.4 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.670208421902179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.670208421902179 | validation: 4.600002423280471]
	TIME [epoch: 10.4 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.016043340399181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.016043340399181 | validation: 3.7805993126029787]
	TIME [epoch: 10.5 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7634392357775264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7634392357775264 | validation: 3.878341865445683]
	TIME [epoch: 10.4 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.877472115635089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.877472115635089 | validation: 5.296515925757635]
	TIME [epoch: 10.5 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.747254106736643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.747254106736643 | validation: 7.363605923737229]
	TIME [epoch: 10.4 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.685410065193847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.685410065193847 | validation: 4.483789141428524]
	TIME [epoch: 10.4 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.192939300956139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.192939300956139 | validation: 4.155230254086238]
	TIME [epoch: 10.4 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.072067860130342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.072067860130342 | validation: 3.920040486070474]
	TIME [epoch: 10.4 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9433434207405527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9433434207405527 | validation: 3.891935756907496]
	TIME [epoch: 10.4 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8859622779097256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8859622779097256 | validation: 3.8171440715706133]
	TIME [epoch: 10.4 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5842376464314034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5842376464314034 | validation: 3.8146025316775263]
	TIME [epoch: 10.5 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.391861797845518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.391861797845518 | validation: 3.984722551667578]
	TIME [epoch: 10.4 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8077023169070996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8077023169070996 | validation: 5.931216529620278]
	TIME [epoch: 10.4 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.658884017342695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.658884017342695 | validation: 5.482429221642665]
	TIME [epoch: 10.4 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.135257237446736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.135257237446736 | validation: 5.350724371215506]
	TIME [epoch: 10.4 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.401186524440198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.401186524440198 | validation: 4.032607333556481]
	TIME [epoch: 10.4 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8613451527116354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8613451527116354 | validation: 3.9840775721433888]
	TIME [epoch: 10.5 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.808146963935652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.808146963935652 | validation: 3.9105111234189738]
	TIME [epoch: 10.4 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7623208400833237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7623208400833237 | validation: 3.9307593073911926]
	TIME [epoch: 10.4 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.007750165671141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.007750165671141 | validation: 4.3872089615613765]
	TIME [epoch: 10.4 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.078203802146715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.078203802146715 | validation: 3.7863735920102988]
	TIME [epoch: 10.4 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61390272947731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.61390272947731 | validation: 3.8576889781287718]
	TIME [epoch: 10.5 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.76199969427421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.76199969427421 | validation: 4.43363315398768]
	TIME [epoch: 10.4 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.276418012499736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.276418012499736 | validation: 4.6368867906812445]
	TIME [epoch: 10.5 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8128025083545323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8128025083545323 | validation: 3.644324837106179]
	TIME [epoch: 10.5 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.429520560087194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.429520560087194 | validation: 3.729430560673293]
	TIME [epoch: 10.5 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.795513594818857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.795513594818857 | validation: 3.5756639372551353]
	TIME [epoch: 10.4 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5361624713425948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5361624713425948 | validation: 3.618005798588044]
	TIME [epoch: 10.4 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.998945678246757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.998945678246757 | validation: 4.021092272001305]
	TIME [epoch: 10.4 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.454349275134568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.454349275134568 | validation: 4.179762330388925]
	TIME [epoch: 10.4 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.687573695465631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.687573695465631 | validation: 5.646736332780986]
	TIME [epoch: 10.4 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.313858660279946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.313858660279946 | validation: 3.783619455736202]
	TIME [epoch: 10.5 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5797263484767647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5797263484767647 | validation: 4.058309961676448]
	TIME [epoch: 10.5 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.611868506018401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.611868506018401 | validation: 5.569796776862108]
	TIME [epoch: 10.5 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.165175572402991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.165175572402991 | validation: 5.804375138372595]
	TIME [epoch: 10.5 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4529436404626015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4529436404626015 | validation: 4.041889785895065]
	TIME [epoch: 10.5 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.158194192309496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.158194192309496 | validation: 4.2620302771860334]
	TIME [epoch: 10.5 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.790294338019757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.790294338019757 | validation: 3.618722786400103]
	TIME [epoch: 10.4 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8392899731537624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8392899731537624 | validation: 4.152445534428493]
	TIME [epoch: 10.4 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9897316776629004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9897316776629004 | validation: 3.9132535714095105]
	TIME [epoch: 10.5 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8485920184564444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8485920184564444 | validation: 3.966489994941485]
	TIME [epoch: 10.4 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.284829737137996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.284829737137996 | validation: 4.359282261756698]
	TIME [epoch: 10.4 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8325087865867866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8325087865867866 | validation: 3.547656373197132]
	TIME [epoch: 10.5 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5813653450287743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5813653450287743 | validation: 3.8259975822387835]
	TIME [epoch: 10.5 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.591414406651904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.591414406651904 | validation: 4.062973666664526]
	TIME [epoch: 10.4 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.092393378623239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.092393378623239 | validation: 6.481446601798313]
	TIME [epoch: 10.4 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7534571025124075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7534571025124075 | validation: 5.178273903370734]
	TIME [epoch: 10.5 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.6535566808182445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6535566808182445 | validation: 5.348690263662511]
	TIME [epoch: 10.5 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166103509876243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.166103509876243 | validation: 4.509689006073459]
	TIME [epoch: 10.5 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.008350662514772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.008350662514772 | validation: 4.762164459159165]
	TIME [epoch: 10.4 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.391548659645125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.391548659645125 | validation: 3.6330741216441425]
	TIME [epoch: 10.4 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5729094187701156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5729094187701156 | validation: 3.474819438148429]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.903156533589796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.903156533589796 | validation: 4.811530731225127]
	TIME [epoch: 10.4 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.052212101763864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.052212101763864 | validation: 4.06963925218052]
	TIME [epoch: 10.5 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7792636728822133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7792636728822133 | validation: 3.637697990320007]
	TIME [epoch: 10.4 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5964358896302984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5964358896302984 | validation: 3.231459864554632]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0611428531285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0611428531285 | validation: 3.031001148906909]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.158212303069726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.158212303069726 | validation: 3.2127827146584615]
	TIME [epoch: 10.5 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1407769227413147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1407769227413147 | validation: 3.4137488909006173]
	TIME [epoch: 10.5 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.128456571453882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.128456571453882 | validation: 3.542280420968793]
	TIME [epoch: 10.5 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.238460907271005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.238460907271005 | validation: 2.8891742584034494]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.393685875641377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.393685875641377 | validation: 4.640995703683447]
	TIME [epoch: 10.5 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3479342832207544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3479342832207544 | validation: 2.5746593841826866]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4202839182116564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4202839182116564 | validation: 6.1122046176116145]
	TIME [epoch: 10.4 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.870187929369593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.870187929369593 | validation: 2.732593958842786]
	TIME [epoch: 10.4 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640329452131462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.640329452131462 | validation: 4.18150045842694]
	TIME [epoch: 10.5 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.900038545519064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.900038545519064 | validation: 5.2100100545843055]
	TIME [epoch: 10.5 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.649616501959624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.649616501959624 | validation: 5.3633784467609145]
	TIME [epoch: 10.5 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.553752459196895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.553752459196895 | validation: 3.073703407281355]
	TIME [epoch: 10.5 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.992963835500015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.992963835500015 | validation: 2.4385351408833227]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6629045794139494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6629045794139494 | validation: 2.4918718322740885]
	TIME [epoch: 10.4 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8728267974623827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8728267974623827 | validation: 2.4568033506547797]
	TIME [epoch: 10.4 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.626085216367059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.626085216367059 | validation: 2.678371450186985]
	TIME [epoch: 10.4 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5454279544830114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5454279544830114 | validation: 2.5190869610868134]
	TIME [epoch: 10.5 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.758081365271553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.758081365271553 | validation: 2.5576049041204634]
	TIME [epoch: 10.4 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3907142065988234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3907142065988234 | validation: 2.5407431231244506]
	TIME [epoch: 10.4 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0647618557176903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0647618557176903 | validation: 3.5850319678905365]
	TIME [epoch: 10.4 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9510503047910164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9510503047910164 | validation: 2.3778498947973192]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3005321798110665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3005321798110665 | validation: 2.2977505852095845]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7236679204580474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7236679204580474 | validation: 2.4383615418598046]
	TIME [epoch: 10.4 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3172670678712466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3172670678712466 | validation: 2.4077770313546782]
	TIME [epoch: 10.5 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1382413718012754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1382413718012754 | validation: 2.0304296516842992]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2004097020266387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2004097020266387 | validation: 2.314824258290058]
	TIME [epoch: 10.4 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9700752763906322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9700752763906322 | validation: 2.968380162193895]
	TIME [epoch: 10.5 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.689754622295865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.689754622295865 | validation: 1.823947115378075]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1708616566144885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1708616566144885 | validation: 2.1579712361762673]
	TIME [epoch: 10.4 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3000119932722063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3000119932722063 | validation: 2.2584599353350256]
	TIME [epoch: 10.5 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.019293426886512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.019293426886512 | validation: 2.600582960622926]
	TIME [epoch: 10.5 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5924700383948682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5924700383948682 | validation: 2.3000312395458566]
	TIME [epoch: 10.4 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.534833600222516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.534833600222516 | validation: 2.8163766010156803]
	TIME [epoch: 10.4 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246521271256092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.246521271256092 | validation: 2.209207102526953]
	TIME [epoch: 10.5 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221130082244456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.221130082244456 | validation: 4.1694480903094915]
	TIME [epoch: 10.4 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9946502881752886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9946502881752886 | validation: 2.330731782326424]
	TIME [epoch: 10.5 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.226185902803328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.226185902803328 | validation: 6.210287716712647]
	TIME [epoch: 10.4 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9075278971491487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9075278971491487 | validation: 3.3899958106154924]
	TIME [epoch: 10.4 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.532040232821464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.532040232821464 | validation: 6.0114611865107355]
	TIME [epoch: 10.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132204586596828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.132204586596828 | validation: 4.072106335986334]
	TIME [epoch: 10.4 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5944368335042833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5944368335042833 | validation: 2.2316037897003596]
	TIME [epoch: 10.4 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9767801737485862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9767801737485862 | validation: 2.937264954589273]
	TIME [epoch: 10.4 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.527433511670767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.527433511670767 | validation: 1.9657638268570272]
	TIME [epoch: 10.5 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9206238371068785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9206238371068785 | validation: 2.2737649277084064]
	TIME [epoch: 10.4 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.853897161982846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.853897161982846 | validation: 2.337165091327183]
	TIME [epoch: 10.4 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9207396825739038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9207396825739038 | validation: 1.8379920497639826]
	TIME [epoch: 10.4 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.90863630886003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.90863630886003 | validation: 4.476140807160863]
	TIME [epoch: 10.4 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0068132034732913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0068132034732913 | validation: 1.860818278921951]
	TIME [epoch: 10.5 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8199178293652232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8199178293652232 | validation: 2.108620856272192]
	TIME [epoch: 10.5 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.777690171833374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.777690171833374 | validation: 2.3761303475867317]
	TIME [epoch: 10.4 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.823118048264726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.823118048264726 | validation: 2.7212063176182855]
	TIME [epoch: 10.5 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1087423436819104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1087423436819104 | validation: 2.1202281828292713]
	TIME [epoch: 10.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5211163323967227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5211163323967227 | validation: 2.4612010529054724]
	TIME [epoch: 10.4 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.202734832302428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.202734832302428 | validation: 2.1252683647002404]
	TIME [epoch: 10.5 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.699856111520777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.699856111520777 | validation: 2.019004302139957]
	TIME [epoch: 10.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.002582865267005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.002582865267005 | validation: 2.3274235681931947]
	TIME [epoch: 10.4 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0275740556560544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0275740556560544 | validation: 1.8044403795887396]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1840450762468273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1840450762468273 | validation: 1.9477551885174849]
	TIME [epoch: 10.4 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0480778403498525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0480778403498525 | validation: 1.7384025521812385]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9295331153905262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9295331153905262 | validation: 3.017443512580266]
	TIME [epoch: 10.4 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2799388747237233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2799388747237233 | validation: 2.023940477199876]
	TIME [epoch: 10.4 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.337423842601218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.337423842601218 | validation: 2.4807116593330925]
	TIME [epoch: 10.4 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8900539550060806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8900539550060806 | validation: 1.3676883838372618]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7383980448146001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7383980448146001 | validation: 1.478752204097625]
	TIME [epoch: 10.4 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9007889441142638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9007889441142638 | validation: 1.7453607991771423]
	TIME [epoch: 10.4 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8495337662059554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8495337662059554 | validation: 2.103507602178446]
	TIME [epoch: 10.5 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1988493779931964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1988493779931964 | validation: 2.2020359821993813]
	TIME [epoch: 10.4 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9254954958972328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9254954958972328 | validation: 1.8822239580077884]
	TIME [epoch: 10.4 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9631106640805716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9631106640805716 | validation: 2.0910384268310884]
	TIME [epoch: 10.4 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.730941214002901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.730941214002901 | validation: 1.6261707331314668]
	TIME [epoch: 10.5 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6538981908245975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6538981908245975 | validation: 2.5319396344931993]
	TIME [epoch: 10.5 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0563600828653352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0563600828653352 | validation: 2.2950524253191036]
	TIME [epoch: 10.4 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.084082981355337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.084082981355337 | validation: 1.8276510990426516]
	TIME [epoch: 10.5 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0349507269988987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0349507269988987 | validation: 1.6720821373262162]
	TIME [epoch: 10.5 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946710558614824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.946710558614824 | validation: 2.151034523690194]
	TIME [epoch: 10.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.29050871352694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.29050871352694 | validation: 2.3117601558897913]
	TIME [epoch: 10.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9658180241269636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9658180241269636 | validation: 1.951962001168413]
	TIME [epoch: 10.5 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7112777492896556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7112777492896556 | validation: 1.8316051612793787]
	TIME [epoch: 10.4 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990512303171036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.990512303171036 | validation: 2.5959257628432524]
	TIME [epoch: 10.5 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266457976837021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.266457976837021 | validation: 2.26815575625687]
	TIME [epoch: 10.5 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0738147311827038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0738147311827038 | validation: 2.609071420258697]
	TIME [epoch: 10.5 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3931659639461467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3931659639461467 | validation: 5.017319554871258]
	TIME [epoch: 10.5 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4295173125918135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4295173125918135 | validation: 2.172582818576436]
	TIME [epoch: 10.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2460181560712575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2460181560712575 | validation: 1.8991240051011806]
	TIME [epoch: 10.5 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8132827274190646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8132827274190646 | validation: 2.6201163792659976]
	TIME [epoch: 10.5 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9838612936691908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9838612936691908 | validation: 1.886734911975732]
	TIME [epoch: 10.4 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5041583201218542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5041583201218542 | validation: 1.79146639830219]
	TIME [epoch: 10.5 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.795432942135171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.795432942135171 | validation: 2.43228634554824]
	TIME [epoch: 10.5 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.15285218254445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.15285218254445 | validation: 2.0151688041370575]
	TIME [epoch: 10.5 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.807968578983752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.807968578983752 | validation: 5.904476676169619]
	TIME [epoch: 10.5 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.538178350947689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.538178350947689 | validation: 3.3216116297725593]
	TIME [epoch: 10.5 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1419767456001186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1419767456001186 | validation: 2.1785388601979934]
	TIME [epoch: 10.5 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.440959154141212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.440959154141212 | validation: 2.8724779941355245]
	TIME [epoch: 10.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.378162940911774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.378162940911774 | validation: 2.4758726665491086]
	TIME [epoch: 10.5 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1938488885525373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1938488885525373 | validation: 3.2684553665640323]
	TIME [epoch: 10.5 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.36918164653053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.36918164653053 | validation: 2.7130347889646864]
	TIME [epoch: 10.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.413501535983637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.413501535983637 | validation: 1.6954898815343504]
	TIME [epoch: 10.5 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.116619766906598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.116619766906598 | validation: 2.270605654586147]
	TIME [epoch: 10.5 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.987962758759028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.987962758759028 | validation: 1.829954483899377]
	TIME [epoch: 10.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6772787571832215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6772787571832215 | validation: 1.698232901463881]
	TIME [epoch: 10.4 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7421056563438513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7421056563438513 | validation: 1.4445798318357113]
	TIME [epoch: 10.5 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5113180269360365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5113180269360365 | validation: 1.6244334957689075]
	TIME [epoch: 10.5 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5186324330704637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5186324330704637 | validation: 1.512214289199627]
	TIME [epoch: 10.4 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4448996661575686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4448996661575686 | validation: 1.243747540360916]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4974844416566828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4974844416566828 | validation: 2.0284012794583903]
	TIME [epoch: 10.4 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.154210979524174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.154210979524174 | validation: 3.5210632844901775]
	TIME [epoch: 10.5 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1195106301125497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1195106301125497 | validation: 1.44679070979056]
	TIME [epoch: 10.4 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744512425160019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.744512425160019 | validation: 1.7379996419584836]
	TIME [epoch: 10.5 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5343399369393982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5343399369393982 | validation: 1.5192360208076208]
	TIME [epoch: 10.4 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6855968295386539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6855968295386539 | validation: 1.598788748792382]
	TIME [epoch: 10.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6550596132699404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6550596132699404 | validation: 1.8698246081207726]
	TIME [epoch: 10.4 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6972895396681928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6972895396681928 | validation: 1.542930864643741]
	TIME [epoch: 10.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.643751568554372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.643751568554372 | validation: 1.5001562998808213]
	TIME [epoch: 10.5 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5778896962487754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5778896962487754 | validation: 1.7350040787501728]
	TIME [epoch: 10.4 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6041489637097226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6041489637097226 | validation: 2.782003521033135]
	TIME [epoch: 10.4 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0885635438496153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0885635438496153 | validation: 1.7602144660556218]
	TIME [epoch: 10.4 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6385062896734344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6385062896734344 | validation: 1.6709502941992225]
	TIME [epoch: 10.5 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6040105489457055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6040105489457055 | validation: 2.104557562851299]
	TIME [epoch: 10.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6417917523380612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6417917523380612 | validation: 1.4121456097565532]
	TIME [epoch: 10.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.747828914879636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.747828914879636 | validation: 1.7299148659459378]
	TIME [epoch: 10.4 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5862949397631207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5862949397631207 | validation: 1.8966350258823546]
	TIME [epoch: 10.5 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7609656937766363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7609656937766363 | validation: 1.483664086080454]
	TIME [epoch: 10.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7677913220543897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7677913220543897 | validation: 1.274621062740613]
	TIME [epoch: 10.5 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7076973088635377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7076973088635377 | validation: 1.6740320744914983]
	TIME [epoch: 10.5 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5728024196517905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5728024196517905 | validation: 1.638884362661762]
	TIME [epoch: 10.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7336791347601839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7336791347601839 | validation: 1.825426119522423]
	TIME [epoch: 10.5 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6601292807503008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6601292807503008 | validation: 2.0286554914256807]
	TIME [epoch: 10.5 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5341925926599136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5341925926599136 | validation: 2.228021267922841]
	TIME [epoch: 10.5 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3678561067670336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3678561067670336 | validation: 2.4052723305278394]
	TIME [epoch: 10.5 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9352713236668397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9352713236668397 | validation: 1.6908382978269607]
	TIME [epoch: 10.5 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.791423174047523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.791423174047523 | validation: 2.0459533838879786]
	TIME [epoch: 10.5 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7798097821480496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7798097821480496 | validation: 1.301489121251399]
	TIME [epoch: 10.5 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6192558479979104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6192558479979104 | validation: 1.6667152618000152]
	TIME [epoch: 10.5 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4544716185994981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4544716185994981 | validation: 1.9738812586811656]
	TIME [epoch: 10.4 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6578224282130058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6578224282130058 | validation: 1.5943056835926308]
	TIME [epoch: 10.5 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6024548665851506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6024548665851506 | validation: 2.361964292691727]
	TIME [epoch: 10.5 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.843338239820452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.843338239820452 | validation: 1.9634724606561182]
	TIME [epoch: 10.5 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.70332169802968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.70332169802968 | validation: 1.5782234181604]
	TIME [epoch: 10.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8045343244792929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8045343244792929 | validation: 2.2047417827344304]
	TIME [epoch: 10.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1220568159047897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1220568159047897 | validation: 1.6061988728445606]
	TIME [epoch: 10.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5996391711134825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5996391711134825 | validation: 2.1903782980309434]
	TIME [epoch: 10.5 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.673935690885506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.673935690885506 | validation: 2.0260171450312425]
	TIME [epoch: 10.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8244011691310504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8244011691310504 | validation: 1.8291193022660717]
	TIME [epoch: 10.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.661527298673817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.661527298673817 | validation: 2.4247121978851287]
	TIME [epoch: 10.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8372167723967991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8372167723967991 | validation: 1.6447389176195475]
	TIME [epoch: 10.4 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4722749915504814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4722749915504814 | validation: 1.3343978786190338]
	TIME [epoch: 10.5 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4410621839423998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4410621839423998 | validation: 1.9344983475954518]
	TIME [epoch: 10.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6153929339707467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6153929339707467 | validation: 1.9215036688988807]
	TIME [epoch: 10.5 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5880409735501997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5880409735501997 | validation: 1.6129249596851338]
	TIME [epoch: 10.5 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.620733885690663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.620733885690663 | validation: 1.5888727822738815]
	TIME [epoch: 10.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6220309221919458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6220309221919458 | validation: 1.5073871620910502]
	TIME [epoch: 10.5 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.507960726971633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.507960726971633 | validation: 2.2751423725042823]
	TIME [epoch: 10.5 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9172889088826899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9172889088826899 | validation: 1.7834643447894019]
	TIME [epoch: 10.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5962277897858121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5962277897858121 | validation: 1.4187780977392561]
	TIME [epoch: 10.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3892247517233538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3892247517233538 | validation: 2.524876955346009]
	TIME [epoch: 10.4 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8007956736714892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8007956736714892 | validation: 1.473844515441238]
	TIME [epoch: 10.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3505936117978106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3505936117978106 | validation: 1.381753515004691]
	TIME [epoch: 10.5 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4918119576105215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4918119576105215 | validation: 1.3917346085204494]
	TIME [epoch: 10.5 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3672637705416988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3672637705416988 | validation: 1.1870515845499838]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996904575737425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2996904575737425 | validation: 3.022417006722112]
	TIME [epoch: 10.4 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.753379270834942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.753379270834942 | validation: 1.8759580012579375]
	TIME [epoch: 10.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7830536642378298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7830536642378298 | validation: 1.5783569146754086]
	TIME [epoch: 10.4 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.669147147940982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.669147147940982 | validation: 1.7448782578487578]
	TIME [epoch: 10.5 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4696475042895003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4696475042895003 | validation: 1.5450399316882264]
	TIME [epoch: 10.5 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5681463819212516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5681463819212516 | validation: 2.989523735652198]
	TIME [epoch: 10.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.00764770018599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.00764770018599 | validation: 2.3960932562195367]
	TIME [epoch: 10.5 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18413158532599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.18413158532599 | validation: 4.682214334189591]
	TIME [epoch: 10.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1247100907545664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1247100907545664 | validation: 1.7846729348085766]
	TIME [epoch: 10.4 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.354308912483242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.354308912483242 | validation: 2.4635046966634464]
	TIME [epoch: 10.4 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9840458865967012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9840458865967012 | validation: 1.590038572538025]
	TIME [epoch: 10.4 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.639807767692065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.639807767692065 | validation: 1.9464694958424045]
	TIME [epoch: 10.5 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6730461691693015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6730461691693015 | validation: 1.480347467559583]
	TIME [epoch: 10.5 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.789524177722393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.789524177722393 | validation: 1.728424651532057]
	TIME [epoch: 10.4 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.66869564131806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.66869564131806 | validation: 1.518741642392413]
	TIME [epoch: 10.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7221037820664435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7221037820664435 | validation: 1.626826020936743]
	TIME [epoch: 10.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6488393279745144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6488393279745144 | validation: 1.4600286604951425]
	TIME [epoch: 10.4 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7559584960141474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7559584960141474 | validation: 2.073795463706875]
	TIME [epoch: 10.4 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6155440420775125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6155440420775125 | validation: 1.6646032779855933]
	TIME [epoch: 10.4 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7374553351477595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7374553351477595 | validation: 1.8105075944572548]
	TIME [epoch: 10.4 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3735272420065083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3735272420065083 | validation: 3.0362070745656484]
	TIME [epoch: 10.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.033700698755699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.033700698755699 | validation: 3.4022475022635787]
	TIME [epoch: 10.4 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1259024338385295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1259024338385295 | validation: 2.2277724156576686]
	TIME [epoch: 10.4 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.49008766670447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.49008766670447 | validation: 2.3983290221745337]
	TIME [epoch: 10.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1412397454007825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1412397454007825 | validation: 1.730118343160065]
	TIME [epoch: 10.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443418901120158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7443418901120158 | validation: 1.9024636413220668]
	TIME [epoch: 10.4 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8760888024568037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8760888024568037 | validation: 1.5104401044612978]
	TIME [epoch: 10.5 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8028659841183559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8028659841183559 | validation: 2.0255518782631308]
	TIME [epoch: 10.4 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.442739846121384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.442739846121384 | validation: 1.5615720343951431]
	TIME [epoch: 10.4 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7499819051842334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7499819051842334 | validation: 1.6047961557921293]
	TIME [epoch: 10.5 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9259997539956928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9259997539956928 | validation: 2.8677982977143905]
	TIME [epoch: 10.5 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4491854481907347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4491854481907347 | validation: 1.8764357336285722]
	TIME [epoch: 10.4 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4685770034350427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4685770034350427 | validation: 3.318457129092067]
	TIME [epoch: 10.4 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1169694397691536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1169694397691536 | validation: 2.0222363555648735]
	TIME [epoch: 10.5 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9449275840520261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9449275840520261 | validation: 1.580405245311066]
	TIME [epoch: 10.5 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8072890532256622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8072890532256622 | validation: 1.809041216832559]
	TIME [epoch: 10.5 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1907846780216707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1907846780216707 | validation: 1.848378505776333]
	TIME [epoch: 10.5 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7538471398616295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7538471398616295 | validation: 2.4041114302368825]
	TIME [epoch: 10.5 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.847441075077534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.847441075077534 | validation: 1.4360255509894204]
	TIME [epoch: 10.5 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5937899614246445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5937899614246445 | validation: 2.1843705289003252]
	TIME [epoch: 10.4 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.628288158492028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.628288158492028 | validation: 1.6910818426239136]
	TIME [epoch: 10.5 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8195980890224117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8195980890224117 | validation: 2.048922792166602]
	TIME [epoch: 10.4 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.101601092062208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.101601092062208 | validation: 1.8598233323481594]
	TIME [epoch: 10.5 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8015980558926241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8015980558926241 | validation: 1.766887011565569]
	TIME [epoch: 10.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7285600160969814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7285600160969814 | validation: 1.9523249190783818]
	TIME [epoch: 10.5 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5717936337602934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5717936337602934 | validation: 1.4329788788097528]
	TIME [epoch: 10.5 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5860266028688428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5860266028688428 | validation: 1.597038132569109]
	TIME [epoch: 10.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3773508129295817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3773508129295817 | validation: 1.558862670850104]
	TIME [epoch: 10.5 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6021635370924898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6021635370924898 | validation: 1.8231746779927827]
	TIME [epoch: 10.5 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.628155938318271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.628155938318271 | validation: 2.0050793001391747]
	TIME [epoch: 10.5 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0260428377112705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0260428377112705 | validation: 2.3248805017123413]
	TIME [epoch: 10.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6513973352128999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6513973352128999 | validation: 1.7277293601860162]
	TIME [epoch: 10.5 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7274421302033005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7274421302033005 | validation: 1.8627674075413756]
	TIME [epoch: 10.5 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9052788729756251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9052788729756251 | validation: 2.1257441736100473]
	TIME [epoch: 10.4 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.818428859543502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.818428859543502 | validation: 1.525619401983316]
	TIME [epoch: 10.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0269443716961177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0269443716961177 | validation: 2.1561523979501587]
	TIME [epoch: 10.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7739225459112302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7739225459112302 | validation: 1.7643021378409756]
	TIME [epoch: 10.5 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.536740856295887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.536740856295887 | validation: 1.4531017876273378]
	TIME [epoch: 10.4 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5056832540640102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5056832540640102 | validation: 1.2990776581353214]
	TIME [epoch: 10.5 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3247074635754488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3247074635754488 | validation: 1.9743909436844325]
	TIME [epoch: 10.5 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.783198459740327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.783198459740327 | validation: 2.1310985387079513]
	TIME [epoch: 10.5 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5389234534928116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5389234534928116 | validation: 1.6519334799410654]
	TIME [epoch: 10.5 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7133596989279778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7133596989279778 | validation: 1.7746778893220443]
	TIME [epoch: 10.5 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.809619744913117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.809619744913117 | validation: 4.121061521547163]
	TIME [epoch: 10.5 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.43262851734617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.43262851734617 | validation: 1.5778671690118262]
	TIME [epoch: 10.4 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.055105173509491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.055105173509491 | validation: 1.5014875330627768]
	TIME [epoch: 10.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5320190695434566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5320190695434566 | validation: 1.693171219104292]
	TIME [epoch: 10.5 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7070576960380035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7070576960380035 | validation: 1.8442521129423444]
	TIME [epoch: 10.5 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6697265405655277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6697265405655277 | validation: 2.1406918241169475]
	TIME [epoch: 10.5 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6146899187555395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6146899187555395 | validation: 1.5147030024712838]
	TIME [epoch: 10.4 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.597878292906413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.597878292906413 | validation: 1.8987302586597468]
	TIME [epoch: 10.5 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196535217016723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.196535217016723 | validation: 2.419875380020099]
	TIME [epoch: 10.4 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.385697602249987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.385697602249987 | validation: 2.481569961867826]
	TIME [epoch: 10.5 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9910718802208298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9910718802208298 | validation: 2.1635317752291905]
	TIME [epoch: 10.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9164734038117253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9164734038117253 | validation: 1.7788347188734575]
	TIME [epoch: 10.5 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.028098491412511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.028098491412511 | validation: 1.6023639519851984]
	TIME [epoch: 10.5 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6716215209791083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6716215209791083 | validation: 2.34653527415099]
	TIME [epoch: 10.5 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.805941435044069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.805941435044069 | validation: 1.583828716018391]
	TIME [epoch: 10.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8269720275569583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8269720275569583 | validation: 1.9192193021439377]
	TIME [epoch: 10.4 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5765814406862215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5765814406862215 | validation: 2.0783207837515607]
	TIME [epoch: 10.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4372243225592953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4372243225592953 | validation: 2.919488634068347]
	TIME [epoch: 10.4 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4182244184510173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4182244184510173 | validation: 2.0954711192271804]
	TIME [epoch: 10.4 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7301936421366897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7301936421366897 | validation: 1.9879231048986397]
	TIME [epoch: 10.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.623022782532876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.623022782532876 | validation: 1.6977579927189037]
	TIME [epoch: 10.4 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.724161617490448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.724161617490448 | validation: 2.526839815885169]
	TIME [epoch: 10.5 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5354554940562886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5354554940562886 | validation: 2.1190070734598048]
	TIME [epoch: 10.5 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.764627711634806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.764627711634806 | validation: 1.6707442434424173]
	TIME [epoch: 10.4 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.861749514100015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.861749514100015 | validation: 1.5926914512663737]
	TIME [epoch: 10.4 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6467849377659782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6467849377659782 | validation: 2.211517773652981]
	TIME [epoch: 10.5 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9692865009938552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9692865009938552 | validation: 4.777398248626004]
	TIME [epoch: 10.5 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.041137026693476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.041137026693476 | validation: 2.126311533801793]
	TIME [epoch: 10.4 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9975876633247303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9975876633247303 | validation: 2.505935344027757]
	TIME [epoch: 10.5 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1472125448330366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1472125448330366 | validation: 1.8221535312891746]
	TIME [epoch: 10.4 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1696756929382284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1696756929382284 | validation: 1.7549879654650327]
	TIME [epoch: 10.5 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0741015743543407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0741015743543407 | validation: 2.103690322692223]
	TIME [epoch: 10.5 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243411728448878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.243411728448878 | validation: 2.3217610940675892]
	TIME [epoch: 10.5 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259139884195282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.259139884195282 | validation: 2.030265131471605]
	TIME [epoch: 10.5 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.864160843192998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.864160843192998 | validation: 2.5552930860483354]
	TIME [epoch: 10.4 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4280031193824083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4280031193824083 | validation: 2.2137129122327157]
	TIME [epoch: 10.5 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8191593954008796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8191593954008796 | validation: 1.9950375414331176]
	TIME [epoch: 10.5 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.35571100280503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.35571100280503 | validation: 2.107490664381589]
	TIME [epoch: 10.5 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5359286858276073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5359286858276073 | validation: 3.520183826577134]
	TIME [epoch: 10.4 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5818165120477103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5818165120477103 | validation: 2.8495484666070094]
	TIME [epoch: 10.5 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4537642191651083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4537642191651083 | validation: 3.8996068915998037]
	TIME [epoch: 10.4 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.930426638174812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.930426638174812 | validation: 5.139932878518025]
	TIME [epoch: 10.5 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6913006766578156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6913006766578156 | validation: 3.9142780468496676]
	TIME [epoch: 10.4 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.489597599153231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.489597599153231 | validation: 2.9015590718483657]
	TIME [epoch: 10.5 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.157913735513426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.157913735513426 | validation: 3.7417639802905422]
	TIME [epoch: 10.4 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.876864284667809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.876864284667809 | validation: 2.565646218214588]
	TIME [epoch: 10.5 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.467316051833803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.467316051833803 | validation: 2.8295366218032707]
	TIME [epoch: 10.5 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.828761232602454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.828761232602454 | validation: 2.4114905419738326]
	TIME [epoch: 10.5 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4403065190196003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4403065190196003 | validation: 2.2480088481501084]
	TIME [epoch: 10.4 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.343056672189152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.343056672189152 | validation: 3.0849403293646094]
	TIME [epoch: 10.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.643779037338975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.643779037338975 | validation: 2.8282757819730286]
	TIME [epoch: 10.5 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5280561643127935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5280561643127935 | validation: 2.288877649142393]
	TIME [epoch: 10.4 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6029148575353482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6029148575353482 | validation: 2.1144602518782625]
	TIME [epoch: 10.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.585397572798744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.585397572798744 | validation: 2.9271343521505346]
	TIME [epoch: 10.5 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6088881106117316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6088881106117316 | validation: 2.0625554842715847]
	TIME [epoch: 10.4 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3779699272913413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3779699272913413 | validation: 1.9386675039376229]
	TIME [epoch: 10.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6951793454432043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6951793454432043 | validation: 2.2077283623023467]
	TIME [epoch: 10.5 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3554881603684406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3554881603684406 | validation: 2.019080108061176]
	TIME [epoch: 10.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.160675962962623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.160675962962623 | validation: 1.8584948485606014]
	TIME [epoch: 10.4 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3081408167431032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3081408167431032 | validation: 1.9200412414253734]
	TIME [epoch: 10.4 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8287193529197765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8287193529197765 | validation: 1.7080453165983824]
	TIME [epoch: 10.4 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7200974661449329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7200974661449329 | validation: 1.9653792349829258]
	TIME [epoch: 10.5 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8253698203518318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8253698203518318 | validation: 2.712005524893426]
	TIME [epoch: 10.5 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2553020842007614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2553020842007614 | validation: 1.8291157694939646]
	TIME [epoch: 10.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.007736694283609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.007736694283609 | validation: 2.9124879453994232]
	TIME [epoch: 10.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.919608797188376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.919608797188376 | validation: 1.9123415384794253]
	TIME [epoch: 10.5 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7399162517185764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7399162517185764 | validation: 1.699241454944076]
	TIME [epoch: 10.4 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18464634413999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.18464634413999 | validation: 1.661901228740487]
	TIME [epoch: 10.5 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7273880953561693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7273880953561693 | validation: 1.8729567982557642]
	TIME [epoch: 10.5 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8079559363256188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8079559363256188 | validation: 1.8784071276340826]
	TIME [epoch: 10.5 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7345650240764237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7345650240764237 | validation: 1.8186057686366273]
	TIME [epoch: 10.4 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5385499274958603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5385499274958603 | validation: 2.2898918491074545]
	TIME [epoch: 10.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.380814093741739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.380814093741739 | validation: 3.187818864685354]
	TIME [epoch: 10.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.091374247300696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.091374247300696 | validation: 2.511829838774618]
	TIME [epoch: 10.4 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3248510860493665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3248510860493665 | validation: 2.28051888121927]
	TIME [epoch: 10.4 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.634382425429503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.634382425429503 | validation: 2.116761844724795]
	TIME [epoch: 10.4 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3890153501316354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3890153501316354 | validation: 1.7733699706935533]
	TIME [epoch: 10.5 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3004896883721964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3004896883721964 | validation: 2.283186805301194]
	TIME [epoch: 10.5 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1786955340462435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1786955340462435 | validation: 2.032712216147135]
	TIME [epoch: 10.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.97743930644757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.97743930644757 | validation: 1.8969470773768464]
	TIME [epoch: 10.5 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7973559351365451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7973559351365451 | validation: 2.173178765065342]
	TIME [epoch: 10.5 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.159398295794601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.159398295794601 | validation: 3.5510792135852647]
	TIME [epoch: 10.4 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6698341767907565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6698341767907565 | validation: 2.231463752318861]
	TIME [epoch: 10.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6935405716733396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6935405716733396 | validation: 2.540868663838466]
	TIME [epoch: 10.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5657920191555874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5657920191555874 | validation: 2.333881342258881]
	TIME [epoch: 10.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.017588089391777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.017588089391777 | validation: 2.840318393247777]
	TIME [epoch: 10.4 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.38596770322848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.38596770322848 | validation: 2.682758444014127]
	TIME [epoch: 10.5 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3541275209049184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3541275209049184 | validation: 1.6581322436177623]
	TIME [epoch: 10.5 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.140105619992008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.140105619992008 | validation: 2.4024923156285087]
	TIME [epoch: 10.5 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.78413777459103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.78413777459103 | validation: 1.6013519543352828]
	TIME [epoch: 10.4 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185266663053135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.185266663053135 | validation: 2.2067919323255927]
	TIME [epoch: 10.5 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9445331590307895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9445331590307895 | validation: 2.1689294174134144]
	TIME [epoch: 10.4 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0336618227049756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0336618227049756 | validation: 2.2540501437948453]
	TIME [epoch: 10.4 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0333893847860245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0333893847860245 | validation: 1.882988053067976]
	TIME [epoch: 10.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6863190915093376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6863190915093376 | validation: 3.3682127321954782]
	TIME [epoch: 10.5 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5758002225202397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5758002225202397 | validation: 2.043154574994393]
	TIME [epoch: 10.4 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6657264694811502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6657264694811502 | validation: 1.803950494467447]
	TIME [epoch: 10.4 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7639028118292408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7639028118292408 | validation: 1.9719925724774459]
	TIME [epoch: 10.5 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9327593442153765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9327593442153765 | validation: 1.5684870612410005]
	TIME [epoch: 10.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6706839932642836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6706839932642836 | validation: 2.9058679936878837]
	TIME [epoch: 10.4 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.932121013689683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.932121013689683 | validation: 1.904160977392233]
	TIME [epoch: 10.4 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3837403881508776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3837403881508776 | validation: 1.7357126446651416]
	TIME [epoch: 10.5 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6077263576329717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6077263576329717 | validation: 1.979868008093938]
	TIME [epoch: 10.5 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6488725968660294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6488725968660294 | validation: 1.5475857538597688]
	TIME [epoch: 10.5 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1007141863189576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1007141863189576 | validation: 1.9151185574926601]
	TIME [epoch: 10.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.071298460833616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.071298460833616 | validation: 2.469460490934816]
	TIME [epoch: 10.4 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.043511676256941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.043511676256941 | validation: 1.988098822292645]
	TIME [epoch: 10.4 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8811894365483284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8811894365483284 | validation: 1.8155050940568338]
	TIME [epoch: 10.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6930800782926845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6930800782926845 | validation: 1.644257445754842]
	TIME [epoch: 10.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6097904109826433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6097904109826433 | validation: 2.0745389623753003]
	TIME [epoch: 10.5 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8843395529474471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8843395529474471 | validation: 1.642087339403588]
	TIME [epoch: 10.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9220189996031656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9220189996031656 | validation: 1.8025001811384875]
	TIME [epoch: 10.4 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8949368186491307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8949368186491307 | validation: 4.242496747818592]
	TIME [epoch: 10.5 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8412419650211214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8412419650211214 | validation: 1.6876183884598521]
	TIME [epoch: 10.5 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.563469240681858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.563469240681858 | validation: 1.7324939449795733]
	TIME [epoch: 10.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6478529198464027		[learning rate: 0.0099755]
	Learning Rate: 0.00997547
	LOSS [training: 1.6478529198464027 | validation: 1.6641528344407122]
	TIME [epoch: 10.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4738310442380782		[learning rate: 0.0099449]
	Learning Rate: 0.00994489
	LOSS [training: 1.4738310442380782 | validation: 2.3986749233111833]
	TIME [epoch: 10.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.722884031743444		[learning rate: 0.0099144]
	Learning Rate: 0.0099144
	LOSS [training: 1.722884031743444 | validation: 1.5121076753842062]
	TIME [epoch: 10.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.981425725177497		[learning rate: 0.009884]
	Learning Rate: 0.00988401
	LOSS [training: 1.981425725177497 | validation: 1.9561559439145293]
	TIME [epoch: 10.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7020545389814294		[learning rate: 0.0098537]
	Learning Rate: 0.00985371
	LOSS [training: 1.7020545389814294 | validation: 1.636083497208841]
	TIME [epoch: 10.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5900358337742166		[learning rate: 0.0098235]
	Learning Rate: 0.00982351
	LOSS [training: 1.5900358337742166 | validation: 1.6987807849412435]
	TIME [epoch: 10.4 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743649902767333		[learning rate: 0.0097934]
	Learning Rate: 0.0097934
	LOSS [training: 1.743649902767333 | validation: 1.6558535857344359]
	TIME [epoch: 10.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4283347116304754		[learning rate: 0.0097634]
	Learning Rate: 0.00976337
	LOSS [training: 1.4283347116304754 | validation: 1.6344364385697756]
	TIME [epoch: 10.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8424649853046262		[learning rate: 0.0097334]
	Learning Rate: 0.00973345
	LOSS [training: 1.8424649853046262 | validation: 3.8903458341462205]
	TIME [epoch: 10.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.407939983527762		[learning rate: 0.0097036]
	Learning Rate: 0.00970361
	LOSS [training: 2.407939983527762 | validation: 2.601284029589707]
	TIME [epoch: 10.4 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7951308040046974		[learning rate: 0.0096739]
	Learning Rate: 0.00967386
	LOSS [training: 1.7951308040046974 | validation: 1.6102652574439091]
	TIME [epoch: 10.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.527195805634753		[learning rate: 0.0096442]
	Learning Rate: 0.00964421
	LOSS [training: 1.527195805634753 | validation: 2.6061699302527996]
	TIME [epoch: 10.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9469277780354464		[learning rate: 0.0096146]
	Learning Rate: 0.00961465
	LOSS [training: 1.9469277780354464 | validation: 1.5693036815857793]
	TIME [epoch: 10.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4664991459390044		[learning rate: 0.0095852]
	Learning Rate: 0.00958517
	LOSS [training: 1.4664991459390044 | validation: 1.9655561230741645]
	TIME [epoch: 10.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0727204406654254		[learning rate: 0.0095558]
	Learning Rate: 0.00955579
	LOSS [training: 2.0727204406654254 | validation: 1.925669449849591]
	TIME [epoch: 10.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5890446889881322		[learning rate: 0.0095265]
	Learning Rate: 0.0095265
	LOSS [training: 1.5890446889881322 | validation: 2.1145265574149277]
	TIME [epoch: 10.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7750346250233178		[learning rate: 0.0094973]
	Learning Rate: 0.0094973
	LOSS [training: 1.7750346250233178 | validation: 1.8892567835842424]
	TIME [epoch: 10.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.566208209813692		[learning rate: 0.0094682]
	Learning Rate: 0.00946818
	LOSS [training: 1.566208209813692 | validation: 2.1700097775779]
	TIME [epoch: 10.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9634607958627392		[learning rate: 0.0094392]
	Learning Rate: 0.00943916
	LOSS [training: 1.9634607958627392 | validation: 3.197778786379863]
	TIME [epoch: 10.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9182286496907583		[learning rate: 0.0094102]
	Learning Rate: 0.00941022
	LOSS [training: 1.9182286496907583 | validation: 2.1737257232894733]
	TIME [epoch: 10.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0354354034476105		[learning rate: 0.0093814]
	Learning Rate: 0.00938138
	LOSS [training: 2.0354354034476105 | validation: 1.786886394287178]
	TIME [epoch: 10.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.493339072232203		[learning rate: 0.0093526]
	Learning Rate: 0.00935262
	LOSS [training: 1.493339072232203 | validation: 1.6249455438087455]
	TIME [epoch: 10.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6049404648369112		[learning rate: 0.009324]
	Learning Rate: 0.00932395
	LOSS [training: 1.6049404648369112 | validation: 1.624891578095758]
	TIME [epoch: 10.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6434699540337245		[learning rate: 0.0092954]
	Learning Rate: 0.00929537
	LOSS [training: 1.6434699540337245 | validation: 3.436070241717141]
	TIME [epoch: 10.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.355912455649858		[learning rate: 0.0092669]
	Learning Rate: 0.00926687
	LOSS [training: 2.355912455649858 | validation: 1.7516901548284267]
	TIME [epoch: 10.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6453621924208737		[learning rate: 0.0092385]
	Learning Rate: 0.00923847
	LOSS [training: 1.6453621924208737 | validation: 1.4357664807555102]
	TIME [epoch: 10.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7021650174034793		[learning rate: 0.0092101]
	Learning Rate: 0.00921015
	LOSS [training: 1.7021650174034793 | validation: 2.467113747782061]
	TIME [epoch: 10.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8816167499925893		[learning rate: 0.0091819]
	Learning Rate: 0.00918192
	LOSS [training: 1.8816167499925893 | validation: 1.331178605674078]
	TIME [epoch: 10.4 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8045284960080825		[learning rate: 0.0091538]
	Learning Rate: 0.00915377
	LOSS [training: 1.8045284960080825 | validation: 2.350823054836421]
	TIME [epoch: 10.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6985094060545372		[learning rate: 0.0091257]
	Learning Rate: 0.00912571
	LOSS [training: 1.6985094060545372 | validation: 1.7028330608608844]
	TIME [epoch: 10.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4431782724427393		[learning rate: 0.0090977]
	Learning Rate: 0.00909774
	LOSS [training: 1.4431782724427393 | validation: 1.6231965355268607]
	TIME [epoch: 10.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1368805771786077		[learning rate: 0.0090698]
	Learning Rate: 0.00906985
	LOSS [training: 2.1368805771786077 | validation: 1.8321868053707306]
	TIME [epoch: 10.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.360344823414633		[learning rate: 0.009042]
	Learning Rate: 0.00904205
	LOSS [training: 2.360344823414633 | validation: 2.360177248642251]
	TIME [epoch: 10.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.661427256262579		[learning rate: 0.0090143]
	Learning Rate: 0.00901433
	LOSS [training: 1.661427256262579 | validation: 2.0467257420011156]
	TIME [epoch: 10.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2363597630923917		[learning rate: 0.0089867]
	Learning Rate: 0.00898669
	LOSS [training: 2.2363597630923917 | validation: 3.0825188587487196]
	TIME [epoch: 10.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2296380282569492		[learning rate: 0.0089591]
	Learning Rate: 0.00895915
	LOSS [training: 2.2296380282569492 | validation: 1.8088452006123106]
	TIME [epoch: 10.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.234830373747001		[learning rate: 0.0089317]
	Learning Rate: 0.00893168
	LOSS [training: 2.234830373747001 | validation: 3.506517540022703]
	TIME [epoch: 10.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4634401692937433		[learning rate: 0.0089043]
	Learning Rate: 0.0089043
	LOSS [training: 2.4634401692937433 | validation: 2.624168629749519]
	TIME [epoch: 10.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.875259693322866		[learning rate: 0.008877]
	Learning Rate: 0.00887701
	LOSS [training: 1.875259693322866 | validation: 1.5824112345682195]
	TIME [epoch: 10.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5245114043377102		[learning rate: 0.0088498]
	Learning Rate: 0.0088498
	LOSS [training: 1.5245114043377102 | validation: 1.5772249470203301]
	TIME [epoch: 10.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.296491040405132		[learning rate: 0.0088227]
	Learning Rate: 0.00882267
	LOSS [training: 2.296491040405132 | validation: 1.6411209748984186]
	TIME [epoch: 10.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5450907262613427		[learning rate: 0.0087956]
	Learning Rate: 0.00879562
	LOSS [training: 1.5450907262613427 | validation: 1.3515897855510284]
	TIME [epoch: 10.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5658848130804035		[learning rate: 0.0087687]
	Learning Rate: 0.00876866
	LOSS [training: 1.5658848130804035 | validation: 1.6099980095217683]
	TIME [epoch: 10.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5761782395058956		[learning rate: 0.0087418]
	Learning Rate: 0.00874178
	LOSS [training: 2.5761782395058956 | validation: 2.568562474784765]
	TIME [epoch: 10.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2613291111749296		[learning rate: 0.008715]
	Learning Rate: 0.00871499
	LOSS [training: 2.2613291111749296 | validation: 2.502435606511357]
	TIME [epoch: 10.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.169810140331724		[learning rate: 0.0086883]
	Learning Rate: 0.00868827
	LOSS [training: 2.169810140331724 | validation: 1.6873994860737298]
	TIME [epoch: 10.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.669170670498044		[learning rate: 0.0086616]
	Learning Rate: 0.00866164
	LOSS [training: 1.669170670498044 | validation: 1.6588704853820984]
	TIME [epoch: 10.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.561112952436974		[learning rate: 0.0086351]
	Learning Rate: 0.00863509
	LOSS [training: 1.561112952436974 | validation: 2.022542872203035]
	TIME [epoch: 10.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7765975278634851		[learning rate: 0.0086086]
	Learning Rate: 0.00860862
	LOSS [training: 1.7765975278634851 | validation: 2.396572995718098]
	TIME [epoch: 10.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9915884251633666		[learning rate: 0.0085822]
	Learning Rate: 0.00858223
	LOSS [training: 1.9915884251633666 | validation: 2.8438519856853395]
	TIME [epoch: 10.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7995534791633727		[learning rate: 0.0085559]
	Learning Rate: 0.00855592
	LOSS [training: 1.7995534791633727 | validation: 1.3124136343447312]
	TIME [epoch: 10.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6842441276813787		[learning rate: 0.0085297]
	Learning Rate: 0.00852969
	LOSS [training: 1.6842441276813787 | validation: 2.4885293819818846]
	TIME [epoch: 10.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.790160370006879		[learning rate: 0.0085035]
	Learning Rate: 0.00850354
	LOSS [training: 1.790160370006879 | validation: 1.5624992961277775]
	TIME [epoch: 10.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6521090489202375		[learning rate: 0.0084775]
	Learning Rate: 0.00847748
	LOSS [training: 1.6521090489202375 | validation: 2.019747986129268]
	TIME [epoch: 10.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4527091834595507		[learning rate: 0.0084515]
	Learning Rate: 0.00845149
	LOSS [training: 1.4527091834595507 | validation: 1.8355514776858235]
	TIME [epoch: 10.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7688026109095998		[learning rate: 0.0084256]
	Learning Rate: 0.00842558
	LOSS [training: 1.7688026109095998 | validation: 1.8919878303513162]
	TIME [epoch: 10.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7278954980213808		[learning rate: 0.0083998]
	Learning Rate: 0.00839976
	LOSS [training: 1.7278954980213808 | validation: 1.709418871361542]
	TIME [epoch: 10.4 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5620826162684707		[learning rate: 0.008374]
	Learning Rate: 0.00837401
	LOSS [training: 1.5620826162684707 | validation: 1.3309804564074903]
	TIME [epoch: 10.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.704830309854831		[learning rate: 0.0083483]
	Learning Rate: 0.00834834
	LOSS [training: 1.704830309854831 | validation: 2.5750598223799623]
	TIME [epoch: 10.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5654557152415862		[learning rate: 0.0083227]
	Learning Rate: 0.00832275
	LOSS [training: 1.5654557152415862 | validation: 1.397214115392235]
	TIME [epoch: 10.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3630343620375318		[learning rate: 0.0082972]
	Learning Rate: 0.00829723
	LOSS [training: 1.3630343620375318 | validation: 2.901086973534037]
	TIME [epoch: 10.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9584070517470864		[learning rate: 0.0082718]
	Learning Rate: 0.0082718
	LOSS [training: 1.9584070517470864 | validation: 2.1963678682092334]
	TIME [epoch: 10.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7481477744629146		[learning rate: 0.0082464]
	Learning Rate: 0.00824644
	LOSS [training: 1.7481477744629146 | validation: 1.782278831812808]
	TIME [epoch: 10.5 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4874538826205417		[learning rate: 0.0082212]
	Learning Rate: 0.00822116
	LOSS [training: 1.4874538826205417 | validation: 2.28445898808749]
	TIME [epoch: 10.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8162118629116164		[learning rate: 0.008196]
	Learning Rate: 0.00819596
	LOSS [training: 1.8162118629116164 | validation: 1.5966851624256282]
	TIME [epoch: 10.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5929095313744284		[learning rate: 0.0081708]
	Learning Rate: 0.00817084
	LOSS [training: 1.5929095313744284 | validation: 1.8659476052167123]
	TIME [epoch: 10.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.442701364978885		[learning rate: 0.0081458]
	Learning Rate: 0.00814579
	LOSS [training: 1.442701364978885 | validation: 1.4177185717848455]
	TIME [epoch: 10.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4214973742747055		[learning rate: 0.0081208]
	Learning Rate: 0.00812082
	LOSS [training: 1.4214973742747055 | validation: 1.2387532124134695]
	TIME [epoch: 10.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4282693908542021		[learning rate: 0.0080959]
	Learning Rate: 0.00809593
	LOSS [training: 1.4282693908542021 | validation: 1.8151657834486514]
	TIME [epoch: 10.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4705284537524406		[learning rate: 0.0080711]
	Learning Rate: 0.00807111
	LOSS [training: 1.4705284537524406 | validation: 1.7968824048051284]
	TIME [epoch: 10.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4554888145856837		[learning rate: 0.0080464]
	Learning Rate: 0.00804637
	LOSS [training: 1.4554888145856837 | validation: 1.8902399557619063]
	TIME [epoch: 10.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6659056396403593		[learning rate: 0.0080217]
	Learning Rate: 0.0080217
	LOSS [training: 1.6659056396403593 | validation: 1.5407125967986115]
	TIME [epoch: 10.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7904082616445518		[learning rate: 0.0079971]
	Learning Rate: 0.00799711
	LOSS [training: 1.7904082616445518 | validation: 1.8775367774013263]
	TIME [epoch: 10.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.904788581400819		[learning rate: 0.0079726]
	Learning Rate: 0.0079726
	LOSS [training: 2.904788581400819 | validation: 2.487460032981641]
	TIME [epoch: 10.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4206210561167185		[learning rate: 0.0079482]
	Learning Rate: 0.00794816
	LOSS [training: 2.4206210561167185 | validation: 2.4064915504346347]
	TIME [epoch: 10.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.350345669587182		[learning rate: 0.0079238]
	Learning Rate: 0.0079238
	LOSS [training: 2.350345669587182 | validation: 2.295760173422634]
	TIME [epoch: 10.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7104382330032486		[learning rate: 0.0078995]
	Learning Rate: 0.00789951
	LOSS [training: 1.7104382330032486 | validation: 1.3723680598733388]
	TIME [epoch: 10.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.36413953961341		[learning rate: 0.0078753]
	Learning Rate: 0.00787529
	LOSS [training: 1.36413953961341 | validation: 2.012342358375702]
	TIME [epoch: 10.4 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751507251558484		[learning rate: 0.0078512]
	Learning Rate: 0.00785115
	LOSS [training: 1.751507251558484 | validation: 1.9508615937947076]
	TIME [epoch: 10.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.626436748309097		[learning rate: 0.0078271]
	Learning Rate: 0.00782708
	LOSS [training: 1.626436748309097 | validation: 1.4520380458004842]
	TIME [epoch: 10.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1261080838268227		[learning rate: 0.0078031]
	Learning Rate: 0.00780309
	LOSS [training: 2.1261080838268227 | validation: 2.1683468262505152]
	TIME [epoch: 10.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.722909709388204		[learning rate: 0.0077792]
	Learning Rate: 0.00777917
	LOSS [training: 1.722909709388204 | validation: 2.614896999033458]
	TIME [epoch: 10.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8708232654080923		[learning rate: 0.0077553]
	Learning Rate: 0.00775532
	LOSS [training: 1.8708232654080923 | validation: 2.056559199350908]
	TIME [epoch: 10.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6540991061546393		[learning rate: 0.0077316]
	Learning Rate: 0.00773155
	LOSS [training: 1.6540991061546393 | validation: 2.0622077852039506]
	TIME [epoch: 10.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.795303347428152		[learning rate: 0.0077079]
	Learning Rate: 0.00770785
	LOSS [training: 1.795303347428152 | validation: 1.7595909415833375]
	TIME [epoch: 10.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4999914244070855		[learning rate: 0.0076842]
	Learning Rate: 0.00768422
	LOSS [training: 1.4999914244070855 | validation: 2.3821482278938837]
	TIME [epoch: 10.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9207429411363477		[learning rate: 0.0076607]
	Learning Rate: 0.00766067
	LOSS [training: 1.9207429411363477 | validation: 2.3393175206157064]
	TIME [epoch: 10.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.669626805031503		[learning rate: 0.0076372]
	Learning Rate: 0.00763718
	LOSS [training: 1.669626805031503 | validation: 1.7949186031519662]
	TIME [epoch: 10.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.498698359868741		[learning rate: 0.0076138]
	Learning Rate: 0.00761377
	LOSS [training: 1.498698359868741 | validation: 2.049490515763931]
	TIME [epoch: 10.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.103929386466185		[learning rate: 0.0075904]
	Learning Rate: 0.00759043
	LOSS [training: 2.103929386466185 | validation: 2.057565421529513]
	TIME [epoch: 10.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.008506266230842		[learning rate: 0.0075672]
	Learning Rate: 0.00756717
	LOSS [training: 2.008506266230842 | validation: 1.526477917856691]
	TIME [epoch: 10.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.632741106335164		[learning rate: 0.007544]
	Learning Rate: 0.00754397
	LOSS [training: 1.632741106335164 | validation: 1.453729451203156]
	TIME [epoch: 10.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6415726305543754		[learning rate: 0.0075208]
	Learning Rate: 0.00752085
	LOSS [training: 1.6415726305543754 | validation: 2.4458051100244185]
	TIME [epoch: 10.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0252711738273836		[learning rate: 0.0074978]
	Learning Rate: 0.00749779
	LOSS [training: 2.0252711738273836 | validation: 1.6580528834319932]
	TIME [epoch: 10.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5688075356035651		[learning rate: 0.0074748]
	Learning Rate: 0.00747481
	LOSS [training: 1.5688075356035651 | validation: 1.6866904002711152]
	TIME [epoch: 10.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.753922337758517		[learning rate: 0.0074519]
	Learning Rate: 0.00745189
	LOSS [training: 1.753922337758517 | validation: 1.4384041032070247]
	TIME [epoch: 10.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6229688057160825		[learning rate: 0.0074291]
	Learning Rate: 0.00742905
	LOSS [training: 1.6229688057160825 | validation: 1.592171116598503]
	TIME [epoch: 10.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.606293068114025		[learning rate: 0.0074063]
	Learning Rate: 0.00740628
	LOSS [training: 1.606293068114025 | validation: 2.2846220504549057]
	TIME [epoch: 10.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0047874367641567		[learning rate: 0.0073836]
	Learning Rate: 0.00738357
	LOSS [training: 2.0047874367641567 | validation: 2.1701698634210147]
	TIME [epoch: 10.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7363306147240527		[learning rate: 0.0073609]
	Learning Rate: 0.00736094
	LOSS [training: 3.7363306147240527 | validation: 4.072526235361412]
	TIME [epoch: 10.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.363289729802376		[learning rate: 0.0073384]
	Learning Rate: 0.00733838
	LOSS [training: 3.363289729802376 | validation: 2.1532436610793417]
	TIME [epoch: 10.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6675660278688724		[learning rate: 0.0073159]
	Learning Rate: 0.00731588
	LOSS [training: 2.6675660278688724 | validation: 2.308115837024895]
	TIME [epoch: 10.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.71249218501712		[learning rate: 0.0072935]
	Learning Rate: 0.00729345
	LOSS [training: 2.71249218501712 | validation: 2.8080056062320726]
	TIME [epoch: 10.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5119416824948675		[learning rate: 0.0072711]
	Learning Rate: 0.0072711
	LOSS [training: 2.5119416824948675 | validation: 1.8095412847112702]
	TIME [epoch: 10.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.626876790976307		[learning rate: 0.0072488]
	Learning Rate: 0.00724881
	LOSS [training: 1.626876790976307 | validation: 1.4446071183943072]
	TIME [epoch: 10.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8871819873168783		[learning rate: 0.0072266]
	Learning Rate: 0.00722659
	LOSS [training: 1.8871819873168783 | validation: 2.4744858777183834]
	TIME [epoch: 10.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.334408351708984		[learning rate: 0.0072044]
	Learning Rate: 0.00720444
	LOSS [training: 2.334408351708984 | validation: 1.3785826104820307]
	TIME [epoch: 10.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4774068804989606		[learning rate: 0.0071824]
	Learning Rate: 0.00718235
	LOSS [training: 1.4774068804989606 | validation: 1.2982884642272836]
	TIME [epoch: 10.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7715689320137877		[learning rate: 0.0071603]
	Learning Rate: 0.00716033
	LOSS [training: 1.7715689320137877 | validation: 1.6841378066818606]
	TIME [epoch: 10.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5751123412533246		[learning rate: 0.0071384]
	Learning Rate: 0.00713838
	LOSS [training: 1.5751123412533246 | validation: 1.396836431209885]
	TIME [epoch: 10.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.845522072296967		[learning rate: 0.0071165]
	Learning Rate: 0.0071165
	LOSS [training: 1.845522072296967 | validation: 1.4688560881876935]
	TIME [epoch: 10.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.61836850210967		[learning rate: 0.0070947]
	Learning Rate: 0.00709469
	LOSS [training: 1.61836850210967 | validation: 1.694086246675799]
	TIME [epoch: 10.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.763820479349652		[learning rate: 0.0070729]
	Learning Rate: 0.00707294
	LOSS [training: 1.763820479349652 | validation: 1.5053289520480968]
	TIME [epoch: 10.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4933977642525176		[learning rate: 0.0070513]
	Learning Rate: 0.00705126
	LOSS [training: 1.4933977642525176 | validation: 1.343165513061301]
	TIME [epoch: 10.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4749040362607393		[learning rate: 0.0070296]
	Learning Rate: 0.00702964
	LOSS [training: 1.4749040362607393 | validation: 1.1362242999465368]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3017212199031747		[learning rate: 0.0070081]
	Learning Rate: 0.00700809
	LOSS [training: 1.3017212199031747 | validation: 1.141574057611164]
	TIME [epoch: 10.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7333924276492905		[learning rate: 0.0069866]
	Learning Rate: 0.00698661
	LOSS [training: 1.7333924276492905 | validation: 4.790221161514501]
	TIME [epoch: 10.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775821056494652		[learning rate: 0.0069652]
	Learning Rate: 0.0069652
	LOSS [training: 4.775821056494652 | validation: 3.852133038152619]
	TIME [epoch: 10.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8059089999389197		[learning rate: 0.0069438]
	Learning Rate: 0.00694384
	LOSS [training: 2.8059089999389197 | validation: 1.1837321697523184]
	TIME [epoch: 10.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5322630411821274		[learning rate: 0.0069226]
	Learning Rate: 0.00692256
	LOSS [training: 1.5322630411821274 | validation: 1.003308157120635]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6828593275052013		[learning rate: 0.0069013]
	Learning Rate: 0.00690134
	LOSS [training: 2.6828593275052013 | validation: 2.4272251246197647]
	TIME [epoch: 10.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6815366200188833		[learning rate: 0.0068802]
	Learning Rate: 0.00688018
	LOSS [training: 1.6815366200188833 | validation: 1.3886915982283763]
	TIME [epoch: 10.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.519722213713843		[learning rate: 0.0068591]
	Learning Rate: 0.00685909
	LOSS [training: 1.519722213713843 | validation: 1.4111971493014677]
	TIME [epoch: 10.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5602687412287346		[learning rate: 0.0068381]
	Learning Rate: 0.00683807
	LOSS [training: 1.5602687412287346 | validation: 1.1753433639540136]
	TIME [epoch: 10.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.309220496753485		[learning rate: 0.0068171]
	Learning Rate: 0.0068171
	LOSS [training: 1.309220496753485 | validation: 1.228475135294715]
	TIME [epoch: 10.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.446672281970839		[learning rate: 0.0067962]
	Learning Rate: 0.00679621
	LOSS [training: 1.446672281970839 | validation: 1.6325753409999262]
	TIME [epoch: 10.4 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.699732429605659		[learning rate: 0.0067754]
	Learning Rate: 0.00677537
	LOSS [training: 1.699732429605659 | validation: 3.2069571300876345]
	TIME [epoch: 10.4 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7341753121341217		[learning rate: 0.0067546]
	Learning Rate: 0.00675461
	LOSS [training: 1.7341753121341217 | validation: 1.1729977748590519]
	TIME [epoch: 10.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1729765298570047		[learning rate: 0.0067339]
	Learning Rate: 0.0067339
	LOSS [training: 1.1729765298570047 | validation: 1.3180959011850177]
	TIME [epoch: 10.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6605678520219054		[learning rate: 0.0067133]
	Learning Rate: 0.00671326
	LOSS [training: 1.6605678520219054 | validation: 4.085314507525629]
	TIME [epoch: 10.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.789200022161036		[learning rate: 0.0066927]
	Learning Rate: 0.00669268
	LOSS [training: 2.789200022161036 | validation: 1.3017524314653144]
	TIME [epoch: 10.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3482468584272542		[learning rate: 0.0066722]
	Learning Rate: 0.00667216
	LOSS [training: 1.3482468584272542 | validation: 1.4067398082077913]
	TIME [epoch: 10.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.480708940354138		[learning rate: 0.0066517]
	Learning Rate: 0.00665171
	LOSS [training: 1.480708940354138 | validation: 1.3432989124217962]
	TIME [epoch: 10.4 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4329239525537516		[learning rate: 0.0066313]
	Learning Rate: 0.00663132
	LOSS [training: 1.4329239525537516 | validation: 1.3804661563436829]
	TIME [epoch: 10.4 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3017286477513978		[learning rate: 0.006611]
	Learning Rate: 0.00661099
	LOSS [training: 1.3017286477513978 | validation: 1.638851466607764]
	TIME [epoch: 10.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3170518430874938		[learning rate: 0.0065907]
	Learning Rate: 0.00659073
	LOSS [training: 1.3170518430874938 | validation: 1.1746096495908216]
	TIME [epoch: 10.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1978329841702164		[learning rate: 0.0065705]
	Learning Rate: 0.00657052
	LOSS [training: 1.1978329841702164 | validation: 1.2057240117434938]
	TIME [epoch: 10.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3839370835432951		[learning rate: 0.0065504]
	Learning Rate: 0.00655038
	LOSS [training: 1.3839370835432951 | validation: 1.6742157695539397]
	TIME [epoch: 10.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.653562904620502		[learning rate: 0.0065303]
	Learning Rate: 0.0065303
	LOSS [training: 1.653562904620502 | validation: 2.379639902291008]
	TIME [epoch: 10.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.683045255084514		[learning rate: 0.0065103]
	Learning Rate: 0.00651028
	LOSS [training: 1.683045255084514 | validation: 1.672836352184782]
	TIME [epoch: 10.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8345394164195343		[learning rate: 0.0064903]
	Learning Rate: 0.00649033
	LOSS [training: 1.8345394164195343 | validation: 1.584002250595146]
	TIME [epoch: 10.4 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4997957804496984		[learning rate: 0.0064704]
	Learning Rate: 0.00647043
	LOSS [training: 1.4997957804496984 | validation: 1.397815321436541]
	TIME [epoch: 10.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.938484316181744		[learning rate: 0.0064506]
	Learning Rate: 0.0064506
	LOSS [training: 2.938484316181744 | validation: 3.1250376174241614]
	TIME [epoch: 10.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.666521518942167		[learning rate: 0.0064308]
	Learning Rate: 0.00643082
	LOSS [training: 2.666521518942167 | validation: 1.4099653350306085]
	TIME [epoch: 10.4 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2974686510266082		[learning rate: 0.0064111]
	Learning Rate: 0.00641111
	LOSS [training: 1.2974686510266082 | validation: 1.1967031970390485]
	TIME [epoch: 10.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1051792683564965		[learning rate: 0.0063915]
	Learning Rate: 0.00639146
	LOSS [training: 1.1051792683564965 | validation: 1.5955906444704748]
	TIME [epoch: 10.4 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8578475859730599		[learning rate: 0.0063719]
	Learning Rate: 0.00637187
	LOSS [training: 1.8578475859730599 | validation: 1.5675386484535871]
	TIME [epoch: 10.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2822112280155387		[learning rate: 0.0063523]
	Learning Rate: 0.00635233
	LOSS [training: 1.2822112280155387 | validation: 1.3906743423232741]
	TIME [epoch: 10.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3622042966728123		[learning rate: 0.0063329]
	Learning Rate: 0.00633286
	LOSS [training: 1.3622042966728123 | validation: 1.220792066212904]
	TIME [epoch: 10.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4498381683686414		[learning rate: 0.0063134]
	Learning Rate: 0.00631345
	LOSS [training: 1.4498381683686414 | validation: 1.9636944150804874]
	TIME [epoch: 10.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.568972590530166		[learning rate: 0.0062941]
	Learning Rate: 0.0062941
	LOSS [training: 1.568972590530166 | validation: 2.997782853494485]
	TIME [epoch: 10.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9355355145031428		[learning rate: 0.0062748]
	Learning Rate: 0.0062748
	LOSS [training: 1.9355355145031428 | validation: 1.8570254627849028]
	TIME [epoch: 10.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5094875616214696		[learning rate: 0.0062556]
	Learning Rate: 0.00625557
	LOSS [training: 1.5094875616214696 | validation: 1.5243331923343315]
	TIME [epoch: 10.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5939091407319013		[learning rate: 0.0062364]
	Learning Rate: 0.00623639
	LOSS [training: 1.5939091407319013 | validation: 2.555789685005859]
	TIME [epoch: 10.5 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394635674683347		[learning rate: 0.0062173]
	Learning Rate: 0.00621727
	LOSS [training: 1.9394635674683347 | validation: 1.571328037089878]
	TIME [epoch: 10.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7894933898768528		[learning rate: 0.0061982]
	Learning Rate: 0.00619822
	LOSS [training: 1.7894933898768528 | validation: 2.9847986835888425]
	TIME [epoch: 10.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.071188955690377		[learning rate: 0.0061792]
	Learning Rate: 0.00617922
	LOSS [training: 2.071188955690377 | validation: 1.2989473441211323]
	TIME [epoch: 10.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2856922617956101		[learning rate: 0.0061603]
	Learning Rate: 0.00616027
	LOSS [training: 1.2856922617956101 | validation: 1.2449973219079988]
	TIME [epoch: 10.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2265046142484768		[learning rate: 0.0061414]
	Learning Rate: 0.00614139
	LOSS [training: 1.2265046142484768 | validation: 1.3249005522321777]
	TIME [epoch: 10.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4047951948641892		[learning rate: 0.0061226]
	Learning Rate: 0.00612256
	LOSS [training: 1.4047951948641892 | validation: 1.6987001437445315]
	TIME [epoch: 10.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752482967050381		[learning rate: 0.0061038]
	Learning Rate: 0.0061038
	LOSS [training: 1.752482967050381 | validation: 3.109321730766103]
	TIME [epoch: 10.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.471658374021799		[learning rate: 0.0060851]
	Learning Rate: 0.00608508
	LOSS [training: 2.471658374021799 | validation: 1.8649535404696027]
	TIME [epoch: 10.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.754537673994091		[learning rate: 0.0060664]
	Learning Rate: 0.00606643
	LOSS [training: 1.754537673994091 | validation: 1.546849993995804]
	TIME [epoch: 10.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249018159508376		[learning rate: 0.0060478]
	Learning Rate: 0.00604784
	LOSS [training: 2.249018159508376 | validation: 1.5854672788918658]
	TIME [epoch: 10.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5409583062625047		[learning rate: 0.0060293]
	Learning Rate: 0.0060293
	LOSS [training: 1.5409583062625047 | validation: 1.405493070937104]
	TIME [epoch: 10.4 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7559729557476285		[learning rate: 0.0060108]
	Learning Rate: 0.00601081
	LOSS [training: 1.7559729557476285 | validation: 2.274240459898475]
	TIME [epoch: 10.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.54069672223588		[learning rate: 0.0059924]
	Learning Rate: 0.00599239
	LOSS [training: 1.54069672223588 | validation: 1.7793891872321177]
	TIME [epoch: 10.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3755398766789442		[learning rate: 0.005974]
	Learning Rate: 0.00597402
	LOSS [training: 1.3755398766789442 | validation: 1.2625001257250406]
	TIME [epoch: 10.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3089921997544143		[learning rate: 0.0059557]
	Learning Rate: 0.00595571
	LOSS [training: 1.3089921997544143 | validation: 1.6636339250933987]
	TIME [epoch: 10.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4557478486094968		[learning rate: 0.0059375]
	Learning Rate: 0.00593745
	LOSS [training: 1.4557478486094968 | validation: 2.2733390457410207]
	TIME [epoch: 10.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9254766254423383		[learning rate: 0.0059192]
	Learning Rate: 0.00591925
	LOSS [training: 1.9254766254423383 | validation: 1.7473530559417927]
	TIME [epoch: 10.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4147494808254721		[learning rate: 0.0059011]
	Learning Rate: 0.0059011
	LOSS [training: 1.4147494808254721 | validation: 2.9735029524952963]
	TIME [epoch: 10.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.161812834618119		[learning rate: 0.005883]
	Learning Rate: 0.00588302
	LOSS [training: 2.161812834618119 | validation: 1.8672822067574693]
	TIME [epoch: 10.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4636268005250772		[learning rate: 0.005865]
	Learning Rate: 0.00586498
	LOSS [training: 1.4636268005250772 | validation: 1.4673992603790087]
	TIME [epoch: 10.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4131128451187418		[learning rate: 0.005847]
	Learning Rate: 0.005847
	LOSS [training: 1.4131128451187418 | validation: 2.064426041006226]
	TIME [epoch: 10.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5575281589894832		[learning rate: 0.0058291]
	Learning Rate: 0.00582908
	LOSS [training: 1.5575281589894832 | validation: 1.7811126998070024]
	TIME [epoch: 10.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6979084477479365		[learning rate: 0.0058112]
	Learning Rate: 0.00581121
	LOSS [training: 1.6979084477479365 | validation: 2.802708197167457]
	TIME [epoch: 10.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.796583882671743		[learning rate: 0.0057934]
	Learning Rate: 0.0057934
	LOSS [training: 1.796583882671743 | validation: 1.6324468408594657]
	TIME [epoch: 10.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9040960431825407		[learning rate: 0.0057756]
	Learning Rate: 0.00577564
	LOSS [training: 1.9040960431825407 | validation: 1.9947042939798167]
	TIME [epoch: 10.5 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.742889189061058		[learning rate: 0.0057579]
	Learning Rate: 0.00575793
	LOSS [training: 2.742889189061058 | validation: 2.7573835951939]
	TIME [epoch: 10.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8713991598077186		[learning rate: 0.0057403]
	Learning Rate: 0.00574028
	LOSS [training: 1.8713991598077186 | validation: 1.7052283559128338]
	TIME [epoch: 10.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7025336466484486		[learning rate: 0.0057227]
	Learning Rate: 0.00572269
	LOSS [training: 1.7025336466484486 | validation: 1.8210642675137214]
	TIME [epoch: 10.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7864734671185496		[learning rate: 0.0057051]
	Learning Rate: 0.00570514
	LOSS [training: 1.7864734671185496 | validation: 2.0635108107430327]
	TIME [epoch: 10.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4269818192976724		[learning rate: 0.0056877]
	Learning Rate: 0.00568766
	LOSS [training: 1.4269818192976724 | validation: 1.503965065480191]
	TIME [epoch: 10.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2844360922073996		[learning rate: 0.0056702]
	Learning Rate: 0.00567022
	LOSS [training: 1.2844360922073996 | validation: 1.384224061240522]
	TIME [epoch: 10.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2274800104234806		[learning rate: 0.0056528]
	Learning Rate: 0.00565284
	LOSS [training: 1.2274800104234806 | validation: 1.5154773076076715]
	TIME [epoch: 10.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6346643217333252		[learning rate: 0.0056355]
	Learning Rate: 0.00563551
	LOSS [training: 1.6346643217333252 | validation: 1.6437587144716181]
	TIME [epoch: 10.5 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4378122918267757		[learning rate: 0.0056182]
	Learning Rate: 0.00561824
	LOSS [training: 1.4378122918267757 | validation: 1.580829116494753]
	TIME [epoch: 10.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.553388020353763		[learning rate: 0.005601]
	Learning Rate: 0.00560101
	LOSS [training: 1.553388020353763 | validation: 2.541585278029037]
	TIME [epoch: 10.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.53770788586955		[learning rate: 0.0055838]
	Learning Rate: 0.00558384
	LOSS [training: 1.53770788586955 | validation: 1.5129843507264098]
	TIME [epoch: 10.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3483304585611442		[learning rate: 0.0055667]
	Learning Rate: 0.00556673
	LOSS [training: 1.3483304585611442 | validation: 2.599859976936529]
	TIME [epoch: 10.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8753646995890647		[learning rate: 0.0055497]
	Learning Rate: 0.00554966
	LOSS [training: 2.8753646995890647 | validation: 2.081824385220001]
	TIME [epoch: 10.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0558403362326994		[learning rate: 0.0055327]
	Learning Rate: 0.00553265
	LOSS [training: 2.0558403362326994 | validation: 2.168039061734453]
	TIME [epoch: 10.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5538666724971268		[learning rate: 0.0055157]
	Learning Rate: 0.00551569
	LOSS [training: 1.5538666724971268 | validation: 2.414731575892151]
	TIME [epoch: 10.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6994125288562731		[learning rate: 0.0054988]
	Learning Rate: 0.00549878
	LOSS [training: 1.6994125288562731 | validation: 1.312995950797706]
	TIME [epoch: 10.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6572661757109644		[learning rate: 0.0054819]
	Learning Rate: 0.00548193
	LOSS [training: 1.6572661757109644 | validation: 1.4588611003329317]
	TIME [epoch: 10.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5562752982559647		[learning rate: 0.0054651]
	Learning Rate: 0.00546512
	LOSS [training: 1.5562752982559647 | validation: 1.5511775749220522]
	TIME [epoch: 10.4 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74956091707874		[learning rate: 0.0054484]
	Learning Rate: 0.00544837
	LOSS [training: 1.74956091707874 | validation: 2.19361147796606]
	TIME [epoch: 10.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.479099344125896		[learning rate: 0.0054317]
	Learning Rate: 0.00543167
	LOSS [training: 1.479099344125896 | validation: 1.4973629397340764]
	TIME [epoch: 10.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.189607730195342		[learning rate: 0.005415]
	Learning Rate: 0.00541502
	LOSS [training: 1.189607730195342 | validation: 1.0842162837597373]
	TIME [epoch: 10.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5424148011270433		[learning rate: 0.0053984]
	Learning Rate: 0.00539842
	LOSS [training: 1.5424148011270433 | validation: 1.4963776823768498]
	TIME [epoch: 10.4 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1849043160741002		[learning rate: 0.0053819]
	Learning Rate: 0.00538187
	LOSS [training: 1.1849043160741002 | validation: 1.2867896187619396]
	TIME [epoch: 10.4 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1566175552625118		[learning rate: 0.0053654]
	Learning Rate: 0.00536537
	LOSS [training: 1.1566175552625118 | validation: 1.3439342085626664]
	TIME [epoch: 10.4 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.20679873964859		[learning rate: 0.0053489]
	Learning Rate: 0.00534893
	LOSS [training: 1.20679873964859 | validation: 1.1876887250569885]
	TIME [epoch: 10.4 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3849378905926348		[learning rate: 0.0053325]
	Learning Rate: 0.00533253
	LOSS [training: 1.3849378905926348 | validation: 1.3352456926118645]
	TIME [epoch: 10.4 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2852327250897058		[learning rate: 0.0053162]
	Learning Rate: 0.00531618
	LOSS [training: 1.2852327250897058 | validation: 1.4089881188284448]
	TIME [epoch: 10.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3226499356569021		[learning rate: 0.0052999]
	Learning Rate: 0.00529989
	LOSS [training: 1.3226499356569021 | validation: 1.4073808188553323]
	TIME [epoch: 10.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7575621456349981		[learning rate: 0.0052836]
	Learning Rate: 0.00528364
	LOSS [training: 1.7575621456349981 | validation: 2.3033420893881926]
	TIME [epoch: 10.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.437745549123075		[learning rate: 0.0052674]
	Learning Rate: 0.00526744
	LOSS [training: 1.437745549123075 | validation: 1.432144441193747]
	TIME [epoch: 10.4 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4241907588505511		[learning rate: 0.0052513]
	Learning Rate: 0.0052513
	LOSS [training: 1.4241907588505511 | validation: 1.211033045682817]
	TIME [epoch: 10.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6617035193991687		[learning rate: 0.0052352]
	Learning Rate: 0.0052352
	LOSS [training: 2.6617035193991687 | validation: 1.7315829300420211]
	TIME [epoch: 10.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4828682563361986		[learning rate: 0.0052192]
	Learning Rate: 0.00521915
	LOSS [training: 1.4828682563361986 | validation: 1.7808259984470352]
	TIME [epoch: 10.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.487927297484934		[learning rate: 0.0052032]
	Learning Rate: 0.00520315
	LOSS [training: 1.487927297484934 | validation: 1.4103831324712846]
	TIME [epoch: 10.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6852658636775142		[learning rate: 0.0051872]
	Learning Rate: 0.0051872
	LOSS [training: 1.6852658636775142 | validation: 3.7485418652990536]
	TIME [epoch: 10.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6444147824305047		[learning rate: 0.0051713]
	Learning Rate: 0.0051713
	LOSS [training: 2.6444147824305047 | validation: 2.549031705569666]
	TIME [epoch: 10.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9159414959361374		[learning rate: 0.0051555]
	Learning Rate: 0.00515545
	LOSS [training: 1.9159414959361374 | validation: 1.8292248889113305]
	TIME [epoch: 10.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9391467667489555		[learning rate: 0.0051396]
	Learning Rate: 0.00513965
	LOSS [training: 1.9391467667489555 | validation: 2.9745441409907096]
	TIME [epoch: 10.4 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8836169573977983		[learning rate: 0.0051239]
	Learning Rate: 0.00512389
	LOSS [training: 1.8836169573977983 | validation: 1.5617157529885208]
	TIME [epoch: 10.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2404976569732837		[learning rate: 0.0051082]
	Learning Rate: 0.00510819
	LOSS [training: 1.2404976569732837 | validation: 1.5607916875017807]
	TIME [epoch: 10.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.547248993543394		[learning rate: 0.0050925]
	Learning Rate: 0.00509253
	LOSS [training: 1.547248993543394 | validation: 1.8900130001936182]
	TIME [epoch: 10.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4179934599377995		[learning rate: 0.0050769]
	Learning Rate: 0.00507692
	LOSS [training: 2.4179934599377995 | validation: 4.001790278288143]
	TIME [epoch: 10.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.361866096122146		[learning rate: 0.0050614]
	Learning Rate: 0.00506135
	LOSS [training: 3.361866096122146 | validation: 2.140106307503621]
	TIME [epoch: 10.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6505772475285774		[learning rate: 0.0050458]
	Learning Rate: 0.00504584
	LOSS [training: 1.6505772475285774 | validation: 1.5342432131041892]
	TIME [epoch: 10.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6344234300176304		[learning rate: 0.0050304]
	Learning Rate: 0.00503037
	LOSS [training: 1.6344234300176304 | validation: 1.6439183229181902]
	TIME [epoch: 10.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3517047871316386		[learning rate: 0.005015]
	Learning Rate: 0.00501495
	LOSS [training: 1.3517047871316386 | validation: 1.2985265132792123]
	TIME [epoch: 10.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3258640397425059		[learning rate: 0.0049996]
	Learning Rate: 0.00499958
	LOSS [training: 1.3258640397425059 | validation: 2.1438488836011915]
	TIME [epoch: 10.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6619327852411412		[learning rate: 0.0049843]
	Learning Rate: 0.00498425
	LOSS [training: 1.6619327852411412 | validation: 1.4553231284035126]
	TIME [epoch: 10.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2852339816747258		[learning rate: 0.004969]
	Learning Rate: 0.00496897
	LOSS [training: 1.2852339816747258 | validation: 1.2626850569821702]
	TIME [epoch: 10.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.668880576509356		[learning rate: 0.0049537]
	Learning Rate: 0.00495374
	LOSS [training: 1.668880576509356 | validation: 1.4869239308465672]
	TIME [epoch: 10.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1952125127742017		[learning rate: 0.0049386]
	Learning Rate: 0.00493856
	LOSS [training: 1.1952125127742017 | validation: 1.3156051862828488]
	TIME [epoch: 10.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.237336130964597		[learning rate: 0.0049234]
	Learning Rate: 0.00492342
	LOSS [training: 1.237336130964597 | validation: 1.2729824958431224]
	TIME [epoch: 10.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3168067415530342		[learning rate: 0.0049083]
	Learning Rate: 0.00490832
	LOSS [training: 1.3168067415530342 | validation: 1.4162181105356075]
	TIME [epoch: 10.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5012627561372223		[learning rate: 0.0048933]
	Learning Rate: 0.00489328
	LOSS [training: 1.5012627561372223 | validation: 1.562468961125359]
	TIME [epoch: 10.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.062853638518023		[learning rate: 0.0048783]
	Learning Rate: 0.00487828
	LOSS [training: 1.062853638518023 | validation: 1.0461388878481057]
	TIME [epoch: 10.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.176292380264964		[learning rate: 0.0048633]
	Learning Rate: 0.00486333
	LOSS [training: 1.176292380264964 | validation: 1.2494865894562248]
	TIME [epoch: 10.4 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.22241122806736		[learning rate: 0.0048484]
	Learning Rate: 0.00484842
	LOSS [training: 1.22241122806736 | validation: 1.4684624201624934]
	TIME [epoch: 10.4 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.127075135898425		[learning rate: 0.0048336]
	Learning Rate: 0.00483355
	LOSS [training: 1.127075135898425 | validation: 1.4387073304797242]
	TIME [epoch: 10.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0484081231243263		[learning rate: 0.0048187]
	Learning Rate: 0.00481874
	LOSS [training: 1.0484081231243263 | validation: 1.4414275754604102]
	TIME [epoch: 10.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5618649710709929		[learning rate: 0.004804]
	Learning Rate: 0.00480397
	LOSS [training: 1.5618649710709929 | validation: 1.7769746840980383]
	TIME [epoch: 10.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3490425655100506		[learning rate: 0.0047892]
	Learning Rate: 0.00478924
	LOSS [training: 1.3490425655100506 | validation: 1.4548829432208419]
	TIME [epoch: 10.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1730999434934497		[learning rate: 0.0047746]
	Learning Rate: 0.00477456
	LOSS [training: 1.1730999434934497 | validation: 1.3655442235817237]
	TIME [epoch: 10.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0879045650455756		[learning rate: 0.0047599]
	Learning Rate: 0.00475992
	LOSS [training: 1.0879045650455756 | validation: 1.2750961301700505]
	TIME [epoch: 10.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3569997539097685		[learning rate: 0.0047453]
	Learning Rate: 0.00474533
	LOSS [training: 1.3569997539097685 | validation: 1.5013647172612077]
	TIME [epoch: 10.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.820823085394612		[learning rate: 0.0047308]
	Learning Rate: 0.00473079
	LOSS [training: 1.820823085394612 | validation: 1.736362608922855]
	TIME [epoch: 10.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5855273304539002		[learning rate: 0.0047163]
	Learning Rate: 0.00471628
	LOSS [training: 1.5855273304539002 | validation: 1.3953806285662316]
	TIME [epoch: 10.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.606329700596025		[learning rate: 0.0047018]
	Learning Rate: 0.00470183
	LOSS [training: 1.606329700596025 | validation: 1.3810411995696108]
	TIME [epoch: 10.4 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3348632830692635		[learning rate: 0.0046874]
	Learning Rate: 0.00468741
	LOSS [training: 1.3348632830692635 | validation: 1.7086311533283673]
	TIME [epoch: 10.4 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8473461704196505		[learning rate: 0.004673]
	Learning Rate: 0.00467305
	LOSS [training: 1.8473461704196505 | validation: 2.1608233979155322]
	TIME [epoch: 10.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7318267941075096		[learning rate: 0.0046587]
	Learning Rate: 0.00465872
	LOSS [training: 1.7318267941075096 | validation: 1.6231794464692424]
	TIME [epoch: 10.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.647651823883669		[learning rate: 0.0046444]
	Learning Rate: 0.00464444
	LOSS [training: 1.647651823883669 | validation: 1.9806568024661662]
	TIME [epoch: 10.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.529882005813789		[learning rate: 0.0046302]
	Learning Rate: 0.0046302
	LOSS [training: 1.529882005813789 | validation: 1.3580189228261537]
	TIME [epoch: 10.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1881644761812797		[learning rate: 0.004616]
	Learning Rate: 0.00461601
	LOSS [training: 1.1881644761812797 | validation: 1.3840493483234326]
	TIME [epoch: 10.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2983904812195077		[learning rate: 0.0046019]
	Learning Rate: 0.00460186
	LOSS [training: 1.2983904812195077 | validation: 1.5606013278115043]
	TIME [epoch: 10.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.258765592440359		[learning rate: 0.0045878]
	Learning Rate: 0.00458775
	LOSS [training: 1.258765592440359 | validation: 2.367262985733063]
	TIME [epoch: 10.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.907239115817185		[learning rate: 0.0045737]
	Learning Rate: 0.00457369
	LOSS [training: 1.907239115817185 | validation: 1.703153262374769]
	TIME [epoch: 10.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.62604885799268		[learning rate: 0.0045597]
	Learning Rate: 0.00455967
	LOSS [training: 1.62604885799268 | validation: 1.7055049719790902]
	TIME [epoch: 10.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6011073028302198		[learning rate: 0.0045457]
	Learning Rate: 0.00454569
	LOSS [training: 1.6011073028302198 | validation: 0.9940377826337203]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_757.pth
	Model improved!!!
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.111459839214303		[learning rate: 0.0045318]
	Learning Rate: 0.00453176
	LOSS [training: 1.111459839214303 | validation: 1.697213097632154]
	TIME [epoch: 10.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3663186201947375		[learning rate: 0.0045179]
	Learning Rate: 0.00451787
	LOSS [training: 1.3663186201947375 | validation: 1.0025281063320741]
	TIME [epoch: 10.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9734529819681649		[learning rate: 0.004504]
	Learning Rate: 0.00450402
	LOSS [training: 0.9734529819681649 | validation: 1.1410540862231313]
	TIME [epoch: 10.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1659134551228698		[learning rate: 0.0044902]
	Learning Rate: 0.00449021
	LOSS [training: 1.1659134551228698 | validation: 1.405369160917865]
	TIME [epoch: 10.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2505165692580056		[learning rate: 0.0044764]
	Learning Rate: 0.00447645
	LOSS [training: 1.2505165692580056 | validation: 1.7460399863998188]
	TIME [epoch: 10.4 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4325518645186954		[learning rate: 0.0044627]
	Learning Rate: 0.00446272
	LOSS [training: 1.4325518645186954 | validation: 1.2580311068754464]
	TIME [epoch: 10.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1013684489033333		[learning rate: 0.004449]
	Learning Rate: 0.00444904
	LOSS [training: 1.1013684489033333 | validation: 1.1924843212400769]
	TIME [epoch: 10.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0355483006157615		[learning rate: 0.0044354]
	Learning Rate: 0.0044354
	LOSS [training: 1.0355483006157615 | validation: 1.6616684034606743]
	TIME [epoch: 10.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2771721282103772		[learning rate: 0.0044218]
	Learning Rate: 0.00442181
	LOSS [training: 1.2771721282103772 | validation: 1.7674264584859678]
	TIME [epoch: 10.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2697760103209412		[learning rate: 0.0044083]
	Learning Rate: 0.00440825
	LOSS [training: 1.2697760103209412 | validation: 1.2452530871571934]
	TIME [epoch: 10.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.187534165408144		[learning rate: 0.0043947]
	Learning Rate: 0.00439474
	LOSS [training: 1.187534165408144 | validation: 1.098293849150366]
	TIME [epoch: 10.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0858691871817858		[learning rate: 0.0043813]
	Learning Rate: 0.00438127
	LOSS [training: 1.0858691871817858 | validation: 1.101748930728369]
	TIME [epoch: 10.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2475682289793104		[learning rate: 0.0043678]
	Learning Rate: 0.00436784
	LOSS [training: 1.2475682289793104 | validation: 1.3419792822499912]
	TIME [epoch: 10.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1826168706247415		[learning rate: 0.0043544]
	Learning Rate: 0.00435445
	LOSS [training: 1.1826168706247415 | validation: 0.9947262426878127]
	TIME [epoch: 10.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9524351567062215		[learning rate: 0.0043411]
	Learning Rate: 0.0043411
	LOSS [training: 0.9524351567062215 | validation: 1.1191657733235862]
	TIME [epoch: 10.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.977203837781871		[learning rate: 0.0043278]
	Learning Rate: 0.0043278
	LOSS [training: 0.977203837781871 | validation: 1.1493684065587144]
	TIME [epoch: 10.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3537180827532573		[learning rate: 0.0043145]
	Learning Rate: 0.00431453
	LOSS [training: 1.3537180827532573 | validation: 1.0306505648443187]
	TIME [epoch: 10.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1595674048854552		[learning rate: 0.0043013]
	Learning Rate: 0.0043013
	LOSS [training: 1.1595674048854552 | validation: 1.0022043067285193]
	TIME [epoch: 10.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9590362345989517		[learning rate: 0.0042881]
	Learning Rate: 0.00428812
	LOSS [training: 0.9590362345989517 | validation: 1.5685596973163456]
	TIME [epoch: 10.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.083395133849508		[learning rate: 0.004275]
	Learning Rate: 0.00427497
	LOSS [training: 1.083395133849508 | validation: 1.368118902321809]
	TIME [epoch: 10.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0963679941047946		[learning rate: 0.0042619]
	Learning Rate: 0.00426187
	LOSS [training: 1.0963679941047946 | validation: 1.0060381065648207]
	TIME [epoch: 10.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.111540540174169		[learning rate: 0.0042488]
	Learning Rate: 0.0042488
	LOSS [training: 1.111540540174169 | validation: 1.32165745990794]
	TIME [epoch: 10.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0047599018476119		[learning rate: 0.0042358]
	Learning Rate: 0.00423578
	LOSS [training: 1.0047599018476119 | validation: 1.0823886088186088]
	TIME [epoch: 10.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9306467028642436		[learning rate: 0.0042228]
	Learning Rate: 0.00422279
	LOSS [training: 0.9306467028642436 | validation: 0.975620312093189]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.147613241188364		[learning rate: 0.0042098]
	Learning Rate: 0.00420985
	LOSS [training: 1.147613241188364 | validation: 2.0792285734942912]
	TIME [epoch: 10.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4728777719088675		[learning rate: 0.0041969]
	Learning Rate: 0.00419695
	LOSS [training: 1.4728777719088675 | validation: 1.4655100909898602]
	TIME [epoch: 10.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1316891612766893		[learning rate: 0.0041841]
	Learning Rate: 0.00418408
	LOSS [training: 1.1316891612766893 | validation: 1.3134622276555876]
	TIME [epoch: 10.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0408009913065266		[learning rate: 0.0041713]
	Learning Rate: 0.00417125
	LOSS [training: 1.0408009913065266 | validation: 1.0045729619934987]
	TIME [epoch: 10.5 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9790852130559928		[learning rate: 0.0041585]
	Learning Rate: 0.00415847
	LOSS [training: 0.9790852130559928 | validation: 1.0459970197313408]
	TIME [epoch: 10.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.082402557446682		[learning rate: 0.0041457]
	Learning Rate: 0.00414572
	LOSS [training: 1.082402557446682 | validation: 1.3031013587627325]
	TIME [epoch: 10.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0421307580420458		[learning rate: 0.004133]
	Learning Rate: 0.00413301
	LOSS [training: 1.0421307580420458 | validation: 1.0772138866973149]
	TIME [epoch: 10.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9168861926832704		[learning rate: 0.0041203]
	Learning Rate: 0.00412034
	LOSS [training: 0.9168861926832704 | validation: 1.0112615300396357]
	TIME [epoch: 10.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0203373194681975		[learning rate: 0.0041077]
	Learning Rate: 0.00410771
	LOSS [training: 1.0203373194681975 | validation: 1.0802562138274814]
	TIME [epoch: 10.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9290216384677853		[learning rate: 0.0040951]
	Learning Rate: 0.00409512
	LOSS [training: 0.9290216384677853 | validation: 0.9946694986446977]
	TIME [epoch: 10.4 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8500281081341875		[learning rate: 0.0040826]
	Learning Rate: 0.00408257
	LOSS [training: 0.8500281081341875 | validation: 1.3169277642637849]
	TIME [epoch: 10.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0549551576606429		[learning rate: 0.0040701]
	Learning Rate: 0.00407005
	LOSS [training: 1.0549551576606429 | validation: 0.9009724750754381]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_793.pth
	Model improved!!!
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9575666172996644		[learning rate: 0.0040576]
	Learning Rate: 0.00405758
	LOSS [training: 0.9575666172996644 | validation: 0.9537114041226837]
	TIME [epoch: 10.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7335316561982617		[learning rate: 0.0040451]
	Learning Rate: 0.00404514
	LOSS [training: 1.7335316561982617 | validation: 2.20878292659426]
	TIME [epoch: 10.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5396405498103962		[learning rate: 0.0040327]
	Learning Rate: 0.00403274
	LOSS [training: 1.5396405498103962 | validation: 1.7808124967443302]
	TIME [epoch: 10.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1078684747547745		[learning rate: 0.0040204]
	Learning Rate: 0.00402038
	LOSS [training: 1.1078684747547745 | validation: 1.0071690277670644]
	TIME [epoch: 10.4 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8390810747567059		[learning rate: 0.0040081]
	Learning Rate: 0.00400805
	LOSS [training: 0.8390810747567059 | validation: 0.9794894823861904]
	TIME [epoch: 10.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9006957476538112		[learning rate: 0.0039958]
	Learning Rate: 0.00399577
	LOSS [training: 0.9006957476538112 | validation: 1.082069062047694]
	TIME [epoch: 10.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2104882844677636		[learning rate: 0.0039835]
	Learning Rate: 0.00398352
	LOSS [training: 1.2104882844677636 | validation: 1.3125261929496508]
	TIME [epoch: 10.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3192065059651932		[learning rate: 0.0039713]
	Learning Rate: 0.00397131
	LOSS [training: 1.3192065059651932 | validation: 1.1548059597766656]
	TIME [epoch: 10.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9584757927326446		[learning rate: 0.0039591]
	Learning Rate: 0.00395913
	LOSS [training: 0.9584757927326446 | validation: 1.321073875533183]
	TIME [epoch: 10.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9164950016897313		[learning rate: 0.003947]
	Learning Rate: 0.003947
	LOSS [training: 0.9164950016897313 | validation: 1.676681408305474]
	TIME [epoch: 10.5 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1933137353390209		[learning rate: 0.0039349]
	Learning Rate: 0.0039349
	LOSS [training: 1.1933137353390209 | validation: 1.0719170102818993]
	TIME [epoch: 10.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9300605798388629		[learning rate: 0.0039228]
	Learning Rate: 0.00392283
	LOSS [training: 0.9300605798388629 | validation: 1.4302712385996006]
	TIME [epoch: 10.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0265587747182625		[learning rate: 0.0039108]
	Learning Rate: 0.00391081
	LOSS [training: 1.0265587747182625 | validation: 1.1126332841268012]
	TIME [epoch: 10.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1482193880370546		[learning rate: 0.0038988]
	Learning Rate: 0.00389882
	LOSS [training: 1.1482193880370546 | validation: 1.3508268262127865]
	TIME [epoch: 10.5 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4340265242558874		[learning rate: 0.0038869]
	Learning Rate: 0.00388687
	LOSS [training: 1.4340265242558874 | validation: 1.75911940516629]
	TIME [epoch: 10.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3094430112655941		[learning rate: 0.003875]
	Learning Rate: 0.00387495
	LOSS [training: 1.3094430112655941 | validation: 1.0379326958081645]
	TIME [epoch: 10.5 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0728350956667487		[learning rate: 0.0038631]
	Learning Rate: 0.00386308
	LOSS [training: 1.0728350956667487 | validation: 1.0255171489420014]
	TIME [epoch: 10.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.02981909665302		[learning rate: 0.0038512]
	Learning Rate: 0.00385123
	LOSS [training: 1.02981909665302 | validation: 1.0929774344888403]
	TIME [epoch: 10.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8955893681083262		[learning rate: 0.0038394]
	Learning Rate: 0.00383943
	LOSS [training: 0.8955893681083262 | validation: 0.9845368903137427]
	TIME [epoch: 10.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8623495385450134		[learning rate: 0.0038277]
	Learning Rate: 0.00382766
	LOSS [training: 0.8623495385450134 | validation: 0.9335880490058655]
	TIME [epoch: 10.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9350558102066536		[learning rate: 0.0038159]
	Learning Rate: 0.00381593
	LOSS [training: 0.9350558102066536 | validation: 1.0216620512056411]
	TIME [epoch: 10.5 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8520686868389088		[learning rate: 0.0038042]
	Learning Rate: 0.00380423
	LOSS [training: 0.8520686868389088 | validation: 1.176984884088201]
	TIME [epoch: 10.5 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2807259966253148		[learning rate: 0.0037926]
	Learning Rate: 0.00379257
	LOSS [training: 1.2807259966253148 | validation: 0.975176907049598]
	TIME [epoch: 10.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0331079804888552		[learning rate: 0.0037809]
	Learning Rate: 0.00378094
	LOSS [training: 1.0331079804888552 | validation: 0.9998007091214776]
	TIME [epoch: 10.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1133562379888877		[learning rate: 0.0037694]
	Learning Rate: 0.00376935
	LOSS [training: 1.1133562379888877 | validation: 1.2870362265384407]
	TIME [epoch: 10.5 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1327284599518475		[learning rate: 0.0037578]
	Learning Rate: 0.0037578
	LOSS [training: 1.1327284599518475 | validation: 1.0283785902100775]
	TIME [epoch: 10.5 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.057189155060706		[learning rate: 0.0037463]
	Learning Rate: 0.00374628
	LOSS [training: 1.057189155060706 | validation: 1.2440463088988472]
	TIME [epoch: 10.5 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3596945079814373		[learning rate: 0.0037348]
	Learning Rate: 0.00373479
	LOSS [training: 1.3596945079814373 | validation: 1.437517827689655]
	TIME [epoch: 10.5 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.108828081761057		[learning rate: 0.0037233]
	Learning Rate: 0.00372335
	LOSS [training: 1.108828081761057 | validation: 1.1415610690015183]
	TIME [epoch: 10.5 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9199458969443729		[learning rate: 0.0037119]
	Learning Rate: 0.00371193
	LOSS [training: 0.9199458969443729 | validation: 1.5001144214702735]
	TIME [epoch: 10.5 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1052978581596307		[learning rate: 0.0037006]
	Learning Rate: 0.00370055
	LOSS [training: 1.1052978581596307 | validation: 1.0433882248321191]
	TIME [epoch: 10.5 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0051472081090795		[learning rate: 0.0036892]
	Learning Rate: 0.00368921
	LOSS [training: 1.0051472081090795 | validation: 0.9624678597875326]
	TIME [epoch: 10.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8405442301237624		[learning rate: 0.0036779]
	Learning Rate: 0.0036779
	LOSS [training: 0.8405442301237624 | validation: 1.019193917534027]
	TIME [epoch: 10.5 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0685401104689667		[learning rate: 0.0036666]
	Learning Rate: 0.00366663
	LOSS [training: 1.0685401104689667 | validation: 1.5019187754733043]
	TIME [epoch: 10.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6552903100143534		[learning rate: 0.0036554]
	Learning Rate: 0.00365539
	LOSS [training: 1.6552903100143534 | validation: 1.5981642113473271]
	TIME [epoch: 10.5 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2998301244156845		[learning rate: 0.0036442]
	Learning Rate: 0.00364418
	LOSS [training: 1.2998301244156845 | validation: 0.9579948522127901]
	TIME [epoch: 10.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.970048576673744		[learning rate: 0.003633]
	Learning Rate: 0.00363301
	LOSS [training: 0.970048576673744 | validation: 1.4852392389421232]
	TIME [epoch: 10.5 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.974164323301359		[learning rate: 0.0036219]
	Learning Rate: 0.00362187
	LOSS [training: 0.974164323301359 | validation: 1.0903263740572111]
	TIME [epoch: 10.4 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.939795589181499		[learning rate: 0.0036108]
	Learning Rate: 0.00361077
	LOSS [training: 0.939795589181499 | validation: 0.976309961697431]
	TIME [epoch: 10.5 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9536678680533758		[learning rate: 0.0035997]
	Learning Rate: 0.0035997
	LOSS [training: 0.9536678680533758 | validation: 1.3003107173103763]
	TIME [epoch: 10.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9983184017871292		[learning rate: 0.0035887]
	Learning Rate: 0.00358867
	LOSS [training: 0.9983184017871292 | validation: 0.86894838500803]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9542171629553515		[learning rate: 0.0035777]
	Learning Rate: 0.00357767
	LOSS [training: 0.9542171629553515 | validation: 1.2113534049008192]
	TIME [epoch: 10.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4801640691277467		[learning rate: 0.0035667]
	Learning Rate: 0.0035667
	LOSS [training: 1.4801640691277467 | validation: 1.0496614834217552]
	TIME [epoch: 10.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0563999733931293		[learning rate: 0.0035558]
	Learning Rate: 0.00355577
	LOSS [training: 1.0563999733931293 | validation: 1.152701695503539]
	TIME [epoch: 10.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0269618618350074		[learning rate: 0.0035449]
	Learning Rate: 0.00354487
	LOSS [training: 1.0269618618350074 | validation: 1.0855041863021777]
	TIME [epoch: 10.5 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9124326224985844		[learning rate: 0.003534]
	Learning Rate: 0.003534
	LOSS [training: 0.9124326224985844 | validation: 1.662634146233345]
	TIME [epoch: 10.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2358451899281693		[learning rate: 0.0035232]
	Learning Rate: 0.00352317
	LOSS [training: 1.2358451899281693 | validation: 1.4593335801001523]
	TIME [epoch: 10.5 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2036486507689403		[learning rate: 0.0035124]
	Learning Rate: 0.00351237
	LOSS [training: 1.2036486507689403 | validation: 1.053619096854195]
	TIME [epoch: 10.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0292595146475867		[learning rate: 0.0035016]
	Learning Rate: 0.0035016
	LOSS [training: 1.0292595146475867 | validation: 1.1106415749908785]
	TIME [epoch: 10.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.050912360916776		[learning rate: 0.0034909]
	Learning Rate: 0.00349087
	LOSS [training: 1.050912360916776 | validation: 1.0685406047560404]
	TIME [epoch: 10.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8874310483940991		[learning rate: 0.0034802]
	Learning Rate: 0.00348017
	LOSS [training: 0.8874310483940991 | validation: 0.8424441665463346]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_844.pth
	Model improved!!!
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0960211462723437		[learning rate: 0.0034695]
	Learning Rate: 0.0034695
	LOSS [training: 1.0960211462723437 | validation: 1.072995501901171]
	TIME [epoch: 10.5 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0129678398630906		[learning rate: 0.0034589]
	Learning Rate: 0.00345886
	LOSS [training: 1.0129678398630906 | validation: 0.8189859044247731]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_846.pth
	Model improved!!!
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8010405024385545		[learning rate: 0.0034483]
	Learning Rate: 0.00344826
	LOSS [training: 0.8010405024385545 | validation: 0.8089608639821705]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9000160122561672		[learning rate: 0.0034377]
	Learning Rate: 0.00343769
	LOSS [training: 0.9000160122561672 | validation: 0.7919892324118769]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9102341299879804		[learning rate: 0.0034272]
	Learning Rate: 0.00342715
	LOSS [training: 0.9102341299879804 | validation: 0.966353130324975]
	TIME [epoch: 10.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9804152522046874		[learning rate: 0.0034166]
	Learning Rate: 0.00341665
	LOSS [training: 0.9804152522046874 | validation: 0.848893559580486]
	TIME [epoch: 10.4 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9039593268743016		[learning rate: 0.0034062]
	Learning Rate: 0.00340617
	LOSS [training: 0.9039593268743016 | validation: 0.9469138050792251]
	TIME [epoch: 10.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0326404504247126		[learning rate: 0.0033957]
	Learning Rate: 0.00339573
	LOSS [training: 1.0326404504247126 | validation: 1.2933765003829791]
	TIME [epoch: 10.5 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.113572717871037		[learning rate: 0.0033853]
	Learning Rate: 0.00338532
	LOSS [training: 1.113572717871037 | validation: 0.7952290087615399]
	TIME [epoch: 10.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8670766597704989		[learning rate: 0.0033749]
	Learning Rate: 0.00337494
	LOSS [training: 0.8670766597704989 | validation: 1.0188470276539585]
	TIME [epoch: 10.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9527957218846718		[learning rate: 0.0033646]
	Learning Rate: 0.0033646
	LOSS [training: 0.9527957218846718 | validation: 1.6358223842109754]
	TIME [epoch: 10.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0982417578671906		[learning rate: 0.0033543]
	Learning Rate: 0.00335428
	LOSS [training: 1.0982417578671906 | validation: 0.7794684373791761]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_856.pth
	Model improved!!!
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8657406773158195		[learning rate: 0.003344]
	Learning Rate: 0.003344
	LOSS [training: 0.8657406773158195 | validation: 1.19103997648808]
	TIME [epoch: 10.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.00242366082728		[learning rate: 0.0033338]
	Learning Rate: 0.00333375
	LOSS [training: 1.00242366082728 | validation: 0.9800252427955877]
	TIME [epoch: 10.5 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9526854602261989		[learning rate: 0.0033235]
	Learning Rate: 0.00332353
	LOSS [training: 0.9526854602261989 | validation: 0.7480647435980299]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9003380450372536		[learning rate: 0.0033133]
	Learning Rate: 0.00331334
	LOSS [training: 0.9003380450372536 | validation: 0.8155016219941854]
	TIME [epoch: 10.4 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7852996269732622		[learning rate: 0.0033032]
	Learning Rate: 0.00330319
	LOSS [training: 0.7852996269732622 | validation: 1.3470316792979171]
	TIME [epoch: 10.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0921159982693367		[learning rate: 0.0032931]
	Learning Rate: 0.00329306
	LOSS [training: 1.0921159982693367 | validation: 1.5143116696255154]
	TIME [epoch: 10.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0763174866857368		[learning rate: 0.003283]
	Learning Rate: 0.00328297
	LOSS [training: 1.0763174866857368 | validation: 0.9719633958243656]
	TIME [epoch: 10.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9122841057487227		[learning rate: 0.0032729]
	Learning Rate: 0.0032729
	LOSS [training: 0.9122841057487227 | validation: 1.3989578236823508]
	TIME [epoch: 10.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2417877911888424		[learning rate: 0.0032629]
	Learning Rate: 0.00326287
	LOSS [training: 1.2417877911888424 | validation: 1.3985075335529462]
	TIME [epoch: 10.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4189403739725592		[learning rate: 0.0032529]
	Learning Rate: 0.00325287
	LOSS [training: 1.4189403739725592 | validation: 1.217980455558676]
	TIME [epoch: 10.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007241703324277		[learning rate: 0.0032429]
	Learning Rate: 0.0032429
	LOSS [training: 1.3007241703324277 | validation: 0.8863724408182906]
	TIME [epoch: 10.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9966241016596346		[learning rate: 0.003233]
	Learning Rate: 0.00323296
	LOSS [training: 0.9966241016596346 | validation: 0.9012843801265789]
	TIME [epoch: 10.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.919460128756978		[learning rate: 0.003223]
	Learning Rate: 0.00322305
	LOSS [training: 0.919460128756978 | validation: 0.8340782453665762]
	TIME [epoch: 10.4 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9597003046479978		[learning rate: 0.0032132]
	Learning Rate: 0.00321317
	LOSS [training: 0.9597003046479978 | validation: 0.7297228408124459]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_870.pth
	Model improved!!!
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8514379953192254		[learning rate: 0.0032033]
	Learning Rate: 0.00320332
	LOSS [training: 0.8514379953192254 | validation: 0.9576815250644317]
	TIME [epoch: 10.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9326172425307323		[learning rate: 0.0031935]
	Learning Rate: 0.0031935
	LOSS [training: 0.9326172425307323 | validation: 1.4959005980713522]
	TIME [epoch: 10.4 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.054969755688364		[learning rate: 0.0031837]
	Learning Rate: 0.00318371
	LOSS [training: 1.054969755688364 | validation: 1.354376804482322]
	TIME [epoch: 10.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9474713614430961		[learning rate: 0.0031739]
	Learning Rate: 0.00317395
	LOSS [training: 0.9474713614430961 | validation: 1.0549174170411082]
	TIME [epoch: 10.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.881906452567242		[learning rate: 0.0031642]
	Learning Rate: 0.00316422
	LOSS [training: 0.881906452567242 | validation: 0.9569420186894886]
	TIME [epoch: 10.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9185222087042986		[learning rate: 0.0031545]
	Learning Rate: 0.00315452
	LOSS [training: 0.9185222087042986 | validation: 1.0547918112727173]
	TIME [epoch: 10.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.975464544691856		[learning rate: 0.0031449]
	Learning Rate: 0.00314485
	LOSS [training: 0.975464544691856 | validation: 0.9981417315148224]
	TIME [epoch: 10.4 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.025282458903245		[learning rate: 0.0031352]
	Learning Rate: 0.00313521
	LOSS [training: 1.025282458903245 | validation: 1.0757453496393692]
	TIME [epoch: 10.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9777777132925467		[learning rate: 0.0031256]
	Learning Rate: 0.0031256
	LOSS [training: 0.9777777132925467 | validation: 1.067839757702044]
	TIME [epoch: 10.5 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8777505820567635		[learning rate: 0.003116]
	Learning Rate: 0.00311602
	LOSS [training: 0.8777505820567635 | validation: 0.9264248580087173]
	TIME [epoch: 10.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8597361971499063		[learning rate: 0.0031065]
	Learning Rate: 0.00310647
	LOSS [training: 0.8597361971499063 | validation: 1.018518014656424]
	TIME [epoch: 10.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9303977129271221		[learning rate: 0.0030969]
	Learning Rate: 0.00309694
	LOSS [training: 0.9303977129271221 | validation: 1.292554760752301]
	TIME [epoch: 10.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.974003332786055		[learning rate: 0.0030875]
	Learning Rate: 0.00308745
	LOSS [training: 0.974003332786055 | validation: 0.9730132097745293]
	TIME [epoch: 10.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7913820646499137		[learning rate: 0.003078]
	Learning Rate: 0.00307799
	LOSS [training: 0.7913820646499137 | validation: 1.0259882998347598]
	TIME [epoch: 10.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.877997675766572		[learning rate: 0.0030686]
	Learning Rate: 0.00306855
	LOSS [training: 0.877997675766572 | validation: 1.0669676477996946]
	TIME [epoch: 10.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7969957159070193		[learning rate: 0.0030591]
	Learning Rate: 0.00305914
	LOSS [training: 0.7969957159070193 | validation: 0.9672177732033072]
	TIME [epoch: 10.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8192668564488883		[learning rate: 0.0030498]
	Learning Rate: 0.00304977
	LOSS [training: 0.8192668564488883 | validation: 0.8719404822317367]
	TIME [epoch: 10.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8382204218088708		[learning rate: 0.0030404]
	Learning Rate: 0.00304042
	LOSS [training: 0.8382204218088708 | validation: 0.7969183098219589]
	TIME [epoch: 10.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8931467231938768		[learning rate: 0.0030311]
	Learning Rate: 0.0030311
	LOSS [training: 0.8931467231938768 | validation: 1.1670740315917334]
	TIME [epoch: 10.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0323932315244893		[learning rate: 0.0030218]
	Learning Rate: 0.00302181
	LOSS [training: 1.0323932315244893 | validation: 1.8174792659542485]
	TIME [epoch: 10.4 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2083629646487277		[learning rate: 0.0030125]
	Learning Rate: 0.00301254
	LOSS [training: 1.2083629646487277 | validation: 0.9229104185881081]
	TIME [epoch: 10.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7929190448514438		[learning rate: 0.0030033]
	Learning Rate: 0.00300331
	LOSS [training: 0.7929190448514438 | validation: 1.0524598518806905]
	TIME [epoch: 10.5 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0109702213325806		[learning rate: 0.0029941]
	Learning Rate: 0.0029941
	LOSS [training: 1.0109702213325806 | validation: 1.4206192013417884]
	TIME [epoch: 10.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.206886713446019		[learning rate: 0.0029849]
	Learning Rate: 0.00298492
	LOSS [training: 1.206886713446019 | validation: 1.7091089899184924]
	TIME [epoch: 10.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5154756534990583		[learning rate: 0.0029758]
	Learning Rate: 0.00297577
	LOSS [training: 1.5154756534990583 | validation: 1.6474554787915219]
	TIME [epoch: 10.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0773996420179994		[learning rate: 0.0029667]
	Learning Rate: 0.00296665
	LOSS [training: 1.0773996420179994 | validation: 0.9860971216269593]
	TIME [epoch: 10.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9538581168470168		[learning rate: 0.0029576]
	Learning Rate: 0.00295756
	LOSS [training: 0.9538581168470168 | validation: 0.9082354910092911]
	TIME [epoch: 10.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9790009223247556		[learning rate: 0.0029485]
	Learning Rate: 0.00294849
	LOSS [training: 0.9790009223247556 | validation: 1.0260323294933074]
	TIME [epoch: 10.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9839169173024696		[learning rate: 0.0029395]
	Learning Rate: 0.00293945
	LOSS [training: 0.9839169173024696 | validation: 0.969936815665679]
	TIME [epoch: 10.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8839495438520342		[learning rate: 0.0029304]
	Learning Rate: 0.00293044
	LOSS [training: 0.8839495438520342 | validation: 0.9581708465160566]
	TIME [epoch: 10.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9926650455100304		[learning rate: 0.0029215]
	Learning Rate: 0.00292146
	LOSS [training: 0.9926650455100304 | validation: 1.4335793527523089]
	TIME [epoch: 10.5 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5128255105283828		[learning rate: 0.0029125]
	Learning Rate: 0.0029125
	LOSS [training: 1.5128255105283828 | validation: 1.3054738888365878]
	TIME [epoch: 10.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.246438298300971		[learning rate: 0.0029036]
	Learning Rate: 0.00290358
	LOSS [training: 1.246438298300971 | validation: 1.0239212222315661]
	TIME [epoch: 10.5 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.136676930992234		[learning rate: 0.0028947]
	Learning Rate: 0.00289468
	LOSS [training: 1.136676930992234 | validation: 1.3909288106170257]
	TIME [epoch: 10.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2880321170674818		[learning rate: 0.0028858]
	Learning Rate: 0.0028858
	LOSS [training: 1.2880321170674818 | validation: 1.1908653912729346]
	TIME [epoch: 10.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.342565757619016		[learning rate: 0.002877]
	Learning Rate: 0.00287696
	LOSS [training: 1.342565757619016 | validation: 1.2827078446434548]
	TIME [epoch: 10.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0459819937825012		[learning rate: 0.0028681]
	Learning Rate: 0.00286814
	LOSS [training: 1.0459819937825012 | validation: 1.9235454993418757]
	TIME [epoch: 10.5 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5428529997086655		[learning rate: 0.0028593]
	Learning Rate: 0.00285935
	LOSS [training: 1.5428529997086655 | validation: 1.7835521393221734]
	TIME [epoch: 10.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2603528562964315		[learning rate: 0.0028506]
	Learning Rate: 0.00285058
	LOSS [training: 1.2603528562964315 | validation: 1.5687441630045118]
	TIME [epoch: 10.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2426369612224284		[learning rate: 0.0028418]
	Learning Rate: 0.00284184
	LOSS [training: 1.2426369612224284 | validation: 1.0810976231146838]
	TIME [epoch: 10.5 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9247570007917943		[learning rate: 0.0028331]
	Learning Rate: 0.00283313
	LOSS [training: 0.9247570007917943 | validation: 1.0624796477361618]
	TIME [epoch: 10.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9063519940007272		[learning rate: 0.0028244]
	Learning Rate: 0.00282445
	LOSS [training: 0.9063519940007272 | validation: 1.0574656496426023]
	TIME [epoch: 10.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0852814905087407		[learning rate: 0.0028158]
	Learning Rate: 0.00281579
	LOSS [training: 1.0852814905087407 | validation: 1.1004692239979745]
	TIME [epoch: 10.5 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.789401944952419		[learning rate: 0.0028072]
	Learning Rate: 0.00280716
	LOSS [training: 0.789401944952419 | validation: 0.8729501663847518]
	TIME [epoch: 10.5 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8547301480695049		[learning rate: 0.0027986]
	Learning Rate: 0.00279855
	LOSS [training: 0.8547301480695049 | validation: 1.0345627182093384]
	TIME [epoch: 10.5 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8782796957956206		[learning rate: 0.00279]
	Learning Rate: 0.00278997
	LOSS [training: 0.8782796957956206 | validation: 1.835066060299703]
	TIME [epoch: 10.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4394462860990564		[learning rate: 0.0027814]
	Learning Rate: 0.00278142
	LOSS [training: 1.4394462860990564 | validation: 0.9349570027155918]
	TIME [epoch: 10.5 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2346059651610175		[learning rate: 0.0027729]
	Learning Rate: 0.00277289
	LOSS [training: 1.2346059651610175 | validation: 1.0216778713500867]
	TIME [epoch: 10.5 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4351228332281203		[learning rate: 0.0027644]
	Learning Rate: 0.00276439
	LOSS [training: 1.4351228332281203 | validation: 1.2291461549899125]
	TIME [epoch: 10.5 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1257319447993277		[learning rate: 0.0027559]
	Learning Rate: 0.00275592
	LOSS [training: 1.1257319447993277 | validation: 0.9117154377531351]
	TIME [epoch: 10.5 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9198288995440373		[learning rate: 0.0027475]
	Learning Rate: 0.00274747
	LOSS [training: 0.9198288995440373 | validation: 1.0970357615860808]
	TIME [epoch: 10.5 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4667927390694946		[learning rate: 0.002739]
	Learning Rate: 0.00273905
	LOSS [training: 1.4667927390694946 | validation: 1.7683839251511535]
	TIME [epoch: 10.4 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0901567171958466		[learning rate: 0.0027307]
	Learning Rate: 0.00273065
	LOSS [training: 1.0901567171958466 | validation: 1.3991102592017006]
	TIME [epoch: 10.5 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5693206433460165		[learning rate: 0.0027223]
	Learning Rate: 0.00272228
	LOSS [training: 1.5693206433460165 | validation: 1.0441908452943958]
	TIME [epoch: 10.5 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8700937102231014		[learning rate: 0.0027139]
	Learning Rate: 0.00271394
	LOSS [training: 0.8700937102231014 | validation: 1.0011668662261681]
	TIME [epoch: 10.5 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0324568411761352		[learning rate: 0.0027056]
	Learning Rate: 0.00270562
	LOSS [training: 1.0324568411761352 | validation: 0.9436609316328821]
	TIME [epoch: 10.5 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9164796563460691		[learning rate: 0.0026973]
	Learning Rate: 0.00269733
	LOSS [training: 0.9164796563460691 | validation: 0.9451724911260633]
	TIME [epoch: 10.5 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9221762429651352		[learning rate: 0.0026891]
	Learning Rate: 0.00268906
	LOSS [training: 0.9221762429651352 | validation: 2.1592550745398706]
	TIME [epoch: 10.5 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1718492017966424		[learning rate: 0.0026808]
	Learning Rate: 0.00268081
	LOSS [training: 1.1718492017966424 | validation: 0.9308609239603538]
	TIME [epoch: 10.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1407282646736594		[learning rate: 0.0026726]
	Learning Rate: 0.0026726
	LOSS [training: 1.1407282646736594 | validation: 1.4586165145830237]
	TIME [epoch: 10.5 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0517817013323494		[learning rate: 0.0026644]
	Learning Rate: 0.0026644
	LOSS [training: 1.0517817013323494 | validation: 0.8546087810998273]
	TIME [epoch: 10.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9486394514999585		[learning rate: 0.0026562]
	Learning Rate: 0.00265624
	LOSS [training: 0.9486394514999585 | validation: 0.9891244174614521]
	TIME [epoch: 10.5 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9079458858683591		[learning rate: 0.0026481]
	Learning Rate: 0.00264809
	LOSS [training: 0.9079458858683591 | validation: 1.0528124610077672]
	TIME [epoch: 10.5 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9075042881151887		[learning rate: 0.00264]
	Learning Rate: 0.00263998
	LOSS [training: 0.9075042881151887 | validation: 0.9152823146140034]
	TIME [epoch: 10.4 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8865523499467247		[learning rate: 0.0026319]
	Learning Rate: 0.00263188
	LOSS [training: 0.8865523499467247 | validation: 0.9005179928633535]
	TIME [epoch: 10.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7597598074374348		[learning rate: 0.0026238]
	Learning Rate: 0.00262382
	LOSS [training: 0.7597598074374348 | validation: 0.8368631355136498]
	TIME [epoch: 10.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8537108855322174		[learning rate: 0.0026158]
	Learning Rate: 0.00261577
	LOSS [training: 0.8537108855322174 | validation: 0.8884529650212315]
	TIME [epoch: 10.5 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7632279339500686		[learning rate: 0.0026078]
	Learning Rate: 0.00260775
	LOSS [training: 0.7632279339500686 | validation: 0.9265750842041502]
	TIME [epoch: 10.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.718910002076094		[learning rate: 0.0025998]
	Learning Rate: 0.00259976
	LOSS [training: 1.718910002076094 | validation: 0.8499408148047156]
	TIME [epoch: 10.5 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8957227165451032		[learning rate: 0.0025918]
	Learning Rate: 0.00259179
	LOSS [training: 0.8957227165451032 | validation: 1.1025912028407976]
	TIME [epoch: 10.5 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9633986783089039		[learning rate: 0.0025838]
	Learning Rate: 0.00258385
	LOSS [training: 0.9633986783089039 | validation: 0.9115460077540424]
	TIME [epoch: 10.4 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7640017054553803		[learning rate: 0.0025759]
	Learning Rate: 0.00257593
	LOSS [training: 0.7640017054553803 | validation: 0.8738425252933232]
	TIME [epoch: 10.5 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8656382450208027		[learning rate: 0.002568]
	Learning Rate: 0.00256803
	LOSS [training: 0.8656382450208027 | validation: 0.9226945254133836]
	TIME [epoch: 10.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7454917459887559		[learning rate: 0.0025602]
	Learning Rate: 0.00256016
	LOSS [training: 0.7454917459887559 | validation: 0.8855773241708346]
	TIME [epoch: 10.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7940126188010509		[learning rate: 0.0025523]
	Learning Rate: 0.00255231
	LOSS [training: 0.7940126188010509 | validation: 0.836076618960859]
	TIME [epoch: 10.4 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.860077164096752		[learning rate: 0.0025445]
	Learning Rate: 0.00254449
	LOSS [training: 0.860077164096752 | validation: 1.0593067454300797]
	TIME [epoch: 10.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.82024059495796		[learning rate: 0.0025367]
	Learning Rate: 0.00253669
	LOSS [training: 0.82024059495796 | validation: 0.8600740629973908]
	TIME [epoch: 10.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8553205129073588		[learning rate: 0.0025289]
	Learning Rate: 0.00252891
	LOSS [training: 0.8553205129073588 | validation: 1.154131754556084]
	TIME [epoch: 10.4 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9932295792831525		[learning rate: 0.0025212]
	Learning Rate: 0.00252116
	LOSS [training: 0.9932295792831525 | validation: 1.070183283405133]
	TIME [epoch: 10.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0330116642839813		[learning rate: 0.0025134]
	Learning Rate: 0.00251343
	LOSS [training: 1.0330116642839813 | validation: 1.1418556168186516]
	TIME [epoch: 10.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.107441906976259		[learning rate: 0.0025057]
	Learning Rate: 0.00250572
	LOSS [training: 1.107441906976259 | validation: 0.9167699431872575]
	TIME [epoch: 10.4 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8437124848062332		[learning rate: 0.002498]
	Learning Rate: 0.00249804
	LOSS [training: 0.8437124848062332 | validation: 0.8277537508112721]
	TIME [epoch: 10.4 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0675021441149435		[learning rate: 0.0024904]
	Learning Rate: 0.00249039
	LOSS [training: 1.0675021441149435 | validation: 0.8832080102010824]
	TIME [epoch: 10.4 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8741166088812719		[learning rate: 0.0024828]
	Learning Rate: 0.00248275
	LOSS [training: 0.8741166088812719 | validation: 0.8320098570824104]
	TIME [epoch: 10.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0008844932074965		[learning rate: 0.0024751]
	Learning Rate: 0.00247514
	LOSS [training: 1.0008844932074965 | validation: 1.0863689011733804]
	TIME [epoch: 10.5 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8701246226675261		[learning rate: 0.0024676]
	Learning Rate: 0.00246755
	LOSS [training: 0.8701246226675261 | validation: 0.7823712494421164]
	TIME [epoch: 10.4 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8073029817785656		[learning rate: 0.00246]
	Learning Rate: 0.00245999
	LOSS [training: 0.8073029817785656 | validation: 0.9421376795876583]
	TIME [epoch: 10.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7968812968405682		[learning rate: 0.0024524]
	Learning Rate: 0.00245245
	LOSS [training: 0.7968812968405682 | validation: 0.7386832734530535]
	TIME [epoch: 10.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7380547743751947		[learning rate: 0.0024449]
	Learning Rate: 0.00244493
	LOSS [training: 0.7380547743751947 | validation: 0.744790762483305]
	TIME [epoch: 10.5 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.759623704022389		[learning rate: 0.0024374]
	Learning Rate: 0.00243744
	LOSS [training: 0.759623704022389 | validation: 0.7154299657118631]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_960.pth
	Model improved!!!
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7880091233348134		[learning rate: 0.00243]
	Learning Rate: 0.00242996
	LOSS [training: 0.7880091233348134 | validation: 1.1089491119802528]
	TIME [epoch: 10.5 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9542300527167621		[learning rate: 0.0024225]
	Learning Rate: 0.00242252
	LOSS [training: 0.9542300527167621 | validation: 1.1960843243895378]
	TIME [epoch: 10.4 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9175188572035726		[learning rate: 0.0024151]
	Learning Rate: 0.00241509
	LOSS [training: 0.9175188572035726 | validation: 0.9225413633098112]
	TIME [epoch: 10.5 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7268445715473801		[learning rate: 0.0024077]
	Learning Rate: 0.00240769
	LOSS [training: 0.7268445715473801 | validation: 0.8683453319005051]
	TIME [epoch: 10.5 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7154011068960862		[learning rate: 0.0024003]
	Learning Rate: 0.00240031
	LOSS [training: 0.7154011068960862 | validation: 0.8000626831970868]
	TIME [epoch: 10.5 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7419326129331164		[learning rate: 0.0023929]
	Learning Rate: 0.00239295
	LOSS [training: 0.7419326129331164 | validation: 0.7814197426877474]
	TIME [epoch: 10.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7737842333512004		[learning rate: 0.0023856]
	Learning Rate: 0.00238561
	LOSS [training: 0.7737842333512004 | validation: 0.8021250888247332]
	TIME [epoch: 10.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699576318238198		[learning rate: 0.0023783]
	Learning Rate: 0.0023783
	LOSS [training: 0.699576318238198 | validation: 0.8128662519800643]
	TIME [epoch: 10.5 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7357234205474483		[learning rate: 0.002371]
	Learning Rate: 0.00237101
	LOSS [training: 0.7357234205474483 | validation: 1.0969753741918422]
	TIME [epoch: 10.5 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1262247534843006		[learning rate: 0.0023637]
	Learning Rate: 0.00236374
	LOSS [training: 1.1262247534843006 | validation: 1.151398882290685]
	TIME [epoch: 10.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9885278018520701		[learning rate: 0.0023565]
	Learning Rate: 0.0023565
	LOSS [training: 0.9885278018520701 | validation: 0.7939577940184007]
	TIME [epoch: 10.5 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.713493504965372		[learning rate: 0.0023493]
	Learning Rate: 0.00234927
	LOSS [training: 0.713493504965372 | validation: 0.6669075943772639]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_972.pth
	Model improved!!!
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7590431494968881		[learning rate: 0.0023421]
	Learning Rate: 0.00234207
	LOSS [training: 0.7590431494968881 | validation: 0.660205084482017]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_973.pth
	Model improved!!!
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7972948286835573		[learning rate: 0.0023349]
	Learning Rate: 0.00233489
	LOSS [training: 0.7972948286835573 | validation: 0.6651194906210239]
	TIME [epoch: 10.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6763374944899263		[learning rate: 0.0023277]
	Learning Rate: 0.00232773
	LOSS [training: 0.6763374944899263 | validation: 0.7364330550802807]
	TIME [epoch: 10.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9677700724783775		[learning rate: 0.0023206]
	Learning Rate: 0.0023206
	LOSS [training: 0.9677700724783775 | validation: 1.0403447108061283]
	TIME [epoch: 10.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1814875178727489		[learning rate: 0.0023135]
	Learning Rate: 0.00231348
	LOSS [training: 1.1814875178727489 | validation: 0.7552204020414507]
	TIME [epoch: 10.4 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1842334137607073		[learning rate: 0.0023064]
	Learning Rate: 0.00230639
	LOSS [training: 1.1842334137607073 | validation: 0.9283876301962276]
	TIME [epoch: 10.4 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8854250203032998		[learning rate: 0.0022993]
	Learning Rate: 0.00229932
	LOSS [training: 0.8854250203032998 | validation: 1.2046168729815312]
	TIME [epoch: 10.4 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9680760619381203		[learning rate: 0.0022923]
	Learning Rate: 0.00229227
	LOSS [training: 0.9680760619381203 | validation: 0.707970502067563]
	TIME [epoch: 10.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7435044953021013		[learning rate: 0.0022852]
	Learning Rate: 0.00228525
	LOSS [training: 0.7435044953021013 | validation: 0.7089518495705344]
	TIME [epoch: 10.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7260508508993715		[learning rate: 0.0022782]
	Learning Rate: 0.00227824
	LOSS [training: 0.7260508508993715 | validation: 0.7045933388343683]
	TIME [epoch: 10.4 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8431018800948303		[learning rate: 0.0022713]
	Learning Rate: 0.00227126
	LOSS [training: 0.8431018800948303 | validation: 0.6790668101681606]
	TIME [epoch: 10.4 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7296298793942712		[learning rate: 0.0022643]
	Learning Rate: 0.0022643
	LOSS [training: 0.7296298793942712 | validation: 0.7199562128047859]
	TIME [epoch: 10.4 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.815280967399748		[learning rate: 0.0022574]
	Learning Rate: 0.00225736
	LOSS [training: 0.815280967399748 | validation: 0.8690828160301939]
	TIME [epoch: 10.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8163679328686276		[learning rate: 0.0022504]
	Learning Rate: 0.00225044
	LOSS [training: 0.8163679328686276 | validation: 0.79290990603175]
	TIME [epoch: 10.4 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7556178175746828		[learning rate: 0.0022435]
	Learning Rate: 0.00224354
	LOSS [training: 0.7556178175746828 | validation: 0.7654277244197405]
	TIME [epoch: 10.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.781478462820614		[learning rate: 0.0022367]
	Learning Rate: 0.00223666
	LOSS [training: 0.781478462820614 | validation: 0.7068574855925964]
	TIME [epoch: 10.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7178509720971936		[learning rate: 0.0022298]
	Learning Rate: 0.0022298
	LOSS [training: 0.7178509720971936 | validation: 0.7754664566558569]
	TIME [epoch: 10.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7546012692078927		[learning rate: 0.002223]
	Learning Rate: 0.00222297
	LOSS [training: 0.7546012692078927 | validation: 0.7617110834682626]
	TIME [epoch: 10.5 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8322849112978135		[learning rate: 0.0022162]
	Learning Rate: 0.00221615
	LOSS [training: 0.8322849112978135 | validation: 0.8217235295053996]
	TIME [epoch: 10.5 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.068346078440229		[learning rate: 0.0022094]
	Learning Rate: 0.00220936
	LOSS [training: 1.068346078440229 | validation: 1.0491653756478494]
	TIME [epoch: 10.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.857690737363713		[learning rate: 0.0022026]
	Learning Rate: 0.00220259
	LOSS [training: 0.857690737363713 | validation: 0.7368187754780342]
	TIME [epoch: 10.5 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6522888005135375		[learning rate: 0.0021958]
	Learning Rate: 0.00219584
	LOSS [training: 0.6522888005135375 | validation: 0.7610600972548375]
	TIME [epoch: 10.5 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7487132754877213		[learning rate: 0.0021891]
	Learning Rate: 0.00218911
	LOSS [training: 0.7487132754877213 | validation: 1.0411128607752504]
	TIME [epoch: 10.5 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7915036091108141		[learning rate: 0.0021824]
	Learning Rate: 0.00218239
	LOSS [training: 0.7915036091108141 | validation: 0.7676124118289568]
	TIME [epoch: 10.4 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8143434765556306		[learning rate: 0.0021757]
	Learning Rate: 0.00217571
	LOSS [training: 0.8143434765556306 | validation: 1.063154187701714]
	TIME [epoch: 10.5 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9841985198580486		[learning rate: 0.002169]
	Learning Rate: 0.00216904
	LOSS [training: 0.9841985198580486 | validation: 0.8662913502958197]
	TIME [epoch: 10.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7934953799795245		[learning rate: 0.0021624]
	Learning Rate: 0.00216239
	LOSS [training: 0.7934953799795245 | validation: 0.7879840942162977]
	TIME [epoch: 10.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9683905736168359		[learning rate: 0.0021558]
	Learning Rate: 0.00215576
	LOSS [training: 0.9683905736168359 | validation: 1.288349684018648]
	TIME [epoch: 10.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3698640627208163		[learning rate: 0.0021491]
	Learning Rate: 0.00214915
	LOSS [training: 1.3698640627208163 | validation: 0.9063653660690183]
	TIME [epoch: 10.5 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8886199113359867		[learning rate: 0.0021426]
	Learning Rate: 0.00214256
	LOSS [training: 0.8886199113359867 | validation: 0.9263799488246481]
	TIME [epoch: 10.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7806668060618206		[learning rate: 0.002136]
	Learning Rate: 0.00213599
	LOSS [training: 0.7806668060618206 | validation: 0.9581566117008998]
	TIME [epoch: 10.5 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0421591874744265		[learning rate: 0.0021294]
	Learning Rate: 0.00212945
	LOSS [training: 1.0421591874744265 | validation: 1.1110592235098988]
	TIME [epoch: 10.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8268031313461133		[learning rate: 0.0021229]
	Learning Rate: 0.00212292
	LOSS [training: 0.8268031313461133 | validation: 0.8670886861215124]
	TIME [epoch: 10.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8289217165111186		[learning rate: 0.0021164]
	Learning Rate: 0.00211641
	LOSS [training: 0.8289217165111186 | validation: 1.187352859385146]
	TIME [epoch: 10.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0248129235293753		[learning rate: 0.0021099]
	Learning Rate: 0.00210992
	LOSS [training: 1.0248129235293753 | validation: 0.7515266855020485]
	TIME [epoch: 10.5 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8457215272871293		[learning rate: 0.0021035]
	Learning Rate: 0.00210346
	LOSS [training: 0.8457215272871293 | validation: 0.8955400259312947]
	TIME [epoch: 10.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8613994088345913		[learning rate: 0.002097]
	Learning Rate: 0.00209701
	LOSS [training: 0.8613994088345913 | validation: 0.9342084369826966]
	TIME [epoch: 10.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7834522555760323		[learning rate: 0.0020906]
	Learning Rate: 0.00209058
	LOSS [training: 0.7834522555760323 | validation: 0.7330926619778344]
	TIME [epoch: 10.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7687216552903607		[learning rate: 0.0020842]
	Learning Rate: 0.00208417
	LOSS [training: 0.7687216552903607 | validation: 1.2050485630641974]
	TIME [epoch: 10.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1166227234800952		[learning rate: 0.0020778]
	Learning Rate: 0.00207778
	LOSS [training: 1.1166227234800952 | validation: 0.9852166586938136]
	TIME [epoch: 10.5 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8780154163186275		[learning rate: 0.0020714]
	Learning Rate: 0.00207141
	LOSS [training: 0.8780154163186275 | validation: 0.8489733102258924]
	TIME [epoch: 10.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7222983160512447		[learning rate: 0.0020651]
	Learning Rate: 0.00206506
	LOSS [training: 0.7222983160512447 | validation: 0.8927660125656223]
	TIME [epoch: 10.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7170947346288368		[learning rate: 0.0020587]
	Learning Rate: 0.00205873
	LOSS [training: 0.7170947346288368 | validation: 1.0433455393285744]
	TIME [epoch: 10.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7308035611709643		[learning rate: 0.0020524]
	Learning Rate: 0.00205242
	LOSS [training: 0.7308035611709643 | validation: 0.7428905641179006]
	TIME [epoch: 10.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7633389531769647		[learning rate: 0.0020461]
	Learning Rate: 0.00204613
	LOSS [training: 0.7633389531769647 | validation: 0.8304259310997049]
	TIME [epoch: 10.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8666044457077827		[learning rate: 0.0020399]
	Learning Rate: 0.00203986
	LOSS [training: 0.8666044457077827 | validation: 1.1352537125107867]
	TIME [epoch: 10.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0487341580666167		[learning rate: 0.0020336]
	Learning Rate: 0.00203361
	LOSS [training: 1.0487341580666167 | validation: 1.3182090811035079]
	TIME [epoch: 10.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1148155964945066		[learning rate: 0.0020274]
	Learning Rate: 0.00202737
	LOSS [training: 1.1148155964945066 | validation: 1.2680285895724788]
	TIME [epoch: 10.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9303244636377489		[learning rate: 0.0020212]
	Learning Rate: 0.00202116
	LOSS [training: 0.9303244636377489 | validation: 0.973780464197981]
	TIME [epoch: 10.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.761186817700312		[learning rate: 0.002015]
	Learning Rate: 0.00201496
	LOSS [training: 0.761186817700312 | validation: 0.7294159730317938]
	TIME [epoch: 10.5 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.741175613906351		[learning rate: 0.0020088]
	Learning Rate: 0.00200878
	LOSS [training: 0.741175613906351 | validation: 0.6804242292309354]
	TIME [epoch: 10.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7202865715265769		[learning rate: 0.0020026]
	Learning Rate: 0.00200263
	LOSS [training: 0.7202865715265769 | validation: 0.7709673773256078]
	TIME [epoch: 10.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8452788527180164		[learning rate: 0.0019965]
	Learning Rate: 0.00199649
	LOSS [training: 0.8452788527180164 | validation: 1.101985307845988]
	TIME [epoch: 10.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8167343419938285		[learning rate: 0.0019904]
	Learning Rate: 0.00199037
	LOSS [training: 0.8167343419938285 | validation: 0.6515939681167825]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1026.pth
	Model improved!!!
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.775550160850429		[learning rate: 0.0019843]
	Learning Rate: 0.00198427
	LOSS [training: 0.775550160850429 | validation: 0.6769058134866495]
	TIME [epoch: 10.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6219162635725548		[learning rate: 0.0019782]
	Learning Rate: 0.00197818
	LOSS [training: 0.6219162635725548 | validation: 1.351510613909645]
	TIME [epoch: 10.4 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8266189543135672		[learning rate: 0.0019721]
	Learning Rate: 0.00197212
	LOSS [training: 0.8266189543135672 | validation: 0.7670841630038429]
	TIME [epoch: 10.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9223288321928965		[learning rate: 0.0019661]
	Learning Rate: 0.00196607
	LOSS [training: 0.9223288321928965 | validation: 0.982687387763079]
	TIME [epoch: 10.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8715881299205552		[learning rate: 0.00196]
	Learning Rate: 0.00196005
	LOSS [training: 0.8715881299205552 | validation: 0.801383160677188]
	TIME [epoch: 10.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7063631663024124		[learning rate: 0.001954]
	Learning Rate: 0.00195404
	LOSS [training: 0.7063631663024124 | validation: 0.8531489908643031]
	TIME [epoch: 10.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7578741949524226		[learning rate: 0.001948]
	Learning Rate: 0.00194805
	LOSS [training: 0.7578741949524226 | validation: 0.8247675608235503]
	TIME [epoch: 10.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8893026377476243		[learning rate: 0.0019421]
	Learning Rate: 0.00194208
	LOSS [training: 0.8893026377476243 | validation: 0.752065028417319]
	TIME [epoch: 10.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8751710863422056		[learning rate: 0.0019361]
	Learning Rate: 0.00193612
	LOSS [training: 0.8751710863422056 | validation: 1.1313982271724152]
	TIME [epoch: 10.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0056969464164456		[learning rate: 0.0019302]
	Learning Rate: 0.00193019
	LOSS [training: 1.0056969464164456 | validation: 0.6566010882624076]
	TIME [epoch: 10.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7147010750588704		[learning rate: 0.0019243]
	Learning Rate: 0.00192427
	LOSS [training: 0.7147010750588704 | validation: 0.9710592961931812]
	TIME [epoch: 10.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7464311908027499		[learning rate: 0.0019184]
	Learning Rate: 0.00191837
	LOSS [training: 0.7464311908027499 | validation: 0.7420667938855227]
	TIME [epoch: 10.5 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.81526237391191		[learning rate: 0.0019125]
	Learning Rate: 0.00191249
	LOSS [training: 0.81526237391191 | validation: 0.9175269428142363]
	TIME [epoch: 10.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.754373325243843		[learning rate: 0.0019066]
	Learning Rate: 0.00190663
	LOSS [training: 0.754373325243843 | validation: 0.7605401382162247]
	TIME [epoch: 10.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7261992317901751		[learning rate: 0.0019008]
	Learning Rate: 0.00190079
	LOSS [training: 0.7261992317901751 | validation: 0.7843686204707884]
	TIME [epoch: 10.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7320481986880744		[learning rate: 0.001895]
	Learning Rate: 0.00189496
	LOSS [training: 0.7320481986880744 | validation: 0.7926042819398671]
	TIME [epoch: 10.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0441069192243613		[learning rate: 0.0018892]
	Learning Rate: 0.00188915
	LOSS [training: 1.0441069192243613 | validation: 0.9119003513128486]
	TIME [epoch: 10.5 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8565635281525749		[learning rate: 0.0018834]
	Learning Rate: 0.00188336
	LOSS [training: 0.8565635281525749 | validation: 0.9121790669703782]
	TIME [epoch: 10.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7347558369119745		[learning rate: 0.0018776]
	Learning Rate: 0.00187759
	LOSS [training: 0.7347558369119745 | validation: 0.7180489654424949]
	TIME [epoch: 10.5 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8257435372221013		[learning rate: 0.0018718]
	Learning Rate: 0.00187183
	LOSS [training: 0.8257435372221013 | validation: 0.860876111094613]
	TIME [epoch: 10.5 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7302153797092478		[learning rate: 0.0018661]
	Learning Rate: 0.00186609
	LOSS [training: 0.7302153797092478 | validation: 0.8427721658716086]
	TIME [epoch: 10.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7564024951487544		[learning rate: 0.0018604]
	Learning Rate: 0.00186037
	LOSS [training: 0.7564024951487544 | validation: 0.8208439479986064]
	TIME [epoch: 10.5 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7575252329801303		[learning rate: 0.0018547]
	Learning Rate: 0.00185467
	LOSS [training: 0.7575252329801303 | validation: 1.0729885595411348]
	TIME [epoch: 10.5 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8265150338814052		[learning rate: 0.001849]
	Learning Rate: 0.00184898
	LOSS [training: 0.8265150338814052 | validation: 0.8713716521506932]
	TIME [epoch: 10.5 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7384645706612644		[learning rate: 0.0018433]
	Learning Rate: 0.00184332
	LOSS [training: 0.7384645706612644 | validation: 0.8547627969096171]
	TIME [epoch: 10.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.685989674296		[learning rate: 0.0018377]
	Learning Rate: 0.00183767
	LOSS [training: 0.685989674296 | validation: 0.7383511480093347]
	TIME [epoch: 10.5 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6422967214243469		[learning rate: 0.001832]
	Learning Rate: 0.00183203
	LOSS [training: 0.6422967214243469 | validation: 0.652480898587029]
	TIME [epoch: 10.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7316697523729427		[learning rate: 0.0018264]
	Learning Rate: 0.00182642
	LOSS [training: 0.7316697523729427 | validation: 0.73860823528459]
	TIME [epoch: 10.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6532728108873709		[learning rate: 0.0018208]
	Learning Rate: 0.00182082
	LOSS [training: 0.6532728108873709 | validation: 0.6562167041311685]
	TIME [epoch: 10.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8500749815384848		[learning rate: 0.0018152]
	Learning Rate: 0.00181524
	LOSS [training: 0.8500749815384848 | validation: 0.7774931437605558]
	TIME [epoch: 10.5 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8035777576045838		[learning rate: 0.0018097]
	Learning Rate: 0.00180967
	LOSS [training: 0.8035777576045838 | validation: 0.759200250316217]
	TIME [epoch: 10.5 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.685348885242328		[learning rate: 0.0018041]
	Learning Rate: 0.00180412
	LOSS [training: 0.685348885242328 | validation: 0.7163380330191756]
	TIME [epoch: 10.5 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6748708237765904		[learning rate: 0.0017986]
	Learning Rate: 0.00179859
	LOSS [training: 0.6748708237765904 | validation: 1.3834518904126727]
	TIME [epoch: 10.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9024378088664765		[learning rate: 0.0017931]
	Learning Rate: 0.00179308
	LOSS [training: 0.9024378088664765 | validation: 0.8272632559622904]
	TIME [epoch: 10.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8031998835712184		[learning rate: 0.0017876]
	Learning Rate: 0.00178758
	LOSS [training: 0.8031998835712184 | validation: 1.1248842895022253]
	TIME [epoch: 10.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8461529430781484		[learning rate: 0.0017821]
	Learning Rate: 0.00178211
	LOSS [training: 0.8461529430781484 | validation: 0.7539343405122775]
	TIME [epoch: 10.5 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6805922267190875		[learning rate: 0.0017766]
	Learning Rate: 0.00177664
	LOSS [training: 0.6805922267190875 | validation: 0.8940196269896319]
	TIME [epoch: 10.5 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6716589016702266		[learning rate: 0.0017712]
	Learning Rate: 0.0017712
	LOSS [training: 0.6716589016702266 | validation: 0.6323372361365044]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1064.pth
	Model improved!!!
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6199609457461615		[learning rate: 0.0017658]
	Learning Rate: 0.00176577
	LOSS [training: 0.6199609457461615 | validation: 0.7532138533810583]
	TIME [epoch: 10.5 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7337475995365283		[learning rate: 0.0017604]
	Learning Rate: 0.00176035
	LOSS [training: 0.7337475995365283 | validation: 0.7347643596547226]
	TIME [epoch: 10.5 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6904294820764653		[learning rate: 0.001755]
	Learning Rate: 0.00175496
	LOSS [training: 0.6904294820764653 | validation: 0.9099333226465715]
	TIME [epoch: 10.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7164206033006099		[learning rate: 0.0017496]
	Learning Rate: 0.00174958
	LOSS [training: 0.7164206033006099 | validation: 0.7850261418232205]
	TIME [epoch: 10.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7470179440162659		[learning rate: 0.0017442]
	Learning Rate: 0.00174421
	LOSS [training: 0.7470179440162659 | validation: 0.8917160219684169]
	TIME [epoch: 10.5 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7658786719083281		[learning rate: 0.0017389]
	Learning Rate: 0.00173887
	LOSS [training: 0.7658786719083281 | validation: 0.9851777365960338]
	TIME [epoch: 10.5 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6791928584504353		[learning rate: 0.0017335]
	Learning Rate: 0.00173354
	LOSS [training: 0.6791928584504353 | validation: 0.8094804963965185]
	TIME [epoch: 10.5 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0567093213745284		[learning rate: 0.0017282]
	Learning Rate: 0.00172822
	LOSS [training: 1.0567093213745284 | validation: 0.9875822329805474]
	TIME [epoch: 10.5 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7297757917260317		[learning rate: 0.0017229]
	Learning Rate: 0.00172293
	LOSS [training: 0.7297757917260317 | validation: 0.966252722707437]
	TIME [epoch: 10.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8158450927335256		[learning rate: 0.0017176]
	Learning Rate: 0.00171764
	LOSS [training: 0.8158450927335256 | validation: 0.8173877481216388]
	TIME [epoch: 10.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6435174885938444		[learning rate: 0.0017124]
	Learning Rate: 0.00171238
	LOSS [training: 0.6435174885938444 | validation: 0.7115572876808348]
	TIME [epoch: 10.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7441495425640907		[learning rate: 0.0017071]
	Learning Rate: 0.00170713
	LOSS [training: 0.7441495425640907 | validation: 0.8508929310283186]
	TIME [epoch: 10.5 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7585524581692711		[learning rate: 0.0017019]
	Learning Rate: 0.0017019
	LOSS [training: 0.7585524581692711 | validation: 0.7546378103638096]
	TIME [epoch: 10.5 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7522260121705742		[learning rate: 0.0016967]
	Learning Rate: 0.00169668
	LOSS [training: 0.7522260121705742 | validation: 0.8594566141848402]
	TIME [epoch: 10.5 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8233974175606871		[learning rate: 0.0016915]
	Learning Rate: 0.00169148
	LOSS [training: 0.8233974175606871 | validation: 0.9539038793448655]
	TIME [epoch: 10.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9019453412451547		[learning rate: 0.0016863]
	Learning Rate: 0.00168629
	LOSS [training: 0.9019453412451547 | validation: 0.8068445152297588]
	TIME [epoch: 10.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7346909028028723		[learning rate: 0.0016811]
	Learning Rate: 0.00168113
	LOSS [training: 0.7346909028028723 | validation: 0.7758625707844817]
	TIME [epoch: 10.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5212427643904052		[learning rate: 0.001676]
	Learning Rate: 0.00167597
	LOSS [training: 1.5212427643904052 | validation: 1.2340598474792301]
	TIME [epoch: 10.5 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8602045803114828		[learning rate: 0.0016708]
	Learning Rate: 0.00167083
	LOSS [training: 0.8602045803114828 | validation: 0.6748999157201216]
	TIME [epoch: 10.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9147347601449198		[learning rate: 0.0016657]
	Learning Rate: 0.00166571
	LOSS [training: 0.9147347601449198 | validation: 0.9530369096152275]
	TIME [epoch: 10.5 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7515252033858223		[learning rate: 0.0016606]
	Learning Rate: 0.00166061
	LOSS [training: 0.7515252033858223 | validation: 0.7750431631459097]
	TIME [epoch: 10.5 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.938159736139611		[learning rate: 0.0016555]
	Learning Rate: 0.00165552
	LOSS [training: 0.938159736139611 | validation: 0.7554696659546261]
	TIME [epoch: 10.5 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6994817543375538		[learning rate: 0.0016504]
	Learning Rate: 0.00165044
	LOSS [training: 0.6994817543375538 | validation: 0.6672731081084924]
	TIME [epoch: 10.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7112416454594487		[learning rate: 0.0016454]
	Learning Rate: 0.00164538
	LOSS [training: 0.7112416454594487 | validation: 0.8343539921703581]
	TIME [epoch: 10.5 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7604209938808325		[learning rate: 0.0016403]
	Learning Rate: 0.00164034
	LOSS [training: 0.7604209938808325 | validation: 0.7713204116864144]
	TIME [epoch: 10.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7903617246081237		[learning rate: 0.0016353]
	Learning Rate: 0.00163531
	LOSS [training: 0.7903617246081237 | validation: 0.695894095462148]
	TIME [epoch: 10.5 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7065320346011121		[learning rate: 0.0016303]
	Learning Rate: 0.0016303
	LOSS [training: 0.7065320346011121 | validation: 0.8254490447255469]
	TIME [epoch: 10.5 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7230033127366232		[learning rate: 0.0016253]
	Learning Rate: 0.0016253
	LOSS [training: 0.7230033127366232 | validation: 0.8274509058980911]
	TIME [epoch: 10.5 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7088433416829151		[learning rate: 0.0016203]
	Learning Rate: 0.00162032
	LOSS [training: 0.7088433416829151 | validation: 0.8511876754554649]
	TIME [epoch: 10.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6813308553643298		[learning rate: 0.0016154]
	Learning Rate: 0.00161535
	LOSS [training: 0.6813308553643298 | validation: 0.8553663208354356]
	TIME [epoch: 10.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7211947425616494		[learning rate: 0.0016104]
	Learning Rate: 0.0016104
	LOSS [training: 0.7211947425616494 | validation: 1.1822020848356383]
	TIME [epoch: 10.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8445749832621564		[learning rate: 0.0016055]
	Learning Rate: 0.00160546
	LOSS [training: 0.8445749832621564 | validation: 0.9984025633753991]
	TIME [epoch: 10.5 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1954510942978418		[learning rate: 0.0016005]
	Learning Rate: 0.00160054
	LOSS [training: 1.1954510942978418 | validation: 1.399809399076961]
	TIME [epoch: 10.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9760086033747062		[learning rate: 0.0015956]
	Learning Rate: 0.00159563
	LOSS [training: 0.9760086033747062 | validation: 0.8056515504125628]
	TIME [epoch: 10.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8883177254315668		[learning rate: 0.0015907]
	Learning Rate: 0.00159074
	LOSS [training: 0.8883177254315668 | validation: 1.2939672580698482]
	TIME [epoch: 10.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8124502526920789		[learning rate: 0.0015859]
	Learning Rate: 0.00158587
	LOSS [training: 0.8124502526920789 | validation: 0.7419978200414282]
	TIME [epoch: 10.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6452563284160953		[learning rate: 0.001581]
	Learning Rate: 0.00158101
	LOSS [training: 0.6452563284160953 | validation: 0.7673343901975811]
	TIME [epoch: 10.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6330809519647503		[learning rate: 0.0015762]
	Learning Rate: 0.00157616
	LOSS [training: 0.6330809519647503 | validation: 0.7637844302687938]
	TIME [epoch: 10.5 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.928479087230083		[learning rate: 0.0015713]
	Learning Rate: 0.00157133
	LOSS [training: 0.928479087230083 | validation: 0.639921230518744]
	TIME [epoch: 10.5 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7748549484786695		[learning rate: 0.0015665]
	Learning Rate: 0.00156651
	LOSS [training: 0.7748549484786695 | validation: 1.2016797301688713]
	TIME [epoch: 10.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8894180472827268		[learning rate: 0.0015617]
	Learning Rate: 0.00156171
	LOSS [training: 0.8894180472827268 | validation: 0.9127456440384902]
	TIME [epoch: 10.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7366936003734234		[learning rate: 0.0015569]
	Learning Rate: 0.00155692
	LOSS [training: 0.7366936003734234 | validation: 0.7773260103819175]
	TIME [epoch: 10.5 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8881087388559804		[learning rate: 0.0015521]
	Learning Rate: 0.00155215
	LOSS [training: 0.8881087388559804 | validation: 0.7989025342929474]
	TIME [epoch: 10.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7951378003714085		[learning rate: 0.0015474]
	Learning Rate: 0.00154739
	LOSS [training: 0.7951378003714085 | validation: 0.7984827984339978]
	TIME [epoch: 10.5 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7448785770189741		[learning rate: 0.0015426]
	Learning Rate: 0.00154265
	LOSS [training: 0.7448785770189741 | validation: 0.942910642144926]
	TIME [epoch: 10.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7319244504541496		[learning rate: 0.0015379]
	Learning Rate: 0.00153792
	LOSS [training: 0.7319244504541496 | validation: 0.8770676925105219]
	TIME [epoch: 10.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7888086710905048		[learning rate: 0.0015332]
	Learning Rate: 0.0015332
	LOSS [training: 0.7888086710905048 | validation: 0.9879728091740446]
	TIME [epoch: 10.5 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7792721353993329		[learning rate: 0.0015285]
	Learning Rate: 0.0015285
	LOSS [training: 0.7792721353993329 | validation: 1.081268034473643]
	TIME [epoch: 10.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1059050142495555		[learning rate: 0.0015238]
	Learning Rate: 0.00152382
	LOSS [training: 1.1059050142495555 | validation: 1.2969816719957212]
	TIME [epoch: 10.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.777466740320747		[learning rate: 0.0015191]
	Learning Rate: 0.00151915
	LOSS [training: 0.777466740320747 | validation: 0.8776242089728726]
	TIME [epoch: 10.5 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6895143247750992		[learning rate: 0.0015145]
	Learning Rate: 0.00151449
	LOSS [training: 0.6895143247750992 | validation: 0.7222155314803248]
	TIME [epoch: 10.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7301692957029658		[learning rate: 0.0015098]
	Learning Rate: 0.00150985
	LOSS [training: 0.7301692957029658 | validation: 0.7568532446666825]
	TIME [epoch: 10.4 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7402744915251522		[learning rate: 0.0015052]
	Learning Rate: 0.00150522
	LOSS [training: 0.7402744915251522 | validation: 0.7627967755572047]
	TIME [epoch: 10.5 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6778471394351332		[learning rate: 0.0015006]
	Learning Rate: 0.00150061
	LOSS [training: 0.6778471394351332 | validation: 0.6883965833919947]
	TIME [epoch: 10.5 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.662152817005244		[learning rate: 0.001496]
	Learning Rate: 0.00149601
	LOSS [training: 0.662152817005244 | validation: 0.7531285406394796]
	TIME [epoch: 10.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7481331387496921		[learning rate: 0.0014914]
	Learning Rate: 0.00149142
	LOSS [training: 0.7481331387496921 | validation: 0.7390261591455339]
	TIME [epoch: 10.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7568607541761209		[learning rate: 0.0014868]
	Learning Rate: 0.00148685
	LOSS [training: 0.7568607541761209 | validation: 1.6265380019200169]
	TIME [epoch: 10.4 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2248134053321402		[learning rate: 0.0014823]
	Learning Rate: 0.00148229
	LOSS [training: 1.2248134053321402 | validation: 0.8024187029793645]
	TIME [epoch: 10.5 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6845420003148728		[learning rate: 0.0014777]
	Learning Rate: 0.00147775
	LOSS [training: 0.6845420003148728 | validation: 1.132054895622362]
	TIME [epoch: 10.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7329175885869554		[learning rate: 0.0014732]
	Learning Rate: 0.00147322
	LOSS [training: 0.7329175885869554 | validation: 0.7306964544216495]
	TIME [epoch: 10.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6892605211843877		[learning rate: 0.0014687]
	Learning Rate: 0.0014687
	LOSS [training: 0.6892605211843877 | validation: 1.3619536149951503]
	TIME [epoch: 10.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0565481105473058		[learning rate: 0.0014642]
	Learning Rate: 0.0014642
	LOSS [training: 1.0565481105473058 | validation: 1.2357146083847486]
	TIME [epoch: 10.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8632818687207362		[learning rate: 0.0014597]
	Learning Rate: 0.00145971
	LOSS [training: 0.8632818687207362 | validation: 0.9346276119162493]
	TIME [epoch: 10.5 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1281024609098784		[learning rate: 0.0014552]
	Learning Rate: 0.00145524
	LOSS [training: 1.1281024609098784 | validation: 1.0031739148221082]
	TIME [epoch: 10.5 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7737238412614846		[learning rate: 0.0014508]
	Learning Rate: 0.00145077
	LOSS [training: 0.7737238412614846 | validation: 1.1648889203666335]
	TIME [epoch: 10.5 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8367837430072204		[learning rate: 0.0014463]
	Learning Rate: 0.00144633
	LOSS [training: 0.8367837430072204 | validation: 1.0385179322592268]
	TIME [epoch: 10.5 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7382064545357692		[learning rate: 0.0014419]
	Learning Rate: 0.00144189
	LOSS [training: 0.7382064545357692 | validation: 0.9156988708261747]
	TIME [epoch: 10.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.725561854201721		[learning rate: 0.0014375]
	Learning Rate: 0.00143747
	LOSS [training: 0.725561854201721 | validation: 1.3537429598086552]
	TIME [epoch: 10.5 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9440967055141616		[learning rate: 0.0014331]
	Learning Rate: 0.00143307
	LOSS [training: 0.9440967055141616 | validation: 0.9611490693368443]
	TIME [epoch: 10.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9705130738988983		[learning rate: 0.0014287]
	Learning Rate: 0.00142867
	LOSS [training: 0.9705130738988983 | validation: 2.0953790040964515]
	TIME [epoch: 10.5 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1664634526062656		[learning rate: 0.0014243]
	Learning Rate: 0.0014243
	LOSS [training: 2.1664634526062656 | validation: 0.9134560438562414]
	TIME [epoch: 10.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9095687130942665		[learning rate: 0.0014199]
	Learning Rate: 0.00141993
	LOSS [training: 0.9095687130942665 | validation: 0.9202626250814328]
	TIME [epoch: 10.5 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7556864923142375		[learning rate: 0.0014156]
	Learning Rate: 0.00141558
	LOSS [training: 1.7556864923142375 | validation: 1.0599321823850922]
	TIME [epoch: 10.4 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8718134916664597		[learning rate: 0.0014112]
	Learning Rate: 0.00141124
	LOSS [training: 0.8718134916664597 | validation: 0.8201785247814146]
	TIME [epoch: 10.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8028525825118951		[learning rate: 0.0014069]
	Learning Rate: 0.00140691
	LOSS [training: 0.8028525825118951 | validation: 0.9122125237462183]
	TIME [epoch: 10.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7185575916405744		[learning rate: 0.0014026]
	Learning Rate: 0.0014026
	LOSS [training: 0.7185575916405744 | validation: 1.017320261121279]
	TIME [epoch: 10.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7671938995502481		[learning rate: 0.0013983]
	Learning Rate: 0.0013983
	LOSS [training: 0.7671938995502481 | validation: 1.0127036727354293]
	TIME [epoch: 10.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908664776081581		[learning rate: 0.001394]
	Learning Rate: 0.00139401
	LOSS [training: 0.6908664776081581 | validation: 0.9237944776746858]
	TIME [epoch: 10.5 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6486810302592343		[learning rate: 0.0013897]
	Learning Rate: 0.00138974
	LOSS [training: 0.6486810302592343 | validation: 0.9362978963196149]
	TIME [epoch: 10.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6186767964129615		[learning rate: 0.0013855]
	Learning Rate: 0.00138548
	LOSS [training: 0.6186767964129615 | validation: 0.7413406730597356]
	TIME [epoch: 10.5 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6168617264332582		[learning rate: 0.0013812]
	Learning Rate: 0.00138123
	LOSS [training: 0.6168617264332582 | validation: 0.9892175303998434]
	TIME [epoch: 10.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6825410289734352		[learning rate: 0.001377]
	Learning Rate: 0.001377
	LOSS [training: 0.6825410289734352 | validation: 0.6693482719475322]
	TIME [epoch: 10.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.703414693497981		[learning rate: 0.0013728]
	Learning Rate: 0.00137278
	LOSS [training: 0.703414693497981 | validation: 0.658666213273416]
	TIME [epoch: 10.5 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6565262031629511		[learning rate: 0.0013686]
	Learning Rate: 0.00136857
	LOSS [training: 0.6565262031629511 | validation: 0.7207178305171372]
	TIME [epoch: 10.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.689115449331793		[learning rate: 0.0013644]
	Learning Rate: 0.00136437
	LOSS [training: 0.689115449331793 | validation: 0.7939581078314711]
	TIME [epoch: 10.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7040017612852864		[learning rate: 0.0013602]
	Learning Rate: 0.00136019
	LOSS [training: 0.7040017612852864 | validation: 0.6756422201671484]
	TIME [epoch: 10.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6149145369086593		[learning rate: 0.001356]
	Learning Rate: 0.00135602
	LOSS [training: 0.6149145369086593 | validation: 0.621243270335351]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1151.pth
	Model improved!!!
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6367089550860728		[learning rate: 0.0013519]
	Learning Rate: 0.00135187
	LOSS [training: 0.6367089550860728 | validation: 0.8739542198805534]
	TIME [epoch: 10.4 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.828193375391999		[learning rate: 0.0013477]
	Learning Rate: 0.00134772
	LOSS [training: 0.828193375391999 | validation: 1.0339754364193623]
	TIME [epoch: 10.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.845029825192247		[learning rate: 0.0013436]
	Learning Rate: 0.00134359
	LOSS [training: 0.845029825192247 | validation: 0.7543821241314883]
	TIME [epoch: 10.5 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.675144509621718		[learning rate: 0.0013395]
	Learning Rate: 0.00133947
	LOSS [training: 0.675144509621718 | validation: 0.6304923258240254]
	TIME [epoch: 10.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6690013083177899		[learning rate: 0.0013354]
	Learning Rate: 0.00133536
	LOSS [training: 0.6690013083177899 | validation: 0.6928718350955329]
	TIME [epoch: 10.5 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0172584806009544		[learning rate: 0.0013313]
	Learning Rate: 0.00133127
	LOSS [training: 1.0172584806009544 | validation: 0.6646795384564095]
	TIME [epoch: 10.4 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6740628025673712		[learning rate: 0.0013272]
	Learning Rate: 0.00132719
	LOSS [training: 0.6740628025673712 | validation: 0.7261088089028723]
	TIME [epoch: 10.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7013959307021448		[learning rate: 0.0013231]
	Learning Rate: 0.00132312
	LOSS [training: 0.7013959307021448 | validation: 0.7102191552304905]
	TIME [epoch: 10.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6802578969188555		[learning rate: 0.0013191]
	Learning Rate: 0.00131907
	LOSS [training: 0.6802578969188555 | validation: 0.650752357524436]
	TIME [epoch: 10.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6686790458941043		[learning rate: 0.001315]
	Learning Rate: 0.00131502
	LOSS [training: 0.6686790458941043 | validation: 0.7750126390994383]
	TIME [epoch: 10.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6785007279601614		[learning rate: 0.001311]
	Learning Rate: 0.00131099
	LOSS [training: 0.6785007279601614 | validation: 0.8064684098911377]
	TIME [epoch: 10.5 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8484207108424426		[learning rate: 0.001307]
	Learning Rate: 0.00130697
	LOSS [training: 0.8484207108424426 | validation: 0.8236065842661497]
	TIME [epoch: 10.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9450775757973122		[learning rate: 0.001303]
	Learning Rate: 0.00130297
	LOSS [training: 0.9450775757973122 | validation: 0.7311312597364099]
	TIME [epoch: 10.5 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7017756017830716		[learning rate: 0.001299]
	Learning Rate: 0.00129897
	LOSS [training: 0.7017756017830716 | validation: 0.7683625664269377]
	TIME [epoch: 10.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6995134645607907		[learning rate: 0.001295]
	Learning Rate: 0.00129499
	LOSS [training: 0.6995134645607907 | validation: 0.6274739089858469]
	TIME [epoch: 10.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7257205748992239		[learning rate: 0.001291]
	Learning Rate: 0.00129102
	LOSS [training: 0.7257205748992239 | validation: 0.6629133426187112]
	TIME [epoch: 10.5 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8770926261270467		[learning rate: 0.0012871]
	Learning Rate: 0.00128706
	LOSS [training: 0.8770926261270467 | validation: 0.8646858025356713]
	TIME [epoch: 10.5 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0591793127127833		[learning rate: 0.0012831]
	Learning Rate: 0.00128312
	LOSS [training: 1.0591793127127833 | validation: 0.7132593963232868]
	TIME [epoch: 10.5 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6670578209804003		[learning rate: 0.0012792]
	Learning Rate: 0.00127918
	LOSS [training: 0.6670578209804003 | validation: 0.7758166044799056]
	TIME [epoch: 10.5 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6454833976682669		[learning rate: 0.0012753]
	Learning Rate: 0.00127526
	LOSS [training: 0.6454833976682669 | validation: 0.6846681994367273]
	TIME [epoch: 10.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6568895655779172		[learning rate: 0.0012714]
	Learning Rate: 0.00127135
	LOSS [training: 0.6568895655779172 | validation: 0.9092240439403881]
	TIME [epoch: 10.5 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8023515219093674		[learning rate: 0.0012675]
	Learning Rate: 0.00126746
	LOSS [training: 0.8023515219093674 | validation: 0.7251653067069666]
	TIME [epoch: 10.5 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.747243113217142		[learning rate: 0.0012636]
	Learning Rate: 0.00126357
	LOSS [training: 1.747243113217142 | validation: 2.6134391675954096]
	TIME [epoch: 10.5 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.296951924952421		[learning rate: 0.0012597]
	Learning Rate: 0.0012597
	LOSS [training: 2.296951924952421 | validation: 1.6204634088162349]
	TIME [epoch: 10.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0481893163377136		[learning rate: 0.0012558]
	Learning Rate: 0.00125584
	LOSS [training: 2.0481893163377136 | validation: 1.4551271292004417]
	TIME [epoch: 10.5 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.762093779625658		[learning rate: 0.001252]
	Learning Rate: 0.00125199
	LOSS [training: 1.762093779625658 | validation: 1.4870493585097848]
	TIME [epoch: 10.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.595823249932574		[learning rate: 0.0012481]
	Learning Rate: 0.00124815
	LOSS [training: 1.595823249932574 | validation: 1.4457713063325384]
	TIME [epoch: 10.5 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7688962411745557		[learning rate: 0.0012443]
	Learning Rate: 0.00124432
	LOSS [training: 1.7688962411745557 | validation: 0.9060769301874938]
	TIME [epoch: 10.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.350708639803405		[learning rate: 0.0012405]
	Learning Rate: 0.00124051
	LOSS [training: 1.350708639803405 | validation: 1.5391101029207261]
	TIME [epoch: 10.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.730143802203149		[learning rate: 0.0012367]
	Learning Rate: 0.00123671
	LOSS [training: 1.730143802203149 | validation: 1.4206594112371291]
	TIME [epoch: 10.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2382749208023314		[learning rate: 0.0012329]
	Learning Rate: 0.00123292
	LOSS [training: 1.2382749208023314 | validation: 0.864623352724496]
	TIME [epoch: 10.5 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8647117986617656		[learning rate: 0.0012291]
	Learning Rate: 0.00122914
	LOSS [training: 0.8647117986617656 | validation: 1.3846677006823072]
	TIME [epoch: 10.5 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1146787065340182		[learning rate: 0.0012254]
	Learning Rate: 0.00122537
	LOSS [training: 1.1146787065340182 | validation: 1.024340140359063]
	TIME [epoch: 10.5 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0873346133012398		[learning rate: 0.0012216]
	Learning Rate: 0.00122161
	LOSS [training: 1.0873346133012398 | validation: 1.4377633713330034]
	TIME [epoch: 10.5 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2081085166910457		[learning rate: 0.0012179]
	Learning Rate: 0.00121787
	LOSS [training: 2.2081085166910457 | validation: 1.1734356880689951]
	TIME [epoch: 10.5 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9541480320451701		[learning rate: 0.0012141]
	Learning Rate: 0.00121413
	LOSS [training: 0.9541480320451701 | validation: 0.8024852747676926]
	TIME [epoch: 10.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7126652496823551		[learning rate: 0.0012104]
	Learning Rate: 0.00121041
	LOSS [training: 0.7126652496823551 | validation: 0.6073338863075937]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1188.pth
	Model improved!!!
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6380038465794153		[learning rate: 0.0012067]
	Learning Rate: 0.0012067
	LOSS [training: 0.6380038465794153 | validation: 0.7658859339070502]
	TIME [epoch: 10.5 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6793415579037997		[learning rate: 0.001203]
	Learning Rate: 0.001203
	LOSS [training: 0.6793415579037997 | validation: 0.688767188102004]
	TIME [epoch: 10.5 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6118321194533076		[learning rate: 0.0011993]
	Learning Rate: 0.00119932
	LOSS [training: 0.6118321194533076 | validation: 0.7650808043111682]
	TIME [epoch: 10.5 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7081677579615648		[learning rate: 0.0011956]
	Learning Rate: 0.00119564
	LOSS [training: 0.7081677579615648 | validation: 0.5996171871805156]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1192.pth
	Model improved!!!
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5865885045141156		[learning rate: 0.001192]
	Learning Rate: 0.00119197
	LOSS [training: 0.5865885045141156 | validation: 0.623018012095888]
	TIME [epoch: 10.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.674945281719534		[learning rate: 0.0011883]
	Learning Rate: 0.00118832
	LOSS [training: 0.674945281719534 | validation: 0.645520464621018]
	TIME [epoch: 10.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6421342488980178		[learning rate: 0.0011847]
	Learning Rate: 0.00118468
	LOSS [training: 0.6421342488980178 | validation: 0.671196446268888]
	TIME [epoch: 10.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6256500193189319		[learning rate: 0.001181]
	Learning Rate: 0.00118105
	LOSS [training: 0.6256500193189319 | validation: 0.6135392869032444]
	TIME [epoch: 10.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6259582587315329		[learning rate: 0.0011774]
	Learning Rate: 0.00117743
	LOSS [training: 0.6259582587315329 | validation: 0.9548944121870838]
	TIME [epoch: 10.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7190024947012452		[learning rate: 0.0011738]
	Learning Rate: 0.00117382
	LOSS [training: 0.7190024947012452 | validation: 0.6738066433831006]
	TIME [epoch: 10.5 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7028727565797813		[learning rate: 0.0011702]
	Learning Rate: 0.00117022
	LOSS [training: 0.7028727565797813 | validation: 0.6332008149935012]
	TIME [epoch: 10.5 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6264987024337516		[learning rate: 0.0011666]
	Learning Rate: 0.00116663
	LOSS [training: 0.6264987024337516 | validation: 0.7024062202539285]
	TIME [epoch: 10.5 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7604123092634433		[learning rate: 0.0011631]
	Learning Rate: 0.00116305
	LOSS [training: 0.7604123092634433 | validation: 0.7972211765725901]
	TIME [epoch: 10.5 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6909709333378848		[learning rate: 0.0011595]
	Learning Rate: 0.00115949
	LOSS [training: 0.6909709333378848 | validation: 0.7783272858703458]
	TIME [epoch: 10.5 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.632852880916572		[learning rate: 0.0011559]
	Learning Rate: 0.00115593
	LOSS [training: 0.632852880916572 | validation: 0.7038603251607947]
	TIME [epoch: 10.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6162682543384719		[learning rate: 0.0011524]
	Learning Rate: 0.00115239
	LOSS [training: 0.6162682543384719 | validation: 0.5858449441325628]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1204.pth
	Model improved!!!
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6315007893491374		[learning rate: 0.0011489]
	Learning Rate: 0.00114886
	LOSS [training: 0.6315007893491374 | validation: 0.9243757499331627]
	TIME [epoch: 10.5 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9325880377776568		[learning rate: 0.0011453]
	Learning Rate: 0.00114534
	LOSS [training: 0.9325880377776568 | validation: 0.6992975586057041]
	TIME [epoch: 10.5 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6814375139123043		[learning rate: 0.0011418]
	Learning Rate: 0.00114183
	LOSS [training: 0.6814375139123043 | validation: 0.6344908426566715]
	TIME [epoch: 10.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6737099964872117		[learning rate: 0.0011383]
	Learning Rate: 0.00113833
	LOSS [training: 0.6737099964872117 | validation: 0.7487479656463681]
	TIME [epoch: 10.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.737427893621721		[learning rate: 0.0011348]
	Learning Rate: 0.00113484
	LOSS [training: 0.737427893621721 | validation: 0.7905566497514691]
	TIME [epoch: 10.5 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9210149643214305		[learning rate: 0.0011314]
	Learning Rate: 0.00113136
	LOSS [training: 0.9210149643214305 | validation: 1.1153254533071646]
	TIME [epoch: 10.5 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.055730870106872		[learning rate: 0.0011279]
	Learning Rate: 0.00112789
	LOSS [training: 1.055730870106872 | validation: 0.9822079864186869]
	TIME [epoch: 10.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.832141706774762		[learning rate: 0.0011244]
	Learning Rate: 0.00112443
	LOSS [training: 0.832141706774762 | validation: 0.9324768269327903]
	TIME [epoch: 10.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7694031396189758		[learning rate: 0.001121]
	Learning Rate: 0.00112099
	LOSS [training: 0.7694031396189758 | validation: 0.6053259368517305]
	TIME [epoch: 10.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6110175663167153		[learning rate: 0.0011175]
	Learning Rate: 0.00111755
	LOSS [training: 0.6110175663167153 | validation: 0.6895527981277739]
	TIME [epoch: 10.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7427900533378101		[learning rate: 0.0011141]
	Learning Rate: 0.00111412
	LOSS [training: 0.7427900533378101 | validation: 0.7208215090335542]
	TIME [epoch: 10.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6766783730293439		[learning rate: 0.0011107]
	Learning Rate: 0.00111071
	LOSS [training: 0.6766783730293439 | validation: 0.6809525578712402]
	TIME [epoch: 10.5 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7436505884562499		[learning rate: 0.0011073]
	Learning Rate: 0.0011073
	LOSS [training: 0.7436505884562499 | validation: 0.7673513567733845]
	TIME [epoch: 10.5 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9578821478376043		[learning rate: 0.0011039]
	Learning Rate: 0.00110391
	LOSS [training: 0.9578821478376043 | validation: 0.9652446096558386]
	TIME [epoch: 10.5 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8723464818501793		[learning rate: 0.0011005]
	Learning Rate: 0.00110053
	LOSS [training: 0.8723464818501793 | validation: 0.7753439207150681]
	TIME [epoch: 10.4 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7337328011904698		[learning rate: 0.0010972]
	Learning Rate: 0.00109715
	LOSS [training: 0.7337328011904698 | validation: 0.7949745998426079]
	TIME [epoch: 10.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7017001024483356		[learning rate: 0.0010938]
	Learning Rate: 0.00109379
	LOSS [training: 0.7017001024483356 | validation: 0.7353040937336272]
	TIME [epoch: 10.5 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.695083541851816		[learning rate: 0.0010904]
	Learning Rate: 0.00109044
	LOSS [training: 0.695083541851816 | validation: 0.656701002999028]
	TIME [epoch: 10.5 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6230643552402151		[learning rate: 0.0010871]
	Learning Rate: 0.00108709
	LOSS [training: 0.6230643552402151 | validation: 0.7071880008662129]
	TIME [epoch: 10.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6679764825347655		[learning rate: 0.0010838]
	Learning Rate: 0.00108376
	LOSS [training: 0.6679764825347655 | validation: 0.6317806267879267]
	TIME [epoch: 10.5 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6174245431383893		[learning rate: 0.0010804]
	Learning Rate: 0.00108044
	LOSS [training: 0.6174245431383893 | validation: 0.5829555012838106]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1225.pth
	Model improved!!!
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5854545767536787		[learning rate: 0.0010771]
	Learning Rate: 0.00107713
	LOSS [training: 0.5854545767536787 | validation: 0.688269230512255]
	TIME [epoch: 10.5 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7025053780922474		[learning rate: 0.0010738]
	Learning Rate: 0.00107382
	LOSS [training: 0.7025053780922474 | validation: 0.5799252284243628]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1227.pth
	Model improved!!!
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6256318393684488		[learning rate: 0.0010705]
	Learning Rate: 0.00107053
	LOSS [training: 0.6256318393684488 | validation: 0.7809128411557288]
	TIME [epoch: 10.5 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6400947647023487		[learning rate: 0.0010673]
	Learning Rate: 0.00106725
	LOSS [training: 0.6400947647023487 | validation: 0.6280037990680841]
	TIME [epoch: 10.5 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5813288204216233		[learning rate: 0.001064]
	Learning Rate: 0.00106398
	LOSS [training: 0.5813288204216233 | validation: 0.5702793159768027]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1230.pth
	Model improved!!!
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.589647544887449		[learning rate: 0.0010607]
	Learning Rate: 0.00106072
	LOSS [training: 0.589647544887449 | validation: 0.6149327022399156]
	TIME [epoch: 10.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6035440025779235		[learning rate: 0.0010575]
	Learning Rate: 0.00105747
	LOSS [training: 0.6035440025779235 | validation: 0.6739778136254022]
	TIME [epoch: 10.5 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6660175206302369		[learning rate: 0.0010542]
	Learning Rate: 0.00105422
	LOSS [training: 0.6660175206302369 | validation: 0.7382460658076799]
	TIME [epoch: 10.5 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6131546466770476		[learning rate: 0.001051]
	Learning Rate: 0.00105099
	LOSS [training: 0.6131546466770476 | validation: 0.6251629203565731]
	TIME [epoch: 10.5 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5929566960025625		[learning rate: 0.0010478]
	Learning Rate: 0.00104777
	LOSS [training: 0.5929566960025625 | validation: 0.6152799396693236]
	TIME [epoch: 10.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6057147383274621		[learning rate: 0.0010446]
	Learning Rate: 0.00104456
	LOSS [training: 0.6057147383274621 | validation: 0.6637282281157889]
	TIME [epoch: 10.5 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7196935733309857		[learning rate: 0.0010414]
	Learning Rate: 0.00104136
	LOSS [training: 0.7196935733309857 | validation: 0.625179195066229]
	TIME [epoch: 10.4 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6018324117612249		[learning rate: 0.0010382]
	Learning Rate: 0.00103817
	LOSS [training: 0.6018324117612249 | validation: 0.7300097044763664]
	TIME [epoch: 10.4 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6910091637997725		[learning rate: 0.001035]
	Learning Rate: 0.00103498
	LOSS [training: 0.6910091637997725 | validation: 0.6108734782775712]
	TIME [epoch: 10.5 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.612986132722951		[learning rate: 0.0010318]
	Learning Rate: 0.00103181
	LOSS [training: 0.612986132722951 | validation: 0.6252970090371178]
	TIME [epoch: 10.4 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.621948177829295		[learning rate: 0.0010286]
	Learning Rate: 0.00102865
	LOSS [training: 0.621948177829295 | validation: 0.5941748982527991]
	TIME [epoch: 10.5 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6148017694748023		[learning rate: 0.0010255]
	Learning Rate: 0.00102549
	LOSS [training: 0.6148017694748023 | validation: 0.5921187732445049]
	TIME [epoch: 10.4 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6265186100350503		[learning rate: 0.0010224]
	Learning Rate: 0.00102235
	LOSS [training: 0.6265186100350503 | validation: 0.5977751328287595]
	TIME [epoch: 10.5 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5986138465255929		[learning rate: 0.0010192]
	Learning Rate: 0.00101922
	LOSS [training: 0.5986138465255929 | validation: 0.682953984010222]
	TIME [epoch: 10.4 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5805086752830679		[learning rate: 0.0010161]
	Learning Rate: 0.00101609
	LOSS [training: 0.5805086752830679 | validation: 0.6714474237603766]
	TIME [epoch: 10.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6073120934090407		[learning rate: 0.001013]
	Learning Rate: 0.00101298
	LOSS [training: 0.6073120934090407 | validation: 0.8001753241424714]
	TIME [epoch: 10.5 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6904234419841958		[learning rate: 0.0010099]
	Learning Rate: 0.00100987
	LOSS [training: 0.6904234419841958 | validation: 0.819461000522559]
	TIME [epoch: 10.5 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6862648841921208		[learning rate: 0.0010068]
	Learning Rate: 0.00100678
	LOSS [training: 0.6862648841921208 | validation: 0.7627108822552299]
	TIME [epoch: 10.5 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6996425307795524		[learning rate: 0.0010037]
	Learning Rate: 0.00100369
	LOSS [training: 0.6996425307795524 | validation: 0.7800476987489282]
	TIME [epoch: 10.5 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6569623137369144		[learning rate: 0.0010006]
	Learning Rate: 0.00100061
	LOSS [training: 0.6569623137369144 | validation: 0.658962211591091]
	TIME [epoch: 10.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6243174424679419		[learning rate: 0.00099755]
	Learning Rate: 0.000997547
	LOSS [training: 0.6243174424679419 | validation: 0.7103290405186532]
	TIME [epoch: 10.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6062481728977468		[learning rate: 0.00099449]
	Learning Rate: 0.000994489
	LOSS [training: 0.6062481728977468 | validation: 0.593369957999169]
	TIME [epoch: 10.5 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6659928058092892		[learning rate: 0.00099144]
	Learning Rate: 0.00099144
	LOSS [training: 0.6659928058092892 | validation: 0.5782610427956961]
	TIME [epoch: 10.4 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6130158484599517		[learning rate: 0.0009884]
	Learning Rate: 0.000988401
	LOSS [training: 0.6130158484599517 | validation: 0.6729974007666687]
	TIME [epoch: 10.5 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6342486123529373		[learning rate: 0.00098537]
	Learning Rate: 0.000985371
	LOSS [training: 0.6342486123529373 | validation: 0.6873843382301502]
	TIME [epoch: 10.4 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.606827113927371		[learning rate: 0.00098235]
	Learning Rate: 0.000982351
	LOSS [training: 0.606827113927371 | validation: 0.8012321263765793]
	TIME [epoch: 10.5 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6560919452535601		[learning rate: 0.00097934]
	Learning Rate: 0.000979339
	LOSS [training: 0.6560919452535601 | validation: 0.5963626993424396]
	TIME [epoch: 10.4 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6027653304751233		[learning rate: 0.00097634]
	Learning Rate: 0.000976337
	LOSS [training: 0.6027653304751233 | validation: 0.5689218710665682]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1258.pth
	Model improved!!!
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.571374241878971		[learning rate: 0.00097334]
	Learning Rate: 0.000973345
	LOSS [training: 0.571374241878971 | validation: 0.6077295669044773]
	TIME [epoch: 10.5 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5867914063850681		[learning rate: 0.00097036]
	Learning Rate: 0.000970361
	LOSS [training: 0.5867914063850681 | validation: 0.5916039236554215]
	TIME [epoch: 10.5 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6010893599223912		[learning rate: 0.00096739]
	Learning Rate: 0.000967386
	LOSS [training: 0.6010893599223912 | validation: 0.730496640016984]
	TIME [epoch: 10.5 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.646913429425336		[learning rate: 0.00096442]
	Learning Rate: 0.000964421
	LOSS [training: 0.646913429425336 | validation: 0.6921063392313856]
	TIME [epoch: 10.5 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6102478661566964		[learning rate: 0.00096146]
	Learning Rate: 0.000961464
	LOSS [training: 0.6102478661566964 | validation: 0.5992896611843327]
	TIME [epoch: 10.5 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5882354493214171		[learning rate: 0.00095852]
	Learning Rate: 0.000958517
	LOSS [training: 0.5882354493214171 | validation: 0.6951720185991471]
	TIME [epoch: 10.5 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.640057585876202		[learning rate: 0.00095558]
	Learning Rate: 0.000955579
	LOSS [training: 0.640057585876202 | validation: 0.6909454248332411]
	TIME [epoch: 10.5 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7098648398172509		[learning rate: 0.00095265]
	Learning Rate: 0.00095265
	LOSS [training: 0.7098648398172509 | validation: 0.7592558270902725]
	TIME [epoch: 10.5 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7097362104453416		[learning rate: 0.00094973]
	Learning Rate: 0.00094973
	LOSS [training: 0.7097362104453416 | validation: 0.8000921208357278]
	TIME [epoch: 10.5 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7023179459251939		[learning rate: 0.00094682]
	Learning Rate: 0.000946818
	LOSS [training: 0.7023179459251939 | validation: 0.6715277658179342]
	TIME [epoch: 10.5 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5831474821415493		[learning rate: 0.00094392]
	Learning Rate: 0.000943916
	LOSS [training: 0.5831474821415493 | validation: 0.6638087910315313]
	TIME [epoch: 10.5 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5859222255987262		[learning rate: 0.00094102]
	Learning Rate: 0.000941023
	LOSS [training: 0.5859222255987262 | validation: 0.6928987186659374]
	TIME [epoch: 10.5 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6867932391580949		[learning rate: 0.00093814]
	Learning Rate: 0.000938138
	LOSS [training: 0.6867932391580949 | validation: 0.7811039163761909]
	TIME [epoch: 10.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6556518383947665		[learning rate: 0.00093526]
	Learning Rate: 0.000935262
	LOSS [training: 0.6556518383947665 | validation: 0.9236115687119949]
	TIME [epoch: 10.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7589790542420223		[learning rate: 0.0009324]
	Learning Rate: 0.000932395
	LOSS [training: 0.7589790542420223 | validation: 0.6399134408758268]
	TIME [epoch: 10.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.589313152522398		[learning rate: 0.00092954]
	Learning Rate: 0.000929537
	LOSS [training: 0.589313152522398 | validation: 0.6100876975099884]
	TIME [epoch: 10.5 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6382124647805878		[learning rate: 0.00092669]
	Learning Rate: 0.000926688
	LOSS [training: 0.6382124647805878 | validation: 0.6337354973280589]
	TIME [epoch: 10.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.68150707283389		[learning rate: 0.00092385]
	Learning Rate: 0.000923847
	LOSS [training: 0.68150707283389 | validation: 0.7883491961543306]
	TIME [epoch: 10.5 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7471586869733854		[learning rate: 0.00092101]
	Learning Rate: 0.000921015
	LOSS [training: 0.7471586869733854 | validation: 0.7172830853864249]
	TIME [epoch: 10.5 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6173501189597734		[learning rate: 0.00091819]
	Learning Rate: 0.000918192
	LOSS [training: 0.6173501189597734 | validation: 0.6592383405906721]
	TIME [epoch: 10.5 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5915322045591886		[learning rate: 0.00091538]
	Learning Rate: 0.000915377
	LOSS [training: 0.5915322045591886 | validation: 0.6343991840045411]
	TIME [epoch: 10.5 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6494223672423993		[learning rate: 0.00091257]
	Learning Rate: 0.000912571
	LOSS [training: 0.6494223672423993 | validation: 0.6948973215340607]
	TIME [epoch: 10.5 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6828657017401677		[learning rate: 0.00090977]
	Learning Rate: 0.000909774
	LOSS [training: 0.6828657017401677 | validation: 0.7320879152427354]
	TIME [epoch: 10.5 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6241046898268356		[learning rate: 0.00090698]
	Learning Rate: 0.000906985
	LOSS [training: 0.6241046898268356 | validation: 0.6641151403681016]
	TIME [epoch: 10.5 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5968476832778525		[learning rate: 0.0009042]
	Learning Rate: 0.000904204
	LOSS [training: 0.5968476832778525 | validation: 0.661627251197121]
	TIME [epoch: 10.5 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6089954897802935		[learning rate: 0.00090143]
	Learning Rate: 0.000901433
	LOSS [training: 0.6089954897802935 | validation: 0.8962036100445624]
	TIME [epoch: 10.5 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7570494111527324		[learning rate: 0.00089867]
	Learning Rate: 0.000898669
	LOSS [training: 0.7570494111527324 | validation: 0.8485432389146667]
	TIME [epoch: 10.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6784344262678794		[learning rate: 0.00089591]
	Learning Rate: 0.000895915
	LOSS [training: 0.6784344262678794 | validation: 0.8394371280380684]
	TIME [epoch: 10.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980219422622371		[learning rate: 0.00089317]
	Learning Rate: 0.000893168
	LOSS [training: 0.6980219422622371 | validation: 0.7198447084967532]
	TIME [epoch: 10.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6210140638625224		[learning rate: 0.00089043]
	Learning Rate: 0.00089043
	LOSS [training: 0.6210140638625224 | validation: 0.9060242898687153]
	TIME [epoch: 10.5 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7468061770017278		[learning rate: 0.0008877]
	Learning Rate: 0.000887701
	LOSS [training: 0.7468061770017278 | validation: 1.012784622131866]
	TIME [epoch: 10.5 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7956476372014003		[learning rate: 0.00088498]
	Learning Rate: 0.00088498
	LOSS [training: 0.7956476372014003 | validation: 1.0396549315986567]
	TIME [epoch: 10.5 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7216542279464554		[learning rate: 0.00088227]
	Learning Rate: 0.000882267
	LOSS [training: 0.7216542279464554 | validation: 0.6870248650764978]
	TIME [epoch: 10.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5779874368098998		[learning rate: 0.00087956]
	Learning Rate: 0.000879562
	LOSS [training: 0.5779874368098998 | validation: 0.6683008137070013]
	TIME [epoch: 10.5 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6197465740104212		[learning rate: 0.00087687]
	Learning Rate: 0.000876866
	LOSS [training: 0.6197465740104212 | validation: 0.722449839669842]
	TIME [epoch: 10.5 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5720158550804171		[learning rate: 0.00087418]
	Learning Rate: 0.000874178
	LOSS [training: 0.5720158550804171 | validation: 0.6558431514133127]
	TIME [epoch: 10.5 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5676366895214725		[learning rate: 0.0008715]
	Learning Rate: 0.000871498
	LOSS [training: 0.5676366895214725 | validation: 0.7134225598558986]
	TIME [epoch: 10.5 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5800483993021576		[learning rate: 0.00086883]
	Learning Rate: 0.000868827
	LOSS [training: 0.5800483993021576 | validation: 0.7239403475157267]
	TIME [epoch: 10.5 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6067030071516787		[learning rate: 0.00086616]
	Learning Rate: 0.000866164
	LOSS [training: 0.6067030071516787 | validation: 0.6561350455788519]
	TIME [epoch: 10.5 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6590548565370222		[learning rate: 0.00086351]
	Learning Rate: 0.000863509
	LOSS [training: 0.6590548565370222 | validation: 0.6440248142357103]
	TIME [epoch: 10.5 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6453631037094839		[learning rate: 0.00086086]
	Learning Rate: 0.000860861
	LOSS [training: 0.6453631037094839 | validation: 0.6159679357963779]
	TIME [epoch: 10.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6199097491918815		[learning rate: 0.00085822]
	Learning Rate: 0.000858223
	LOSS [training: 0.6199097491918815 | validation: 0.6581481678056961]
	TIME [epoch: 10.5 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.613871327074418		[learning rate: 0.00085559]
	Learning Rate: 0.000855592
	LOSS [training: 0.613871327074418 | validation: 0.7979411365335666]
	TIME [epoch: 10.5 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6770491474975323		[learning rate: 0.00085297]
	Learning Rate: 0.000852969
	LOSS [training: 0.6770491474975323 | validation: 0.6896732591846991]
	TIME [epoch: 10.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942785102708473		[learning rate: 0.00085035]
	Learning Rate: 0.000850354
	LOSS [training: 0.6942785102708473 | validation: 0.6658617935436937]
	TIME [epoch: 10.5 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6252168492129109		[learning rate: 0.00084775]
	Learning Rate: 0.000847748
	LOSS [training: 0.6252168492129109 | validation: 0.6514866864234432]
	TIME [epoch: 10.5 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5971830144615264		[learning rate: 0.00084515]
	Learning Rate: 0.000845149
	LOSS [training: 0.5971830144615264 | validation: 0.7332149931616573]
	TIME [epoch: 10.5 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6263981907530779		[learning rate: 0.00084256]
	Learning Rate: 0.000842558
	LOSS [training: 0.6263981907530779 | validation: 0.7530264456281006]
	TIME [epoch: 10.5 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6637791856332173		[learning rate: 0.00083998]
	Learning Rate: 0.000839976
	LOSS [training: 0.6637791856332173 | validation: 0.6403566708231844]
	TIME [epoch: 10.4 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6269333326419552		[learning rate: 0.0008374]
	Learning Rate: 0.000837401
	LOSS [training: 0.6269333326419552 | validation: 0.6587064549471308]
	TIME [epoch: 10.5 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5848214666803078		[learning rate: 0.00083483]
	Learning Rate: 0.000834834
	LOSS [training: 0.5848214666803078 | validation: 0.6364150715913159]
	TIME [epoch: 10.5 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6030970331565755		[learning rate: 0.00083227]
	Learning Rate: 0.000832274
	LOSS [training: 0.6030970331565755 | validation: 0.7832473880020581]
	TIME [epoch: 10.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5875147957072386		[learning rate: 0.00082972]
	Learning Rate: 0.000829723
	LOSS [training: 0.5875147957072386 | validation: 0.6762861532705022]
	TIME [epoch: 10.5 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5396611702169262		[learning rate: 0.00082718]
	Learning Rate: 0.00082718
	LOSS [training: 0.5396611702169262 | validation: 0.5843529298833517]
	TIME [epoch: 10.5 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5510253688927295		[learning rate: 0.00082464]
	Learning Rate: 0.000824644
	LOSS [training: 0.5510253688927295 | validation: 0.6372661598502986]
	TIME [epoch: 10.5 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5603692486816095		[learning rate: 0.00082212]
	Learning Rate: 0.000822116
	LOSS [training: 0.5603692486816095 | validation: 0.7101615150705005]
	TIME [epoch: 10.5 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.564522879453646		[learning rate: 0.0008196]
	Learning Rate: 0.000819596
	LOSS [training: 0.564522879453646 | validation: 0.5972528099022896]
	TIME [epoch: 10.5 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5424047491781763		[learning rate: 0.00081708]
	Learning Rate: 0.000817084
	LOSS [training: 0.5424047491781763 | validation: 0.599228336624357]
	TIME [epoch: 10.5 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5546226780979111		[learning rate: 0.00081458]
	Learning Rate: 0.000814579
	LOSS [training: 0.5546226780979111 | validation: 0.6131728186950984]
	TIME [epoch: 10.5 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5928721526086844		[learning rate: 0.00081208]
	Learning Rate: 0.000812082
	LOSS [training: 0.5928721526086844 | validation: 0.5714010924008511]
	TIME [epoch: 10.5 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5860623277589673		[learning rate: 0.00080959]
	Learning Rate: 0.000809593
	LOSS [training: 0.5860623277589673 | validation: 0.646902851235385]
	TIME [epoch: 10.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5727294825014434		[learning rate: 0.00080711]
	Learning Rate: 0.000807111
	LOSS [training: 0.5727294825014434 | validation: 0.7385687435352576]
	TIME [epoch: 10.4 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6125197918819333		[learning rate: 0.00080464]
	Learning Rate: 0.000804637
	LOSS [training: 0.6125197918819333 | validation: 0.6916499278564677]
	TIME [epoch: 10.5 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5871029748843248		[learning rate: 0.00080217]
	Learning Rate: 0.00080217
	LOSS [training: 0.5871029748843248 | validation: 0.6365321871001371]
	TIME [epoch: 10.5 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5393977290219796		[learning rate: 0.00079971]
	Learning Rate: 0.000799712
	LOSS [training: 0.5393977290219796 | validation: 0.5977532677815127]
	TIME [epoch: 10.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5541559417425168		[learning rate: 0.00079726]
	Learning Rate: 0.00079726
	LOSS [training: 0.5541559417425168 | validation: 0.6407748168494886]
	TIME [epoch: 10.5 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5709665996325298		[learning rate: 0.00079482]
	Learning Rate: 0.000794816
	LOSS [training: 0.5709665996325298 | validation: 0.6983014758657201]
	TIME [epoch: 10.5 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.619479976408788		[learning rate: 0.00079238]
	Learning Rate: 0.00079238
	LOSS [training: 0.619479976408788 | validation: 0.7450499370991901]
	TIME [epoch: 10.5 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6026214658359879		[learning rate: 0.00078995]
	Learning Rate: 0.000789951
	LOSS [training: 0.6026214658359879 | validation: 0.600259905026144]
	TIME [epoch: 10.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6365828194057883		[learning rate: 0.00078753]
	Learning Rate: 0.000787529
	LOSS [training: 0.6365828194057883 | validation: 0.6385336716263068]
	TIME [epoch: 10.5 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5963077940489653		[learning rate: 0.00078512]
	Learning Rate: 0.000785115
	LOSS [training: 0.5963077940489653 | validation: 0.649078636969817]
	TIME [epoch: 10.5 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925187784902853		[learning rate: 0.00078271]
	Learning Rate: 0.000782708
	LOSS [training: 0.6925187784902853 | validation: 0.7064133112195242]
	TIME [epoch: 10.5 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6529348818550691		[learning rate: 0.00078031]
	Learning Rate: 0.000780309
	LOSS [training: 0.6529348818550691 | validation: 0.7360810034518137]
	TIME [epoch: 10.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6540851578432425		[learning rate: 0.00077792]
	Learning Rate: 0.000777917
	LOSS [training: 0.6540851578432425 | validation: 0.5947515575096691]
	TIME [epoch: 10.5 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5926577609238686		[learning rate: 0.00077553]
	Learning Rate: 0.000775533
	LOSS [training: 0.5926577609238686 | validation: 0.5867768008156878]
	TIME [epoch: 10.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5739276981609139		[learning rate: 0.00077316]
	Learning Rate: 0.000773155
	LOSS [training: 0.5739276981609139 | validation: 0.5564915139771562]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1334.pth
	Model improved!!!
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5499101797339003		[learning rate: 0.00077079]
	Learning Rate: 0.000770785
	LOSS [training: 0.5499101797339003 | validation: 0.5731235402979091]
	TIME [epoch: 10.4 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5508988295692971		[learning rate: 0.00076842]
	Learning Rate: 0.000768422
	LOSS [training: 0.5508988295692971 | validation: 0.6948771392181959]
	TIME [epoch: 10.5 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5558407503711897		[learning rate: 0.00076607]
	Learning Rate: 0.000766067
	LOSS [training: 0.5558407503711897 | validation: 0.6436065052446648]
	TIME [epoch: 10.5 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5620692022061414		[learning rate: 0.00076372]
	Learning Rate: 0.000763719
	LOSS [training: 0.5620692022061414 | validation: 0.5400567957907227]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1338.pth
	Model improved!!!
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5870042403712369		[learning rate: 0.00076138]
	Learning Rate: 0.000761377
	LOSS [training: 0.5870042403712369 | validation: 0.5869965079405003]
	TIME [epoch: 10.5 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6105649938455141		[learning rate: 0.00075904]
	Learning Rate: 0.000759043
	LOSS [training: 0.6105649938455141 | validation: 0.5452631577709883]
	TIME [epoch: 10.5 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6631511779465378		[learning rate: 0.00075672]
	Learning Rate: 0.000756717
	LOSS [training: 0.6631511779465378 | validation: 0.8076754113152653]
	TIME [epoch: 10.5 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6470063708422066		[learning rate: 0.0007544]
	Learning Rate: 0.000754397
	LOSS [training: 0.6470063708422066 | validation: 0.6982437224844591]
	TIME [epoch: 10.5 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5872891437560698		[learning rate: 0.00075208]
	Learning Rate: 0.000752084
	LOSS [training: 0.5872891437560698 | validation: 0.6978180477362718]
	TIME [epoch: 10.5 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6388668079522217		[learning rate: 0.00074978]
	Learning Rate: 0.000749779
	LOSS [training: 0.6388668079522217 | validation: 0.7628618236817958]
	TIME [epoch: 10.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7178811889836199		[learning rate: 0.00074748]
	Learning Rate: 0.000747481
	LOSS [training: 0.7178811889836199 | validation: 0.8915820136597725]
	TIME [epoch: 10.5 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6273920797441463		[learning rate: 0.00074519]
	Learning Rate: 0.000745189
	LOSS [training: 0.6273920797441463 | validation: 0.7273532980401822]
	TIME [epoch: 10.5 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5560827457688485		[learning rate: 0.0007429]
	Learning Rate: 0.000742905
	LOSS [training: 0.5560827457688485 | validation: 0.6221181009919555]
	TIME [epoch: 10.5 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5443265789103652		[learning rate: 0.00074063]
	Learning Rate: 0.000740628
	LOSS [training: 0.5443265789103652 | validation: 0.652681535441757]
	TIME [epoch: 10.5 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5779730675897043		[learning rate: 0.00073836]
	Learning Rate: 0.000738357
	LOSS [training: 0.5779730675897043 | validation: 0.696949877004287]
	TIME [epoch: 10.5 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5520584141774851		[learning rate: 0.00073609]
	Learning Rate: 0.000736094
	LOSS [training: 0.5520584141774851 | validation: 0.754758641607298]
	TIME [epoch: 10.5 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5526498900825604		[learning rate: 0.00073384]
	Learning Rate: 0.000733838
	LOSS [training: 0.5526498900825604 | validation: 0.776560543499574]
	TIME [epoch: 10.5 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6439127017528974		[learning rate: 0.00073159]
	Learning Rate: 0.000731588
	LOSS [training: 0.6439127017528974 | validation: 0.7281658421310198]
	TIME [epoch: 10.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.630234381740063		[learning rate: 0.00072935]
	Learning Rate: 0.000729345
	LOSS [training: 0.630234381740063 | validation: 0.6429028755614178]
	TIME [epoch: 10.5 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5659904229446711		[learning rate: 0.00072711]
	Learning Rate: 0.00072711
	LOSS [training: 0.5659904229446711 | validation: 0.6374248826178587]
	TIME [epoch: 10.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5491315930544983		[learning rate: 0.00072488]
	Learning Rate: 0.000724881
	LOSS [training: 0.5491315930544983 | validation: 0.7268600702916658]
	TIME [epoch: 10.5 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5917266444832581		[learning rate: 0.00072266]
	Learning Rate: 0.000722659
	LOSS [training: 0.5917266444832581 | validation: 0.7853147791197097]
	TIME [epoch: 10.5 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.645211427054638		[learning rate: 0.00072044]
	Learning Rate: 0.000720444
	LOSS [training: 0.645211427054638 | validation: 0.5913312913798334]
	TIME [epoch: 10.5 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5807386288083667		[learning rate: 0.00071824]
	Learning Rate: 0.000718235
	LOSS [training: 0.5807386288083667 | validation: 0.6227508709368318]
	TIME [epoch: 10.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5656196426536996		[learning rate: 0.00071603]
	Learning Rate: 0.000716033
	LOSS [training: 0.5656196426536996 | validation: 0.6480355110576937]
	TIME [epoch: 10.5 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6008072314174316		[learning rate: 0.00071384]
	Learning Rate: 0.000713839
	LOSS [training: 0.6008072314174316 | validation: 0.7047035846744579]
	TIME [epoch: 10.5 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6377695277749122		[learning rate: 0.00071165]
	Learning Rate: 0.00071165
	LOSS [training: 0.6377695277749122 | validation: 0.7388430271090182]
	TIME [epoch: 10.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.621282517225109		[learning rate: 0.00070947]
	Learning Rate: 0.000709469
	LOSS [training: 0.621282517225109 | validation: 0.7539086216415912]
	TIME [epoch: 10.5 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.653844087880464		[learning rate: 0.00070729]
	Learning Rate: 0.000707294
	LOSS [training: 0.653844087880464 | validation: 0.7968719607203404]
	TIME [epoch: 10.5 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6712519895862485		[learning rate: 0.00070513]
	Learning Rate: 0.000705126
	LOSS [training: 0.6712519895862485 | validation: 0.714302556824689]
	TIME [epoch: 10.5 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5871326218084549		[learning rate: 0.00070296]
	Learning Rate: 0.000702964
	LOSS [training: 0.5871326218084549 | validation: 0.8091635692936376]
	TIME [epoch: 10.5 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6361438226752291		[learning rate: 0.00070081]
	Learning Rate: 0.00070081
	LOSS [training: 0.6361438226752291 | validation: 0.7032245913666796]
	TIME [epoch: 10.5 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5644778503665352		[learning rate: 0.00069866]
	Learning Rate: 0.000698661
	LOSS [training: 0.5644778503665352 | validation: 0.718044866523264]
	TIME [epoch: 10.5 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.574126391585678		[learning rate: 0.00069652]
	Learning Rate: 0.000696519
	LOSS [training: 0.574126391585678 | validation: 0.7688370986193392]
	TIME [epoch: 10.5 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6371715768113402		[learning rate: 0.00069438]
	Learning Rate: 0.000694384
	LOSS [training: 0.6371715768113402 | validation: 1.0081081410979817]
	TIME [epoch: 10.5 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6692628846035327		[learning rate: 0.00069226]
	Learning Rate: 0.000692256
	LOSS [training: 0.6692628846035327 | validation: 0.7582447601545027]
	TIME [epoch: 10.5 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6932263477642414		[learning rate: 0.00069013]
	Learning Rate: 0.000690134
	LOSS [training: 0.6932263477642414 | validation: 0.655687055369956]
	TIME [epoch: 10.5 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5850630998865068		[learning rate: 0.00068802]
	Learning Rate: 0.000688018
	LOSS [training: 0.5850630998865068 | validation: 0.6727073561569051]
	TIME [epoch: 10.5 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5780844900776185		[learning rate: 0.00068591]
	Learning Rate: 0.000685909
	LOSS [training: 0.5780844900776185 | validation: 0.6196125814986486]
	TIME [epoch: 10.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5501250575081096		[learning rate: 0.00068381]
	Learning Rate: 0.000683807
	LOSS [training: 0.5501250575081096 | validation: 0.6997752621543866]
	TIME [epoch: 10.5 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6068994307372149		[learning rate: 0.00068171]
	Learning Rate: 0.000681711
	LOSS [training: 0.6068994307372149 | validation: 0.7007381103112692]
	TIME [epoch: 10.5 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6060791583495827		[learning rate: 0.00067962]
	Learning Rate: 0.000679621
	LOSS [training: 0.6060791583495827 | validation: 0.6680282019660677]
	TIME [epoch: 10.5 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6731948888307308		[learning rate: 0.00067754]
	Learning Rate: 0.000677538
	LOSS [training: 0.6731948888307308 | validation: 0.6675681681304687]
	TIME [epoch: 10.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5742472271752017		[learning rate: 0.00067546]
	Learning Rate: 0.000675461
	LOSS [training: 0.5742472271752017 | validation: 0.5713757361713248]
	TIME [epoch: 10.5 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5385879218459334		[learning rate: 0.00067339]
	Learning Rate: 0.00067339
	LOSS [training: 0.5385879218459334 | validation: 0.7072598462859895]
	TIME [epoch: 10.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6161785073671813		[learning rate: 0.00067133]
	Learning Rate: 0.000671326
	LOSS [training: 0.6161785073671813 | validation: 0.7559048427209888]
	TIME [epoch: 10.5 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6406531354319395		[learning rate: 0.00066927]
	Learning Rate: 0.000669268
	LOSS [training: 0.6406531354319395 | validation: 0.7370650268914866]
	TIME [epoch: 10.5 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6203591797812008		[learning rate: 0.00066722]
	Learning Rate: 0.000667216
	LOSS [training: 0.6203591797812008 | validation: 0.7429537680151366]
	TIME [epoch: 10.4 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6117408574095412		[learning rate: 0.00066517]
	Learning Rate: 0.000665171
	LOSS [training: 0.6117408574095412 | validation: 0.6678337193402161]
	TIME [epoch: 10.5 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6549151144983479		[learning rate: 0.00066313]
	Learning Rate: 0.000663132
	LOSS [training: 0.6549151144983479 | validation: 0.9085906203164912]
	TIME [epoch: 10.4 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6763877459273567		[learning rate: 0.0006611]
	Learning Rate: 0.000661099
	LOSS [training: 0.6763877459273567 | validation: 0.7541232317109503]
	TIME [epoch: 10.5 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6669357048535073		[learning rate: 0.00065907]
	Learning Rate: 0.000659073
	LOSS [training: 0.6669357048535073 | validation: 0.7760012893963234]
	TIME [epoch: 10.5 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6628294416226082		[learning rate: 0.00065705]
	Learning Rate: 0.000657052
	LOSS [training: 0.6628294416226082 | validation: 0.7569829535830277]
	TIME [epoch: 10.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6541043463602527		[learning rate: 0.00065504]
	Learning Rate: 0.000655038
	LOSS [training: 0.6541043463602527 | validation: 0.7453477304086801]
	TIME [epoch: 10.5 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6209026062400258		[learning rate: 0.00065303]
	Learning Rate: 0.00065303
	LOSS [training: 0.6209026062400258 | validation: 0.6813499304260214]
	TIME [epoch: 10.5 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.663610898779678		[learning rate: 0.00065103]
	Learning Rate: 0.000651028
	LOSS [training: 0.663610898779678 | validation: 0.7484005726840204]
	TIME [epoch: 10.5 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6495778040121225		[learning rate: 0.00064903]
	Learning Rate: 0.000649033
	LOSS [training: 0.6495778040121225 | validation: 0.7126428931208729]
	TIME [epoch: 10.5 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6586471029754417		[learning rate: 0.00064704]
	Learning Rate: 0.000647043
	LOSS [training: 0.6586471029754417 | validation: 0.6806371818951676]
	TIME [epoch: 10.5 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976304003066626		[learning rate: 0.00064506]
	Learning Rate: 0.00064506
	LOSS [training: 0.6976304003066626 | validation: 0.7298560180663531]
	TIME [epoch: 10.5 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6237324369998192		[learning rate: 0.00064308]
	Learning Rate: 0.000643082
	LOSS [training: 0.6237324369998192 | validation: 0.5903626767176228]
	TIME [epoch: 10.4 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5962818129338492		[learning rate: 0.00064111]
	Learning Rate: 0.000641111
	LOSS [training: 0.5962818129338492 | validation: 0.6136711356154148]
	TIME [epoch: 10.5 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6389881465727401		[learning rate: 0.00063915]
	Learning Rate: 0.000639146
	LOSS [training: 0.6389881465727401 | validation: 0.6340948390603209]
	TIME [epoch: 10.5 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6459178604564675		[learning rate: 0.00063719]
	Learning Rate: 0.000637187
	LOSS [training: 0.6459178604564675 | validation: 0.5667744039596679]
	TIME [epoch: 10.5 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.60803893777764		[learning rate: 0.00063523]
	Learning Rate: 0.000635233
	LOSS [training: 0.60803893777764 | validation: 0.5806992000315033]
	TIME [epoch: 10.5 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5946688970536431		[learning rate: 0.00063329]
	Learning Rate: 0.000633286
	LOSS [training: 0.5946688970536431 | validation: 0.5232913477986333]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1399.pth
	Model improved!!!
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5515161750979841		[learning rate: 0.00063134]
	Learning Rate: 0.000631345
	LOSS [training: 0.5515161750979841 | validation: 0.5386641099043615]
	TIME [epoch: 10.5 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5445914790758433		[learning rate: 0.00062941]
	Learning Rate: 0.000629409
	LOSS [training: 0.5445914790758433 | validation: 0.5700528908748459]
	TIME [epoch: 10.5 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6158554116756075		[learning rate: 0.00062748]
	Learning Rate: 0.00062748
	LOSS [training: 0.6158554116756075 | validation: 0.5554500020732611]
	TIME [epoch: 10.5 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5449910314435786		[learning rate: 0.00062556]
	Learning Rate: 0.000625557
	LOSS [training: 0.5449910314435786 | validation: 0.5504988090708762]
	TIME [epoch: 10.5 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5462040578291936		[learning rate: 0.00062364]
	Learning Rate: 0.000623639
	LOSS [training: 0.5462040578291936 | validation: 0.6106760947756877]
	TIME [epoch: 10.5 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5797443783619513		[learning rate: 0.00062173]
	Learning Rate: 0.000621727
	LOSS [training: 0.5797443783619513 | validation: 0.6430787573691145]
	TIME [epoch: 10.5 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5501106448574248		[learning rate: 0.00061982]
	Learning Rate: 0.000619821
	LOSS [training: 0.5501106448574248 | validation: 0.6142758539364703]
	TIME [epoch: 10.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5222186918968588		[learning rate: 0.00061792]
	Learning Rate: 0.000617922
	LOSS [training: 0.5222186918968588 | validation: 0.6230528001896477]
	TIME [epoch: 10.5 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5431530890570905		[learning rate: 0.00061603]
	Learning Rate: 0.000616027
	LOSS [training: 0.5431530890570905 | validation: 0.5670575060286115]
	TIME [epoch: 10.5 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5452333188092717		[learning rate: 0.00061414]
	Learning Rate: 0.000614139
	LOSS [training: 0.5452333188092717 | validation: 0.7254175474546466]
	TIME [epoch: 10.5 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6518978657061825		[learning rate: 0.00061226]
	Learning Rate: 0.000612256
	LOSS [training: 0.6518978657061825 | validation: 0.801296986252378]
	TIME [epoch: 10.5 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7091008840952823		[learning rate: 0.00061038]
	Learning Rate: 0.00061038
	LOSS [training: 0.7091008840952823 | validation: 0.7902893721099645]
	TIME [epoch: 10.5 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6433447001949637		[learning rate: 0.00060851]
	Learning Rate: 0.000608509
	LOSS [training: 0.6433447001949637 | validation: 0.7923620625463055]
	TIME [epoch: 10.4 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6057736771294903		[learning rate: 0.00060664]
	Learning Rate: 0.000606643
	LOSS [training: 0.6057736771294903 | validation: 0.6023813521123471]
	TIME [epoch: 10.5 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5423903906937771		[learning rate: 0.00060478]
	Learning Rate: 0.000604784
	LOSS [training: 0.5423903906937771 | validation: 0.6719648314630436]
	TIME [epoch: 10.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5827085600562275		[learning rate: 0.00060293]
	Learning Rate: 0.00060293
	LOSS [training: 0.5827085600562275 | validation: 0.6639948046893247]
	TIME [epoch: 10.5 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5354543324065968		[learning rate: 0.00060108]
	Learning Rate: 0.000601081
	LOSS [training: 0.5354543324065968 | validation: 0.5822656722722723]
	TIME [epoch: 10.5 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5223810290486004		[learning rate: 0.00059924]
	Learning Rate: 0.000599239
	LOSS [training: 0.5223810290486004 | validation: 0.6242884950338126]
	TIME [epoch: 10.5 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5534346428367994		[learning rate: 0.0005974]
	Learning Rate: 0.000597402
	LOSS [training: 0.5534346428367994 | validation: 0.741372142974426]
	TIME [epoch: 10.5 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6031508545104669		[learning rate: 0.00059557]
	Learning Rate: 0.000595571
	LOSS [training: 0.6031508545104669 | validation: 0.6019480733938484]
	TIME [epoch: 10.5 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5291757413752614		[learning rate: 0.00059375]
	Learning Rate: 0.000593745
	LOSS [training: 0.5291757413752614 | validation: 0.5634100710211155]
	TIME [epoch: 10.5 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5271778815406691		[learning rate: 0.00059192]
	Learning Rate: 0.000591925
	LOSS [training: 0.5271778815406691 | validation: 0.578746718084063]
	TIME [epoch: 10.5 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5382086025465234		[learning rate: 0.00059011]
	Learning Rate: 0.00059011
	LOSS [training: 0.5382086025465234 | validation: 0.6585385404024571]
	TIME [epoch: 10.5 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5592247589839725		[learning rate: 0.0005883]
	Learning Rate: 0.000588302
	LOSS [training: 0.5592247589839725 | validation: 0.6572533494176789]
	TIME [epoch: 10.5 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5592986376680267		[learning rate: 0.0005865]
	Learning Rate: 0.000586498
	LOSS [training: 0.5592986376680267 | validation: 0.7058836164631606]
	TIME [epoch: 10.5 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6055432926399077		[learning rate: 0.0005847]
	Learning Rate: 0.0005847
	LOSS [training: 0.6055432926399077 | validation: 0.7612187459439643]
	TIME [epoch: 10.5 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5819636119543444		[learning rate: 0.00058291]
	Learning Rate: 0.000582908
	LOSS [training: 0.5819636119543444 | validation: 0.6976020946563787]
	TIME [epoch: 10.5 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6228548917527329		[learning rate: 0.00058112]
	Learning Rate: 0.000581121
	LOSS [training: 0.6228548917527329 | validation: 0.6189375466797412]
	TIME [epoch: 10.5 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5594523516569374		[learning rate: 0.00057934]
	Learning Rate: 0.00057934
	LOSS [training: 0.5594523516569374 | validation: 0.6745067686094615]
	TIME [epoch: 10.5 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5713394929726185		[learning rate: 0.00057756]
	Learning Rate: 0.000577564
	LOSS [training: 0.5713394929726185 | validation: 0.5841868811204879]
	TIME [epoch: 10.5 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7134894243375377		[learning rate: 0.00057579]
	Learning Rate: 0.000575793
	LOSS [training: 0.7134894243375377 | validation: 0.7622134780166998]
	TIME [epoch: 10.5 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6415666717267599		[learning rate: 0.00057403]
	Learning Rate: 0.000574028
	LOSS [training: 0.6415666717267599 | validation: 0.805373166017874]
	TIME [epoch: 10.5 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6441021392937241		[learning rate: 0.00057227]
	Learning Rate: 0.000572269
	LOSS [training: 0.6441021392937241 | validation: 0.8452676498318694]
	TIME [epoch: 10.5 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5895679038856922		[learning rate: 0.00057051]
	Learning Rate: 0.000570514
	LOSS [training: 0.5895679038856922 | validation: 0.638232336626881]
	TIME [epoch: 10.5 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6169496299360817		[learning rate: 0.00056877]
	Learning Rate: 0.000568766
	LOSS [training: 0.6169496299360817 | validation: 0.646024483343938]
	TIME [epoch: 10.5 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5774452897831103		[learning rate: 0.00056702]
	Learning Rate: 0.000567022
	LOSS [training: 0.5774452897831103 | validation: 0.5817682207656286]
	TIME [epoch: 10.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5752418739471522		[learning rate: 0.00056528]
	Learning Rate: 0.000565284
	LOSS [training: 0.5752418739471522 | validation: 0.5969294985736628]
	TIME [epoch: 10.5 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5541938519258104		[learning rate: 0.00056355]
	Learning Rate: 0.000563551
	LOSS [training: 0.5541938519258104 | validation: 0.6327615416210741]
	TIME [epoch: 10.5 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5931014152695331		[learning rate: 0.00056182]
	Learning Rate: 0.000561824
	LOSS [training: 0.5931014152695331 | validation: 0.5785079379051917]
	TIME [epoch: 10.5 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.563405393501935		[learning rate: 0.0005601]
	Learning Rate: 0.000560101
	LOSS [training: 0.563405393501935 | validation: 0.5926173048035019]
	TIME [epoch: 10.5 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5830505445748877		[learning rate: 0.00055838]
	Learning Rate: 0.000558385
	LOSS [training: 0.5830505445748877 | validation: 0.5632648153176509]
	TIME [epoch: 10.5 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5378051688048158		[learning rate: 0.00055667]
	Learning Rate: 0.000556673
	LOSS [training: 0.5378051688048158 | validation: 0.5889282118061905]
	TIME [epoch: 10.5 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5407898216048401		[learning rate: 0.00055497]
	Learning Rate: 0.000554966
	LOSS [training: 0.5407898216048401 | validation: 0.5795616354722866]
	TIME [epoch: 10.5 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5492238554893508		[learning rate: 0.00055327]
	Learning Rate: 0.000553265
	LOSS [training: 0.5492238554893508 | validation: 0.6147907397205443]
	TIME [epoch: 10.5 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5654951812762264		[learning rate: 0.00055157]
	Learning Rate: 0.000551569
	LOSS [training: 0.5654951812762264 | validation: 0.606891946401416]
	TIME [epoch: 10.5 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5295369408643296		[learning rate: 0.00054988]
	Learning Rate: 0.000549878
	LOSS [training: 0.5295369408643296 | validation: 0.5852721921969278]
	TIME [epoch: 10.5 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5099720452896658		[learning rate: 0.00054819]
	Learning Rate: 0.000548193
	LOSS [training: 0.5099720452896658 | validation: 0.5847603520681577]
	TIME [epoch: 10.5 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5361805245648885		[learning rate: 0.00054651]
	Learning Rate: 0.000546512
	LOSS [training: 0.5361805245648885 | validation: 0.6054006996524833]
	TIME [epoch: 10.5 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5518043927683292		[learning rate: 0.00054484]
	Learning Rate: 0.000544837
	LOSS [training: 0.5518043927683292 | validation: 0.5956545988181823]
	TIME [epoch: 10.5 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5423693623670373		[learning rate: 0.00054317]
	Learning Rate: 0.000543167
	LOSS [training: 0.5423693623670373 | validation: 0.7247083268805897]
	TIME [epoch: 10.5 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6111620643321863		[learning rate: 0.0005415]
	Learning Rate: 0.000541502
	LOSS [training: 0.6111620643321863 | validation: 0.5574411026696885]
	TIME [epoch: 10.5 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.52487226749353		[learning rate: 0.00053984]
	Learning Rate: 0.000539842
	LOSS [training: 0.52487226749353 | validation: 0.5801497634963141]
	TIME [epoch: 10.5 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5678372147110087		[learning rate: 0.00053819]
	Learning Rate: 0.000538187
	LOSS [training: 0.5678372147110087 | validation: 0.6147651809122283]
	TIME [epoch: 10.5 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5766938104305044		[learning rate: 0.00053654]
	Learning Rate: 0.000536537
	LOSS [training: 0.5766938104305044 | validation: 0.5997621825582969]
	TIME [epoch: 10.5 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5879142794411959		[learning rate: 0.00053489]
	Learning Rate: 0.000534893
	LOSS [training: 0.5879142794411959 | validation: 0.6241842033437294]
	TIME [epoch: 10.5 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5193532291200155		[learning rate: 0.00053325]
	Learning Rate: 0.000533253
	LOSS [training: 0.5193532291200155 | validation: 0.5913455964405341]
	TIME [epoch: 10.5 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5365919878159199		[learning rate: 0.00053162]
	Learning Rate: 0.000531618
	LOSS [training: 0.5365919878159199 | validation: 0.5651563753578703]
	TIME [epoch: 10.5 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5566991367419534		[learning rate: 0.00052999]
	Learning Rate: 0.000529989
	LOSS [training: 0.5566991367419534 | validation: 0.643984307447407]
	TIME [epoch: 10.5 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5218921513721745		[learning rate: 0.00052836]
	Learning Rate: 0.000528364
	LOSS [training: 0.5218921513721745 | validation: 0.5779722188229774]
	TIME [epoch: 10.5 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5271139807705728		[learning rate: 0.00052674]
	Learning Rate: 0.000526744
	LOSS [training: 0.5271139807705728 | validation: 0.6314486244327439]
	TIME [epoch: 10.5 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5212728665726223		[learning rate: 0.00052513]
	Learning Rate: 0.00052513
	LOSS [training: 0.5212728665726223 | validation: 0.5857737975249847]
	TIME [epoch: 10.5 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5090939139938789		[learning rate: 0.00052352]
	Learning Rate: 0.00052352
	LOSS [training: 0.5090939139938789 | validation: 0.5980857084461392]
	TIME [epoch: 10.5 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5041902002147787		[learning rate: 0.00052192]
	Learning Rate: 0.000521915
	LOSS [training: 0.5041902002147787 | validation: 0.5513088602258432]
	TIME [epoch: 10.5 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5644050392415312		[learning rate: 0.00052032]
	Learning Rate: 0.000520315
	LOSS [training: 0.5644050392415312 | validation: 0.6912328807534007]
	TIME [epoch: 10.5 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5543746152190779		[learning rate: 0.00051872]
	Learning Rate: 0.00051872
	LOSS [training: 0.5543746152190779 | validation: 0.6701974632297728]
	TIME [epoch: 10.5 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5657835011235438		[learning rate: 0.00051713]
	Learning Rate: 0.00051713
	LOSS [training: 0.5657835011235438 | validation: 0.7715823929257971]
	TIME [epoch: 10.5 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6476000382146289		[learning rate: 0.00051555]
	Learning Rate: 0.000515545
	LOSS [training: 0.6476000382146289 | validation: 0.721664421356781]
	TIME [epoch: 10.5 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5904921971057766		[learning rate: 0.00051396]
	Learning Rate: 0.000513965
	LOSS [training: 0.5904921971057766 | validation: 0.6996700710587354]
	TIME [epoch: 10.5 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.594577552775849		[learning rate: 0.00051239]
	Learning Rate: 0.000512389
	LOSS [training: 0.594577552775849 | validation: 0.7181577291275903]
	TIME [epoch: 10.5 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6138581725121254		[learning rate: 0.00051082]
	Learning Rate: 0.000510818
	LOSS [training: 0.6138581725121254 | validation: 0.7917488506810029]
	TIME [epoch: 10.5 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5719315176375834		[learning rate: 0.00050925]
	Learning Rate: 0.000509253
	LOSS [training: 0.5719315176375834 | validation: 0.7086689397229183]
	TIME [epoch: 10.5 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5941497259928245		[learning rate: 0.00050769]
	Learning Rate: 0.000507692
	LOSS [training: 0.5941497259928245 | validation: 0.722844581706259]
	TIME [epoch: 10.5 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5986634827592896		[learning rate: 0.00050614]
	Learning Rate: 0.000506135
	LOSS [training: 0.5986634827592896 | validation: 0.6626003358499536]
	TIME [epoch: 10.5 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6335334066116775		[learning rate: 0.00050458]
	Learning Rate: 0.000504584
	LOSS [training: 0.6335334066116775 | validation: 0.7026250069690764]
	TIME [epoch: 10.5 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6052092010786059		[learning rate: 0.00050304]
	Learning Rate: 0.000503037
	LOSS [training: 0.6052092010786059 | validation: 0.6851839569468094]
	TIME [epoch: 10.5 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5884018605400427		[learning rate: 0.00050149]
	Learning Rate: 0.000501495
	LOSS [training: 0.5884018605400427 | validation: 0.6505802304251708]
	TIME [epoch: 10.5 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5454028397140798		[learning rate: 0.00049996]
	Learning Rate: 0.000499958
	LOSS [training: 0.5454028397140798 | validation: 0.7143527501078991]
	TIME [epoch: 10.5 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6146646324454309		[learning rate: 0.00049843]
	Learning Rate: 0.000498425
	LOSS [training: 0.6146646324454309 | validation: 0.6036150528347843]
	TIME [epoch: 10.5 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5512055107971273		[learning rate: 0.0004969]
	Learning Rate: 0.000496897
	LOSS [training: 0.5512055107971273 | validation: 0.6931339146284643]
	TIME [epoch: 10.5 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5664162649549507		[learning rate: 0.00049537]
	Learning Rate: 0.000495374
	LOSS [training: 0.5664162649549507 | validation: 0.6897095578097127]
	TIME [epoch: 10.5 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5859808286205805		[learning rate: 0.00049386]
	Learning Rate: 0.000493856
	LOSS [training: 0.5859808286205805 | validation: 0.6314192673358643]
	TIME [epoch: 10.5 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5456512010297706		[learning rate: 0.00049234]
	Learning Rate: 0.000492342
	LOSS [training: 0.5456512010297706 | validation: 0.5950517606702371]
	TIME [epoch: 10.5 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5316832436402057		[learning rate: 0.00049083]
	Learning Rate: 0.000490832
	LOSS [training: 0.5316832436402057 | validation: 0.5645597748465861]
	TIME [epoch: 10.5 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5611417422345111		[learning rate: 0.00048933]
	Learning Rate: 0.000489328
	LOSS [training: 0.5611417422345111 | validation: 0.6428694783916933]
	TIME [epoch: 10.5 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5963048891365912		[learning rate: 0.00048783]
	Learning Rate: 0.000487828
	LOSS [training: 0.5963048891365912 | validation: 0.5614893022126392]
	TIME [epoch: 10.5 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5681451243105458		[learning rate: 0.00048633]
	Learning Rate: 0.000486333
	LOSS [training: 0.5681451243105458 | validation: 0.668501701988601]
	TIME [epoch: 10.5 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6282966529006465		[learning rate: 0.00048484]
	Learning Rate: 0.000484842
	LOSS [training: 0.6282966529006465 | validation: 0.6536187497912513]
	TIME [epoch: 10.5 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6766472961951788		[learning rate: 0.00048336]
	Learning Rate: 0.000483356
	LOSS [training: 0.6766472961951788 | validation: 0.655968714581387]
	TIME [epoch: 10.5 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.609528402625207		[learning rate: 0.00048187]
	Learning Rate: 0.000481874
	LOSS [training: 0.609528402625207 | validation: 0.5767376750139126]
	TIME [epoch: 10.5 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5567473057545476		[learning rate: 0.0004804]
	Learning Rate: 0.000480397
	LOSS [training: 0.5567473057545476 | validation: 0.664058677867765]
	TIME [epoch: 10.5 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5673281970319124		[learning rate: 0.00047892]
	Learning Rate: 0.000478924
	LOSS [training: 0.5673281970319124 | validation: 0.6156734750870755]
	TIME [epoch: 10.5 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.536993464712652		[learning rate: 0.00047746]
	Learning Rate: 0.000477456
	LOSS [training: 0.536993464712652 | validation: 0.6279109373817974]
	TIME [epoch: 10.5 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5405701029847599		[learning rate: 0.00047599]
	Learning Rate: 0.000475992
	LOSS [training: 0.5405701029847599 | validation: 0.6353445866207482]
	TIME [epoch: 10.5 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5452226246579102		[learning rate: 0.00047453]
	Learning Rate: 0.000474533
	LOSS [training: 0.5452226246579102 | validation: 0.6836045941978063]
	TIME [epoch: 10.5 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5258972271204216		[learning rate: 0.00047308]
	Learning Rate: 0.000473079
	LOSS [training: 0.5258972271204216 | validation: 0.6350614684310704]
	TIME [epoch: 10.5 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5775076358105505		[learning rate: 0.00047163]
	Learning Rate: 0.000471628
	LOSS [training: 0.5775076358105505 | validation: 0.6041288554527022]
	TIME [epoch: 10.4 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5203379458721213		[learning rate: 0.00047018]
	Learning Rate: 0.000470183
	LOSS [training: 0.5203379458721213 | validation: 0.5821603720847575]
	TIME [epoch: 10.5 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4985073468812672		[learning rate: 0.00046874]
	Learning Rate: 0.000468741
	LOSS [training: 0.4985073468812672 | validation: 0.6011786770232037]
	TIME [epoch: 10.5 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5313315817478278		[learning rate: 0.0004673]
	Learning Rate: 0.000467304
	LOSS [training: 0.5313315817478278 | validation: 0.6428450799854792]
	TIME [epoch: 10.5 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5379053872641792		[learning rate: 0.00046587]
	Learning Rate: 0.000465872
	LOSS [training: 0.5379053872641792 | validation: 0.6735185416854926]
	TIME [epoch: 10.5 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5386052803768545		[learning rate: 0.00046444]
	Learning Rate: 0.000464444
	LOSS [training: 0.5386052803768545 | validation: 0.7320196253834337]
	TIME [epoch: 10.5 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5367808126440354		[learning rate: 0.00046302]
	Learning Rate: 0.00046302
	LOSS [training: 0.5367808126440354 | validation: 0.6013368606612287]
	TIME [epoch: 10.5 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5086221830654012		[learning rate: 0.0004616]
	Learning Rate: 0.000461601
	LOSS [training: 0.5086221830654012 | validation: 0.6187269541016136]
	TIME [epoch: 10.5 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5186177452839683		[learning rate: 0.00046019]
	Learning Rate: 0.000460186
	LOSS [training: 0.5186177452839683 | validation: 0.5970982701503096]
	TIME [epoch: 10.5 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5311424961909204		[learning rate: 0.00045878]
	Learning Rate: 0.000458775
	LOSS [training: 0.5311424961909204 | validation: 0.6330946180936949]
	TIME [epoch: 10.5 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.592144234054209		[learning rate: 0.00045737]
	Learning Rate: 0.000457369
	LOSS [training: 0.592144234054209 | validation: 0.7342571136369037]
	TIME [epoch: 10.5 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5914014069257059		[learning rate: 0.00045597]
	Learning Rate: 0.000455967
	LOSS [training: 0.5914014069257059 | validation: 0.758205215595631]
	TIME [epoch: 10.5 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5441355924475062		[learning rate: 0.00045457]
	Learning Rate: 0.000454569
	LOSS [training: 0.5441355924475062 | validation: 0.6848799787734124]
	TIME [epoch: 10.5 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5534036901291524		[learning rate: 0.00045318]
	Learning Rate: 0.000453176
	LOSS [training: 0.5534036901291524 | validation: 0.6374530231620982]
	TIME [epoch: 10.5 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5158152246716156		[learning rate: 0.00045179]
	Learning Rate: 0.000451787
	LOSS [training: 0.5158152246716156 | validation: 0.6668116659873661]
	TIME [epoch: 10.5 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5182870549985945		[learning rate: 0.0004504]
	Learning Rate: 0.000450402
	LOSS [training: 0.5182870549985945 | validation: 0.6398130953855484]
	TIME [epoch: 10.5 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5309541483382823		[learning rate: 0.00044902]
	Learning Rate: 0.000449021
	LOSS [training: 0.5309541483382823 | validation: 0.7326322686931118]
	TIME [epoch: 10.5 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5735651421952588		[learning rate: 0.00044764]
	Learning Rate: 0.000447645
	LOSS [training: 0.5735651421952588 | validation: 0.7239338025306394]
	TIME [epoch: 10.5 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.550456062396453		[learning rate: 0.00044627]
	Learning Rate: 0.000446272
	LOSS [training: 0.550456062396453 | validation: 0.6404024530677256]
	TIME [epoch: 10.5 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5191225339700261		[learning rate: 0.0004449]
	Learning Rate: 0.000444904
	LOSS [training: 0.5191225339700261 | validation: 0.6478150562241467]
	TIME [epoch: 10.5 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6317033181011079		[learning rate: 0.00044354]
	Learning Rate: 0.000443541
	LOSS [training: 0.6317033181011079 | validation: 0.7233442127539036]
	TIME [epoch: 10.5 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5921298079168484		[learning rate: 0.00044218]
	Learning Rate: 0.000442181
	LOSS [training: 0.5921298079168484 | validation: 0.6400542653229105]
	TIME [epoch: 10.5 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5604665919141225		[learning rate: 0.00044083]
	Learning Rate: 0.000440825
	LOSS [training: 0.5604665919141225 | validation: 0.723573695436713]
	TIME [epoch: 10.4 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5904003959998503		[learning rate: 0.00043947]
	Learning Rate: 0.000439474
	LOSS [training: 0.5904003959998503 | validation: 0.751872433696398]
	TIME [epoch: 10.5 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.567453116531172		[learning rate: 0.00043813]
	Learning Rate: 0.000438127
	LOSS [training: 0.567453116531172 | validation: 0.7506633166245874]
	TIME [epoch: 10.5 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5635978230915304		[learning rate: 0.00043678]
	Learning Rate: 0.000436784
	LOSS [training: 0.5635978230915304 | validation: 0.7902282772622607]
	TIME [epoch: 10.5 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5526531536693355		[learning rate: 0.00043544]
	Learning Rate: 0.000435445
	LOSS [training: 0.5526531536693355 | validation: 0.6815902521946332]
	TIME [epoch: 10.5 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5324995139815644		[learning rate: 0.00043411]
	Learning Rate: 0.00043411
	LOSS [training: 0.5324995139815644 | validation: 0.6689859253896328]
	TIME [epoch: 10.5 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5227434294912241		[learning rate: 0.00043278]
	Learning Rate: 0.00043278
	LOSS [training: 0.5227434294912241 | validation: 0.6204440350174977]
	TIME [epoch: 10.5 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5080779783221361		[learning rate: 0.00043145]
	Learning Rate: 0.000431453
	LOSS [training: 0.5080779783221361 | validation: 0.6071232118038583]
	TIME [epoch: 10.5 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5034619309558097		[learning rate: 0.00043013]
	Learning Rate: 0.00043013
	LOSS [training: 0.5034619309558097 | validation: 0.5970083937719779]
	TIME [epoch: 10.4 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5374921477424128		[learning rate: 0.00042881]
	Learning Rate: 0.000428812
	LOSS [training: 0.5374921477424128 | validation: 0.6184700893537572]
	TIME [epoch: 10.5 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5339892270149831		[learning rate: 0.0004275]
	Learning Rate: 0.000427497
	LOSS [training: 0.5339892270149831 | validation: 0.6505095613323378]
	TIME [epoch: 10.5 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5285972769867592		[learning rate: 0.00042619]
	Learning Rate: 0.000426187
	LOSS [training: 0.5285972769867592 | validation: 0.6746830761061695]
	TIME [epoch: 10.5 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.519381525953358		[learning rate: 0.00042488]
	Learning Rate: 0.00042488
	LOSS [training: 0.519381525953358 | validation: 0.6578113416343194]
	TIME [epoch: 10.5 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5240776953666575		[learning rate: 0.00042358]
	Learning Rate: 0.000423578
	LOSS [training: 0.5240776953666575 | validation: 0.667510509595576]
	TIME [epoch: 10.5 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.55125964030626		[learning rate: 0.00042228]
	Learning Rate: 0.000422279
	LOSS [training: 0.55125964030626 | validation: 0.610642296292984]
	TIME [epoch: 10.5 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5323391563223323		[learning rate: 0.00042098]
	Learning Rate: 0.000420985
	LOSS [training: 0.5323391563223323 | validation: 0.690172119696787]
	TIME [epoch: 10.5 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5678218264276845		[learning rate: 0.00041969]
	Learning Rate: 0.000419694
	LOSS [training: 0.5678218264276845 | validation: 0.6650307038231027]
	TIME [epoch: 10.5 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5715368964399599		[learning rate: 0.00041841]
	Learning Rate: 0.000418408
	LOSS [training: 0.5715368964399599 | validation: 0.8151908679701145]
	TIME [epoch: 10.5 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6016037754947519		[learning rate: 0.00041713]
	Learning Rate: 0.000417125
	LOSS [training: 0.6016037754947519 | validation: 0.7249393164750453]
	TIME [epoch: 10.5 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.580378252395735		[learning rate: 0.00041585]
	Learning Rate: 0.000415847
	LOSS [training: 0.580378252395735 | validation: 0.7139464123311825]
	TIME [epoch: 10.5 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5637695077889551		[learning rate: 0.00041457]
	Learning Rate: 0.000414572
	LOSS [training: 0.5637695077889551 | validation: 0.6611038791157863]
	TIME [epoch: 10.5 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5839366698010323		[learning rate: 0.0004133]
	Learning Rate: 0.000413301
	LOSS [training: 0.5839366698010323 | validation: 0.7141058997408574]
	TIME [epoch: 10.5 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6128646680433177		[learning rate: 0.00041203]
	Learning Rate: 0.000412034
	LOSS [training: 0.6128646680433177 | validation: 0.6978563354130398]
	TIME [epoch: 10.5 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5952354688937168		[learning rate: 0.00041077]
	Learning Rate: 0.000410771
	LOSS [training: 0.5952354688937168 | validation: 0.8026243999790199]
	TIME [epoch: 10.5 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6293207297293114		[learning rate: 0.00040951]
	Learning Rate: 0.000409512
	LOSS [training: 0.6293207297293114 | validation: 0.7784338739174333]
	TIME [epoch: 10.5 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.579329980006764		[learning rate: 0.00040826]
	Learning Rate: 0.000408257
	LOSS [training: 0.579329980006764 | validation: 0.6683850701535383]
	TIME [epoch: 10.4 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5409952711533164		[learning rate: 0.00040701]
	Learning Rate: 0.000407005
	LOSS [training: 0.5409952711533164 | validation: 0.6607698959991926]
	TIME [epoch: 10.5 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5618624897358052		[learning rate: 0.00040576]
	Learning Rate: 0.000405758
	LOSS [training: 0.5618624897358052 | validation: 0.6684753012780408]
	TIME [epoch: 10.5 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.553744363027309		[learning rate: 0.00040451]
	Learning Rate: 0.000404514
	LOSS [training: 0.553744363027309 | validation: 0.6672606398286621]
	TIME [epoch: 10.4 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5345778235589547		[learning rate: 0.00040327]
	Learning Rate: 0.000403274
	LOSS [training: 0.5345778235589547 | validation: 0.6260706827847143]
	TIME [epoch: 10.5 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4977371850524187		[learning rate: 0.00040204]
	Learning Rate: 0.000402038
	LOSS [training: 0.4977371850524187 | validation: 0.6085108894728076]
	TIME [epoch: 10.5 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4851247041655574		[learning rate: 0.00040081]
	Learning Rate: 0.000400805
	LOSS [training: 0.4851247041655574 | validation: 0.5767003245150244]
	TIME [epoch: 10.5 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48544427979063987		[learning rate: 0.00039958]
	Learning Rate: 0.000399577
	LOSS [training: 0.48544427979063987 | validation: 0.581993904709958]
	TIME [epoch: 10.5 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5532159508543668		[learning rate: 0.00039835]
	Learning Rate: 0.000398352
	LOSS [training: 0.5532159508543668 | validation: 0.7150809663137448]
	TIME [epoch: 10.5 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5707429067276532		[learning rate: 0.00039713]
	Learning Rate: 0.000397131
	LOSS [training: 0.5707429067276532 | validation: 0.6381376590368987]
	TIME [epoch: 10.5 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5185789549967703		[learning rate: 0.00039591]
	Learning Rate: 0.000395913
	LOSS [training: 0.5185789549967703 | validation: 0.5653200565655015]
	TIME [epoch: 10.5 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5173888493799524		[learning rate: 0.0003947]
	Learning Rate: 0.0003947
	LOSS [training: 0.5173888493799524 | validation: 0.5573618205333978]
	TIME [epoch: 10.5 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5333148077072392		[learning rate: 0.00039349]
	Learning Rate: 0.00039349
	LOSS [training: 0.5333148077072392 | validation: 0.5945353134367793]
	TIME [epoch: 10.4 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5047687695579983		[learning rate: 0.00039228]
	Learning Rate: 0.000392283
	LOSS [training: 0.5047687695579983 | validation: 0.5779147037864544]
	TIME [epoch: 10.5 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47699883578587626		[learning rate: 0.00039108]
	Learning Rate: 0.000391081
	LOSS [training: 0.47699883578587626 | validation: 0.5675240446321198]
	TIME [epoch: 10.5 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4710606870546932		[learning rate: 0.00038988]
	Learning Rate: 0.000389882
	LOSS [training: 0.4710606870546932 | validation: 0.5881457171420487]
	TIME [epoch: 10.5 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4870276776126832		[learning rate: 0.00038869]
	Learning Rate: 0.000388687
	LOSS [training: 0.4870276776126832 | validation: 0.5888078020964908]
	TIME [epoch: 10.5 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4869763216357854		[learning rate: 0.0003875]
	Learning Rate: 0.000387495
	LOSS [training: 0.4869763216357854 | validation: 0.5826005948006345]
	TIME [epoch: 10.5 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4872764940622444		[learning rate: 0.00038631]
	Learning Rate: 0.000386308
	LOSS [training: 0.4872764940622444 | validation: 0.6298001608952982]
	TIME [epoch: 10.5 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5036899858109869		[learning rate: 0.00038512]
	Learning Rate: 0.000385123
	LOSS [training: 0.5036899858109869 | validation: 0.5846715841362558]
	TIME [epoch: 10.5 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5129793725658665		[learning rate: 0.00038394]
	Learning Rate: 0.000383943
	LOSS [training: 0.5129793725658665 | validation: 0.5985046747184327]
	TIME [epoch: 10.5 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5317781986945562		[learning rate: 0.00038277]
	Learning Rate: 0.000382766
	LOSS [training: 0.5317781986945562 | validation: 0.6695434448414759]
	TIME [epoch: 10.4 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5557918655122238		[learning rate: 0.00038159]
	Learning Rate: 0.000381593
	LOSS [training: 0.5557918655122238 | validation: 0.6595325309330968]
	TIME [epoch: 10.4 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5367934136860297		[learning rate: 0.00038042]
	Learning Rate: 0.000380423
	LOSS [training: 0.5367934136860297 | validation: 0.6656417991385767]
	TIME [epoch: 10.5 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5151870353922994		[learning rate: 0.00037926]
	Learning Rate: 0.000379257
	LOSS [training: 0.5151870353922994 | validation: 0.663113141562844]
	TIME [epoch: 10.4 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5028245067519713		[learning rate: 0.00037809]
	Learning Rate: 0.000378094
	LOSS [training: 0.5028245067519713 | validation: 0.6564619022725614]
	TIME [epoch: 10.5 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5068093067495449		[learning rate: 0.00037694]
	Learning Rate: 0.000376935
	LOSS [training: 0.5068093067495449 | validation: 0.6420948823733058]
	TIME [epoch: 10.5 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5232113851990791		[learning rate: 0.00037578]
	Learning Rate: 0.00037578
	LOSS [training: 0.5232113851990791 | validation: 0.668422509107857]
	TIME [epoch: 10.5 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5259933083322642		[learning rate: 0.00037463]
	Learning Rate: 0.000374628
	LOSS [training: 0.5259933083322642 | validation: 0.6267781446652887]
	TIME [epoch: 10.4 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5035431837526096		[learning rate: 0.00037348]
	Learning Rate: 0.000373479
	LOSS [training: 0.5035431837526096 | validation: 0.6224290534720727]
	TIME [epoch: 10.5 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.528977952093744		[learning rate: 0.00037233]
	Learning Rate: 0.000372335
	LOSS [training: 0.528977952093744 | validation: 0.6288119756810441]
	TIME [epoch: 10.5 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5394551448470775		[learning rate: 0.00037119]
	Learning Rate: 0.000371193
	LOSS [training: 0.5394551448470775 | validation: 0.6102298424178177]
	TIME [epoch: 10.5 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5077649224139659		[learning rate: 0.00037006]
	Learning Rate: 0.000370055
	LOSS [training: 0.5077649224139659 | validation: 0.6074381210460655]
	TIME [epoch: 10.4 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4848186460984111		[learning rate: 0.00036892]
	Learning Rate: 0.000368921
	LOSS [training: 0.4848186460984111 | validation: 0.5672665334350044]
	TIME [epoch: 10.5 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4757884227595294		[learning rate: 0.00036779]
	Learning Rate: 0.00036779
	LOSS [training: 0.4757884227595294 | validation: 0.5669295440748884]
	TIME [epoch: 10.5 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4771006128944921		[learning rate: 0.00036666]
	Learning Rate: 0.000366663
	LOSS [training: 0.4771006128944921 | validation: 0.5903124335969404]
	TIME [epoch: 10.5 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5016598002597018		[learning rate: 0.00036554]
	Learning Rate: 0.000365539
	LOSS [training: 0.5016598002597018 | validation: 0.5607501140351644]
	TIME [epoch: 10.4 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4844762550709375		[learning rate: 0.00036442]
	Learning Rate: 0.000364418
	LOSS [training: 0.4844762550709375 | validation: 0.5564616690311658]
	TIME [epoch: 10.5 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47863681471505626		[learning rate: 0.0003633]
	Learning Rate: 0.000363301
	LOSS [training: 0.47863681471505626 | validation: 0.5791812669683302]
	TIME [epoch: 10.5 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4905679080992364		[learning rate: 0.00036219]
	Learning Rate: 0.000362187
	LOSS [training: 0.4905679080992364 | validation: 0.5598155752074642]
	TIME [epoch: 10.4 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4938293725358003		[learning rate: 0.00036108]
	Learning Rate: 0.000361077
	LOSS [training: 0.4938293725358003 | validation: 0.5581090241613955]
	TIME [epoch: 10.4 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5055815377903061		[learning rate: 0.00035997]
	Learning Rate: 0.00035997
	LOSS [training: 0.5055815377903061 | validation: 0.5869189744084811]
	TIME [epoch: 10.5 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5454291024141191		[learning rate: 0.00035887]
	Learning Rate: 0.000358867
	LOSS [training: 0.5454291024141191 | validation: 0.5607613193636486]
	TIME [epoch: 10.5 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4982108345045594		[learning rate: 0.00035777]
	Learning Rate: 0.000357767
	LOSS [training: 0.4982108345045594 | validation: 0.6165388280308747]
	TIME [epoch: 10.5 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5052476916206794		[learning rate: 0.00035667]
	Learning Rate: 0.00035667
	LOSS [training: 0.5052476916206794 | validation: 0.5702643630672184]
	TIME [epoch: 10.5 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5439360979722224		[learning rate: 0.00035558]
	Learning Rate: 0.000355577
	LOSS [training: 0.5439360979722224 | validation: 0.5914406009471742]
	TIME [epoch: 10.5 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5251157851735575		[learning rate: 0.00035449]
	Learning Rate: 0.000354487
	LOSS [training: 0.5251157851735575 | validation: 0.626092306011382]
	TIME [epoch: 10.4 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5558819119397828		[learning rate: 0.0003534]
	Learning Rate: 0.0003534
	LOSS [training: 0.5558819119397828 | validation: 0.6279929368208571]
	TIME [epoch: 10.5 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5404639734217951		[learning rate: 0.00035232]
	Learning Rate: 0.000352317
	LOSS [training: 0.5404639734217951 | validation: 0.6200809979512438]
	TIME [epoch: 10.5 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5558334490308043		[learning rate: 0.00035124]
	Learning Rate: 0.000351237
	LOSS [training: 0.5558334490308043 | validation: 0.6468814434088636]
	TIME [epoch: 10.5 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5569657964566639		[learning rate: 0.00035016]
	Learning Rate: 0.00035016
	LOSS [training: 0.5569657964566639 | validation: 0.6301406707905644]
	TIME [epoch: 10.5 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5351016322728313		[learning rate: 0.00034909]
	Learning Rate: 0.000349087
	LOSS [training: 0.5351016322728313 | validation: 0.6401625039280266]
	TIME [epoch: 10.5 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5118974177231691		[learning rate: 0.00034802]
	Learning Rate: 0.000348017
	LOSS [training: 0.5118974177231691 | validation: 0.5768605367108808]
	TIME [epoch: 10.5 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.537585906989414		[learning rate: 0.00034695]
	Learning Rate: 0.00034695
	LOSS [training: 0.537585906989414 | validation: 0.6580609000761273]
	TIME [epoch: 10.5 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5353200861323726		[learning rate: 0.00034589]
	Learning Rate: 0.000345886
	LOSS [training: 0.5353200861323726 | validation: 0.6732325210885316]
	TIME [epoch: 10.5 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5514988021342234		[learning rate: 0.00034483]
	Learning Rate: 0.000344826
	LOSS [training: 0.5514988021342234 | validation: 0.7296662300448611]
	TIME [epoch: 10.5 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5800233157672885		[learning rate: 0.00034377]
	Learning Rate: 0.000343769
	LOSS [training: 0.5800233157672885 | validation: 0.6931993213824541]
	TIME [epoch: 10.5 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5626196308570596		[learning rate: 0.00034272]
	Learning Rate: 0.000342715
	LOSS [training: 0.5626196308570596 | validation: 0.6645786177879848]
	TIME [epoch: 10.5 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.609273922939265		[learning rate: 0.00034166]
	Learning Rate: 0.000341665
	LOSS [training: 0.609273922939265 | validation: 0.6685728790106031]
	TIME [epoch: 10.5 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5709973466445424		[learning rate: 0.00034062]
	Learning Rate: 0.000340617
	LOSS [training: 0.5709973466445424 | validation: 0.6524006819473729]
	TIME [epoch: 10.5 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.574942704657923		[learning rate: 0.00033957]
	Learning Rate: 0.000339573
	LOSS [training: 0.574942704657923 | validation: 0.653224039138522]
	TIME [epoch: 10.5 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5843891329091024		[learning rate: 0.00033853]
	Learning Rate: 0.000338532
	LOSS [training: 0.5843891329091024 | validation: 0.694556415229634]
	TIME [epoch: 10.5 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5637583587128572		[learning rate: 0.00033749]
	Learning Rate: 0.000337494
	LOSS [training: 0.5637583587128572 | validation: 0.6320683328832015]
	TIME [epoch: 10.4 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.588391827835882		[learning rate: 0.00033646]
	Learning Rate: 0.00033646
	LOSS [training: 0.588391827835882 | validation: 0.7496886025087507]
	TIME [epoch: 10.5 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6116006328086815		[learning rate: 0.00033543]
	Learning Rate: 0.000335428
	LOSS [training: 0.6116006328086815 | validation: 0.7070315464801206]
	TIME [epoch: 10.5 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.559268663396192		[learning rate: 0.0003344]
	Learning Rate: 0.0003344
	LOSS [training: 0.559268663396192 | validation: 0.632124074358199]
	TIME [epoch: 10.5 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5383001538377086		[learning rate: 0.00033338]
	Learning Rate: 0.000333375
	LOSS [training: 0.5383001538377086 | validation: 0.6320433475353698]
	TIME [epoch: 10.4 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5311334974398112		[learning rate: 0.00033235]
	Learning Rate: 0.000332353
	LOSS [training: 0.5311334974398112 | validation: 0.6304356864743818]
	TIME [epoch: 10.5 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5001985116897546		[learning rate: 0.00033133]
	Learning Rate: 0.000331334
	LOSS [training: 0.5001985116897546 | validation: 0.5462503940002188]
	TIME [epoch: 10.5 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5008145720679223		[learning rate: 0.00033032]
	Learning Rate: 0.000330319
	LOSS [training: 0.5008145720679223 | validation: 0.5566235513945845]
	TIME [epoch: 10.4 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48328472074095935		[learning rate: 0.00032931]
	Learning Rate: 0.000329306
	LOSS [training: 0.48328472074095935 | validation: 0.5674303686156094]
	TIME [epoch: 10.5 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4855991173171237		[learning rate: 0.0003283]
	Learning Rate: 0.000328297
	LOSS [training: 0.4855991173171237 | validation: 0.5651412093903301]
	TIME [epoch: 10.5 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46794176375365665		[learning rate: 0.00032729]
	Learning Rate: 0.00032729
	LOSS [training: 0.46794176375365665 | validation: 0.5745719199528285]
	TIME [epoch: 10.4 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.495323993260737		[learning rate: 0.00032629]
	Learning Rate: 0.000326287
	LOSS [training: 0.495323993260737 | validation: 0.6086190603132129]
	TIME [epoch: 10.4 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5271371456760587		[learning rate: 0.00032529]
	Learning Rate: 0.000325287
	LOSS [training: 0.5271371456760587 | validation: 0.6040428103509246]
	TIME [epoch: 10.5 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5062827998071128		[learning rate: 0.00032429]
	Learning Rate: 0.00032429
	LOSS [training: 0.5062827998071128 | validation: 0.5944913197888178]
	TIME [epoch: 10.4 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4778255693458268		[learning rate: 0.0003233]
	Learning Rate: 0.000323296
	LOSS [training: 0.4778255693458268 | validation: 0.5953297851425452]
	TIME [epoch: 10.4 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4864976062476785		[learning rate: 0.0003223]
	Learning Rate: 0.000322305
	LOSS [training: 0.4864976062476785 | validation: 0.5874697880075843]
	TIME [epoch: 10.5 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5072475072143839		[learning rate: 0.00032132]
	Learning Rate: 0.000321317
	LOSS [training: 0.5072475072143839 | validation: 0.6272864306590062]
	TIME [epoch: 10.5 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5054561199046931		[learning rate: 0.00032033]
	Learning Rate: 0.000320332
	LOSS [training: 0.5054561199046931 | validation: 0.6237443497160525]
	TIME [epoch: 10.4 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4932502495086453		[learning rate: 0.00031935]
	Learning Rate: 0.00031935
	LOSS [training: 0.4932502495086453 | validation: 0.6431558152138146]
	TIME [epoch: 10.5 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5341486521850879		[learning rate: 0.00031837]
	Learning Rate: 0.000318371
	LOSS [training: 0.5341486521850879 | validation: 0.6084433623335176]
	TIME [epoch: 10.5 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5227126649672968		[learning rate: 0.00031739]
	Learning Rate: 0.000317395
	LOSS [training: 0.5227126649672968 | validation: 0.6053315884289916]
	TIME [epoch: 10.5 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5433587633851159		[learning rate: 0.00031642]
	Learning Rate: 0.000316422
	LOSS [training: 0.5433587633851159 | validation: 0.5524753548440465]
	TIME [epoch: 10.4 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5009851849502553		[learning rate: 0.00031545]
	Learning Rate: 0.000315452
	LOSS [training: 0.5009851849502553 | validation: 0.5983097458848388]
	TIME [epoch: 10.5 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5340251686236221		[learning rate: 0.00031449]
	Learning Rate: 0.000314485
	LOSS [training: 0.5340251686236221 | validation: 0.5881127323582507]
	TIME [epoch: 10.5 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4913022249598738		[learning rate: 0.00031352]
	Learning Rate: 0.000313521
	LOSS [training: 0.4913022249598738 | validation: 0.5439867264960279]
	TIME [epoch: 10.4 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4808747162292696		[learning rate: 0.00031256]
	Learning Rate: 0.00031256
	LOSS [training: 0.4808747162292696 | validation: 0.5680220971387313]
	TIME [epoch: 10.5 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48215484219353116		[learning rate: 0.0003116]
	Learning Rate: 0.000311602
	LOSS [training: 0.48215484219353116 | validation: 0.5691900421454902]
	TIME [epoch: 10.5 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5026421695680671		[learning rate: 0.00031065]
	Learning Rate: 0.000310647
	LOSS [training: 0.5026421695680671 | validation: 0.5870121537945313]
	TIME [epoch: 10.5 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49592440983959907		[learning rate: 0.00030969]
	Learning Rate: 0.000309694
	LOSS [training: 0.49592440983959907 | validation: 0.5417941788236353]
	TIME [epoch: 10.5 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5140628044207631		[learning rate: 0.00030874]
	Learning Rate: 0.000308745
	LOSS [training: 0.5140628044207631 | validation: 0.5484249543480894]
	TIME [epoch: 10.5 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5000895355723652		[learning rate: 0.0003078]
	Learning Rate: 0.000307799
	LOSS [training: 0.5000895355723652 | validation: 0.627623528018148]
	TIME [epoch: 10.5 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49385747761076837		[learning rate: 0.00030686]
	Learning Rate: 0.000306855
	LOSS [training: 0.49385747761076837 | validation: 0.5972489449981555]
	TIME [epoch: 10.4 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5291767972531696		[learning rate: 0.00030591]
	Learning Rate: 0.000305914
	LOSS [training: 0.5291767972531696 | validation: 0.5925240308815419]
	TIME [epoch: 10.5 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5216831696375711		[learning rate: 0.00030498]
	Learning Rate: 0.000304977
	LOSS [training: 0.5216831696375711 | validation: 0.620110706982765]
	TIME [epoch: 10.5 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5205992421979976		[learning rate: 0.00030404]
	Learning Rate: 0.000304042
	LOSS [training: 0.5205992421979976 | validation: 0.5934270109994892]
	TIME [epoch: 10.5 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5138987628816538		[learning rate: 0.00030311]
	Learning Rate: 0.00030311
	LOSS [training: 0.5138987628816538 | validation: 0.5756928391881492]
	TIME [epoch: 10.4 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.510311297918169		[learning rate: 0.00030218]
	Learning Rate: 0.000302181
	LOSS [training: 0.510311297918169 | validation: 0.6154935122881696]
	TIME [epoch: 10.5 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5070591214204452		[learning rate: 0.00030125]
	Learning Rate: 0.000301254
	LOSS [training: 0.5070591214204452 | validation: 0.6527070039648969]
	TIME [epoch: 10.5 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5220396130738046		[learning rate: 0.00030033]
	Learning Rate: 0.000300331
	LOSS [training: 0.5220396130738046 | validation: 0.5934527507401052]
	TIME [epoch: 10.5 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5028140422528231		[learning rate: 0.00029941]
	Learning Rate: 0.00029941
	LOSS [training: 0.5028140422528231 | validation: 0.6156701278067476]
	TIME [epoch: 10.5 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.514646280987228		[learning rate: 0.00029849]
	Learning Rate: 0.000298492
	LOSS [training: 0.514646280987228 | validation: 0.6105615282759156]
	TIME [epoch: 10.5 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5104474490245794		[learning rate: 0.00029758]
	Learning Rate: 0.000297577
	LOSS [training: 0.5104474490245794 | validation: 0.6592409893870944]
	TIME [epoch: 10.5 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5219765505252267		[learning rate: 0.00029667]
	Learning Rate: 0.000296665
	LOSS [training: 0.5219765505252267 | validation: 0.6013816943639673]
	TIME [epoch: 10.5 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49546584301186203		[learning rate: 0.00029576]
	Learning Rate: 0.000295756
	LOSS [training: 0.49546584301186203 | validation: 0.5537220510782414]
	TIME [epoch: 10.5 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4805075797313754		[learning rate: 0.00029485]
	Learning Rate: 0.000294849
	LOSS [training: 0.4805075797313754 | validation: 0.5703102009619022]
	TIME [epoch: 10.5 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47953930076496026		[learning rate: 0.00029395]
	Learning Rate: 0.000293945
	LOSS [training: 0.47953930076496026 | validation: 0.5670881401879188]
	TIME [epoch: 10.5 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.493546884045576		[learning rate: 0.00029304]
	Learning Rate: 0.000293044
	LOSS [training: 0.493546884045576 | validation: 0.5577713551419134]
	TIME [epoch: 10.4 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.474519126958457		[learning rate: 0.00029215]
	Learning Rate: 0.000292146
	LOSS [training: 0.474519126958457 | validation: 0.6013238282140363]
	TIME [epoch: 10.4 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47019540099417745		[learning rate: 0.00029125]
	Learning Rate: 0.00029125
	LOSS [training: 0.47019540099417745 | validation: 0.567447909949791]
	TIME [epoch: 10.5 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4789336225333921		[learning rate: 0.00029036]
	Learning Rate: 0.000290358
	LOSS [training: 0.4789336225333921 | validation: 0.5638278241043648]
	TIME [epoch: 10.5 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4884394305623535		[learning rate: 0.00028947]
	Learning Rate: 0.000289468
	LOSS [training: 0.4884394305623535 | validation: 0.516183285721613]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1654.pth
	Model improved!!!
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48496243849689913		[learning rate: 0.00028858]
	Learning Rate: 0.00028858
	LOSS [training: 0.48496243849689913 | validation: 0.5598700524025086]
	TIME [epoch: 10.5 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4994901232189622		[learning rate: 0.0002877]
	Learning Rate: 0.000287696
	LOSS [training: 0.4994901232189622 | validation: 0.629415346664912]
	TIME [epoch: 10.5 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5290686954988175		[learning rate: 0.00028681]
	Learning Rate: 0.000286814
	LOSS [training: 0.5290686954988175 | validation: 0.662889872349231]
	TIME [epoch: 10.5 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5057730464729037		[learning rate: 0.00028593]
	Learning Rate: 0.000285935
	LOSS [training: 0.5057730464729037 | validation: 0.6053880941180225]
	TIME [epoch: 10.5 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47639567193659305		[learning rate: 0.00028506]
	Learning Rate: 0.000285058
	LOSS [training: 0.47639567193659305 | validation: 0.6085393912723933]
	TIME [epoch: 10.5 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48520742637880704		[learning rate: 0.00028418]
	Learning Rate: 0.000284184
	LOSS [training: 0.48520742637880704 | validation: 0.6252907969112913]
	TIME [epoch: 10.5 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4730505136142479		[learning rate: 0.00028331]
	Learning Rate: 0.000283313
	LOSS [training: 0.4730505136142479 | validation: 0.6110670481213621]
	TIME [epoch: 10.5 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46495261066477844		[learning rate: 0.00028244]
	Learning Rate: 0.000282445
	LOSS [training: 0.46495261066477844 | validation: 0.59883462075853]
	TIME [epoch: 10.5 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49299593510004297		[learning rate: 0.00028158]
	Learning Rate: 0.000281579
	LOSS [training: 0.49299593510004297 | validation: 0.6266823160084722]
	TIME [epoch: 10.5 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.497630519181809		[learning rate: 0.00028072]
	Learning Rate: 0.000280716
	LOSS [training: 0.497630519181809 | validation: 0.5957686152954369]
	TIME [epoch: 10.5 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45802856171399464		[learning rate: 0.00027986]
	Learning Rate: 0.000279855
	LOSS [training: 0.45802856171399464 | validation: 0.6090345628030558]
	TIME [epoch: 10.5 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48323189109209447		[learning rate: 0.000279]
	Learning Rate: 0.000278997
	LOSS [training: 0.48323189109209447 | validation: 0.6076245177891632]
	TIME [epoch: 10.5 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47841855579209414		[learning rate: 0.00027814]
	Learning Rate: 0.000278142
	LOSS [training: 0.47841855579209414 | validation: 0.6158429180687083]
	TIME [epoch: 10.5 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5050279358550392		[learning rate: 0.00027729]
	Learning Rate: 0.000277289
	LOSS [training: 0.5050279358550392 | validation: 0.6597374949386798]
	TIME [epoch: 10.4 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5111338146363764		[learning rate: 0.00027644]
	Learning Rate: 0.000276439
	LOSS [training: 0.5111338146363764 | validation: 0.5947392543718643]
	TIME [epoch: 10.5 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5023447022851816		[learning rate: 0.00027559]
	Learning Rate: 0.000275592
	LOSS [training: 0.5023447022851816 | validation: 0.6065551851113226]
	TIME [epoch: 10.5 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5009340727753792		[learning rate: 0.00027475]
	Learning Rate: 0.000274747
	LOSS [training: 0.5009340727753792 | validation: 0.576377960910955]
	TIME [epoch: 10.5 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4865114939610845		[learning rate: 0.00027391]
	Learning Rate: 0.000273905
	LOSS [training: 0.4865114939610845 | validation: 0.5662413227175283]
	TIME [epoch: 10.4 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4833758596942489		[learning rate: 0.00027307]
	Learning Rate: 0.000273065
	LOSS [training: 0.4833758596942489 | validation: 0.6040250117358356]
	TIME [epoch: 10.5 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5088961725846085		[learning rate: 0.00027223]
	Learning Rate: 0.000272228
	LOSS [training: 0.5088961725846085 | validation: 0.6890771615012713]
	TIME [epoch: 10.5 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5180516464945305		[learning rate: 0.00027139]
	Learning Rate: 0.000271394
	LOSS [training: 0.5180516464945305 | validation: 0.6779120450281354]
	TIME [epoch: 10.5 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4932785257732896		[learning rate: 0.00027056]
	Learning Rate: 0.000270562
	LOSS [training: 0.4932785257732896 | validation: 0.6305049410298559]
	TIME [epoch: 10.5 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48948688440002963		[learning rate: 0.00026973]
	Learning Rate: 0.000269733
	LOSS [training: 0.48948688440002963 | validation: 0.6281390968922559]
	TIME [epoch: 10.4 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4834244088948262		[learning rate: 0.00026891]
	Learning Rate: 0.000268906
	LOSS [training: 0.4834244088948262 | validation: 0.5723741652094967]
	TIME [epoch: 10.5 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4781553447865545		[learning rate: 0.00026808]
	Learning Rate: 0.000268081
	LOSS [training: 0.4781553447865545 | validation: 0.6301243709531359]
	TIME [epoch: 10.5 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4873106630780657		[learning rate: 0.00026726]
	Learning Rate: 0.00026726
	LOSS [training: 0.4873106630780657 | validation: 0.6227039238412531]
	TIME [epoch: 10.5 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4778509708722162		[learning rate: 0.00026644]
	Learning Rate: 0.00026644
	LOSS [training: 0.4778509708722162 | validation: 0.6315528633726595]
	TIME [epoch: 10.5 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4777169195715178		[learning rate: 0.00026562]
	Learning Rate: 0.000265624
	LOSS [training: 0.4777169195715178 | validation: 0.5959101996311514]
	TIME [epoch: 10.5 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47503459896105704		[learning rate: 0.00026481]
	Learning Rate: 0.000264809
	LOSS [training: 0.47503459896105704 | validation: 0.608561710196164]
	TIME [epoch: 10.5 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4755286025850027		[learning rate: 0.000264]
	Learning Rate: 0.000263998
	LOSS [training: 0.4755286025850027 | validation: 0.58780775710313]
	TIME [epoch: 10.5 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48862510726963704		[learning rate: 0.00026319]
	Learning Rate: 0.000263188
	LOSS [training: 0.48862510726963704 | validation: 0.6399290669437933]
	TIME [epoch: 10.5 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5314089943373966		[learning rate: 0.00026238]
	Learning Rate: 0.000262382
	LOSS [training: 0.5314089943373966 | validation: 0.6184908205593324]
	TIME [epoch: 10.4 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47668082765962827		[learning rate: 0.00026158]
	Learning Rate: 0.000261577
	LOSS [training: 0.47668082765962827 | validation: 0.5844588373790751]
	TIME [epoch: 10.4 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47711223172883416		[learning rate: 0.00026078]
	Learning Rate: 0.000260775
	LOSS [training: 0.47711223172883416 | validation: 0.6172498578096106]
	TIME [epoch: 10.4 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4831674289362805		[learning rate: 0.00025998]
	Learning Rate: 0.000259976
	LOSS [training: 0.4831674289362805 | validation: 0.6096122184633319]
	TIME [epoch: 10.5 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5146963797584034		[learning rate: 0.00025918]
	Learning Rate: 0.000259179
	LOSS [training: 0.5146963797584034 | validation: 0.613713048454635]
	TIME [epoch: 10.5 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.473706264695852		[learning rate: 0.00025838]
	Learning Rate: 0.000258385
	LOSS [training: 0.473706264695852 | validation: 0.5963841488176665]
	TIME [epoch: 10.5 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4696287683986224		[learning rate: 0.00025759]
	Learning Rate: 0.000257593
	LOSS [training: 0.4696287683986224 | validation: 0.5962531921456892]
	TIME [epoch: 10.5 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47584221804602833		[learning rate: 0.0002568]
	Learning Rate: 0.000256803
	LOSS [training: 0.47584221804602833 | validation: 0.5897426220163888]
	TIME [epoch: 10.4 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48444229978583175		[learning rate: 0.00025602]
	Learning Rate: 0.000256016
	LOSS [training: 0.48444229978583175 | validation: 0.6212901925411702]
	TIME [epoch: 10.5 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49146372586526244		[learning rate: 0.00025523]
	Learning Rate: 0.000255231
	LOSS [training: 0.49146372586526244 | validation: 0.6018319867872876]
	TIME [epoch: 10.4 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47333627181249655		[learning rate: 0.00025445]
	Learning Rate: 0.000254449
	LOSS [training: 0.47333627181249655 | validation: 0.59600944944946]
	TIME [epoch: 10.5 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4717918219417158		[learning rate: 0.00025367]
	Learning Rate: 0.000253669
	LOSS [training: 0.4717918219417158 | validation: 0.6239848457980108]
	TIME [epoch: 10.4 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.480235457810244		[learning rate: 0.00025289]
	Learning Rate: 0.000252891
	LOSS [training: 0.480235457810244 | validation: 0.5777411400294302]
	TIME [epoch: 10.5 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49803039402213933		[learning rate: 0.00025212]
	Learning Rate: 0.000252116
	LOSS [training: 0.49803039402213933 | validation: 0.6277181096756491]
	TIME [epoch: 10.5 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47532915700431644		[learning rate: 0.00025134]
	Learning Rate: 0.000251343
	LOSS [training: 0.47532915700431644 | validation: 0.6057579665887672]
	TIME [epoch: 10.5 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4953478469607049		[learning rate: 0.00025057]
	Learning Rate: 0.000250572
	LOSS [training: 0.4953478469607049 | validation: 0.6519427953308707]
	TIME [epoch: 10.4 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.523845545598472		[learning rate: 0.0002498]
	Learning Rate: 0.000249804
	LOSS [training: 0.523845545598472 | validation: 0.7265971347132535]
	TIME [epoch: 10.5 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5332993701140899		[learning rate: 0.00024904]
	Learning Rate: 0.000249039
	LOSS [training: 0.5332993701140899 | validation: 0.68279723990513]
	TIME [epoch: 10.5 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49681609277233835		[learning rate: 0.00024828]
	Learning Rate: 0.000248275
	LOSS [training: 0.49681609277233835 | validation: 0.6362180986974305]
	TIME [epoch: 10.5 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5108287663153763		[learning rate: 0.00024751]
	Learning Rate: 0.000247514
	LOSS [training: 0.5108287663153763 | validation: 0.6141518345225769]
	TIME [epoch: 10.5 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48468464636467495		[learning rate: 0.00024676]
	Learning Rate: 0.000246755
	LOSS [training: 0.48468464636467495 | validation: 0.574050845577118]
	TIME [epoch: 10.5 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47747360233736347		[learning rate: 0.000246]
	Learning Rate: 0.000245999
	LOSS [training: 0.47747360233736347 | validation: 0.6287083419335804]
	TIME [epoch: 10.5 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4688602706001069		[learning rate: 0.00024524]
	Learning Rate: 0.000245245
	LOSS [training: 0.4688602706001069 | validation: 0.6162655918439934]
	TIME [epoch: 10.5 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48142062645837713		[learning rate: 0.00024449]
	Learning Rate: 0.000244493
	LOSS [training: 0.48142062645837713 | validation: 0.6082943450511266]
	TIME [epoch: 10.5 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4854235533158656		[learning rate: 0.00024374]
	Learning Rate: 0.000243744
	LOSS [training: 0.4854235533158656 | validation: 0.5921873457914815]
	TIME [epoch: 10.5 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49314271388108055		[learning rate: 0.000243]
	Learning Rate: 0.000242996
	LOSS [training: 0.49314271388108055 | validation: 0.6024483616930294]
	TIME [epoch: 10.5 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5001302750861771		[learning rate: 0.00024225]
	Learning Rate: 0.000242252
	LOSS [training: 0.5001302750861771 | validation: 0.6448487064432044]
	TIME [epoch: 10.4 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4975201506251382		[learning rate: 0.00024151]
	Learning Rate: 0.000241509
	LOSS [training: 0.4975201506251382 | validation: 0.6358036496848845]
	TIME [epoch: 10.5 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4994143613028916		[learning rate: 0.00024077]
	Learning Rate: 0.000240769
	LOSS [training: 0.4994143613028916 | validation: 0.6697698271708188]
	TIME [epoch: 10.5 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5208484409982597		[learning rate: 0.00024003]
	Learning Rate: 0.000240031
	LOSS [training: 0.5208484409982597 | validation: 0.6859584613871684]
	TIME [epoch: 10.5 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5132637621286553		[learning rate: 0.00023929]
	Learning Rate: 0.000239295
	LOSS [training: 0.5132637621286553 | validation: 0.6707716941260335]
	TIME [epoch: 10.5 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4695478342844953		[learning rate: 0.00023856]
	Learning Rate: 0.000238561
	LOSS [training: 0.4695478342844953 | validation: 0.6011632391807303]
	TIME [epoch: 10.5 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4694731299989866		[learning rate: 0.00023783]
	Learning Rate: 0.00023783
	LOSS [training: 0.4694731299989866 | validation: 0.595522563155337]
	TIME [epoch: 10.5 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47438721452927946		[learning rate: 0.0002371]
	Learning Rate: 0.000237101
	LOSS [training: 0.47438721452927946 | validation: 0.6246355791648559]
	TIME [epoch: 10.5 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4745275125748213		[learning rate: 0.00023637]
	Learning Rate: 0.000236374
	LOSS [training: 0.4745275125748213 | validation: 0.6165432558427079]
	TIME [epoch: 10.5 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49806482663447926		[learning rate: 0.00023565]
	Learning Rate: 0.00023565
	LOSS [training: 0.49806482663447926 | validation: 0.6013793630283868]
	TIME [epoch: 10.5 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47610650760128587		[learning rate: 0.00023493]
	Learning Rate: 0.000234927
	LOSS [training: 0.47610650760128587 | validation: 0.6074973634773213]
	TIME [epoch: 10.5 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4779116091685375		[learning rate: 0.00023421]
	Learning Rate: 0.000234207
	LOSS [training: 0.4779116091685375 | validation: 0.5904342672680162]
	TIME [epoch: 10.5 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4739787632873448		[learning rate: 0.00023349]
	Learning Rate: 0.000233489
	LOSS [training: 0.4739787632873448 | validation: 0.6148107228252137]
	TIME [epoch: 10.5 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48380896877913526		[learning rate: 0.00023277]
	Learning Rate: 0.000232773
	LOSS [training: 0.48380896877913526 | validation: 0.60008535401369]
	TIME [epoch: 10.5 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46441709624985866		[learning rate: 0.00023206]
	Learning Rate: 0.00023206
	LOSS [training: 0.46441709624985866 | validation: 0.5884087317215522]
	TIME [epoch: 10.5 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.473113342274346		[learning rate: 0.00023135]
	Learning Rate: 0.000231348
	LOSS [training: 0.473113342274346 | validation: 0.5815787184246703]
	TIME [epoch: 10.5 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5092877350869744		[learning rate: 0.00023064]
	Learning Rate: 0.000230639
	LOSS [training: 0.5092877350869744 | validation: 0.5927656245141596]
	TIME [epoch: 10.5 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4880756924556312		[learning rate: 0.00022993]
	Learning Rate: 0.000229932
	LOSS [training: 0.4880756924556312 | validation: 0.545327912363556]
	TIME [epoch: 10.5 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4662106991395773		[learning rate: 0.00022923]
	Learning Rate: 0.000229227
	LOSS [training: 0.4662106991395773 | validation: 0.5541906561035408]
	TIME [epoch: 10.5 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47965337888050874		[learning rate: 0.00022852]
	Learning Rate: 0.000228525
	LOSS [training: 0.47965337888050874 | validation: 0.58406302310366]
	TIME [epoch: 10.5 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49490289427691386		[learning rate: 0.00022782]
	Learning Rate: 0.000227824
	LOSS [training: 0.49490289427691386 | validation: 0.5544631050673201]
	TIME [epoch: 10.5 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4897202018293253		[learning rate: 0.00022713]
	Learning Rate: 0.000227126
	LOSS [training: 0.4897202018293253 | validation: 0.6128375202660215]
	TIME [epoch: 10.5 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4946876477601162		[learning rate: 0.00022643]
	Learning Rate: 0.00022643
	LOSS [training: 0.4946876477601162 | validation: 0.5676205117780664]
	TIME [epoch: 10.5 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47719370895460306		[learning rate: 0.00022574]
	Learning Rate: 0.000225736
	LOSS [training: 0.47719370895460306 | validation: 0.5846352710003353]
	TIME [epoch: 10.5 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4713704455307762		[learning rate: 0.00022504]
	Learning Rate: 0.000225044
	LOSS [training: 0.4713704455307762 | validation: 0.5825582839887943]
	TIME [epoch: 10.5 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48742349981498323		[learning rate: 0.00022435]
	Learning Rate: 0.000224354
	LOSS [training: 0.48742349981498323 | validation: 0.5409164816176181]
	TIME [epoch: 10.4 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4717555578755667		[learning rate: 0.00022367]
	Learning Rate: 0.000223666
	LOSS [training: 0.4717555578755667 | validation: 0.5206464998385368]
	TIME [epoch: 10.5 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4573396425333499		[learning rate: 0.00022298]
	Learning Rate: 0.00022298
	LOSS [training: 0.4573396425333499 | validation: 0.5851681805168203]
	TIME [epoch: 10.5 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4677979210994854		[learning rate: 0.0002223]
	Learning Rate: 0.000222297
	LOSS [training: 0.4677979210994854 | validation: 0.5720397345835526]
	TIME [epoch: 10.5 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4797844403803183		[learning rate: 0.00022162]
	Learning Rate: 0.000221615
	LOSS [training: 0.4797844403803183 | validation: 0.5864834163912036]
	TIME [epoch: 10.5 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4685362265458887		[learning rate: 0.00022094]
	Learning Rate: 0.000220936
	LOSS [training: 0.4685362265458887 | validation: 0.5513201328569431]
	TIME [epoch: 10.5 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45821530284964557		[learning rate: 0.00022026]
	Learning Rate: 0.000220259
	LOSS [training: 0.45821530284964557 | validation: 0.5788555091617178]
	TIME [epoch: 10.4 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46183084943856717		[learning rate: 0.00021958]
	Learning Rate: 0.000219584
	LOSS [training: 0.46183084943856717 | validation: 0.5804055462862702]
	TIME [epoch: 10.5 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46121162395871496		[learning rate: 0.00021891]
	Learning Rate: 0.000218911
	LOSS [training: 0.46121162395871496 | validation: 0.5669606452895609]
	TIME [epoch: 10.5 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4840476425405691		[learning rate: 0.00021824]
	Learning Rate: 0.000218239
	LOSS [training: 0.4840476425405691 | validation: 0.6307189047908678]
	TIME [epoch: 10.5 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4847577681576968		[learning rate: 0.00021757]
	Learning Rate: 0.00021757
	LOSS [training: 0.4847577681576968 | validation: 0.5873042757824504]
	TIME [epoch: 10.5 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4662027801324696		[learning rate: 0.0002169]
	Learning Rate: 0.000216904
	LOSS [training: 0.4662027801324696 | validation: 0.5839598033026056]
	TIME [epoch: 10.5 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46104822260194317		[learning rate: 0.00021624]
	Learning Rate: 0.000216239
	LOSS [training: 0.46104822260194317 | validation: 0.6272300587203472]
	TIME [epoch: 10.4 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47588698384361655		[learning rate: 0.00021558]
	Learning Rate: 0.000215576
	LOSS [training: 0.47588698384361655 | validation: 0.5838485797365778]
	TIME [epoch: 10.5 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4749814482681492		[learning rate: 0.00021491]
	Learning Rate: 0.000214915
	LOSS [training: 0.4749814482681492 | validation: 0.5827836555723027]
	TIME [epoch: 10.5 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4846723544825908		[learning rate: 0.00021426]
	Learning Rate: 0.000214256
	LOSS [training: 0.4846723544825908 | validation: 0.5634010189068066]
	TIME [epoch: 10.5 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45993938524823186		[learning rate: 0.0002136]
	Learning Rate: 0.000213599
	LOSS [training: 0.45993938524823186 | validation: 0.5539961636140079]
	TIME [epoch: 10.5 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48414363414212574		[learning rate: 0.00021294]
	Learning Rate: 0.000212945
	LOSS [training: 0.48414363414212574 | validation: 0.570287541359294]
	TIME [epoch: 10.4 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5017462742568406		[learning rate: 0.00021229]
	Learning Rate: 0.000212292
	LOSS [training: 0.5017462742568406 | validation: 0.5683962678385985]
	TIME [epoch: 10.5 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48153030474654435		[learning rate: 0.00021164]
	Learning Rate: 0.000211641
	LOSS [training: 0.48153030474654435 | validation: 0.5232048654253216]
	TIME [epoch: 10.5 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45413046656132805		[learning rate: 0.00021099]
	Learning Rate: 0.000210992
	LOSS [training: 0.45413046656132805 | validation: 0.5833061787603346]
	TIME [epoch: 10.5 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4613044379327277		[learning rate: 0.00021035]
	Learning Rate: 0.000210346
	LOSS [training: 0.4613044379327277 | validation: 0.5646874564902811]
	TIME [epoch: 10.5 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4810293173068921		[learning rate: 0.0002097]
	Learning Rate: 0.000209701
	LOSS [training: 0.4810293173068921 | validation: 0.5517651957378228]
	TIME [epoch: 10.5 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4589634678752768		[learning rate: 0.00020906]
	Learning Rate: 0.000209058
	LOSS [training: 0.4589634678752768 | validation: 0.5176090949230975]
	TIME [epoch: 10.5 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4632881404565765		[learning rate: 0.00020842]
	Learning Rate: 0.000208417
	LOSS [training: 0.4632881404565765 | validation: 0.6150659343862813]
	TIME [epoch: 10.5 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4887548834729939		[learning rate: 0.00020778]
	Learning Rate: 0.000207778
	LOSS [training: 0.4887548834729939 | validation: 0.5895509322462802]
	TIME [epoch: 10.5 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4612360663212911		[learning rate: 0.00020714]
	Learning Rate: 0.000207141
	LOSS [training: 0.4612360663212911 | validation: 0.5761455106357106]
	TIME [epoch: 10.5 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4576610689251649		[learning rate: 0.00020651]
	Learning Rate: 0.000206506
	LOSS [training: 0.4576610689251649 | validation: 0.5568708905530683]
	TIME [epoch: 10.5 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4584823215798665		[learning rate: 0.00020587]
	Learning Rate: 0.000205873
	LOSS [training: 0.4584823215798665 | validation: 0.5556947376015657]
	TIME [epoch: 10.5 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47228300747448915		[learning rate: 0.00020524]
	Learning Rate: 0.000205242
	LOSS [training: 0.47228300747448915 | validation: 0.5608744604853807]
	TIME [epoch: 10.5 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4633006463616587		[learning rate: 0.00020461]
	Learning Rate: 0.000204613
	LOSS [training: 0.4633006463616587 | validation: 0.5690335954699965]
	TIME [epoch: 10.5 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4863985930151351		[learning rate: 0.00020399]
	Learning Rate: 0.000203986
	LOSS [training: 0.4863985930151351 | validation: 0.5906415709371857]
	TIME [epoch: 10.5 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4821669910923525		[learning rate: 0.00020336]
	Learning Rate: 0.00020336
	LOSS [training: 0.4821669910923525 | validation: 0.5567668974460426]
	TIME [epoch: 10.5 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46963932448750006		[learning rate: 0.00020274]
	Learning Rate: 0.000202737
	LOSS [training: 0.46963932448750006 | validation: 0.5684631951144444]
	TIME [epoch: 10.5 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47446560523828996		[learning rate: 0.00020212]
	Learning Rate: 0.000202116
	LOSS [training: 0.47446560523828996 | validation: 0.560776610236548]
	TIME [epoch: 10.5 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4599855247847994		[learning rate: 0.0002015]
	Learning Rate: 0.000201496
	LOSS [training: 0.4599855247847994 | validation: 0.5528949855342831]
	TIME [epoch: 10.5 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4675864568859714		[learning rate: 0.00020088]
	Learning Rate: 0.000200878
	LOSS [training: 0.4675864568859714 | validation: 0.5359700674951554]
	TIME [epoch: 10.5 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.461003820424258		[learning rate: 0.00020026]
	Learning Rate: 0.000200263
	LOSS [training: 0.461003820424258 | validation: 0.5387902517569637]
	TIME [epoch: 10.5 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4528871332842563		[learning rate: 0.00019965]
	Learning Rate: 0.000199649
	LOSS [training: 0.4528871332842563 | validation: 0.5442855148410414]
	TIME [epoch: 10.5 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45621568520875594		[learning rate: 0.00019904]
	Learning Rate: 0.000199037
	LOSS [training: 0.45621568520875594 | validation: 0.5763064834472794]
	TIME [epoch: 10.5 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46875065755554335		[learning rate: 0.00019843]
	Learning Rate: 0.000198427
	LOSS [training: 0.46875065755554335 | validation: 0.555735864037804]
	TIME [epoch: 10.5 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46770164183483054		[learning rate: 0.00019782]
	Learning Rate: 0.000197818
	LOSS [training: 0.46770164183483054 | validation: 0.5444399811235795]
	TIME [epoch: 10.5 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4599377426189874		[learning rate: 0.00019721]
	Learning Rate: 0.000197212
	LOSS [training: 0.4599377426189874 | validation: 0.5766212410342265]
	TIME [epoch: 10.5 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46916724457776826		[learning rate: 0.00019661]
	Learning Rate: 0.000196607
	LOSS [training: 0.46916724457776826 | validation: 0.561772203221058]
	TIME [epoch: 10.5 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46654130791984627		[learning rate: 0.000196]
	Learning Rate: 0.000196005
	LOSS [training: 0.46654130791984627 | validation: 0.5526986734681905]
	TIME [epoch: 10.5 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45440419320726333		[learning rate: 0.0001954]
	Learning Rate: 0.000195404
	LOSS [training: 0.45440419320726333 | validation: 0.5577507299742933]
	TIME [epoch: 10.5 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4647723730260596		[learning rate: 0.0001948]
	Learning Rate: 0.000194805
	LOSS [training: 0.4647723730260596 | validation: 0.5131945410637391]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1783.pth
	Model improved!!!
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46034748230403116		[learning rate: 0.00019421]
	Learning Rate: 0.000194208
	LOSS [training: 0.46034748230403116 | validation: 0.5882213640752239]
	TIME [epoch: 10.4 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4666253494828063		[learning rate: 0.00019361]
	Learning Rate: 0.000193612
	LOSS [training: 0.4666253494828063 | validation: 0.57546772381596]
	TIME [epoch: 10.5 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4673035490974076		[learning rate: 0.00019302]
	Learning Rate: 0.000193019
	LOSS [training: 0.4673035490974076 | validation: 0.5457749384332008]
	TIME [epoch: 10.5 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4732068182874249		[learning rate: 0.00019243]
	Learning Rate: 0.000192427
	LOSS [training: 0.4732068182874249 | validation: 0.5760085137558207]
	TIME [epoch: 10.5 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4686920895441534		[learning rate: 0.00019184]
	Learning Rate: 0.000191837
	LOSS [training: 0.4686920895441534 | validation: 0.5982104270402288]
	TIME [epoch: 10.5 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4773430282857095		[learning rate: 0.00019125]
	Learning Rate: 0.000191249
	LOSS [training: 0.4773430282857095 | validation: 0.5487345699732998]
	TIME [epoch: 10.5 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46626940691737684		[learning rate: 0.00019066]
	Learning Rate: 0.000190663
	LOSS [training: 0.46626940691737684 | validation: 0.5565254869447344]
	TIME [epoch: 10.5 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4730887044816233		[learning rate: 0.00019008]
	Learning Rate: 0.000190079
	LOSS [training: 0.4730887044816233 | validation: 0.5497811086069448]
	TIME [epoch: 10.5 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47541925281800557		[learning rate: 0.0001895]
	Learning Rate: 0.000189496
	LOSS [training: 0.47541925281800557 | validation: 0.5563872105851658]
	TIME [epoch: 10.5 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4605514264329832		[learning rate: 0.00018892]
	Learning Rate: 0.000188915
	LOSS [training: 0.4605514264329832 | validation: 0.5484963898535786]
	TIME [epoch: 10.5 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46091453056068915		[learning rate: 0.00018834]
	Learning Rate: 0.000188336
	LOSS [training: 0.46091453056068915 | validation: 0.5459373089101641]
	TIME [epoch: 10.5 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45558554849082267		[learning rate: 0.00018776]
	Learning Rate: 0.000187759
	LOSS [training: 0.45558554849082267 | validation: 0.5651517040197879]
	TIME [epoch: 10.5 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4519070625548725		[learning rate: 0.00018718]
	Learning Rate: 0.000187183
	LOSS [training: 0.4519070625548725 | validation: 0.568622450246325]
	TIME [epoch: 10.5 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46811488319332534		[learning rate: 0.00018661]
	Learning Rate: 0.000186609
	LOSS [training: 0.46811488319332534 | validation: 0.5478892294021516]
	TIME [epoch: 10.5 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44995093941432696		[learning rate: 0.00018604]
	Learning Rate: 0.000186037
	LOSS [training: 0.44995093941432696 | validation: 0.5582475943221651]
	TIME [epoch: 10.5 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44859859350394266		[learning rate: 0.00018547]
	Learning Rate: 0.000185467
	LOSS [training: 0.44859859350394266 | validation: 0.5200575818644415]
	TIME [epoch: 10.5 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4528651143809258		[learning rate: 0.0001849]
	Learning Rate: 0.000184898
	LOSS [training: 0.4528651143809258 | validation: 0.5902680285555432]
	TIME [epoch: 10.5 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45994627482355116		[learning rate: 0.00018433]
	Learning Rate: 0.000184332
	LOSS [training: 0.45994627482355116 | validation: 0.5502032034886043]
	TIME [epoch: 10.5 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4515809870278312		[learning rate: 0.00018377]
	Learning Rate: 0.000183767
	LOSS [training: 0.4515809870278312 | validation: 0.557334122068009]
	TIME [epoch: 10.5 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4603186978421191		[learning rate: 0.0001832]
	Learning Rate: 0.000183203
	LOSS [training: 0.4603186978421191 | validation: 0.5651942428489316]
	TIME [epoch: 10.4 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46992571674874667		[learning rate: 0.00018264]
	Learning Rate: 0.000182642
	LOSS [training: 0.46992571674874667 | validation: 0.5443573043950337]
	TIME [epoch: 10.5 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4677764675374787		[learning rate: 0.00018208]
	Learning Rate: 0.000182082
	LOSS [training: 0.4677764675374787 | validation: 0.5329015658412501]
	TIME [epoch: 10.5 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45972507782546207		[learning rate: 0.00018152]
	Learning Rate: 0.000181524
	LOSS [training: 0.45972507782546207 | validation: 0.5342855987019144]
	TIME [epoch: 10.4 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45234046364433383		[learning rate: 0.00018097]
	Learning Rate: 0.000180967
	LOSS [training: 0.45234046364433383 | validation: 0.5540192490875958]
	TIME [epoch: 10.5 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4637623511305951		[learning rate: 0.00018041]
	Learning Rate: 0.000180412
	LOSS [training: 0.4637623511305951 | validation: 0.5530245887729632]
	TIME [epoch: 10.5 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.462628774919373		[learning rate: 0.00017986]
	Learning Rate: 0.000179859
	LOSS [training: 0.462628774919373 | validation: 0.5481789392503478]
	TIME [epoch: 10.5 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4648726858007393		[learning rate: 0.00017931]
	Learning Rate: 0.000179308
	LOSS [training: 0.4648726858007393 | validation: 0.5557784031951785]
	TIME [epoch: 10.5 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46751705366509466		[learning rate: 0.00017876]
	Learning Rate: 0.000178758
	LOSS [training: 0.46751705366509466 | validation: 0.5317169453493306]
	TIME [epoch: 10.5 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4625502515892075		[learning rate: 0.00017821]
	Learning Rate: 0.00017821
	LOSS [training: 0.4625502515892075 | validation: 0.5574720877782258]
	TIME [epoch: 10.5 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4530824465495362		[learning rate: 0.00017766]
	Learning Rate: 0.000177664
	LOSS [training: 0.4530824465495362 | validation: 0.5785898085262856]
	TIME [epoch: 10.5 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4707160839232585		[learning rate: 0.00017712]
	Learning Rate: 0.00017712
	LOSS [training: 0.4707160839232585 | validation: 0.571639386991375]
	TIME [epoch: 10.5 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4807431358962922		[learning rate: 0.00017658]
	Learning Rate: 0.000176577
	LOSS [training: 0.4807431358962922 | validation: 0.558029597545099]
	TIME [epoch: 10.5 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4625132275307001		[learning rate: 0.00017604]
	Learning Rate: 0.000176035
	LOSS [training: 0.4625132275307001 | validation: 0.5815668807215445]
	TIME [epoch: 10.5 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4517175110709902		[learning rate: 0.0001755]
	Learning Rate: 0.000175496
	LOSS [training: 0.4517175110709902 | validation: 0.5902959702291222]
	TIME [epoch: 10.5 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4576645418854793		[learning rate: 0.00017496]
	Learning Rate: 0.000174958
	LOSS [training: 0.4576645418854793 | validation: 0.5674420842486714]
	TIME [epoch: 10.5 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4563972275060511		[learning rate: 0.00017442]
	Learning Rate: 0.000174421
	LOSS [training: 0.4563972275060511 | validation: 0.5664657552111264]
	TIME [epoch: 10.5 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4600595942374063		[learning rate: 0.00017389]
	Learning Rate: 0.000173887
	LOSS [training: 0.4600595942374063 | validation: 0.5721078190761066]
	TIME [epoch: 10.5 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45401558034687745		[learning rate: 0.00017335]
	Learning Rate: 0.000173354
	LOSS [training: 0.45401558034687745 | validation: 0.6008322724578998]
	TIME [epoch: 10.5 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45066041178593935		[learning rate: 0.00017282]
	Learning Rate: 0.000172822
	LOSS [training: 0.45066041178593935 | validation: 0.5692599368654462]
	TIME [epoch: 10.5 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4508135926567299		[learning rate: 0.00017229]
	Learning Rate: 0.000172293
	LOSS [training: 0.4508135926567299 | validation: 0.5528296054170877]
	TIME [epoch: 10.5 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47195993570022654		[learning rate: 0.00017176]
	Learning Rate: 0.000171764
	LOSS [training: 0.47195993570022654 | validation: 0.5742542150653486]
	TIME [epoch: 10.5 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45958279051771633		[learning rate: 0.00017124]
	Learning Rate: 0.000171238
	LOSS [training: 0.45958279051771633 | validation: 0.5941378337662048]
	TIME [epoch: 10.5 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.462147116284864		[learning rate: 0.00017071]
	Learning Rate: 0.000170713
	LOSS [training: 0.462147116284864 | validation: 0.6039583238279334]
	TIME [epoch: 10.5 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.456455382553521		[learning rate: 0.00017019]
	Learning Rate: 0.00017019
	LOSS [training: 0.456455382553521 | validation: 0.5823942316178494]
	TIME [epoch: 10.5 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46605279080279904		[learning rate: 0.00016967]
	Learning Rate: 0.000169668
	LOSS [training: 0.46605279080279904 | validation: 0.579986811101111]
	TIME [epoch: 10.5 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45105543884302807		[learning rate: 0.00016915]
	Learning Rate: 0.000169148
	LOSS [training: 0.45105543884302807 | validation: 0.5591897845846224]
	TIME [epoch: 10.5 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.446735745077529		[learning rate: 0.00016863]
	Learning Rate: 0.000168629
	LOSS [training: 0.446735745077529 | validation: 0.5523704737747854]
	TIME [epoch: 10.5 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4466482387863106		[learning rate: 0.00016811]
	Learning Rate: 0.000168112
	LOSS [training: 0.4466482387863106 | validation: 0.5860510055134815]
	TIME [epoch: 10.5 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4514588305400702		[learning rate: 0.0001676]
	Learning Rate: 0.000167597
	LOSS [training: 0.4514588305400702 | validation: 0.560229730847359]
	TIME [epoch: 10.5 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4429274340406019		[learning rate: 0.00016708]
	Learning Rate: 0.000167083
	LOSS [training: 0.4429274340406019 | validation: 0.5705327513695949]
	TIME [epoch: 10.5 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46802396276805186		[learning rate: 0.00016657]
	Learning Rate: 0.000166571
	LOSS [training: 0.46802396276805186 | validation: 0.5822859034645427]
	TIME [epoch: 10.4 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4748912158834283		[learning rate: 0.00016606]
	Learning Rate: 0.000166061
	LOSS [training: 0.4748912158834283 | validation: 0.582414403785622]
	TIME [epoch: 10.5 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47562852371548675		[learning rate: 0.00016555]
	Learning Rate: 0.000165552
	LOSS [training: 0.47562852371548675 | validation: 0.5294112280298182]
	TIME [epoch: 10.5 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4659828168217806		[learning rate: 0.00016504]
	Learning Rate: 0.000165044
	LOSS [training: 0.4659828168217806 | validation: 0.5323289946722237]
	TIME [epoch: 10.5 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45595241619074434		[learning rate: 0.00016454]
	Learning Rate: 0.000164538
	LOSS [training: 0.45595241619074434 | validation: 0.5524067319309383]
	TIME [epoch: 10.5 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47157480960244264		[learning rate: 0.00016403]
	Learning Rate: 0.000164034
	LOSS [training: 0.47157480960244264 | validation: 0.5236162378461255]
	TIME [epoch: 10.5 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4524155287257893		[learning rate: 0.00016353]
	Learning Rate: 0.000163531
	LOSS [training: 0.4524155287257893 | validation: 0.552037281742383]
	TIME [epoch: 10.5 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4533326961247993		[learning rate: 0.00016303]
	Learning Rate: 0.00016303
	LOSS [training: 0.4533326961247993 | validation: 0.5459449825622811]
	TIME [epoch: 10.5 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45661870753017697		[learning rate: 0.00016253]
	Learning Rate: 0.00016253
	LOSS [training: 0.45661870753017697 | validation: 0.5231490038841992]
	TIME [epoch: 10.5 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4552111528470573		[learning rate: 0.00016203]
	Learning Rate: 0.000162032
	LOSS [training: 0.4552111528470573 | validation: 0.5371947895814464]
	TIME [epoch: 10.5 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4583994855464142		[learning rate: 0.00016153]
	Learning Rate: 0.000161535
	LOSS [training: 0.4583994855464142 | validation: 0.603420053457213]
	TIME [epoch: 10.5 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4711901659613817		[learning rate: 0.00016104]
	Learning Rate: 0.00016104
	LOSS [training: 0.4711901659613817 | validation: 0.5601741966438132]
	TIME [epoch: 10.5 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45102516694982314		[learning rate: 0.00016055]
	Learning Rate: 0.000160546
	LOSS [training: 0.45102516694982314 | validation: 0.5699202273799295]
	TIME [epoch: 10.5 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4498358761344826		[learning rate: 0.00016005]
	Learning Rate: 0.000160054
	LOSS [training: 0.4498358761344826 | validation: 0.569390865081046]
	TIME [epoch: 10.5 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45520899670516357		[learning rate: 0.00015956]
	Learning Rate: 0.000159563
	LOSS [training: 0.45520899670516357 | validation: 0.5817106384597323]
	TIME [epoch: 10.5 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45997714561299724		[learning rate: 0.00015907]
	Learning Rate: 0.000159074
	LOSS [training: 0.45997714561299724 | validation: 0.5729415161720838]
	TIME [epoch: 10.5 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.492654124200249		[learning rate: 0.00015859]
	Learning Rate: 0.000158587
	LOSS [training: 0.492654124200249 | validation: 0.601751791624647]
	TIME [epoch: 10.5 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46651728349582144		[learning rate: 0.0001581]
	Learning Rate: 0.000158101
	LOSS [training: 0.46651728349582144 | validation: 0.5797131846708423]
	TIME [epoch: 10.5 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4612839538731944		[learning rate: 0.00015762]
	Learning Rate: 0.000157616
	LOSS [training: 0.4612839538731944 | validation: 0.5845588328317054]
	TIME [epoch: 10.5 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4561619143502235		[learning rate: 0.00015713]
	Learning Rate: 0.000157133
	LOSS [training: 0.4561619143502235 | validation: 0.5685267231325355]
	TIME [epoch: 10.5 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4530925898571693		[learning rate: 0.00015665]
	Learning Rate: 0.000156651
	LOSS [training: 0.4530925898571693 | validation: 0.5836209460130882]
	TIME [epoch: 10.5 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45896611679130245		[learning rate: 0.00015617]
	Learning Rate: 0.000156171
	LOSS [training: 0.45896611679130245 | validation: 0.5512938196606616]
	TIME [epoch: 10.5 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4520524251046515		[learning rate: 0.00015569]
	Learning Rate: 0.000155692
	LOSS [training: 0.4520524251046515 | validation: 0.5644443216468052]
	TIME [epoch: 10.5 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4623010333051997		[learning rate: 0.00015521]
	Learning Rate: 0.000155215
	LOSS [training: 0.4623010333051997 | validation: 0.5655309183672237]
	TIME [epoch: 10.4 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46628104996240083		[learning rate: 0.00015474]
	Learning Rate: 0.000154739
	LOSS [training: 0.46628104996240083 | validation: 0.5705349117282446]
	TIME [epoch: 10.5 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45844115533069185		[learning rate: 0.00015426]
	Learning Rate: 0.000154265
	LOSS [training: 0.45844115533069185 | validation: 0.5570376878416006]
	TIME [epoch: 10.5 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4623008901172643		[learning rate: 0.00015379]
	Learning Rate: 0.000153792
	LOSS [training: 0.4623008901172643 | validation: 0.5496841492444265]
	TIME [epoch: 10.5 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44947184630138814		[learning rate: 0.00015332]
	Learning Rate: 0.00015332
	LOSS [training: 0.44947184630138814 | validation: 0.5485128899832122]
	TIME [epoch: 10.5 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45177202839278163		[learning rate: 0.00015285]
	Learning Rate: 0.00015285
	LOSS [training: 0.45177202839278163 | validation: 0.5536415576967751]
	TIME [epoch: 10.5 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4434521351580411		[learning rate: 0.00015238]
	Learning Rate: 0.000152382
	LOSS [training: 0.4434521351580411 | validation: 0.5607455626925669]
	TIME [epoch: 10.5 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4485464284073542		[learning rate: 0.00015191]
	Learning Rate: 0.000151915
	LOSS [training: 0.4485464284073542 | validation: 0.5650450508714355]
	TIME [epoch: 10.5 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4413417785989715		[learning rate: 0.00015145]
	Learning Rate: 0.000151449
	LOSS [training: 0.4413417785989715 | validation: 0.5537805046999938]
	TIME [epoch: 10.5 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4607267201854016		[learning rate: 0.00015098]
	Learning Rate: 0.000150985
	LOSS [training: 0.4607267201854016 | validation: 0.5633511873611229]
	TIME [epoch: 10.5 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4644956656350866		[learning rate: 0.00015052]
	Learning Rate: 0.000150522
	LOSS [training: 0.4644956656350866 | validation: 0.5714771697113855]
	TIME [epoch: 10.4 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45839333701843044		[learning rate: 0.00015006]
	Learning Rate: 0.000150061
	LOSS [training: 0.45839333701843044 | validation: 0.5932057902612061]
	TIME [epoch: 10.5 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4521597824190657		[learning rate: 0.0001496]
	Learning Rate: 0.000149601
	LOSS [training: 0.4521597824190657 | validation: 0.5815772292928457]
	TIME [epoch: 10.5 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4569824075817904		[learning rate: 0.00014914]
	Learning Rate: 0.000149142
	LOSS [training: 0.4569824075817904 | validation: 0.5772642830193935]
	TIME [epoch: 10.5 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4428380854525805		[learning rate: 0.00014868]
	Learning Rate: 0.000148685
	LOSS [training: 0.4428380854525805 | validation: 0.5985765961926272]
	TIME [epoch: 10.5 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47873233933782144		[learning rate: 0.00014823]
	Learning Rate: 0.000148229
	LOSS [training: 0.47873233933782144 | validation: 0.6319678663526033]
	TIME [epoch: 10.5 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.482692551285188		[learning rate: 0.00014777]
	Learning Rate: 0.000147775
	LOSS [training: 0.482692551285188 | validation: 0.6174361768911976]
	TIME [epoch: 10.5 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4758790682674977		[learning rate: 0.00014732]
	Learning Rate: 0.000147322
	LOSS [training: 0.4758790682674977 | validation: 0.6218271648064733]
	TIME [epoch: 10.5 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4585085551140809		[learning rate: 0.00014687]
	Learning Rate: 0.00014687
	LOSS [training: 0.4585085551140809 | validation: 0.5852736909680375]
	TIME [epoch: 10.5 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46163147683105354		[learning rate: 0.00014642]
	Learning Rate: 0.00014642
	LOSS [training: 0.46163147683105354 | validation: 0.6131111357002859]
	TIME [epoch: 10.5 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4912050934084979		[learning rate: 0.00014597]
	Learning Rate: 0.000145971
	LOSS [training: 0.4912050934084979 | validation: 0.5947891139332404]
	TIME [epoch: 10.5 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46181387290504883		[learning rate: 0.00014552]
	Learning Rate: 0.000145524
	LOSS [training: 0.46181387290504883 | validation: 0.5571946153118085]
	TIME [epoch: 10.5 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4550028616251778		[learning rate: 0.00014508]
	Learning Rate: 0.000145077
	LOSS [training: 0.4550028616251778 | validation: 0.5575468282383124]
	TIME [epoch: 10.5 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4500210879084042		[learning rate: 0.00014463]
	Learning Rate: 0.000144633
	LOSS [training: 0.4500210879084042 | validation: 0.5624905560815984]
	TIME [epoch: 10.5 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4380599825588976		[learning rate: 0.00014419]
	Learning Rate: 0.000144189
	LOSS [training: 0.4380599825588976 | validation: 0.5674918092067819]
	TIME [epoch: 10.5 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45407215808178397		[learning rate: 0.00014375]
	Learning Rate: 0.000143747
	LOSS [training: 0.45407215808178397 | validation: 0.561523040770413]
	TIME [epoch: 10.5 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4566997320689324		[learning rate: 0.00014331]
	Learning Rate: 0.000143307
	LOSS [training: 0.4566997320689324 | validation: 0.5669240050092966]
	TIME [epoch: 10.5 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4663244679065032		[learning rate: 0.00014287]
	Learning Rate: 0.000142867
	LOSS [training: 0.4663244679065032 | validation: 0.5474784810029583]
	TIME [epoch: 10.5 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47624755611695935		[learning rate: 0.00014243]
	Learning Rate: 0.00014243
	LOSS [training: 0.47624755611695935 | validation: 0.5864862273069558]
	TIME [epoch: 10.5 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48352928738136497		[learning rate: 0.00014199]
	Learning Rate: 0.000141993
	LOSS [training: 0.48352928738136497 | validation: 0.5484433736871227]
	TIME [epoch: 10.5 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48754769738065995		[learning rate: 0.00014156]
	Learning Rate: 0.000141558
	LOSS [training: 0.48754769738065995 | validation: 0.5533628366526641]
	TIME [epoch: 10.5 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47266251340403215		[learning rate: 0.00014112]
	Learning Rate: 0.000141124
	LOSS [training: 0.47266251340403215 | validation: 0.5528912982669208]
	TIME [epoch: 10.5 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4710316311646997		[learning rate: 0.00014069]
	Learning Rate: 0.000140691
	LOSS [training: 0.4710316311646997 | validation: 0.5251872244708231]
	TIME [epoch: 10.5 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46534641894446976		[learning rate: 0.00014026]
	Learning Rate: 0.00014026
	LOSS [training: 0.46534641894446976 | validation: 0.5286580487961063]
	TIME [epoch: 10.5 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4714026282270181		[learning rate: 0.00013983]
	Learning Rate: 0.00013983
	LOSS [training: 0.4714026282270181 | validation: 0.5651156525297922]
	TIME [epoch: 10.5 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46672014781350724		[learning rate: 0.0001394]
	Learning Rate: 0.000139401
	LOSS [training: 0.46672014781350724 | validation: 0.5239215069240385]
	TIME [epoch: 10.5 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46369436600020125		[learning rate: 0.00013897]
	Learning Rate: 0.000138974
	LOSS [training: 0.46369436600020125 | validation: 0.5465171004212194]
	TIME [epoch: 10.5 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4638992571500714		[learning rate: 0.00013855]
	Learning Rate: 0.000138548
	LOSS [training: 0.4638992571500714 | validation: 0.517848584181036]
	TIME [epoch: 10.5 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4715974411547223		[learning rate: 0.00013812]
	Learning Rate: 0.000138123
	LOSS [training: 0.4715974411547223 | validation: 0.5284697516617456]
	TIME [epoch: 10.5 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4703713559565651		[learning rate: 0.0001377]
	Learning Rate: 0.0001377
	LOSS [training: 0.4703713559565651 | validation: 0.5247944307716115]
	TIME [epoch: 10.5 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46578635904628884		[learning rate: 0.00013728]
	Learning Rate: 0.000137278
	LOSS [training: 0.46578635904628884 | validation: 0.5465703211764497]
	TIME [epoch: 10.5 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48043712056618465		[learning rate: 0.00013686]
	Learning Rate: 0.000136857
	LOSS [training: 0.48043712056618465 | validation: 0.5490315773111566]
	TIME [epoch: 10.5 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4951541408205715		[learning rate: 0.00013644]
	Learning Rate: 0.000136437
	LOSS [training: 0.4951541408205715 | validation: 0.547712518720081]
	TIME [epoch: 10.5 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4695773097117507		[learning rate: 0.00013602]
	Learning Rate: 0.000136019
	LOSS [training: 0.4695773097117507 | validation: 0.5300035656833616]
	TIME [epoch: 10.5 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45622174775528135		[learning rate: 0.0001356]
	Learning Rate: 0.000135602
	LOSS [training: 0.45622174775528135 | validation: 0.5056319193829651]
	TIME [epoch: 10.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240217_075145/states/model_tr_study5_1901.pth
	Model improved!!!
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4936769023251049		[learning rate: 0.00013519]
	Learning Rate: 0.000135186
	LOSS [training: 0.4936769023251049 | validation: 0.5170571033862561]
	TIME [epoch: 10.5 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5127257063226273		[learning rate: 0.00013477]
	Learning Rate: 0.000134772
	LOSS [training: 0.5127257063226273 | validation: 0.5469278391328607]
	TIME [epoch: 10.5 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49673559268775536		[learning rate: 0.00013436]
	Learning Rate: 0.000134359
	LOSS [training: 0.49673559268775536 | validation: 0.5331939367168707]
	TIME [epoch: 10.5 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4787019878387625		[learning rate: 0.00013395]
	Learning Rate: 0.000133947
	LOSS [training: 0.4787019878387625 | validation: 0.5347834186244608]
	TIME [epoch: 10.5 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4984825290777831		[learning rate: 0.00013354]
	Learning Rate: 0.000133536
	LOSS [training: 0.4984825290777831 | validation: 0.5360829393744565]
	TIME [epoch: 10.5 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47697612163691083		[learning rate: 0.00013313]
	Learning Rate: 0.000133127
	LOSS [training: 0.47697612163691083 | validation: 0.5223888388358725]
	TIME [epoch: 10.5 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45330409917630216		[learning rate: 0.00013272]
	Learning Rate: 0.000132719
	LOSS [training: 0.45330409917630216 | validation: 0.5590453013176978]
	TIME [epoch: 10.5 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4707464551111936		[learning rate: 0.00013231]
	Learning Rate: 0.000132312
	LOSS [training: 0.4707464551111936 | validation: 0.5479552679195718]
	TIME [epoch: 10.5 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4569474149697954		[learning rate: 0.00013191]
	Learning Rate: 0.000131907
	LOSS [training: 0.4569474149697954 | validation: 0.5213163937424046]
	TIME [epoch: 10.5 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45783322725323095		[learning rate: 0.0001315]
	Learning Rate: 0.000131502
	LOSS [training: 0.45783322725323095 | validation: 0.5340001253957241]
	TIME [epoch: 10.5 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48223972658505526		[learning rate: 0.0001311]
	Learning Rate: 0.000131099
	LOSS [training: 0.48223972658505526 | validation: 0.554594688810424]
	TIME [epoch: 10.5 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4614650062821145		[learning rate: 0.0001307]
	Learning Rate: 0.000130697
	LOSS [training: 0.4614650062821145 | validation: 0.5292377305313307]
	TIME [epoch: 10.5 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4582372513281422		[learning rate: 0.0001303]
	Learning Rate: 0.000130297
	LOSS [training: 0.4582372513281422 | validation: 0.5642497762246355]
	TIME [epoch: 10.5 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47227100450203574		[learning rate: 0.0001299]
	Learning Rate: 0.000129897
	LOSS [training: 0.47227100450203574 | validation: 0.5521945922206858]
	TIME [epoch: 10.5 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4813584782683306		[learning rate: 0.0001295]
	Learning Rate: 0.000129499
	LOSS [training: 0.4813584782683306 | validation: 0.5523952897225977]
	TIME [epoch: 10.5 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4769332814550527		[learning rate: 0.0001291]
	Learning Rate: 0.000129102
	LOSS [training: 0.4769332814550527 | validation: 0.564671362435437]
	TIME [epoch: 10.5 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46992732013870364		[learning rate: 0.00012871]
	Learning Rate: 0.000128706
	LOSS [training: 0.46992732013870364 | validation: 0.5783088120007382]
	TIME [epoch: 10.4 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46693530879180567		[learning rate: 0.00012831]
	Learning Rate: 0.000128312
	LOSS [training: 0.46693530879180567 | validation: 0.5876443711560179]
	TIME [epoch: 10.5 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46820954500087913		[learning rate: 0.00012792]
	Learning Rate: 0.000127918
	LOSS [training: 0.46820954500087913 | validation: 0.5418108663905113]
	TIME [epoch: 10.5 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4691523189259869		[learning rate: 0.00012753]
	Learning Rate: 0.000127526
	LOSS [training: 0.4691523189259869 | validation: 0.5802267010052665]
	TIME [epoch: 10.5 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48707401490144375		[learning rate: 0.00012714]
	Learning Rate: 0.000127135
	LOSS [training: 0.48707401490144375 | validation: 0.5367748590465558]
	TIME [epoch: 10.4 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4694238063542432		[learning rate: 0.00012675]
	Learning Rate: 0.000126746
	LOSS [training: 0.4694238063542432 | validation: 0.5124209114088155]
	TIME [epoch: 10.5 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4648858634508935		[learning rate: 0.00012636]
	Learning Rate: 0.000126357
	LOSS [training: 0.4648858634508935 | validation: 0.51496535149672]
	TIME [epoch: 10.5 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4563678075326121		[learning rate: 0.00012597]
	Learning Rate: 0.00012597
	LOSS [training: 0.4563678075326121 | validation: 0.5411222310489248]
	TIME [epoch: 10.4 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4655408596376798		[learning rate: 0.00012558]
	Learning Rate: 0.000125584
	LOSS [training: 0.4655408596376798 | validation: 0.5267475680535152]
	TIME [epoch: 10.4 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46440197865634125		[learning rate: 0.0001252]
	Learning Rate: 0.000125199
	LOSS [training: 0.46440197865634125 | validation: 0.5346779511221682]
	TIME [epoch: 10.5 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47237453104928606		[learning rate: 0.00012481]
	Learning Rate: 0.000124815
	LOSS [training: 0.47237453104928606 | validation: 0.5613658608978243]
	TIME [epoch: 10.5 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4671254430453685		[learning rate: 0.00012443]
	Learning Rate: 0.000124432
	LOSS [training: 0.4671254430453685 | validation: 0.5608641966333606]
	TIME [epoch: 10.4 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47087203298117386		[learning rate: 0.00012405]
	Learning Rate: 0.000124051
	LOSS [training: 0.47087203298117386 | validation: 0.5534065400565507]
	TIME [epoch: 10.4 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46815209465654306		[learning rate: 0.00012367]
	Learning Rate: 0.000123671
	LOSS [training: 0.46815209465654306 | validation: 0.5622145231754389]
	TIME [epoch: 10.4 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46127506002483687		[learning rate: 0.00012329]
	Learning Rate: 0.000123292
	LOSS [training: 0.46127506002483687 | validation: 0.5527231367312925]
	TIME [epoch: 10.4 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4530791556138646		[learning rate: 0.00012291]
	Learning Rate: 0.000122914
	LOSS [training: 0.4530791556138646 | validation: 0.5507461357863525]
	TIME [epoch: 10.4 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44724505581906915		[learning rate: 0.00012254]
	Learning Rate: 0.000122537
	LOSS [training: 0.44724505581906915 | validation: 0.531780484036828]
	TIME [epoch: 10.4 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4573401737346055		[learning rate: 0.00012216]
	Learning Rate: 0.000122161
	LOSS [training: 0.4573401737346055 | validation: 0.5399945781639888]
	TIME [epoch: 10.4 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46351248803618583		[learning rate: 0.00012179]
	Learning Rate: 0.000121787
	LOSS [training: 0.46351248803618583 | validation: 0.5507937421270178]
	TIME [epoch: 10.4 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4526450702761104		[learning rate: 0.00012141]
	Learning Rate: 0.000121413
	LOSS [training: 0.4526450702761104 | validation: 0.5687952786475732]
	TIME [epoch: 10.4 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4607590385565346		[learning rate: 0.00012104]
	Learning Rate: 0.000121041
	LOSS [training: 0.4607590385565346 | validation: 0.5899082019026359]
	TIME [epoch: 10.5 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46700478326477074		[learning rate: 0.00012067]
	Learning Rate: 0.00012067
	LOSS [training: 0.46700478326477074 | validation: 0.5878203238321991]
	TIME [epoch: 10.4 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46389439708463076		[learning rate: 0.0001203]
	Learning Rate: 0.0001203
	LOSS [training: 0.46389439708463076 | validation: 0.5886724078348158]
	TIME [epoch: 10.4 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4431884875074544		[learning rate: 0.00011993]
	Learning Rate: 0.000119932
	LOSS [training: 0.4431884875074544 | validation: 0.5736472792777799]
	TIME [epoch: 10.5 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4565253235565767		[learning rate: 0.00011956]
	Learning Rate: 0.000119564
	LOSS [training: 0.4565253235565767 | validation: 0.5737352997434197]
	TIME [epoch: 10.5 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45202325632920476		[learning rate: 0.0001192]
	Learning Rate: 0.000119197
	LOSS [training: 0.45202325632920476 | validation: 0.5811598893697949]
	TIME [epoch: 10.5 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44505608811542813		[learning rate: 0.00011883]
	Learning Rate: 0.000118832
	LOSS [training: 0.44505608811542813 | validation: 0.5683266155024461]
	TIME [epoch: 10.5 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4401914920253963		[learning rate: 0.00011847]
	Learning Rate: 0.000118468
	LOSS [training: 0.4401914920253963 | validation: 0.5570481828872346]
	TIME [epoch: 10.5 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4508995090358459		[learning rate: 0.0001181]
	Learning Rate: 0.000118105
	LOSS [training: 0.4508995090358459 | validation: 0.569059783294419]
	TIME [epoch: 10.4 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44305267624497613		[learning rate: 0.00011774]
	Learning Rate: 0.000117743
	LOSS [training: 0.44305267624497613 | validation: 0.5708352923718932]
	TIME [epoch: 10.5 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44408110191843664		[learning rate: 0.00011738]
	Learning Rate: 0.000117382
	LOSS [training: 0.44408110191843664 | validation: 0.5609592948424923]
	TIME [epoch: 10.4 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4444435587611733		[learning rate: 0.00011702]
	Learning Rate: 0.000117022
	LOSS [training: 0.4444435587611733 | validation: 0.564493315584196]
	TIME [epoch: 10.5 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4557963330916041		[learning rate: 0.00011666]
	Learning Rate: 0.000116663
	LOSS [training: 0.4557963330916041 | validation: 0.5592309326045433]
	TIME [epoch: 10.5 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4639595155655288		[learning rate: 0.00011631]
	Learning Rate: 0.000116305
	LOSS [training: 0.4639595155655288 | validation: 0.574249153394287]
	TIME [epoch: 10.4 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45753770527435805		[learning rate: 0.00011595]
	Learning Rate: 0.000115949
	LOSS [training: 0.45753770527435805 | validation: 0.5757847929057344]
	TIME [epoch: 10.5 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4531930869443557		[learning rate: 0.00011559]
	Learning Rate: 0.000115593
	LOSS [training: 0.4531930869443557 | validation: 0.5521489884622638]
	TIME [epoch: 10.5 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4496333502943843		[learning rate: 0.00011524]
	Learning Rate: 0.000115239
	LOSS [training: 0.4496333502943843 | validation: 0.5586037826241171]
	TIME [epoch: 10.4 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45270417138277586		[learning rate: 0.00011489]
	Learning Rate: 0.000114886
	LOSS [training: 0.45270417138277586 | validation: 0.5799886205598273]
	TIME [epoch: 10.5 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46371232356337283		[learning rate: 0.00011453]
	Learning Rate: 0.000114534
	LOSS [training: 0.46371232356337283 | validation: 0.5883041724845218]
	TIME [epoch: 10.5 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4539182815456737		[learning rate: 0.00011418]
	Learning Rate: 0.000114183
	LOSS [training: 0.4539182815456737 | validation: 0.5398401836249477]
	TIME [epoch: 10.5 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47397494471911317		[learning rate: 0.00011383]
	Learning Rate: 0.000113833
	LOSS [training: 0.47397494471911317 | validation: 0.5747078734041534]
	TIME [epoch: 10.5 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46541001926168957		[learning rate: 0.00011348]
	Learning Rate: 0.000113484
	LOSS [training: 0.46541001926168957 | validation: 0.5741528997419865]
	TIME [epoch: 10.5 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4679099723483599		[learning rate: 0.00011314]
	Learning Rate: 0.000113136
	LOSS [training: 0.4679099723483599 | validation: 0.5710634097447027]
	TIME [epoch: 10.5 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47394187475415916		[learning rate: 0.00011279]
	Learning Rate: 0.000112789
	LOSS [training: 0.47394187475415916 | validation: 0.5777404108314171]
	TIME [epoch: 10.5 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48783171678842335		[learning rate: 0.00011244]
	Learning Rate: 0.000112443
	LOSS [training: 0.48783171678842335 | validation: 0.6016078221912581]
	TIME [epoch: 10.5 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4763662823199185		[learning rate: 0.0001121]
	Learning Rate: 0.000112099
	LOSS [training: 0.4763662823199185 | validation: 0.5801863099939479]
	TIME [epoch: 10.4 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4635389355751937		[learning rate: 0.00011175]
	Learning Rate: 0.000111755
	LOSS [training: 0.4635389355751937 | validation: 0.5400622877729527]
	TIME [epoch: 10.5 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4570298321085338		[learning rate: 0.00011141]
	Learning Rate: 0.000111412
	LOSS [training: 0.4570298321085338 | validation: 0.576879653677682]
	TIME [epoch: 10.4 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48736051111186807		[learning rate: 0.00011107]
	Learning Rate: 0.000111071
	LOSS [training: 0.48736051111186807 | validation: 0.5676942217113355]
	TIME [epoch: 10.4 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46872064713916106		[learning rate: 0.00011073]
	Learning Rate: 0.00011073
	LOSS [training: 0.46872064713916106 | validation: 0.5725629122459356]
	TIME [epoch: 10.4 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4778282737475861		[learning rate: 0.00011039]
	Learning Rate: 0.000110391
	LOSS [training: 0.4778282737475861 | validation: 0.5836243267511911]
	TIME [epoch: 10.4 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4597424313079985		[learning rate: 0.00011005]
	Learning Rate: 0.000110053
	LOSS [training: 0.4597424313079985 | validation: 0.5499337049983737]
	TIME [epoch: 10.4 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4507319094406812		[learning rate: 0.00010972]
	Learning Rate: 0.000109715
	LOSS [training: 0.4507319094406812 | validation: 0.5664047370077129]
	TIME [epoch: 10.4 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4515142477448336		[learning rate: 0.00010938]
	Learning Rate: 0.000109379
	LOSS [training: 0.4515142477448336 | validation: 0.5525973360257386]
	TIME [epoch: 10.4 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44569537592357095		[learning rate: 0.00010904]
	Learning Rate: 0.000109044
	LOSS [training: 0.44569537592357095 | validation: 0.5491011199400225]
	TIME [epoch: 10.4 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4489678794144359		[learning rate: 0.00010871]
	Learning Rate: 0.000108709
	LOSS [training: 0.4489678794144359 | validation: 0.5634612584940153]
	TIME [epoch: 10.4 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4427403887471435		[learning rate: 0.00010838]
	Learning Rate: 0.000108376
	LOSS [training: 0.4427403887471435 | validation: 0.56273311001719]
	TIME [epoch: 10.5 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45472651122186275		[learning rate: 0.00010804]
	Learning Rate: 0.000108044
	LOSS [training: 0.45472651122186275 | validation: 0.5802145223397964]
	TIME [epoch: 10.4 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4642459291186972		[learning rate: 0.00010771]
	Learning Rate: 0.000107713
	LOSS [training: 0.4642459291186972 | validation: 0.6007772613108598]
	TIME [epoch: 10.4 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4779111939226367		[learning rate: 0.00010738]
	Learning Rate: 0.000107382
	LOSS [training: 0.4779111939226367 | validation: 0.6328938244147037]
	TIME [epoch: 10.4 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4761265909510712		[learning rate: 0.00010705]
	Learning Rate: 0.000107053
	LOSS [training: 0.4761265909510712 | validation: 0.5898848443759204]
	TIME [epoch: 10.4 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.454647767424966		[learning rate: 0.00010673]
	Learning Rate: 0.000106725
	LOSS [training: 0.454647767424966 | validation: 0.605488353297604]
	TIME [epoch: 10.4 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4663556892795766		[learning rate: 0.0001064]
	Learning Rate: 0.000106398
	LOSS [training: 0.4663556892795766 | validation: 0.5918826163670543]
	TIME [epoch: 10.4 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4541859427896224		[learning rate: 0.00010607]
	Learning Rate: 0.000106072
	LOSS [training: 0.4541859427896224 | validation: 0.5880281890932783]
	TIME [epoch: 10.4 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45962608409537226		[learning rate: 0.00010575]
	Learning Rate: 0.000105747
	LOSS [training: 0.45962608409537226 | validation: 0.5828863559414827]
	TIME [epoch: 10.4 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.453869269448258		[learning rate: 0.00010542]
	Learning Rate: 0.000105423
	LOSS [training: 0.453869269448258 | validation: 0.5825286317875629]
	TIME [epoch: 10.4 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44902344080172185		[learning rate: 0.0001051]
	Learning Rate: 0.000105099
	LOSS [training: 0.44902344080172185 | validation: 0.5616409831640745]
	TIME [epoch: 10.4 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43962625006318934		[learning rate: 0.00010478]
	Learning Rate: 0.000104777
	LOSS [training: 0.43962625006318934 | validation: 0.580851090690037]
	TIME [epoch: 10.5 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4593977874795632		[learning rate: 0.00010446]
	Learning Rate: 0.000104456
	LOSS [training: 0.4593977874795632 | validation: 0.5932760569518912]
	TIME [epoch: 10.4 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4468920970186604		[learning rate: 0.00010414]
	Learning Rate: 0.000104136
	LOSS [training: 0.4468920970186604 | validation: 0.5779354965758126]
	TIME [epoch: 10.4 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45486402967286227		[learning rate: 0.00010382]
	Learning Rate: 0.000103817
	LOSS [training: 0.45486402967286227 | validation: 0.5669693955723868]
	TIME [epoch: 10.4 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4457954690884846		[learning rate: 0.0001035]
	Learning Rate: 0.000103498
	LOSS [training: 0.4457954690884846 | validation: 0.6308322270485764]
	TIME [epoch: 10.5 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45571665641012726		[learning rate: 0.00010318]
	Learning Rate: 0.000103181
	LOSS [training: 0.45571665641012726 | validation: 0.5794013308123906]
	TIME [epoch: 10.4 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.453205900836922		[learning rate: 0.00010286]
	Learning Rate: 0.000102865
	LOSS [training: 0.453205900836922 | validation: 0.5690174156154378]
	TIME [epoch: 10.4 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4455307220729452		[learning rate: 0.00010255]
	Learning Rate: 0.000102549
	LOSS [training: 0.4455307220729452 | validation: 0.5609331683025264]
	TIME [epoch: 10.5 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4353877416598479		[learning rate: 0.00010224]
	Learning Rate: 0.000102235
	LOSS [training: 0.4353877416598479 | validation: 0.5703936219352324]
	TIME [epoch: 10.5 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45009950681065247		[learning rate: 0.00010192]
	Learning Rate: 0.000101922
	LOSS [training: 0.45009950681065247 | validation: 0.5841905210306348]
	TIME [epoch: 10.4 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44124860020426143		[learning rate: 0.00010161]
	Learning Rate: 0.000101609
	LOSS [training: 0.44124860020426143 | validation: 0.561869150544878]
	TIME [epoch: 10.4 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4473568934695152		[learning rate: 0.0001013]
	Learning Rate: 0.000101298
	LOSS [training: 0.4473568934695152 | validation: 0.5542408317857428]
	TIME [epoch: 10.5 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45461012406353285		[learning rate: 0.00010099]
	Learning Rate: 0.000100987
	LOSS [training: 0.45461012406353285 | validation: 0.5619542346625097]
	TIME [epoch: 10.4 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44225390511947416		[learning rate: 0.00010068]
	Learning Rate: 0.000100678
	LOSS [training: 0.44225390511947416 | validation: 0.5470739705197067]
	TIME [epoch: 10.4 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4496393913171258		[learning rate: 0.00010037]
	Learning Rate: 0.000100369
	LOSS [training: 0.4496393913171258 | validation: 0.5412266630035196]
	TIME [epoch: 10.5 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4431430152704815		[learning rate: 0.00010006]
	Learning Rate: 0.000100061
	LOSS [training: 0.4431430152704815 | validation: 0.5563092043018644]
	TIME [epoch: 10.5 sec]
Finished training in 21034.554 seconds.
