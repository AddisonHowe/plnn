Args:
Namespace(name='model_tr_study5', outdir='out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1', training_data='data/transition_rate_studies/tr_study5/tr_study5_training/r1', validation_data='data/transition_rate_studies/tr_study5/tr_study5_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 154781771

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.405431368343288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.405431368343288 | validation: 11.230495940132208]
	TIME [epoch: 49.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.175517592069584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.175517592069584 | validation: 10.041750878871804]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.456179832877002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.456179832877002 | validation: 9.513380016939564]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.580905590683775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.580905590683775 | validation: 9.29462486769674]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.806600171197688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.806600171197688 | validation: 8.281258749344246]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.96023753517715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.96023753517715 | validation: 7.836340397767839]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.378111860973268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.378111860973268 | validation: 7.290692894849371]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.163100261578679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.163100261578679 | validation: 6.884981336167466]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.888865406129487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.888865406129487 | validation: 9.633309329913121]
	TIME [epoch: 10.3 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.612760661951166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.612760661951166 | validation: 7.04856634041297]
	TIME [epoch: 10.3 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.340562762038767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.340562762038767 | validation: 6.585303754051613]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.407959344727473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.407959344727473 | validation: 6.5372317613288375]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.992242868093248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.992242868093248 | validation: 7.098224249299142]
	TIME [epoch: 10.3 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.384025328263067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.384025328263067 | validation: 7.288013028681527]
	TIME [epoch: 10.3 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.916863555702109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.916863555702109 | validation: 6.6205252444573]
	TIME [epoch: 10.3 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.534619459190824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.534619459190824 | validation: 6.102908546873021]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.283255669808321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.283255669808321 | validation: 6.0556754894615645]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.178607651865907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.178607651865907 | validation: 5.973193710578025]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.055939669600795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.055939669600795 | validation: 5.580881788086499]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.839781983449348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.839781983449348 | validation: 5.676401937895113]
	TIME [epoch: 10.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.34178213007537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.34178213007537 | validation: 6.657187563636667]
	TIME [epoch: 10.3 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.231922588652343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.231922588652343 | validation: 5.671052777058357]
	TIME [epoch: 10.3 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.020072125549183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.020072125549183 | validation: 5.641903854384128]
	TIME [epoch: 10.3 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7788158200287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7788158200287 | validation: 5.941787896610602]
	TIME [epoch: 10.3 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.850992599744122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.850992599744122 | validation: 5.582455555805673]
	TIME [epoch: 10.3 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.743559197279869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.743559197279869 | validation: 5.810804901961428]
	TIME [epoch: 10.3 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.710052972347485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.710052972347485 | validation: 5.579122285298679]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.089147978042784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.089147978042784 | validation: 5.601539955311107]
	TIME [epoch: 10.3 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.959419989306949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.959419989306949 | validation: 5.471129763750961]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.669446730565572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.669446730565572 | validation: 5.559455375152145]
	TIME [epoch: 10.3 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.662775592381234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.662775592381234 | validation: 5.455464221758514]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.683362399972614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.683362399972614 | validation: 5.52573552742638]
	TIME [epoch: 10.3 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.079760480084184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.079760480084184 | validation: 6.044293721434959]
	TIME [epoch: 10.3 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7952591088647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7952591088647 | validation: 6.061103686358216]
	TIME [epoch: 10.3 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.039719551339849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.039719551339849 | validation: 5.987368871719675]
	TIME [epoch: 10.3 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.882742606280596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.882742606280596 | validation: 5.7344724585111635]
	TIME [epoch: 10.3 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.655392625858248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.655392625858248 | validation: 5.72629412685419]
	TIME [epoch: 10.3 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.564724435169291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.564724435169291 | validation: 5.3575842989272395]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.598462914192678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.598462914192678 | validation: 5.396140070680786]
	TIME [epoch: 10.3 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.614323722192796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.614323722192796 | validation: 5.393454248156652]
	TIME [epoch: 10.3 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.525343906983287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.525343906983287 | validation: 5.561340489977068]
	TIME [epoch: 10.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.66995950111694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.66995950111694 | validation: 5.904790114108278]
	TIME [epoch: 10.3 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.826915928371475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.826915928371475 | validation: 5.538548003957451]
	TIME [epoch: 10.3 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.589249289447721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.589249289447721 | validation: 5.403713078371511]
	TIME [epoch: 10.3 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.957104315060986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.957104315060986 | validation: 8.423637136721773]
	TIME [epoch: 10.3 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.418017496411455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.418017496411455 | validation: 5.959080205602052]
	TIME [epoch: 10.3 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.783694907447745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.783694907447745 | validation: 5.43072098976849]
	TIME [epoch: 10.3 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.628055234547966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.628055234547966 | validation: 5.353116268190531]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.528224464471019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.528224464471019 | validation: 5.373263663961076]
	TIME [epoch: 10.3 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.861009755627753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.861009755627753 | validation: 5.510858283731562]
	TIME [epoch: 10.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.515513384955932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.515513384955932 | validation: 5.421531105928754]
	TIME [epoch: 10.3 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.638732153423246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.638732153423246 | validation: 5.480984605847582]
	TIME [epoch: 10.3 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.471927112372549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.471927112372549 | validation: 5.48568748199702]
	TIME [epoch: 10.3 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.112584710354521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.112584710354521 | validation: 7.0618301101128695]
	TIME [epoch: 10.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.921422926037982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.921422926037982 | validation: 5.712410306261147]
	TIME [epoch: 10.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.972291657135581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.972291657135581 | validation: 5.6461132662044955]
	TIME [epoch: 10.3 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.369445458399257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.369445458399257 | validation: 5.255146104866274]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5296426272276715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5296426272276715 | validation: 5.253330183689391]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.451200838381769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.451200838381769 | validation: 5.485505344827299]
	TIME [epoch: 10.3 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.593946916386124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.593946916386124 | validation: 5.454673079126451]
	TIME [epoch: 10.3 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.588307684741126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.588307684741126 | validation: 5.482849246275853]
	TIME [epoch: 10.3 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.415464183009965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.415464183009965 | validation: 6.028475579264184]
	TIME [epoch: 10.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.300031719204384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.300031719204384 | validation: 5.709688719322424]
	TIME [epoch: 10.3 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.572144336096122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.572144336096122 | validation: 5.954345374046108]
	TIME [epoch: 10.3 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.948568936622875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.948568936622875 | validation: 7.035258079738875]
	TIME [epoch: 10.3 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.23297070965714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.23297070965714 | validation: 5.39956399269168]
	TIME [epoch: 10.3 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.563986486957738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.563986486957738 | validation: 7.787054004776098]
	TIME [epoch: 10.3 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.053358645027854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.053358645027854 | validation: 5.946995046195923]
	TIME [epoch: 10.3 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.956692617998639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.956692617998639 | validation: 5.688803023854425]
	TIME [epoch: 10.3 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.664696857880676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.664696857880676 | validation: 5.271873022945213]
	TIME [epoch: 10.3 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.475824953631452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.475824953631452 | validation: 5.1615549465269135]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.450870286627024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.450870286627024 | validation: 5.2882908883350686]
	TIME [epoch: 10.3 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.358788695047265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.358788695047265 | validation: 5.526530088640706]
	TIME [epoch: 10.3 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.849432482877736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.849432482877736 | validation: 5.474201613780776]
	TIME [epoch: 10.3 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.277450152367102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.277450152367102 | validation: 5.257657327083494]
	TIME [epoch: 10.3 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7264338244874216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7264338244874216 | validation: 5.362326983773443]
	TIME [epoch: 10.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.412255086815494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.412255086815494 | validation: 7.89003801580631]
	TIME [epoch: 10.3 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.63685767723801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.63685767723801 | validation: 5.555359291341288]
	TIME [epoch: 10.3 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.523253964829385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.523253964829385 | validation: 5.333909461812966]
	TIME [epoch: 10.3 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.376832843249003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.376832843249003 | validation: 5.506005497901988]
	TIME [epoch: 10.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.786963970883666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.786963970883666 | validation: 5.2199447552952964]
	TIME [epoch: 10.3 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.277376002966333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.277376002966333 | validation: 6.397041248038491]
	TIME [epoch: 10.3 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.301523453089359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.301523453089359 | validation: 6.8890581635187935]
	TIME [epoch: 10.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.966466948559621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.966466948559621 | validation: 5.106416363345432]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.263969785150552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.263969785150552 | validation: 5.237836895044677]
	TIME [epoch: 10.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.9361860053460775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.9361860053460775 | validation: 6.425169362924837]
	TIME [epoch: 10.3 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.91980125200667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.91980125200667 | validation: 5.554680046448919]
	TIME [epoch: 10.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.404886480097936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.404886480097936 | validation: 5.110980993713515]
	TIME [epoch: 10.3 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.064278971126955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.064278971126955 | validation: 5.521229905120178]
	TIME [epoch: 10.3 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.221717796943622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.221717796943622 | validation: 4.831542004611966]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.796102023317406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.796102023317406 | validation: 5.644846733647816]
	TIME [epoch: 10.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.93086072416901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.93086072416901 | validation: 5.655363963268683]
	TIME [epoch: 10.3 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.508870247723283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.508870247723283 | validation: 4.968461246652853]
	TIME [epoch: 10.3 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5370604267129035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5370604267129035 | validation: 6.524547625307307]
	TIME [epoch: 10.3 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.101052102207315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.101052102207315 | validation: 5.366286802264439]
	TIME [epoch: 10.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.776890446216076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.776890446216076 | validation: 6.509085864583959]
	TIME [epoch: 10.3 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.192506661486524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.192506661486524 | validation: 6.245734955680708]
	TIME [epoch: 10.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.861280202454694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.861280202454694 | validation: 7.8048211141583295]
	TIME [epoch: 10.3 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.899489619942964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.899489619942964 | validation: 5.446851979017159]
	TIME [epoch: 10.3 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.715453053061408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.715453053061408 | validation: 6.350029701949803]
	TIME [epoch: 10.3 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3454820054883285		[learning rate: 0.0099806]
	Learning Rate: 0.00998063
	LOSS [training: 6.3454820054883285 | validation: 5.531020162614753]
	TIME [epoch: 10.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.548768603386277		[learning rate: 0.0099565]
	Learning Rate: 0.00995647
	LOSS [training: 5.548768603386277 | validation: 5.340796749614603]
	TIME [epoch: 10.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.893082538549278		[learning rate: 0.0099324]
	Learning Rate: 0.00993236
	LOSS [training: 5.893082538549278 | validation: 5.96939646236774]
	TIME [epoch: 10.3 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.89351671106464		[learning rate: 0.0099083]
	Learning Rate: 0.00990832
	LOSS [training: 5.89351671106464 | validation: 5.878589871060289]
	TIME [epoch: 10.3 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.617820382387112		[learning rate: 0.0098843]
	Learning Rate: 0.00988433
	LOSS [training: 5.617820382387112 | validation: 6.104479683707525]
	TIME [epoch: 10.3 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.547709725811736		[learning rate: 0.0098604]
	Learning Rate: 0.0098604
	LOSS [training: 5.547709725811736 | validation: 5.325169252268488]
	TIME [epoch: 10.3 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.2666952379309775		[learning rate: 0.0098365]
	Learning Rate: 0.00983653
	LOSS [training: 6.2666952379309775 | validation: 6.971615923928925]
	TIME [epoch: 10.3 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.089207701931629		[learning rate: 0.0098127]
	Learning Rate: 0.00981272
	LOSS [training: 6.089207701931629 | validation: 6.0742792782303]
	TIME [epoch: 10.3 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.893515940875304		[learning rate: 0.009789]
	Learning Rate: 0.00978897
	LOSS [training: 5.893515940875304 | validation: 5.39989857907977]
	TIME [epoch: 10.3 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.384685339243102		[learning rate: 0.0097653]
	Learning Rate: 0.00976527
	LOSS [training: 5.384685339243102 | validation: 5.6758563380406155]
	TIME [epoch: 10.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.980692192349305		[learning rate: 0.0097416]
	Learning Rate: 0.00974163
	LOSS [training: 5.980692192349305 | validation: 5.419615573609512]
	TIME [epoch: 10.3 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3292248673309235		[learning rate: 0.009718]
	Learning Rate: 0.00971805
	LOSS [training: 5.3292248673309235 | validation: 5.182372439613123]
	TIME [epoch: 10.3 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.309391765090522		[learning rate: 0.0096945]
	Learning Rate: 0.00969452
	LOSS [training: 5.309391765090522 | validation: 5.305088339619304]
	TIME [epoch: 10.3 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.133998817018949		[learning rate: 0.0096711]
	Learning Rate: 0.00967105
	LOSS [training: 5.133998817018949 | validation: 6.3940299355403845]
	TIME [epoch: 10.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.55920189175813		[learning rate: 0.0096476]
	Learning Rate: 0.00964764
	LOSS [training: 6.55920189175813 | validation: 5.058816751134714]
	TIME [epoch: 10.3 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.401883600068986		[learning rate: 0.0096243]
	Learning Rate: 0.00962428
	LOSS [training: 5.401883600068986 | validation: 5.255851254316619]
	TIME [epoch: 10.3 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2153541347558345		[learning rate: 0.009601]
	Learning Rate: 0.00960098
	LOSS [training: 5.2153541347558345 | validation: 5.222603575986585]
	TIME [epoch: 10.3 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.002919405323543		[learning rate: 0.0095777]
	Learning Rate: 0.00957774
	LOSS [training: 6.002919405323543 | validation: 5.375929207349665]
	TIME [epoch: 10.3 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.288016601581402		[learning rate: 0.0095546]
	Learning Rate: 0.00955456
	LOSS [training: 5.288016601581402 | validation: 4.910607886438519]
	TIME [epoch: 10.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.214844774114907		[learning rate: 0.0095314]
	Learning Rate: 0.00953143
	LOSS [training: 5.214844774114907 | validation: 4.939953644616025]
	TIME [epoch: 10.3 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.844216874574586		[learning rate: 0.0095084]
	Learning Rate: 0.00950835
	LOSS [training: 4.844216874574586 | validation: 4.756944134476283]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.606317503630256		[learning rate: 0.0094853]
	Learning Rate: 0.00948533
	LOSS [training: 4.606317503630256 | validation: 5.549349115936889]
	TIME [epoch: 10.3 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170691754080549		[learning rate: 0.0094624]
	Learning Rate: 0.00946237
	LOSS [training: 5.170691754080549 | validation: 5.478438696306565]
	TIME [epoch: 10.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.750989501130421		[learning rate: 0.0094395]
	Learning Rate: 0.00943946
	LOSS [training: 4.750989501130421 | validation: 4.473220401653136]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.266831486999855		[learning rate: 0.0094166]
	Learning Rate: 0.00941661
	LOSS [training: 4.266831486999855 | validation: 4.39491466738015]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.235780373920765		[learning rate: 0.0093938]
	Learning Rate: 0.00939382
	LOSS [training: 4.235780373920765 | validation: 3.818277445606953]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.091669565756581		[learning rate: 0.0093711]
	Learning Rate: 0.00937108
	LOSS [training: 4.091669565756581 | validation: 7.202673134773136]
	TIME [epoch: 10.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.627192585917011		[learning rate: 0.0093484]
	Learning Rate: 0.00934839
	LOSS [training: 7.627192585917011 | validation: 7.417934416735687]
	TIME [epoch: 10.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.416038444125187		[learning rate: 0.0093258]
	Learning Rate: 0.00932576
	LOSS [training: 6.416038444125187 | validation: 4.371402647472956]
	TIME [epoch: 10.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.333913497250895		[learning rate: 0.0093032]
	Learning Rate: 0.00930318
	LOSS [training: 4.333913497250895 | validation: 5.7141378960007145]
	TIME [epoch: 10.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.54348170686836		[learning rate: 0.0092807]
	Learning Rate: 0.00928066
	LOSS [training: 4.54348170686836 | validation: 3.6603741999734907]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7795495715988685		[learning rate: 0.0092582]
	Learning Rate: 0.00925819
	LOSS [training: 3.7795495715988685 | validation: 6.74155395882965]
	TIME [epoch: 10.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.714409802559532		[learning rate: 0.0092358]
	Learning Rate: 0.00923578
	LOSS [training: 4.714409802559532 | validation: 3.53118426725394]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.374054534155891		[learning rate: 0.0092134]
	Learning Rate: 0.00921342
	LOSS [training: 4.374054534155891 | validation: 5.469362753145633]
	TIME [epoch: 10.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.346833949586539		[learning rate: 0.0091911]
	Learning Rate: 0.00919112
	LOSS [training: 4.346833949586539 | validation: 3.7490029796400712]
	TIME [epoch: 10.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7254240538578265		[learning rate: 0.0091689]
	Learning Rate: 0.00916887
	LOSS [training: 3.7254240538578265 | validation: 3.3906362331042215]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7055330529473807		[learning rate: 0.0091467]
	Learning Rate: 0.00914667
	LOSS [training: 3.7055330529473807 | validation: 5.127184150646055]
	TIME [epoch: 10.3 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1236436945348265		[learning rate: 0.0091245]
	Learning Rate: 0.00912453
	LOSS [training: 6.1236436945348265 | validation: 6.942754286482653]
	TIME [epoch: 10.3 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.119264919077696		[learning rate: 0.0091024]
	Learning Rate: 0.00910244
	LOSS [training: 7.119264919077696 | validation: 4.421820470324416]
	TIME [epoch: 10.3 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.235533777932771		[learning rate: 0.0090804]
	Learning Rate: 0.00908041
	LOSS [training: 4.235533777932771 | validation: 4.457739046962283]
	TIME [epoch: 10.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.993560699545301		[learning rate: 0.0090584]
	Learning Rate: 0.00905842
	LOSS [training: 3.993560699545301 | validation: 3.52572323167203]
	TIME [epoch: 10.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.288720916365723		[learning rate: 0.0090365]
	Learning Rate: 0.00903649
	LOSS [training: 5.288720916365723 | validation: 8.093649936623367]
	TIME [epoch: 10.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.922268870575574		[learning rate: 0.0090146]
	Learning Rate: 0.00901462
	LOSS [training: 6.922268870575574 | validation: 5.3736757471891465]
	TIME [epoch: 10.3 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.821033163052223		[learning rate: 0.0089928]
	Learning Rate: 0.0089928
	LOSS [training: 4.821033163052223 | validation: 3.779927674500633]
	TIME [epoch: 10.3 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.151709995051448		[learning rate: 0.008971]
	Learning Rate: 0.00897103
	LOSS [training: 4.151709995051448 | validation: 5.104878985286582]
	TIME [epoch: 10.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.938717318301794		[learning rate: 0.0089493]
	Learning Rate: 0.00894931
	LOSS [training: 5.938717318301794 | validation: 4.970471824911899]
	TIME [epoch: 10.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.752538322988438		[learning rate: 0.0089276]
	Learning Rate: 0.00892764
	LOSS [training: 4.752538322988438 | validation: 3.9364859187201855]
	TIME [epoch: 10.3 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8284407927658797		[learning rate: 0.008906]
	Learning Rate: 0.00890603
	LOSS [training: 3.8284407927658797 | validation: 3.399714946706283]
	TIME [epoch: 10.3 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4735613974342945		[learning rate: 0.0088845]
	Learning Rate: 0.00888447
	LOSS [training: 3.4735613974342945 | validation: 3.6304153017230614]
	TIME [epoch: 10.3 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4185353170620303		[learning rate: 0.008863]
	Learning Rate: 0.00886296
	LOSS [training: 3.4185353170620303 | validation: 3.248881647866637]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8769533626751		[learning rate: 0.0088415]
	Learning Rate: 0.00884151
	LOSS [training: 3.8769533626751 | validation: 7.4804155030258315]
	TIME [epoch: 10.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.656174311540264		[learning rate: 0.0088201]
	Learning Rate: 0.0088201
	LOSS [training: 7.656174311540264 | validation: 6.653882961360941]
	TIME [epoch: 10.3 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.702360766102731		[learning rate: 0.0087988]
	Learning Rate: 0.00879875
	LOSS [training: 6.702360766102731 | validation: 7.5773513986367425]
	TIME [epoch: 10.3 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.671311149058695		[learning rate: 0.0087775]
	Learning Rate: 0.00877745
	LOSS [training: 6.671311149058695 | validation: 6.1437770991668526]
	TIME [epoch: 10.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.919885427337692		[learning rate: 0.0087562]
	Learning Rate: 0.0087562
	LOSS [training: 5.919885427337692 | validation: 5.76218804576689]
	TIME [epoch: 10.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.654188246495297		[learning rate: 0.008735]
	Learning Rate: 0.008735
	LOSS [training: 4.654188246495297 | validation: 4.357548694187594]
	TIME [epoch: 10.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.087138373097002		[learning rate: 0.0087139]
	Learning Rate: 0.00871386
	LOSS [training: 4.087138373097002 | validation: 3.416929247142643]
	TIME [epoch: 10.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5179958561963325		[learning rate: 0.0086928]
	Learning Rate: 0.00869276
	LOSS [training: 3.5179958561963325 | validation: 3.3132659516595573]
	TIME [epoch: 10.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.124754240323074		[learning rate: 0.0086717]
	Learning Rate: 0.00867172
	LOSS [training: 3.124754240323074 | validation: 4.098065878812689]
	TIME [epoch: 10.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2078661094327194		[learning rate: 0.0086507]
	Learning Rate: 0.00865073
	LOSS [training: 3.2078661094327194 | validation: 3.6424100782214666]
	TIME [epoch: 10.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2112077141154516		[learning rate: 0.0086298]
	Learning Rate: 0.00862979
	LOSS [training: 3.2112077141154516 | validation: 3.2180119408408006]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.372980648265127		[learning rate: 0.0086089]
	Learning Rate: 0.00860889
	LOSS [training: 3.372980648265127 | validation: 2.693069935347411]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7289843395489335		[learning rate: 0.0085881]
	Learning Rate: 0.00858805
	LOSS [training: 3.7289843395489335 | validation: 4.0692478528651375]
	TIME [epoch: 10.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.015699558642802		[learning rate: 0.0085673]
	Learning Rate: 0.00856726
	LOSS [training: 3.015699558642802 | validation: 3.2338979869592355]
	TIME [epoch: 10.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.326884767899881		[learning rate: 0.0085465]
	Learning Rate: 0.00854652
	LOSS [training: 3.326884767899881 | validation: 2.751664576755344]
	TIME [epoch: 10.3 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4762082661082947		[learning rate: 0.0085258]
	Learning Rate: 0.00852583
	LOSS [training: 2.4762082661082947 | validation: 2.484016013473656]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4527555515064896		[learning rate: 0.0085052]
	Learning Rate: 0.00850519
	LOSS [training: 2.4527555515064896 | validation: 6.716213823945384]
	TIME [epoch: 10.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.863921663936594		[learning rate: 0.0084846]
	Learning Rate: 0.0084846
	LOSS [training: 3.863921663936594 | validation: 2.5707437999758667]
	TIME [epoch: 10.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4034754907932805		[learning rate: 0.0084641]
	Learning Rate: 0.00846406
	LOSS [training: 2.4034754907932805 | validation: 2.2740116846423852]
	TIME [epoch: 10.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3323588838594227		[learning rate: 0.0084436]
	Learning Rate: 0.00844357
	LOSS [training: 2.3323588838594227 | validation: 2.353622960996852]
	TIME [epoch: 10.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2875609217248387		[learning rate: 0.0084231]
	Learning Rate: 0.00842313
	LOSS [training: 2.2875609217248387 | validation: 2.4279220363463168]
	TIME [epoch: 10.3 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5434424045829642		[learning rate: 0.0084027]
	Learning Rate: 0.00840274
	LOSS [training: 2.5434424045829642 | validation: 3.894890649588474]
	TIME [epoch: 10.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4152741815957413		[learning rate: 0.0083824]
	Learning Rate: 0.0083824
	LOSS [training: 2.4152741815957413 | validation: 1.983347265261621]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.755175393936735		[learning rate: 0.0083621]
	Learning Rate: 0.00836211
	LOSS [training: 2.755175393936735 | validation: 2.409661787149681]
	TIME [epoch: 10.3 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3866228410689483		[learning rate: 0.0083419]
	Learning Rate: 0.00834186
	LOSS [training: 2.3866228410689483 | validation: 3.084990210766655]
	TIME [epoch: 10.3 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4075150752504855		[learning rate: 0.0083217]
	Learning Rate: 0.00832167
	LOSS [training: 2.4075150752504855 | validation: 2.1677577077420724]
	TIME [epoch: 10.3 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6580498849146763		[learning rate: 0.0083015]
	Learning Rate: 0.00830153
	LOSS [training: 2.6580498849146763 | validation: 2.3310577713463188]
	TIME [epoch: 10.3 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9260603065021642		[learning rate: 0.0082814]
	Learning Rate: 0.00828143
	LOSS [training: 1.9260603065021642 | validation: 3.0945265070198844]
	TIME [epoch: 10.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6775973513711504		[learning rate: 0.0082614]
	Learning Rate: 0.00826138
	LOSS [training: 2.6775973513711504 | validation: 2.1329810389898753]
	TIME [epoch: 10.3 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9447846994223679		[learning rate: 0.0082414]
	Learning Rate: 0.00824138
	LOSS [training: 1.9447846994223679 | validation: 1.866770365189022]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r1_20240219_183143/states/model_tr_study5_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259638833414347		[learning rate: 0.0082214]
	Learning Rate: 0.00822143
	LOSS [training: 2.259638833414347 | validation: 7.772132661776944]
	TIME [epoch: 10.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.05513634595134		[learning rate: 0.0082015]
	Learning Rate: 0.00820153
	LOSS [training: 7.05513634595134 | validation: 6.186598801559878]
	TIME [epoch: 10.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.904263762497543		[learning rate: 0.0081817]
	Learning Rate: 0.00818167
	LOSS [training: 5.904263762497543 | validation: 6.025707769638177]
	TIME [epoch: 10.3 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.002217635909138		[learning rate: 0.0081619]
	Learning Rate: 0.00816187
	LOSS [training: 6.002217635909138 | validation: 5.970161192035195]
	TIME [epoch: 10.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.848766142956177		[learning rate: 0.0081421]
	Learning Rate: 0.00814211
	LOSS [training: 5.848766142956177 | validation: 6.025606446470067]
	TIME [epoch: 10.3 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8131860685887915		[learning rate: 0.0081224]
	Learning Rate: 0.0081224
	LOSS [training: 5.8131860685887915 | validation: 6.4040203864733]
	TIME [epoch: 10.3 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.208458074515383		[learning rate: 0.0081027]
	Learning Rate: 0.00810273
	LOSS [training: 6.208458074515383 | validation: 6.119056433510013]
	TIME [epoch: 10.3 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.959246093309071		[learning rate: 0.0080831]
	Learning Rate: 0.00808312
	LOSS [training: 5.959246093309071 | validation: 6.248482835050862]
	TIME [epoch: 10.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.983881520439936		[learning rate: 0.0080636]
	Learning Rate: 0.00806355
	LOSS [training: 5.983881520439936 | validation: 5.782966509092937]
	TIME [epoch: 10.3 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.9066859212446845		[learning rate: 0.008044]
	Learning Rate: 0.00804403
	LOSS [training: 5.9066859212446845 | validation: 6.523229730407815]
	TIME [epoch: 10.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0550792175695225		[learning rate: 0.0080246]
	Learning Rate: 0.00802456
	LOSS [training: 6.0550792175695225 | validation: 5.904773965979703]
	TIME [epoch: 10.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.826874711985486		[learning rate: 0.0080051]
	Learning Rate: 0.00800513
	LOSS [training: 5.826874711985486 | validation: 5.934351716727067]
	TIME [epoch: 10.3 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.771632971376677		[learning rate: 0.0079858]
	Learning Rate: 0.00798575
	LOSS [training: 5.771632971376677 | validation: 5.908091903422029]
	TIME [epoch: 10.3 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.703714250702769		[learning rate: 0.0079664]
	Learning Rate: 0.00796642
	LOSS [training: 5.703714250702769 | validation: 5.777253096379972]
	TIME [epoch: 10.3 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.745625876777566		[learning rate: 0.0079471]
	Learning Rate: 0.00794713
	LOSS [training: 5.745625876777566 | validation: 5.70754570485886]
	TIME [epoch: 10.3 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.891267448575377		[learning rate: 0.0079279]
	Learning Rate: 0.00792789
	LOSS [training: 5.891267448575377 | validation: 5.86181776416523]
	TIME [epoch: 10.3 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.862313354164688		[learning rate: 0.0079087]
	Learning Rate: 0.0079087
	LOSS [training: 5.862313354164688 | validation: 5.776365605712975]
	TIME [epoch: 10.3 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.695388918664144		[learning rate: 0.0078896]
	Learning Rate: 0.00788956
	LOSS [training: 5.695388918664144 | validation: 6.088560154365735]
	TIME [epoch: 10.3 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.689943400872311		[learning rate: 0.0078705]
	Learning Rate: 0.00787046
	LOSS [training: 5.689943400872311 | validation: 5.740346513821014]
	TIME [epoch: 10.3 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.69373831269472		[learning rate: 0.0078514]
	Learning Rate: 0.0078514
	LOSS [training: 5.69373831269472 | validation: 5.737935473973817]
	TIME [epoch: 10.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.583333199670339		[learning rate: 0.0078324]
	Learning Rate: 0.0078324
	LOSS [training: 5.583333199670339 | validation: 5.863431594647072]
	TIME [epoch: 10.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.632709249220659		[learning rate: 0.0078134]
	Learning Rate: 0.00781344
	LOSS [training: 5.632709249220659 | validation: 6.087779939513086]
	TIME [epoch: 10.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.790017762642108		[learning rate: 0.0077945]
	Learning Rate: 0.00779452
	LOSS [training: 5.790017762642108 | validation: 5.597186658757539]
	TIME [epoch: 10.3 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7820741426246265		[learning rate: 0.0077757]
	Learning Rate: 0.00777565
	LOSS [training: 5.7820741426246265 | validation: 6.37388383250343]
	TIME [epoch: 10.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7476909962128175		[learning rate: 0.0077568]
	Learning Rate: 0.00775683
	LOSS [training: 5.7476909962128175 | validation: 5.788759923317311]
	TIME [epoch: 10.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.796834718982204		[learning rate: 0.0077381]
	Learning Rate: 0.00773805
	LOSS [training: 5.796834718982204 | validation: 5.6555599000378365]
	TIME [epoch: 10.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.574489721370326		[learning rate: 0.0077193]
	Learning Rate: 0.00771932
	LOSS [training: 5.574489721370326 | validation: 5.92960104499191]
	TIME [epoch: 10.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.72808594309276		[learning rate: 0.0077006]
	Learning Rate: 0.00770063
	LOSS [training: 5.72808594309276 | validation: 5.665012694763166]
	TIME [epoch: 10.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.629049799004638		[learning rate: 0.007682]
	Learning Rate: 0.00768199
	LOSS [training: 5.629049799004638 | validation: 5.721612083342263]
	TIME [epoch: 10.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.756304958552147		[learning rate: 0.0076634]
	Learning Rate: 0.00766339
	LOSS [training: 5.756304958552147 | validation: 6.450702982278203]
	TIME [epoch: 10.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.707151203380052		[learning rate: 0.0076448]
	Learning Rate: 0.00764484
	LOSS [training: 5.707151203380052 | validation: 5.647638474676973]
	TIME [epoch: 10.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.546956832667934		[learning rate: 0.0076263]
	Learning Rate: 0.00762633
	LOSS [training: 5.546956832667934 | validation: 5.817036703391211]
	TIME [epoch: 10.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.55286547780199		[learning rate: 0.0076079]
	Learning Rate: 0.00760787
	LOSS [training: 5.55286547780199 | validation: 5.674241809781497]
	TIME [epoch: 10.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5519336136385435		[learning rate: 0.0075895]
	Learning Rate: 0.00758945
	LOSS [training: 5.5519336136385435 | validation: 6.364764199873877]
	TIME [epoch: 10.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.861297740536303		[learning rate: 0.0075711]
	Learning Rate: 0.00757108
	LOSS [training: 5.861297740536303 | validation: 5.7859694106318855]
	TIME [epoch: 10.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6126400072724865		[learning rate: 0.0075528]
	Learning Rate: 0.00755275
	LOSS [training: 5.6126400072724865 | validation: 6.296806370754848]
	TIME [epoch: 10.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.879713804414667		[learning rate: 0.0075345]
	Learning Rate: 0.00753447
	LOSS [training: 5.879713804414667 | validation: 5.987537027472992]
	TIME [epoch: 10.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.863650344441188		[learning rate: 0.0075162]
	Learning Rate: 0.00751623
	LOSS [training: 5.863650344441188 | validation: 6.191823251010935]
	TIME [epoch: 10.3 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8885704060955035		[learning rate: 0.007498]
	Learning Rate: 0.00749803
	LOSS [training: 5.8885704060955035 | validation: 5.6559979021061775]
	TIME [epoch: 10.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.872136577881692		[learning rate: 0.0074799]
	Learning Rate: 0.00747988
	LOSS [training: 5.872136577881692 | validation: 6.301643441408118]
	TIME [epoch: 10.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8339096252844005		[learning rate: 0.0074618]
	Learning Rate: 0.00746177
	LOSS [training: 5.8339096252844005 | validation: 5.682566457050482]
	TIME [epoch: 10.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.736971373662599		[learning rate: 0.0074437]
	Learning Rate: 0.00744371
	LOSS [training: 5.736971373662599 | validation: 5.755360838721033]
	TIME [epoch: 10.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5968653927703595		[learning rate: 0.0074257]
	Learning Rate: 0.00742569
	LOSS [training: 5.5968653927703595 | validation: 5.617736158924679]
	TIME [epoch: 10.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.528889655429129		[learning rate: 0.0074077]
	Learning Rate: 0.00740771
	LOSS [training: 5.528889655429129 | validation: 5.894499668240026]
	TIME [epoch: 10.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.511050616743645		[learning rate: 0.0073898]
	Learning Rate: 0.00738978
	LOSS [training: 5.511050616743645 | validation: 5.6182956689075185]
	TIME [epoch: 10.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.559281216465594		[learning rate: 0.0073719]
	Learning Rate: 0.00737189
	LOSS [training: 5.559281216465594 | validation: 5.712219571149814]
	TIME [epoch: 10.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.545659607765985		[learning rate: 0.007354]
	Learning Rate: 0.00735405
	LOSS [training: 5.545659607765985 | validation: 5.582407457201432]
	TIME [epoch: 10.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5256327098364135		[learning rate: 0.0073362]
	Learning Rate: 0.00733624
	LOSS [training: 5.5256327098364135 | validation: 5.641139451956101]
	TIME [epoch: 10.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.778177926246525		[learning rate: 0.0073185]
	Learning Rate: 0.00731848
	LOSS [training: 5.778177926246525 | validation: 6.335799417146015]
	TIME [epoch: 10.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8178074600366925		[learning rate: 0.0073008]
	Learning Rate: 0.00730077
	LOSS [training: 5.8178074600366925 | validation: 5.778495961977219]
	TIME [epoch: 10.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6813615579560555		[learning rate: 0.0072831]
	Learning Rate: 0.00728309
	LOSS [training: 5.6813615579560555 | validation: 5.678722596198107]
	TIME [epoch: 10.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5756245889933975		[learning rate: 0.0072655]
	Learning Rate: 0.00726546
	LOSS [training: 5.5756245889933975 | validation: 5.755474875610038]
	TIME [epoch: 10.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.581772644704281		[learning rate: 0.0072479]
	Learning Rate: 0.00724787
	LOSS [training: 5.581772644704281 | validation: 5.732241017275138]
	TIME [epoch: 10.3 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.476628719225628		[learning rate: 0.0072303]
	Learning Rate: 0.00723033
	LOSS [training: 5.476628719225628 | validation: 5.88468576464972]
	TIME [epoch: 10.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6763446995107305		[learning rate: 0.0072128]
	Learning Rate: 0.00721282
	LOSS [training: 5.6763446995107305 | validation: 6.474847788873778]
	TIME [epoch: 10.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.598790395654329		[learning rate: 0.0071954]
	Learning Rate: 0.00719536
	LOSS [training: 5.598790395654329 | validation: 6.03203782925476]
	TIME [epoch: 10.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.558087622181209		[learning rate: 0.0071779]
	Learning Rate: 0.00717794
	LOSS [training: 5.558087622181209 | validation: 5.925470371784184]
	TIME [epoch: 10.3 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6822971668832825		[learning rate: 0.0071606]
	Learning Rate: 0.00716057
	LOSS [training: 5.6822971668832825 | validation: 5.53123792572794]
	TIME [epoch: 10.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.586454055732664		[learning rate: 0.0071432]
	Learning Rate: 0.00714323
	LOSS [training: 5.586454055732664 | validation: 5.652586567124681]
	TIME [epoch: 10.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.533531764630541		[learning rate: 0.0071259]
	Learning Rate: 0.00712594
	LOSS [training: 5.533531764630541 | validation: 5.647190586948796]
	TIME [epoch: 10.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.628192926661252		[learning rate: 0.0071087]
	Learning Rate: 0.00710869
	LOSS [training: 5.628192926661252 | validation: 5.723355730710648]
	TIME [epoch: 10.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.618688919469338		[learning rate: 0.0070915]
	Learning Rate: 0.00709148
	LOSS [training: 5.618688919469338 | validation: 5.65533361268411]
	TIME [epoch: 10.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.561186795119953		[learning rate: 0.0070743]
	Learning Rate: 0.00707431
	LOSS [training: 5.561186795119953 | validation: 5.761388188154674]
	TIME [epoch: 10.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.524929405514216		[learning rate: 0.0070572]
	Learning Rate: 0.00705719
	LOSS [training: 5.524929405514216 | validation: 5.574803564468855]
	TIME [epoch: 10.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.625510745150702		[learning rate: 0.0070401]
	Learning Rate: 0.0070401
	LOSS [training: 5.625510745150702 | validation: 5.843755174260819]
	TIME [epoch: 10.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5421048657687795		[learning rate: 0.0070231]
	Learning Rate: 0.00702306
	LOSS [training: 5.5421048657687795 | validation: 5.576259676441601]
	TIME [epoch: 10.3 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.64158449931242		[learning rate: 0.0070061]
	Learning Rate: 0.00700606
	LOSS [training: 5.64158449931242 | validation: 5.7118373972727206]
	TIME [epoch: 10.3 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5683341923533485		[learning rate: 0.0069891]
	Learning Rate: 0.0069891
	LOSS [training: 5.5683341923533485 | validation: 5.657420253238139]
	TIME [epoch: 10.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.50435125274195		[learning rate: 0.0069722]
	Learning Rate: 0.00697218
	LOSS [training: 5.50435125274195 | validation: 5.766640305728422]
	TIME [epoch: 10.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.587037869898929		[learning rate: 0.0069553]
	Learning Rate: 0.0069553
	LOSS [training: 5.587037869898929 | validation: 5.881676022558729]
	TIME [epoch: 10.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.569887302174727		[learning rate: 0.0069385]
	Learning Rate: 0.00693846
	LOSS [training: 5.569887302174727 | validation: 5.584295319574096]
	TIME [epoch: 10.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.448504503983537		[learning rate: 0.0069217]
	Learning Rate: 0.00692166
	LOSS [training: 5.448504503983537 | validation: 5.77655606806728]
	TIME [epoch: 10.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.548870713900607		[learning rate: 0.0069049]
	Learning Rate: 0.00690491
	LOSS [training: 5.548870713900607 | validation: 5.701505807219778]
	TIME [epoch: 10.3 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.535580318170662		[learning rate: 0.0068882]
	Learning Rate: 0.00688819
	LOSS [training: 5.535580318170662 | validation: 5.704761756379141]
	TIME [epoch: 10.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.459910352803362		[learning rate: 0.0068715]
	Learning Rate: 0.00687152
	LOSS [training: 5.459910352803362 | validation: 5.667776641592582]
	TIME [epoch: 10.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.517139038687171		[learning rate: 0.0068549]
	Learning Rate: 0.00685488
	LOSS [training: 5.517139038687171 | validation: 5.678212510398213]
	TIME [epoch: 10.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.481934676119373		[learning rate: 0.0068383]
	Learning Rate: 0.00683829
	LOSS [training: 5.481934676119373 | validation: 5.549987598798971]
	TIME [epoch: 10.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.542086504348548		[learning rate: 0.0068217]
	Learning Rate: 0.00682173
	LOSS [training: 5.542086504348548 | validation: 5.616076077099102]
	TIME [epoch: 10.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5242966422484985		[learning rate: 0.0068052]
	Learning Rate: 0.00680522
	LOSS [training: 5.5242966422484985 | validation: 5.501798575664838]
	TIME [epoch: 10.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.504605227843184		[learning rate: 0.0067887]
	Learning Rate: 0.00678874
	LOSS [training: 5.504605227843184 | validation: 5.751142927808454]
	TIME [epoch: 10.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4721302807618475		[learning rate: 0.0067723]
	Learning Rate: 0.00677231
	LOSS [training: 5.4721302807618475 | validation: 5.500171317489719]
	TIME [epoch: 10.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.419458256826502		[learning rate: 0.0067559]
	Learning Rate: 0.00675592
	LOSS [training: 5.419458256826502 | validation: 5.540620417116215]
	TIME [epoch: 10.3 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.497600131699833		[learning rate: 0.0067396]
	Learning Rate: 0.00673956
	LOSS [training: 5.497600131699833 | validation: 5.626089777942038]
	TIME [epoch: 10.3 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.492953281037331		[learning rate: 0.0067232]
	Learning Rate: 0.00672325
	LOSS [training: 5.492953281037331 | validation: 5.5580393416898835]
	TIME [epoch: 10.3 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.512502767750963		[learning rate: 0.006707]
	Learning Rate: 0.00670697
	LOSS [training: 5.512502767750963 | validation: 5.886360725896024]
	TIME [epoch: 10.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.527143313031423		[learning rate: 0.0066907]
	Learning Rate: 0.00669073
	LOSS [training: 5.527143313031423 | validation: 5.789834876509099]
	TIME [epoch: 10.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.638255537063303		[learning rate: 0.0066745]
	Learning Rate: 0.00667454
	LOSS [training: 5.638255537063303 | validation: 6.429176486348393]
	TIME [epoch: 10.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.583363700409077		[learning rate: 0.0066584]
	Learning Rate: 0.00665838
	LOSS [training: 5.583363700409077 | validation: 5.65905588748614]
	TIME [epoch: 10.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5128647320256166		[learning rate: 0.0066423]
	Learning Rate: 0.00664226
	LOSS [training: 5.5128647320256166 | validation: 5.5725287224064575]
	TIME [epoch: 10.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.443567778059629		[learning rate: 0.0066262]
	Learning Rate: 0.00662618
	LOSS [training: 5.443567778059629 | validation: 5.835286701538426]
	TIME [epoch: 10.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5045971568234515		[learning rate: 0.0066101]
	Learning Rate: 0.00661014
	LOSS [training: 5.5045971568234515 | validation: 5.742936769092912]
	TIME [epoch: 10.3 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.491393591709893		[learning rate: 0.0065941]
	Learning Rate: 0.00659414
	LOSS [training: 5.491393591709893 | validation: 5.5872478021017296]
	TIME [epoch: 10.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.433302276237116		[learning rate: 0.0065782]
	Learning Rate: 0.00657817
	LOSS [training: 5.433302276237116 | validation: 5.74483506201538]
	TIME [epoch: 10.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.394725402218549		[learning rate: 0.0065622]
	Learning Rate: 0.00656225
	LOSS [training: 5.394725402218549 | validation: 5.580976519960302]
	TIME [epoch: 10.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.562612140417189		[learning rate: 0.0065464]
	Learning Rate: 0.00654636
	LOSS [training: 5.562612140417189 | validation: 5.548099713075937]
	TIME [epoch: 10.3 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.403980177353196		[learning rate: 0.0065305]
	Learning Rate: 0.00653051
	LOSS [training: 5.403980177353196 | validation: 5.573320709574823]
	TIME [epoch: 10.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.653756931991515		[learning rate: 0.0065147]
	Learning Rate: 0.0065147
	LOSS [training: 5.653756931991515 | validation: 5.801072106414449]
	TIME [epoch: 10.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.505392662222329		[learning rate: 0.0064989]
	Learning Rate: 0.00649893
	LOSS [training: 5.505392662222329 | validation: 5.8802573386723065]
	TIME [epoch: 10.3 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.469205172119425		[learning rate: 0.0064832]
	Learning Rate: 0.0064832
	LOSS [training: 5.469205172119425 | validation: 5.607876748151596]
	TIME [epoch: 10.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.41684985697408		[learning rate: 0.0064675]
	Learning Rate: 0.00646751
	LOSS [training: 5.41684985697408 | validation: 5.504305077544943]
	TIME [epoch: 10.3 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.430851374763631		[learning rate: 0.0064518]
	Learning Rate: 0.00645185
	LOSS [training: 5.430851374763631 | validation: 5.542617179945093]
	TIME [epoch: 10.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.681029555837581		[learning rate: 0.0064362]
	Learning Rate: 0.00643623
	LOSS [training: 5.681029555837581 | validation: 5.766781934592318]
	TIME [epoch: 10.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.478139762397552		[learning rate: 0.0064206]
	Learning Rate: 0.00642065
	LOSS [training: 5.478139762397552 | validation: 5.634514834540668]
	TIME [epoch: 10.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.403024463997015		[learning rate: 0.0064051]
	Learning Rate: 0.00640511
	LOSS [training: 5.403024463997015 | validation: 5.5407367057656165]
	TIME [epoch: 10.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.479345107914076		[learning rate: 0.0063896]
	Learning Rate: 0.0063896
	LOSS [training: 5.479345107914076 | validation: 5.595690055917589]
	TIME [epoch: 10.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.470744245802762		[learning rate: 0.0063741]
	Learning Rate: 0.00637413
	LOSS [training: 5.470744245802762 | validation: 5.603577676385799]
	TIME [epoch: 10.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.428268552441915		[learning rate: 0.0063587]
	Learning Rate: 0.0063587
	LOSS [training: 5.428268552441915 | validation: 6.042016118644162]
	TIME [epoch: 10.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.554993760286201		[learning rate: 0.0063433]
	Learning Rate: 0.00634331
	LOSS [training: 5.554993760286201 | validation: 5.551631224199583]
	TIME [epoch: 10.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.496092032284355		[learning rate: 0.006328]
	Learning Rate: 0.00632795
	LOSS [training: 5.496092032284355 | validation: 6.203380202248861]
	TIME [epoch: 10.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.548516446958159		[learning rate: 0.0063126]
	Learning Rate: 0.00631263
	LOSS [training: 5.548516446958159 | validation: 5.5389734744690875]
	TIME [epoch: 10.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.476933189031397		[learning rate: 0.0062974]
	Learning Rate: 0.00629735
	LOSS [training: 5.476933189031397 | validation: 5.591959795775202]
	TIME [epoch: 10.3 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.541646418493689		[learning rate: 0.0062821]
	Learning Rate: 0.00628211
	LOSS [training: 5.541646418493689 | validation: 5.586262419352302]
	TIME [epoch: 10.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.483429534317154		[learning rate: 0.0062669]
	Learning Rate: 0.0062669
	LOSS [training: 5.483429534317154 | validation: 5.544447521624764]
	TIME [epoch: 10.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.530806559520543		[learning rate: 0.0062517]
	Learning Rate: 0.00625173
	LOSS [training: 5.530806559520543 | validation: 5.646486446224673]
	TIME [epoch: 10.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.411635956041485		[learning rate: 0.0062366]
	Learning Rate: 0.00623659
	LOSS [training: 5.411635956041485 | validation: 5.643538325798394]
	TIME [epoch: 10.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.520173640112356		[learning rate: 0.0062215]
	Learning Rate: 0.00622149
	LOSS [training: 5.520173640112356 | validation: 5.489119845353912]
	TIME [epoch: 10.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.578377796690871		[learning rate: 0.0062064]
	Learning Rate: 0.00620643
	LOSS [training: 5.578377796690871 | validation: 6.231392558548243]
	TIME [epoch: 10.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.529785698388822		[learning rate: 0.0061914]
	Learning Rate: 0.00619141
	LOSS [training: 5.529785698388822 | validation: 5.701178532805877]
	TIME [epoch: 10.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.486274321603951		[learning rate: 0.0061764]
	Learning Rate: 0.00617642
	LOSS [training: 5.486274321603951 | validation: 5.770468373868082]
	TIME [epoch: 10.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.494773183874832		[learning rate: 0.0061615]
	Learning Rate: 0.00616147
	LOSS [training: 5.494773183874832 | validation: 5.491063095406291]
	TIME [epoch: 10.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.44987094256363		[learning rate: 0.0061466]
	Learning Rate: 0.00614655
	LOSS [training: 5.44987094256363 | validation: 5.662924257144687]
	TIME [epoch: 10.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.439118072801558		[learning rate: 0.0061317]
	Learning Rate: 0.00613167
	LOSS [training: 5.439118072801558 | validation: 5.459959435439055]
	TIME [epoch: 10.3 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.421963024584306		[learning rate: 0.0061168]
	Learning Rate: 0.00611683
	LOSS [training: 5.421963024584306 | validation: 5.824396514655521]
	TIME [epoch: 10.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.621458441925592		[learning rate: 0.006102]
	Learning Rate: 0.00610202
	LOSS [training: 5.621458441925592 | validation: 5.82382843509518]
	TIME [epoch: 10.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.412431834260664		[learning rate: 0.0060872]
	Learning Rate: 0.00608725
	LOSS [training: 5.412431834260664 | validation: 5.521692420875474]
	TIME [epoch: 10.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.666614102899009		[learning rate: 0.0060725]
	Learning Rate: 0.00607251
	LOSS [training: 5.666614102899009 | validation: 5.558334374984558]
	TIME [epoch: 10.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.500385793902743		[learning rate: 0.0060578]
	Learning Rate: 0.00605781
	LOSS [training: 5.500385793902743 | validation: 5.652664075566417]
	TIME [epoch: 10.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.46648959459519		[learning rate: 0.0060431]
	Learning Rate: 0.00604315
	LOSS [training: 5.46648959459519 | validation: 5.493085688225216]
	TIME [epoch: 10.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.398782936460112		[learning rate: 0.0060285]
	Learning Rate: 0.00602852
	LOSS [training: 5.398782936460112 | validation: 5.6513134036710015]
	TIME [epoch: 10.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4909931098423055		[learning rate: 0.0060139]
	Learning Rate: 0.00601392
	LOSS [training: 5.4909931098423055 | validation: 5.701468624523086]
	TIME [epoch: 10.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4722549982201105		[learning rate: 0.0059994]
	Learning Rate: 0.00599936
	LOSS [training: 5.4722549982201105 | validation: 5.498680425312013]
	TIME [epoch: 10.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4261075688786065		[learning rate: 0.0059848]
	Learning Rate: 0.00598484
	LOSS [training: 5.4261075688786065 | validation: 5.520044273701512]
	TIME [epoch: 10.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.393556785403915		[learning rate: 0.0059704]
	Learning Rate: 0.00597035
	LOSS [training: 5.393556785403915 | validation: 5.613041810595701]
	TIME [epoch: 10.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.633977908794131		[learning rate: 0.0059559]
	Learning Rate: 0.0059559
	LOSS [training: 5.633977908794131 | validation: 5.616471387686987]
	TIME [epoch: 10.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5435745900284825		[learning rate: 0.0059415]
	Learning Rate: 0.00594148
	LOSS [training: 5.5435745900284825 | validation: 6.007388705931305]
	TIME [epoch: 10.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.454007510205143		[learning rate: 0.0059271]
	Learning Rate: 0.0059271
	LOSS [training: 5.454007510205143 | validation: 5.72543334141596]
	TIME [epoch: 10.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.429686438857838		[learning rate: 0.0059127]
	Learning Rate: 0.00591275
	LOSS [training: 5.429686438857838 | validation: 5.480789738370529]
	TIME [epoch: 10.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.499995717754209		[learning rate: 0.0058984]
	Learning Rate: 0.00589844
	LOSS [training: 5.499995717754209 | validation: 5.489192507983485]
	TIME [epoch: 10.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.472931497431101		[learning rate: 0.0058842]
	Learning Rate: 0.00588416
	LOSS [training: 5.472931497431101 | validation: 5.557937411776552]
	TIME [epoch: 10.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.326433822929116		[learning rate: 0.0058699]
	Learning Rate: 0.00586991
	LOSS [training: 5.326433822929116 | validation: 5.79580359188333]
	TIME [epoch: 10.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4494170595542375		[learning rate: 0.0058557]
	Learning Rate: 0.0058557
	LOSS [training: 5.4494170595542375 | validation: 5.533306502060184]
	TIME [epoch: 10.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.402981047761215		[learning rate: 0.0058415]
	Learning Rate: 0.00584153
	LOSS [training: 5.402981047761215 | validation: 5.624010787458139]
	TIME [epoch: 10.3 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.415907013648336		[learning rate: 0.0058274]
	Learning Rate: 0.00582738
	LOSS [training: 5.415907013648336 | validation: 5.4384335653801035]
	TIME [epoch: 10.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.357909550636817		[learning rate: 0.0058133]
	Learning Rate: 0.00581328
	LOSS [training: 5.357909550636817 | validation: 5.7826219588655565]
	TIME [epoch: 10.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.501664072184007		[learning rate: 0.0057992]
	Learning Rate: 0.0057992
	LOSS [training: 5.501664072184007 | validation: 5.588810597468353]
	TIME [epoch: 10.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.399459115560466		[learning rate: 0.0057852]
	Learning Rate: 0.00578517
	LOSS [training: 5.399459115560466 | validation: 6.031695970039825]
	TIME [epoch: 10.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.436913079711109		[learning rate: 0.0057712]
	Learning Rate: 0.00577116
	LOSS [training: 5.436913079711109 | validation: 5.521103800160099]
	TIME [epoch: 10.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.37601042627911		[learning rate: 0.0057572]
	Learning Rate: 0.00575719
	LOSS [training: 5.37601042627911 | validation: 5.590664218696772]
	TIME [epoch: 10.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.419536174923245		[learning rate: 0.0057433]
	Learning Rate: 0.00574325
	LOSS [training: 5.419536174923245 | validation: 5.559607661767761]
	TIME [epoch: 10.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.607250539660356		[learning rate: 0.0057293]
	Learning Rate: 0.00572935
	LOSS [training: 5.607250539660356 | validation: 5.7330621372487895]
	TIME [epoch: 10.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.524594589930327		[learning rate: 0.0057155]
	Learning Rate: 0.00571548
	LOSS [training: 5.524594589930327 | validation: 5.497673908333078]
	TIME [epoch: 10.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.402552579024148		[learning rate: 0.0057016]
	Learning Rate: 0.00570164
	LOSS [training: 5.402552579024148 | validation: 5.688598342040523]
	TIME [epoch: 10.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.350927552361176		[learning rate: 0.0056878]
	Learning Rate: 0.00568784
	LOSS [training: 5.350927552361176 | validation: 5.6689054196266]
	TIME [epoch: 10.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.523392586797286		[learning rate: 0.0056741]
	Learning Rate: 0.00567407
	LOSS [training: 5.523392586797286 | validation: 5.444801779609234]
	TIME [epoch: 10.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.500675389759371		[learning rate: 0.0056603]
	Learning Rate: 0.00566033
	LOSS [training: 5.500675389759371 | validation: 5.7818394605445524]
	TIME [epoch: 10.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.473968523372042		[learning rate: 0.0056466]
	Learning Rate: 0.00564663
	LOSS [training: 5.473968523372042 | validation: 5.515982570802136]
	TIME [epoch: 10.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.422781566498136		[learning rate: 0.005633]
	Learning Rate: 0.00563296
	LOSS [training: 5.422781566498136 | validation: 5.422530579867011]
	TIME [epoch: 10.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.28444034488318		[learning rate: 0.0056193]
	Learning Rate: 0.00561933
	LOSS [training: 5.28444034488318 | validation: 6.008067774146596]
	TIME [epoch: 10.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3726372535229086		[learning rate: 0.0056057]
	Learning Rate: 0.00560572
	LOSS [training: 5.3726372535229086 | validation: 5.592870240648792]
	TIME [epoch: 10.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.461362614669399		[learning rate: 0.0055922]
	Learning Rate: 0.00559215
	LOSS [training: 5.461362614669399 | validation: 6.540361709075778]
	TIME [epoch: 10.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.722797882129987		[learning rate: 0.0055786]
	Learning Rate: 0.00557861
	LOSS [training: 5.722797882129987 | validation: 5.48980572070663]
	TIME [epoch: 10.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.358301073027436		[learning rate: 0.0055651]
	Learning Rate: 0.00556511
	LOSS [training: 5.358301073027436 | validation: 5.498037767633807]
	TIME [epoch: 10.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.38244275633545		[learning rate: 0.0055516]
	Learning Rate: 0.00555164
	LOSS [training: 5.38244275633545 | validation: 5.576349453056191]
	TIME [epoch: 10.3 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.415543971572961		[learning rate: 0.0055382]
	Learning Rate: 0.0055382
	LOSS [training: 5.415543971572961 | validation: 5.568894901332434]
	TIME [epoch: 10.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.319618936023619		[learning rate: 0.0055248]
	Learning Rate: 0.00552479
	LOSS [training: 5.319618936023619 | validation: 5.444441825241934]
	TIME [epoch: 10.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.465699037384884		[learning rate: 0.0055114]
	Learning Rate: 0.00551141
	LOSS [training: 5.465699037384884 | validation: 5.454504710778157]
	TIME [epoch: 10.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.333388359578421		[learning rate: 0.0054981]
	Learning Rate: 0.00549807
	LOSS [training: 5.333388359578421 | validation: 5.893699023138068]
	TIME [epoch: 10.3 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.397267812575305		[learning rate: 0.0054848]
	Learning Rate: 0.00548476
	LOSS [training: 5.397267812575305 | validation: 5.3862241857111535]
	TIME [epoch: 10.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3892922621445525		[learning rate: 0.0054715]
	Learning Rate: 0.00547149
	LOSS [training: 5.3892922621445525 | validation: 5.5011025581080295]
	TIME [epoch: 10.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2895388576809115		[learning rate: 0.0054582]
	Learning Rate: 0.00545824
	LOSS [training: 5.2895388576809115 | validation: 5.68879680034232]
	TIME [epoch: 10.3 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.40648241446631		[learning rate: 0.005445]
	Learning Rate: 0.00544503
	LOSS [training: 5.40648241446631 | validation: 5.506482123352471]
	TIME [epoch: 10.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.370194070826817		[learning rate: 0.0054318]
	Learning Rate: 0.00543184
	LOSS [training: 5.370194070826817 | validation: 5.815180939549618]
	TIME [epoch: 10.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.470201058527316		[learning rate: 0.0054187]
	Learning Rate: 0.0054187
	LOSS [training: 5.470201058527316 | validation: 5.360314068727771]
	TIME [epoch: 10.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4269767330638405		[learning rate: 0.0054056]
	Learning Rate: 0.00540558
	LOSS [training: 5.4269767330638405 | validation: 5.719418661248876]
	TIME [epoch: 10.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.488219840256707		[learning rate: 0.0053925]
	Learning Rate: 0.00539249
	LOSS [training: 5.488219840256707 | validation: 5.6311915314766985]
	TIME [epoch: 10.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.350904677538681		[learning rate: 0.0053794]
	Learning Rate: 0.00537944
	LOSS [training: 5.350904677538681 | validation: 5.409960872969719]
	TIME [epoch: 10.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.344289870678054		[learning rate: 0.0053664]
	Learning Rate: 0.00536641
	LOSS [training: 5.344289870678054 | validation: 5.476440148181332]
	TIME [epoch: 10.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.362498057946539		[learning rate: 0.0053534]
	Learning Rate: 0.00535342
	LOSS [training: 5.362498057946539 | validation: 5.489316499080461]
	TIME [epoch: 10.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.34807133756112		[learning rate: 0.0053405]
	Learning Rate: 0.00534046
	LOSS [training: 5.34807133756112 | validation: 5.940348044225614]
	TIME [epoch: 10.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.485101533030212		[learning rate: 0.0053275]
	Learning Rate: 0.00532754
	LOSS [training: 5.485101533030212 | validation: 5.842608822035195]
	TIME [epoch: 10.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.480180206890988		[learning rate: 0.0053146]
	Learning Rate: 0.00531464
	LOSS [training: 5.480180206890988 | validation: 6.047169467600179]
	TIME [epoch: 10.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.422472099415603		[learning rate: 0.0053018]
	Learning Rate: 0.00530177
	LOSS [training: 5.422472099415603 | validation: 5.727336373227286]
	TIME [epoch: 10.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.42776467950622		[learning rate: 0.0052889]
	Learning Rate: 0.00528894
	LOSS [training: 5.42776467950622 | validation: 5.41056464804509]
	TIME [epoch: 10.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.602356180683126		[learning rate: 0.0052761]
	Learning Rate: 0.00527613
	LOSS [training: 5.602356180683126 | validation: 6.093551978125661]
	TIME [epoch: 10.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5365939159547946		[learning rate: 0.0052634]
	Learning Rate: 0.00526336
	LOSS [training: 5.5365939159547946 | validation: 5.406530995945446]
	TIME [epoch: 10.3 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.335051586032576		[learning rate: 0.0052506]
	Learning Rate: 0.00525062
	LOSS [training: 5.335051586032576 | validation: 5.4585896683025545]
	TIME [epoch: 10.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2989878695585695		[learning rate: 0.0052379]
	Learning Rate: 0.00523791
	LOSS [training: 5.2989878695585695 | validation: 5.38049051412974]
	TIME [epoch: 10.3 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.350443455427988		[learning rate: 0.0052252]
	Learning Rate: 0.00522523
	LOSS [training: 5.350443455427988 | validation: 5.409901835351909]
	TIME [epoch: 10.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.389208984837451		[learning rate: 0.0052126]
	Learning Rate: 0.00521258
	LOSS [training: 5.389208984837451 | validation: 5.498836634851712]
	TIME [epoch: 10.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.319955061656015		[learning rate: 0.0052]
	Learning Rate: 0.00519996
	LOSS [training: 5.319955061656015 | validation: 5.519403251552623]
	TIME [epoch: 10.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.39493704296242		[learning rate: 0.0051874]
	Learning Rate: 0.00518737
	LOSS [training: 5.39493704296242 | validation: 5.523314841044701]
	TIME [epoch: 10.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.359313377476179		[learning rate: 0.0051748]
	Learning Rate: 0.00517481
	LOSS [training: 5.359313377476179 | validation: 5.395314840455919]
	TIME [epoch: 10.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.467272359170678		[learning rate: 0.0051623]
	Learning Rate: 0.00516229
	LOSS [training: 5.467272359170678 | validation: 5.58659370379708]
	TIME [epoch: 10.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.389176669491954		[learning rate: 0.0051498]
	Learning Rate: 0.00514979
	LOSS [training: 5.389176669491954 | validation: 5.349115258659049]
	TIME [epoch: 10.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.308475243802664		[learning rate: 0.0051373]
	Learning Rate: 0.00513732
	LOSS [training: 5.308475243802664 | validation: 5.714207365452469]
	TIME [epoch: 10.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.385032610588954		[learning rate: 0.0051249]
	Learning Rate: 0.00512489
	LOSS [training: 5.385032610588954 | validation: 5.392566710709568]
	TIME [epoch: 10.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.330844272490266		[learning rate: 0.0051125]
	Learning Rate: 0.00511248
	LOSS [training: 5.330844272490266 | validation: 5.47904876403456]
	TIME [epoch: 10.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30841419111132		[learning rate: 0.0051001]
	Learning Rate: 0.0051001
	LOSS [training: 5.30841419111132 | validation: 5.395953285550043]
	TIME [epoch: 10.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.325028177565036		[learning rate: 0.0050878]
	Learning Rate: 0.00508776
	LOSS [training: 5.325028177565036 | validation: 5.670672306392858]
	TIME [epoch: 10.3 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.349298408204666		[learning rate: 0.0050754]
	Learning Rate: 0.00507544
	LOSS [training: 5.349298408204666 | validation: 6.024209466443863]
	TIME [epoch: 10.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.318188776841038		[learning rate: 0.0050632]
	Learning Rate: 0.00506315
	LOSS [training: 5.318188776841038 | validation: 6.3423015126268325]
	TIME [epoch: 10.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.393677105056948		[learning rate: 0.0050509]
	Learning Rate: 0.0050509
	LOSS [training: 5.393677105056948 | validation: 5.612041071472709]
	TIME [epoch: 10.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.888856108678247		[learning rate: 0.0050387]
	Learning Rate: 0.00503867
	LOSS [training: 5.888856108678247 | validation: 6.2133013796464205]
	TIME [epoch: 10.3 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.511011878289312		[learning rate: 0.0050265]
	Learning Rate: 0.00502647
	LOSS [training: 5.511011878289312 | validation: 5.400905291693114]
	TIME [epoch: 10.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.307444147349319		[learning rate: 0.0050143]
	Learning Rate: 0.0050143
	LOSS [training: 5.307444147349319 | validation: 5.47488263075536]
	TIME [epoch: 10.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2359491929606845		[learning rate: 0.0050022]
	Learning Rate: 0.00500216
	LOSS [training: 5.2359491929606845 | validation: 5.372182885010243]
	TIME [epoch: 10.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.292326825268455		[learning rate: 0.0049901]
	Learning Rate: 0.00499005
	LOSS [training: 5.292326825268455 | validation: 5.454962404188231]
	TIME [epoch: 10.3 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.268066417726382		[learning rate: 0.004978]
	Learning Rate: 0.00497797
	LOSS [training: 5.268066417726382 | validation: 5.8455700541095466]
	TIME [epoch: 10.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.328378390680915		[learning rate: 0.0049659]
	Learning Rate: 0.00496592
	LOSS [training: 5.328378390680915 | validation: 5.3884981817771544]
	TIME [epoch: 10.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.338462879955516		[learning rate: 0.0049539]
	Learning Rate: 0.0049539
	LOSS [training: 5.338462879955516 | validation: 5.491987225440283]
	TIME [epoch: 10.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2861643248903025		[learning rate: 0.0049419]
	Learning Rate: 0.00494191
	LOSS [training: 5.2861643248903025 | validation: 5.728750762508699]
	TIME [epoch: 10.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.302082819485668		[learning rate: 0.0049299]
	Learning Rate: 0.00492994
	LOSS [training: 5.302082819485668 | validation: 5.38388779726627]
	TIME [epoch: 10.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2887882914535265		[learning rate: 0.004918]
	Learning Rate: 0.00491801
	LOSS [training: 5.2887882914535265 | validation: 5.45059058882473]
	TIME [epoch: 10.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.488859888835629		[learning rate: 0.0049061]
	Learning Rate: 0.0049061
	LOSS [training: 5.488859888835629 | validation: 5.473489435241637]
	TIME [epoch: 10.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.245427308213057		[learning rate: 0.0048942]
	Learning Rate: 0.00489423
	LOSS [training: 5.245427308213057 | validation: 5.793536955977378]
	TIME [epoch: 10.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2967031365091035		[learning rate: 0.0048824]
	Learning Rate: 0.00488238
	LOSS [training: 5.2967031365091035 | validation: 5.542423874474464]
	TIME [epoch: 10.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.304005824618768		[learning rate: 0.0048706]
	Learning Rate: 0.00487056
	LOSS [training: 5.304005824618768 | validation: 5.363850754072799]
	TIME [epoch: 10.3 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3000332049754		[learning rate: 0.0048588]
	Learning Rate: 0.00485877
	LOSS [training: 5.3000332049754 | validation: 5.615773114831136]
	TIME [epoch: 10.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.300185291242515		[learning rate: 0.004847]
	Learning Rate: 0.00484701
	LOSS [training: 5.300185291242515 | validation: 5.317552073533521]
	TIME [epoch: 10.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.521401834868395		[learning rate: 0.0048353]
	Learning Rate: 0.00483527
	LOSS [training: 5.521401834868395 | validation: 5.70928583115736]
	TIME [epoch: 10.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.468196463291932		[learning rate: 0.0048236]
	Learning Rate: 0.00482357
	LOSS [training: 5.468196463291932 | validation: 5.399937075958444]
	TIME [epoch: 10.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.373534614228274		[learning rate: 0.0048119]
	Learning Rate: 0.00481189
	LOSS [training: 5.373534614228274 | validation: 5.423861824620612]
	TIME [epoch: 10.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.411921787347768		[learning rate: 0.0048002]
	Learning Rate: 0.00480024
	LOSS [training: 5.411921787347768 | validation: 5.352173774320977]
	TIME [epoch: 10.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3034480970289914		[learning rate: 0.0047886]
	Learning Rate: 0.00478862
	LOSS [training: 5.3034480970289914 | validation: 5.654053915964938]
	TIME [epoch: 10.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.410576658451234		[learning rate: 0.004777]
	Learning Rate: 0.00477703
	LOSS [training: 5.410576658451234 | validation: 5.396560321264856]
	TIME [epoch: 10.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3426119825884015		[learning rate: 0.0047655]
	Learning Rate: 0.00476546
	LOSS [training: 5.3426119825884015 | validation: 5.497433733368041]
	TIME [epoch: 10.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.297059943887248		[learning rate: 0.0047539]
	Learning Rate: 0.00475393
	LOSS [training: 5.297059943887248 | validation: 5.338553335556562]
	TIME [epoch: 10.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.292000122598071		[learning rate: 0.0047424]
	Learning Rate: 0.00474242
	LOSS [training: 5.292000122598071 | validation: 5.287662681022763]
	TIME [epoch: 10.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.271736600553725		[learning rate: 0.0047309]
	Learning Rate: 0.00473094
	LOSS [training: 5.271736600553725 | validation: 5.5785728267741135]
	TIME [epoch: 10.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.284852935457316		[learning rate: 0.0047195]
	Learning Rate: 0.00471949
	LOSS [training: 5.284852935457316 | validation: 5.492611741592238]
	TIME [epoch: 10.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3232099839020615		[learning rate: 0.0047081]
	Learning Rate: 0.00470806
	LOSS [training: 5.3232099839020615 | validation: 5.285118370557708]
	TIME [epoch: 10.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.255874747962656		[learning rate: 0.0046967]
	Learning Rate: 0.00469666
	LOSS [training: 5.255874747962656 | validation: 5.899434358381909]
	TIME [epoch: 10.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.430609065248545		[learning rate: 0.0046853]
	Learning Rate: 0.00468529
	LOSS [training: 5.430609065248545 | validation: 5.607738245551063]
	TIME [epoch: 10.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.284686042206374		[learning rate: 0.004674]
	Learning Rate: 0.00467395
	LOSS [training: 5.284686042206374 | validation: 5.407205422657113]
	TIME [epoch: 10.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.313920388372016		[learning rate: 0.0046626]
	Learning Rate: 0.00466264
	LOSS [training: 5.313920388372016 | validation: 5.90487309020027]
	TIME [epoch: 10.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.495788349807316		[learning rate: 0.0046513]
	Learning Rate: 0.00465135
	LOSS [training: 5.495788349807316 | validation: 5.353705563170551]
	TIME [epoch: 10.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.23700956037346		[learning rate: 0.0046401]
	Learning Rate: 0.00464009
	LOSS [training: 5.23700956037346 | validation: 5.58877967909257]
	TIME [epoch: 10.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.290203026502418		[learning rate: 0.0046289]
	Learning Rate: 0.00462886
	LOSS [training: 5.290203026502418 | validation: 5.352237711516723]
	TIME [epoch: 10.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2626623957284355		[learning rate: 0.0046177]
	Learning Rate: 0.00461765
	LOSS [training: 5.2626623957284355 | validation: 5.411975625194096]
	TIME [epoch: 10.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.209262677560025		[learning rate: 0.0046065]
	Learning Rate: 0.00460647
	LOSS [training: 5.209262677560025 | validation: 5.5491837397226025]
	TIME [epoch: 10.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.341580948930604		[learning rate: 0.0045953]
	Learning Rate: 0.00459532
	LOSS [training: 5.341580948930604 | validation: 5.585727438223623]
	TIME [epoch: 10.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.290860392370519		[learning rate: 0.0045842]
	Learning Rate: 0.0045842
	LOSS [training: 5.290860392370519 | validation: 5.479496309962483]
	TIME [epoch: 10.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25502782624293		[learning rate: 0.0045731]
	Learning Rate: 0.0045731
	LOSS [training: 5.25502782624293 | validation: 5.696299491750692]
	TIME [epoch: 10.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.295925652379497		[learning rate: 0.004562]
	Learning Rate: 0.00456203
	LOSS [training: 5.295925652379497 | validation: 5.905443151141385]
	TIME [epoch: 10.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.403485511010136		[learning rate: 0.004551]
	Learning Rate: 0.00455098
	LOSS [training: 5.403485511010136 | validation: 5.676210825734411]
	TIME [epoch: 10.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.37622359482354		[learning rate: 0.00454]
	Learning Rate: 0.00453997
	LOSS [training: 5.37622359482354 | validation: 5.836035104809955]
	TIME [epoch: 10.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30358609826711		[learning rate: 0.004529]
	Learning Rate: 0.00452898
	LOSS [training: 5.30358609826711 | validation: 5.400371736660429]
	TIME [epoch: 10.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.420237048699894		[learning rate: 0.004518]
	Learning Rate: 0.00451801
	LOSS [training: 5.420237048699894 | validation: 5.559190048120118]
	TIME [epoch: 10.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.261760116048647		[learning rate: 0.0045071]
	Learning Rate: 0.00450707
	LOSS [training: 5.261760116048647 | validation: 5.50527786456749]
	TIME [epoch: 10.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2383847846570095		[learning rate: 0.0044962]
	Learning Rate: 0.00449616
	LOSS [training: 5.2383847846570095 | validation: 5.331283058175396]
	TIME [epoch: 10.3 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190186480771416		[learning rate: 0.0044853]
	Learning Rate: 0.00448528
	LOSS [training: 5.190186480771416 | validation: 5.3717345223363155]
	TIME [epoch: 10.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.278271161005397		[learning rate: 0.0044744]
	Learning Rate: 0.00447442
	LOSS [training: 5.278271161005397 | validation: 5.63705403911372]
	TIME [epoch: 10.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.341846584514663		[learning rate: 0.0044636]
	Learning Rate: 0.00446359
	LOSS [training: 5.341846584514663 | validation: 5.51119659847511]
	TIME [epoch: 10.3 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.252930496749072		[learning rate: 0.0044528]
	Learning Rate: 0.00445278
	LOSS [training: 5.252930496749072 | validation: 5.350534620402466]
	TIME [epoch: 10.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.212474134911059		[learning rate: 0.004442]
	Learning Rate: 0.004442
	LOSS [training: 5.212474134911059 | validation: 5.609973420999868]
	TIME [epoch: 10.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.269850896850921		[learning rate: 0.0044313]
	Learning Rate: 0.00443125
	LOSS [training: 5.269850896850921 | validation: 5.295468543459442]
	TIME [epoch: 10.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.248345975207351		[learning rate: 0.0044205]
	Learning Rate: 0.00442052
	LOSS [training: 5.248345975207351 | validation: 5.569194673449041]
	TIME [epoch: 10.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.265360740544333		[learning rate: 0.0044098]
	Learning Rate: 0.00440982
	LOSS [training: 5.265360740544333 | validation: 5.518435867444769]
	TIME [epoch: 10.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.385779522086556		[learning rate: 0.0043991]
	Learning Rate: 0.00439915
	LOSS [training: 5.385779522086556 | validation: 5.2450814460092525]
	TIME [epoch: 10.3 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.624921590365664		[learning rate: 0.0043885]
	Learning Rate: 0.0043885
	LOSS [training: 5.624921590365664 | validation: 6.00387303274276]
	TIME [epoch: 10.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.527210989036682		[learning rate: 0.0043779]
	Learning Rate: 0.00437787
	LOSS [training: 5.527210989036682 | validation: 5.369733702619522]
	TIME [epoch: 10.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.448941405548167		[learning rate: 0.0043673]
	Learning Rate: 0.00436727
	LOSS [training: 5.448941405548167 | validation: 5.817875720755563]
	TIME [epoch: 10.3 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.524486676315242		[learning rate: 0.0043567]
	Learning Rate: 0.0043567
	LOSS [training: 5.524486676315242 | validation: 5.431814087625096]
	TIME [epoch: 10.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.247065518640742		[learning rate: 0.0043462]
	Learning Rate: 0.00434616
	LOSS [training: 5.247065518640742 | validation: 5.379621475147601]
	TIME [epoch: 10.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.252431957004381		[learning rate: 0.0043356]
	Learning Rate: 0.00433563
	LOSS [training: 5.252431957004381 | validation: 5.478336720552134]
	TIME [epoch: 10.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.294974206883515		[learning rate: 0.0043251]
	Learning Rate: 0.00432514
	LOSS [training: 5.294974206883515 | validation: 5.292441675313352]
	TIME [epoch: 10.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223779733441519		[learning rate: 0.0043147]
	Learning Rate: 0.00431467
	LOSS [training: 5.223779733441519 | validation: 5.385675638909909]
	TIME [epoch: 10.3 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.239229725133392		[learning rate: 0.0043042]
	Learning Rate: 0.00430422
	LOSS [training: 5.239229725133392 | validation: 5.412129441720672]
	TIME [epoch: 10.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.232651991683887		[learning rate: 0.0042938]
	Learning Rate: 0.0042938
	LOSS [training: 5.232651991683887 | validation: 5.724879993099868]
	TIME [epoch: 10.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.275491657476121		[learning rate: 0.0042834]
	Learning Rate: 0.00428341
	LOSS [training: 5.275491657476121 | validation: 5.247871506791364]
	TIME [epoch: 10.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.231788879893679		[learning rate: 0.004273]
	Learning Rate: 0.00427304
	LOSS [training: 5.231788879893679 | validation: 5.444415396577486]
	TIME [epoch: 10.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.277182510606848		[learning rate: 0.0042627]
	Learning Rate: 0.00426269
	LOSS [training: 5.277182510606848 | validation: 5.748288879518322]
	TIME [epoch: 10.3 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.308259293451586		[learning rate: 0.0042524]
	Learning Rate: 0.00425238
	LOSS [training: 5.308259293451586 | validation: 5.243939159941389]
	TIME [epoch: 10.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.408588079911764		[learning rate: 0.0042421]
	Learning Rate: 0.00424208
	LOSS [training: 5.408588079911764 | validation: 6.357706557253466]
	TIME [epoch: 10.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.605746993005708		[learning rate: 0.0042318]
	Learning Rate: 0.00423181
	LOSS [training: 5.605746993005708 | validation: 5.415708713237461]
	TIME [epoch: 10.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.354037620358146		[learning rate: 0.0042216]
	Learning Rate: 0.00422157
	LOSS [training: 5.354037620358146 | validation: 5.356657072523162]
	TIME [epoch: 10.3 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.219927218988026		[learning rate: 0.0042113]
	Learning Rate: 0.00421135
	LOSS [training: 5.219927218988026 | validation: 5.403059276920373]
	TIME [epoch: 10.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.244252698478013		[learning rate: 0.0042012]
	Learning Rate: 0.00420115
	LOSS [training: 5.244252698478013 | validation: 5.6852685814894475]
	TIME [epoch: 10.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.358021670221369		[learning rate: 0.004191]
	Learning Rate: 0.00419098
	LOSS [training: 5.358021670221369 | validation: 5.8522884990052475]
	TIME [epoch: 10.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.453649743381429		[learning rate: 0.0041808]
	Learning Rate: 0.00418084
	LOSS [training: 5.453649743381429 | validation: 5.464322726128245]
	TIME [epoch: 10.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.360382812623464		[learning rate: 0.0041707]
	Learning Rate: 0.00417071
	LOSS [training: 5.360382812623464 | validation: 5.384092372244286]
	TIME [epoch: 10.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30644716596953		[learning rate: 0.0041606]
	Learning Rate: 0.00416062
	LOSS [training: 5.30644716596953 | validation: 5.668107772794747]
	TIME [epoch: 10.3 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.306336495214029		[learning rate: 0.0041505]
	Learning Rate: 0.00415055
	LOSS [training: 5.306336495214029 | validation: 5.448762141225234]
	TIME [epoch: 10.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.265035436217142		[learning rate: 0.0041405]
	Learning Rate: 0.0041405
	LOSS [training: 5.265035436217142 | validation: 5.668136223176716]
	TIME [epoch: 10.3 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4193859530349995		[learning rate: 0.0041305]
	Learning Rate: 0.00413047
	LOSS [training: 5.4193859530349995 | validation: 5.386096157212812]
	TIME [epoch: 10.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.32108326808105		[learning rate: 0.0041205]
	Learning Rate: 0.00412048
	LOSS [training: 5.32108326808105 | validation: 5.371243587681777]
	TIME [epoch: 10.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.27483412046499		[learning rate: 0.0041105]
	Learning Rate: 0.0041105
	LOSS [training: 5.27483412046499 | validation: 6.128963422729972]
	TIME [epoch: 10.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.780677012325785		[learning rate: 0.0041005]
	Learning Rate: 0.00410055
	LOSS [training: 5.780677012325785 | validation: 5.806748589400062]
	TIME [epoch: 10.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.480333081981682		[learning rate: 0.0040906]
	Learning Rate: 0.00409062
	LOSS [training: 5.480333081981682 | validation: 5.444948278438362]
	TIME [epoch: 10.3 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.231627041367374		[learning rate: 0.0040807]
	Learning Rate: 0.00408072
	LOSS [training: 5.231627041367374 | validation: 5.491548936107982]
	TIME [epoch: 10.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.265740313242992		[learning rate: 0.0040708]
	Learning Rate: 0.00407084
	LOSS [training: 5.265740313242992 | validation: 5.333244756824988]
	TIME [epoch: 10.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.203635030505176		[learning rate: 0.004061]
	Learning Rate: 0.00406099
	LOSS [training: 5.203635030505176 | validation: 5.383928583378911]
	TIME [epoch: 10.3 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241027370492461		[learning rate: 0.0040512]
	Learning Rate: 0.00405116
	LOSS [training: 5.241027370492461 | validation: 5.39089247787816]
	TIME [epoch: 10.3 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1726412586695165		[learning rate: 0.0040413]
	Learning Rate: 0.00404135
	LOSS [training: 5.1726412586695165 | validation: 5.738973183157592]
	TIME [epoch: 10.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.547053407147896		[learning rate: 0.0040316]
	Learning Rate: 0.00403157
	LOSS [training: 5.547053407147896 | validation: 5.339700914560897]
	TIME [epoch: 10.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.220949103335268		[learning rate: 0.0040218]
	Learning Rate: 0.00402181
	LOSS [training: 5.220949103335268 | validation: 5.352754791865119]
	TIME [epoch: 10.3 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.254990702637583		[learning rate: 0.0040121]
	Learning Rate: 0.00401207
	LOSS [training: 5.254990702637583 | validation: 5.34358205361039]
	TIME [epoch: 10.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.421982246941495		[learning rate: 0.0040024]
	Learning Rate: 0.00400236
	LOSS [training: 5.421982246941495 | validation: 5.463910088247617]
	TIME [epoch: 10.3 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.225092261295378		[learning rate: 0.0039927]
	Learning Rate: 0.00399267
	LOSS [training: 5.225092261295378 | validation: 5.563937234946491]
	TIME [epoch: 10.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.275818451724591		[learning rate: 0.003983]
	Learning Rate: 0.003983
	LOSS [training: 5.275818451724591 | validation: 5.438882780049348]
	TIME [epoch: 10.3 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.236552216466176		[learning rate: 0.0039734]
	Learning Rate: 0.00397336
	LOSS [training: 5.236552216466176 | validation: 5.423312330847104]
	TIME [epoch: 10.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.369993963261579		[learning rate: 0.0039637]
	Learning Rate: 0.00396374
	LOSS [training: 5.369993963261579 | validation: 5.493595650911247]
	TIME [epoch: 10.3 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.281769384521172		[learning rate: 0.0039541]
	Learning Rate: 0.00395415
	LOSS [training: 5.281769384521172 | validation: 5.76950728892019]
	TIME [epoch: 10.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.253465837169463		[learning rate: 0.0039446]
	Learning Rate: 0.00394457
	LOSS [training: 5.253465837169463 | validation: 5.245979068302773]
	TIME [epoch: 10.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.246254474856267		[learning rate: 0.003935]
	Learning Rate: 0.00393502
	LOSS [training: 5.246254474856267 | validation: 5.823105667716853]
	TIME [epoch: 10.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25912394215297		[learning rate: 0.0039255]
	Learning Rate: 0.0039255
	LOSS [training: 5.25912394215297 | validation: 5.4610116144050815]
	TIME [epoch: 10.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.216598106943701		[learning rate: 0.003916]
	Learning Rate: 0.00391599
	LOSS [training: 5.216598106943701 | validation: 5.580686645862941]
	TIME [epoch: 10.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30458228368369		[learning rate: 0.0039065]
	Learning Rate: 0.00390651
	LOSS [training: 5.30458228368369 | validation: 5.282608559948389]
	TIME [epoch: 10.3 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.257791321276936		[learning rate: 0.0038971]
	Learning Rate: 0.00389706
	LOSS [training: 5.257791321276936 | validation: 5.859821041728993]
	TIME [epoch: 10.3 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.436748149315308		[learning rate: 0.0038876]
	Learning Rate: 0.00388762
	LOSS [training: 5.436748149315308 | validation: 5.816303776268368]
	TIME [epoch: 10.3 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.392458338899422		[learning rate: 0.0038782]
	Learning Rate: 0.00387821
	LOSS [training: 5.392458338899422 | validation: 5.390476078529124]
	TIME [epoch: 10.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.270289758571851		[learning rate: 0.0038688]
	Learning Rate: 0.00386882
	LOSS [training: 5.270289758571851 | validation: 5.408623192675118]
	TIME [epoch: 10.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.207857645042411		[learning rate: 0.0038595]
	Learning Rate: 0.00385946
	LOSS [training: 5.207857645042411 | validation: 5.319856929741925]
	TIME [epoch: 10.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.385630407640709		[learning rate: 0.0038501]
	Learning Rate: 0.00385011
	LOSS [training: 5.385630407640709 | validation: 5.368260011591074]
	TIME [epoch: 10.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.370749024850788		[learning rate: 0.0038408]
	Learning Rate: 0.00384079
	LOSS [training: 5.370749024850788 | validation: 5.347421257218805]
	TIME [epoch: 10.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234183290116038		[learning rate: 0.0038315]
	Learning Rate: 0.0038315
	LOSS [training: 5.234183290116038 | validation: 5.2653637404524885]
	TIME [epoch: 10.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193413818798531		[learning rate: 0.0038222]
	Learning Rate: 0.00382222
	LOSS [training: 5.193413818798531 | validation: 5.663112173972447]
	TIME [epoch: 10.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25920117767353		[learning rate: 0.003813]
	Learning Rate: 0.00381297
	LOSS [training: 5.25920117767353 | validation: 5.309887826224481]
	TIME [epoch: 10.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.298505795550018		[learning rate: 0.0038037]
	Learning Rate: 0.00380374
	LOSS [training: 5.298505795550018 | validation: 5.466175408652004]
	TIME [epoch: 10.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.365621774696233		[learning rate: 0.0037945]
	Learning Rate: 0.00379453
	LOSS [training: 5.365621774696233 | validation: 5.410493935034944]
	TIME [epoch: 10.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.472977213438773		[learning rate: 0.0037853]
	Learning Rate: 0.00378534
	LOSS [training: 5.472977213438773 | validation: 5.552210309220647]
	TIME [epoch: 10.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.279550989723703		[learning rate: 0.0037762]
	Learning Rate: 0.00377618
	LOSS [training: 5.279550989723703 | validation: 5.673451343107799]
	TIME [epoch: 10.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2553196933681035		[learning rate: 0.003767]
	Learning Rate: 0.00376704
	LOSS [training: 5.2553196933681035 | validation: 5.469403494385207]
	TIME [epoch: 10.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2941940678634065		[learning rate: 0.0037579]
	Learning Rate: 0.00375792
	LOSS [training: 5.2941940678634065 | validation: 5.3924088396936165]
	TIME [epoch: 10.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2753555391166795		[learning rate: 0.0037488]
	Learning Rate: 0.00374882
	LOSS [training: 5.2753555391166795 | validation: 5.4649846827444915]
	TIME [epoch: 10.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.492279922670205		[learning rate: 0.0037397]
	Learning Rate: 0.00373975
	LOSS [training: 5.492279922670205 | validation: 5.451493792777966]
	TIME [epoch: 10.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.391906662959436		[learning rate: 0.0037307]
	Learning Rate: 0.00373069
	LOSS [training: 5.391906662959436 | validation: 5.532039512891431]
	TIME [epoch: 10.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.260576314207336		[learning rate: 0.0037217]
	Learning Rate: 0.00372166
	LOSS [training: 5.260576314207336 | validation: 5.361285858949002]
	TIME [epoch: 10.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.295272835782169		[learning rate: 0.0037127]
	Learning Rate: 0.00371265
	LOSS [training: 5.295272835782169 | validation: 5.434913604382063]
	TIME [epoch: 10.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.379235015907354		[learning rate: 0.0037037]
	Learning Rate: 0.00370366
	LOSS [training: 5.379235015907354 | validation: 5.847184510268462]
	TIME [epoch: 10.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.620420841919504		[learning rate: 0.0036947]
	Learning Rate: 0.0036947
	LOSS [training: 5.620420841919504 | validation: 5.448246300060093]
	TIME [epoch: 10.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.431286827021452		[learning rate: 0.0036858]
	Learning Rate: 0.00368575
	LOSS [training: 5.431286827021452 | validation: 5.859128826608842]
	TIME [epoch: 10.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.340098855078116		[learning rate: 0.0036768]
	Learning Rate: 0.00367683
	LOSS [training: 5.340098855078116 | validation: 5.599185919303564]
	TIME [epoch: 10.3 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.28135311011309		[learning rate: 0.0036679]
	Learning Rate: 0.00366793
	LOSS [training: 5.28135311011309 | validation: 5.604160845240594]
	TIME [epoch: 10.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2933803208849115		[learning rate: 0.0036591]
	Learning Rate: 0.00365905
	LOSS [training: 5.2933803208849115 | validation: 5.514769619597064]
	TIME [epoch: 10.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.41035082632728		[learning rate: 0.0036502]
	Learning Rate: 0.00365019
	LOSS [training: 5.41035082632728 | validation: 5.420276224715512]
	TIME [epoch: 10.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.330374279340661		[learning rate: 0.0036414]
	Learning Rate: 0.00364136
	LOSS [training: 5.330374279340661 | validation: 5.560034752461356]
	TIME [epoch: 10.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.310642442492761		[learning rate: 0.0036325]
	Learning Rate: 0.00363254
	LOSS [training: 5.310642442492761 | validation: 5.624100462156725]
	TIME [epoch: 10.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30653845141751		[learning rate: 0.0036237]
	Learning Rate: 0.00362375
	LOSS [training: 5.30653845141751 | validation: 5.39874463259661]
	TIME [epoch: 10.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.304478093236719		[learning rate: 0.003615]
	Learning Rate: 0.00361497
	LOSS [training: 5.304478093236719 | validation: 5.3950526748314065]
	TIME [epoch: 10.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.295216883448718		[learning rate: 0.0036062]
	Learning Rate: 0.00360622
	LOSS [training: 5.295216883448718 | validation: 5.395997540525871]
	TIME [epoch: 10.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.272365230426763		[learning rate: 0.0035975]
	Learning Rate: 0.00359749
	LOSS [training: 5.272365230426763 | validation: 5.425506439621048]
	TIME [epoch: 10.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.350107943607657		[learning rate: 0.0035888]
	Learning Rate: 0.00358878
	LOSS [training: 5.350107943607657 | validation: 5.352919683190789]
	TIME [epoch: 10.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.421794074767256		[learning rate: 0.0035801]
	Learning Rate: 0.0035801
	LOSS [training: 5.421794074767256 | validation: 5.387966263517415]
	TIME [epoch: 10.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.29108107907533		[learning rate: 0.0035714]
	Learning Rate: 0.00357143
	LOSS [training: 5.29108107907533 | validation: 5.494333298557478]
	TIME [epoch: 10.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.306092170714166		[learning rate: 0.0035628]
	Learning Rate: 0.00356278
	LOSS [training: 5.306092170714166 | validation: 5.3715898256144285]
	TIME [epoch: 10.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.279343090408106		[learning rate: 0.0035542]
	Learning Rate: 0.00355416
	LOSS [training: 5.279343090408106 | validation: 5.4664854938355845]
	TIME [epoch: 10.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.280592872149785		[learning rate: 0.0035456]
	Learning Rate: 0.00354555
	LOSS [training: 5.280592872149785 | validation: 5.377392562610797]
	TIME [epoch: 10.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182444153951299		[learning rate: 0.003537]
	Learning Rate: 0.00353697
	LOSS [training: 5.182444153951299 | validation: 5.377614495688816]
	TIME [epoch: 10.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.238084246799785		[learning rate: 0.0035284]
	Learning Rate: 0.00352841
	LOSS [training: 5.238084246799785 | validation: 5.333524043584182]
	TIME [epoch: 10.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.216989802750662		[learning rate: 0.0035199]
	Learning Rate: 0.00351987
	LOSS [training: 5.216989802750662 | validation: 5.3472713729380805]
	TIME [epoch: 10.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.157893034278933		[learning rate: 0.0035113]
	Learning Rate: 0.00351135
	LOSS [training: 5.157893034278933 | validation: 5.423952732295598]
	TIME [epoch: 10.3 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.286466197324769		[learning rate: 0.0035028]
	Learning Rate: 0.00350285
	LOSS [training: 5.286466197324769 | validation: 5.595540770324288]
	TIME [epoch: 10.3 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.335549003812302		[learning rate: 0.0034944]
	Learning Rate: 0.00349437
	LOSS [training: 5.335549003812302 | validation: 5.297318756508861]
	TIME [epoch: 10.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.331485624948974		[learning rate: 0.0034859]
	Learning Rate: 0.00348591
	LOSS [training: 5.331485624948974 | validation: 5.6064506183748986]
	TIME [epoch: 10.3 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.317111445169705		[learning rate: 0.0034775]
	Learning Rate: 0.00347747
	LOSS [training: 5.317111445169705 | validation: 5.3983192810762635]
	TIME [epoch: 10.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.308404927831599		[learning rate: 0.003469]
	Learning Rate: 0.00346905
	LOSS [training: 5.308404927831599 | validation: 5.295448061143717]
	TIME [epoch: 10.3 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.248654159744204		[learning rate: 0.0034607]
	Learning Rate: 0.00346065
	LOSS [training: 5.248654159744204 | validation: 5.390458062993928]
	TIME [epoch: 10.3 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.205829585699886		[learning rate: 0.0034523]
	Learning Rate: 0.00345227
	LOSS [training: 5.205829585699886 | validation: 5.26828485160105]
	TIME [epoch: 10.3 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.267896653298163		[learning rate: 0.0034439]
	Learning Rate: 0.00344392
	LOSS [training: 5.267896653298163 | validation: 5.640610859445018]
	TIME [epoch: 10.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.335437527728812		[learning rate: 0.0034356]
	Learning Rate: 0.00343558
	LOSS [training: 5.335437527728812 | validation: 5.665854700306706]
	TIME [epoch: 10.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.337376287541886		[learning rate: 0.0034273]
	Learning Rate: 0.00342726
	LOSS [training: 5.337376287541886 | validation: 5.480375327814386]
	TIME [epoch: 10.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.269181484719618		[learning rate: 0.003419]
	Learning Rate: 0.00341897
	LOSS [training: 5.269181484719618 | validation: 5.402194765855975]
	TIME [epoch: 10.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.26938787771133		[learning rate: 0.0034107]
	Learning Rate: 0.00341069
	LOSS [training: 5.26938787771133 | validation: 5.3137475857536405]
	TIME [epoch: 10.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4008137566146175		[learning rate: 0.0034024]
	Learning Rate: 0.00340243
	LOSS [training: 5.4008137566146175 | validation: 5.339172706092935]
	TIME [epoch: 10.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.288456487886075		[learning rate: 0.0033942]
	Learning Rate: 0.0033942
	LOSS [training: 5.288456487886075 | validation: 5.558437620005618]
	TIME [epoch: 10.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.293027870946177		[learning rate: 0.003386]
	Learning Rate: 0.00338598
	LOSS [training: 5.293027870946177 | validation: 5.682374908877198]
	TIME [epoch: 10.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.348213818327821		[learning rate: 0.0033778]
	Learning Rate: 0.00337778
	LOSS [training: 5.348213818327821 | validation: 5.629211639742953]
	TIME [epoch: 10.3 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.292973120453822		[learning rate: 0.0033696]
	Learning Rate: 0.0033696
	LOSS [training: 5.292973120453822 | validation: 5.3961537904860215]
	TIME [epoch: 10.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.219952825435632		[learning rate: 0.0033614]
	Learning Rate: 0.00336145
	LOSS [training: 5.219952825435632 | validation: 5.442812370605668]
	TIME [epoch: 10.3 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.225137593879483		[learning rate: 0.0033533]
	Learning Rate: 0.00335331
	LOSS [training: 5.225137593879483 | validation: 5.376957182202985]
	TIME [epoch: 10.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.23953441209243		[learning rate: 0.0033452]
	Learning Rate: 0.00334519
	LOSS [training: 5.23953441209243 | validation: 5.339192836935292]
	TIME [epoch: 10.3 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183750456274462		[learning rate: 0.0033371]
	Learning Rate: 0.00333709
	LOSS [training: 5.183750456274462 | validation: 5.952553607478298]
	TIME [epoch: 10.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.356846858731457		[learning rate: 0.003329]
	Learning Rate: 0.00332902
	LOSS [training: 5.356846858731457 | validation: 5.2762294697447025]
	TIME [epoch: 10.3 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.194108525027993		[learning rate: 0.003321]
	Learning Rate: 0.00332096
	LOSS [training: 5.194108525027993 | validation: 5.321542037860415]
	TIME [epoch: 10.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.229896775586451		[learning rate: 0.0033129]
	Learning Rate: 0.00331292
	LOSS [training: 5.229896775586451 | validation: 5.3178955241467385]
	TIME [epoch: 10.3 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2068243204077		[learning rate: 0.0033049]
	Learning Rate: 0.0033049
	LOSS [training: 5.2068243204077 | validation: 5.3483961012337975]
	TIME [epoch: 10.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.239651438397397		[learning rate: 0.0032969]
	Learning Rate: 0.0032969
	LOSS [training: 5.239651438397397 | validation: 5.48176860654449]
	TIME [epoch: 10.3 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2270468069283265		[learning rate: 0.0032889]
	Learning Rate: 0.00328891
	LOSS [training: 5.2270468069283265 | validation: 5.518231746018669]
	TIME [epoch: 10.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.260985130897927		[learning rate: 0.003281]
	Learning Rate: 0.00328095
	LOSS [training: 5.260985130897927 | validation: 5.37506753590292]
	TIME [epoch: 10.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.384351735497321		[learning rate: 0.003273]
	Learning Rate: 0.00327301
	LOSS [training: 5.384351735497321 | validation: 5.350572070581645]
	TIME [epoch: 10.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2879136942912055		[learning rate: 0.0032651]
	Learning Rate: 0.00326509
	LOSS [training: 5.2879136942912055 | validation: 5.512931939181369]
	TIME [epoch: 10.3 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177243684776374		[learning rate: 0.0032572]
	Learning Rate: 0.00325718
	LOSS [training: 5.177243684776374 | validation: 5.325016404347922]
	TIME [epoch: 10.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.231981472740784		[learning rate: 0.0032493]
	Learning Rate: 0.0032493
	LOSS [training: 5.231981472740784 | validation: 5.543909086567965]
	TIME [epoch: 10.3 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.319585904954373		[learning rate: 0.0032414]
	Learning Rate: 0.00324143
	LOSS [training: 5.319585904954373 | validation: 6.283385310959984]
	TIME [epoch: 10.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.585038323428263		[learning rate: 0.0032336]
	Learning Rate: 0.00323358
	LOSS [training: 5.585038323428263 | validation: 5.298661506057174]
	TIME [epoch: 10.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3509980741906755		[learning rate: 0.0032258]
	Learning Rate: 0.00322576
	LOSS [training: 5.3509980741906755 | validation: 5.847195044376304]
	TIME [epoch: 10.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.324022036190803		[learning rate: 0.0032179]
	Learning Rate: 0.00321795
	LOSS [training: 5.324022036190803 | validation: 5.6068146325673665]
	TIME [epoch: 10.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.244856958828576		[learning rate: 0.0032102]
	Learning Rate: 0.00321016
	LOSS [training: 5.244856958828576 | validation: 5.284585015680134]
	TIME [epoch: 10.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1992309623356		[learning rate: 0.0032024]
	Learning Rate: 0.00320239
	LOSS [training: 5.1992309623356 | validation: 5.335866184575214]
	TIME [epoch: 10.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167973610806989		[learning rate: 0.0031946]
	Learning Rate: 0.00319463
	LOSS [training: 5.167973610806989 | validation: 5.560214651752331]
	TIME [epoch: 10.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184630281470697		[learning rate: 0.0031869]
	Learning Rate: 0.0031869
	LOSS [training: 5.184630281470697 | validation: 5.32660135082649]
	TIME [epoch: 10.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167899489918127		[learning rate: 0.0031792]
	Learning Rate: 0.00317918
	LOSS [training: 5.167899489918127 | validation: 5.478921948318841]
	TIME [epoch: 10.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.272470930885406		[learning rate: 0.0031715]
	Learning Rate: 0.00317149
	LOSS [training: 5.272470930885406 | validation: 5.620297349422294]
	TIME [epoch: 10.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3268871763690955		[learning rate: 0.0031638]
	Learning Rate: 0.00316381
	LOSS [training: 5.3268871763690955 | validation: 5.320085521711702]
	TIME [epoch: 10.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.200878763847622		[learning rate: 0.0031562]
	Learning Rate: 0.00315615
	LOSS [training: 5.200878763847622 | validation: 5.582148437322287]
	TIME [epoch: 10.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.225744105364235		[learning rate: 0.0031485]
	Learning Rate: 0.00314851
	LOSS [training: 5.225744105364235 | validation: 5.5162643620397445]
	TIME [epoch: 10.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.216624332753104		[learning rate: 0.0031409]
	Learning Rate: 0.00314089
	LOSS [training: 5.216624332753104 | validation: 5.583959463996832]
	TIME [epoch: 10.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.353205013271891		[learning rate: 0.0031333]
	Learning Rate: 0.00313329
	LOSS [training: 5.353205013271891 | validation: 5.627992964985096]
	TIME [epoch: 10.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.314425214155948		[learning rate: 0.0031257]
	Learning Rate: 0.0031257
	LOSS [training: 5.314425214155948 | validation: 5.29118558743997]
	TIME [epoch: 10.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.251161419130915		[learning rate: 0.0031181]
	Learning Rate: 0.00311813
	LOSS [training: 5.251161419130915 | validation: 5.372008326406666]
	TIME [epoch: 10.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1739781559565685		[learning rate: 0.0031106]
	Learning Rate: 0.00311059
	LOSS [training: 5.1739781559565685 | validation: 5.313581059282862]
	TIME [epoch: 10.3 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.207685059138196		[learning rate: 0.0031031]
	Learning Rate: 0.00310305
	LOSS [training: 5.207685059138196 | validation: 5.332712838868095]
	TIME [epoch: 10.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.327867756534775		[learning rate: 0.0030955]
	Learning Rate: 0.00309554
	LOSS [training: 5.327867756534775 | validation: 5.28894358381372]
	TIME [epoch: 10.3 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.204725176491044		[learning rate: 0.003088]
	Learning Rate: 0.00308805
	LOSS [training: 5.204725176491044 | validation: 5.3376034517165065]
	TIME [epoch: 10.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192228014392088		[learning rate: 0.0030806]
	Learning Rate: 0.00308057
	LOSS [training: 5.192228014392088 | validation: 5.503860775610349]
	TIME [epoch: 10.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197364794726701		[learning rate: 0.0030731]
	Learning Rate: 0.00307312
	LOSS [training: 5.197364794726701 | validation: 5.351228968801155]
	TIME [epoch: 10.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.109744964944413		[learning rate: 0.0030657]
	Learning Rate: 0.00306568
	LOSS [training: 5.109744964944413 | validation: 5.758533873300022]
	TIME [epoch: 10.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.251194100941736		[learning rate: 0.0030583]
	Learning Rate: 0.00305825
	LOSS [training: 5.251194100941736 | validation: 5.377696365868438]
	TIME [epoch: 10.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.226124671677126		[learning rate: 0.0030509]
	Learning Rate: 0.00305085
	LOSS [training: 5.226124671677126 | validation: 5.526532633980743]
	TIME [epoch: 10.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223406760555487		[learning rate: 0.0030435]
	Learning Rate: 0.00304347
	LOSS [training: 5.223406760555487 | validation: 5.333815064886462]
	TIME [epoch: 10.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163960089773831		[learning rate: 0.0030361]
	Learning Rate: 0.0030361
	LOSS [training: 5.163960089773831 | validation: 5.271503831398767]
	TIME [epoch: 10.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.120649844006506		[learning rate: 0.0030287]
	Learning Rate: 0.00302875
	LOSS [training: 5.120649844006506 | validation: 5.422858499272741]
	TIME [epoch: 10.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.297227931627207		[learning rate: 0.0030214]
	Learning Rate: 0.00302142
	LOSS [training: 5.297227931627207 | validation: 5.184922316862738]
	TIME [epoch: 10.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.157435719374641		[learning rate: 0.0030141]
	Learning Rate: 0.0030141
	LOSS [training: 5.157435719374641 | validation: 5.455788715160661]
	TIME [epoch: 10.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163741465994245		[learning rate: 0.0030068]
	Learning Rate: 0.0030068
	LOSS [training: 5.163741465994245 | validation: 5.324046508938128]
	TIME [epoch: 10.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1321647837459805		[learning rate: 0.0029995]
	Learning Rate: 0.00299953
	LOSS [training: 5.1321647837459805 | validation: 5.174223699468379]
	TIME [epoch: 10.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.107582345219953		[learning rate: 0.0029923]
	Learning Rate: 0.00299226
	LOSS [training: 5.107582345219953 | validation: 5.5959868044445145]
	TIME [epoch: 10.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165539233213805		[learning rate: 0.002985]
	Learning Rate: 0.00298502
	LOSS [training: 5.165539233213805 | validation: 5.202490348037909]
	TIME [epoch: 10.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.135708281521239		[learning rate: 0.0029778]
	Learning Rate: 0.00297779
	LOSS [training: 5.135708281521239 | validation: 5.35158946079474]
	TIME [epoch: 10.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.162171466622314		[learning rate: 0.0029706]
	Learning Rate: 0.00297059
	LOSS [training: 5.162171466622314 | validation: 5.281885835668082]
	TIME [epoch: 10.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164841838205277		[learning rate: 0.0029634]
	Learning Rate: 0.00296339
	LOSS [training: 5.164841838205277 | validation: 5.530446544971863]
	TIME [epoch: 10.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1986089126918555		[learning rate: 0.0029562]
	Learning Rate: 0.00295622
	LOSS [training: 5.1986089126918555 | validation: 5.274467951735276]
	TIME [epoch: 10.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.123709821069097		[learning rate: 0.0029491]
	Learning Rate: 0.00294906
	LOSS [training: 5.123709821069097 | validation: 5.366039190210605]
	TIME [epoch: 10.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1851808415498875		[learning rate: 0.0029419]
	Learning Rate: 0.00294192
	LOSS [training: 5.1851808415498875 | validation: 5.349599952556262]
	TIME [epoch: 10.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.169139400698185		[learning rate: 0.0029348]
	Learning Rate: 0.0029348
	LOSS [training: 5.169139400698185 | validation: 5.429081530380568]
	TIME [epoch: 10.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1726239966074825		[learning rate: 0.0029277]
	Learning Rate: 0.0029277
	LOSS [training: 5.1726239966074825 | validation: 5.2592254596963315]
	TIME [epoch: 10.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.204093987547071		[learning rate: 0.0029206]
	Learning Rate: 0.00292061
	LOSS [training: 5.204093987547071 | validation: 5.368442706291309]
	TIME [epoch: 10.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.198514366643145		[learning rate: 0.0029135]
	Learning Rate: 0.00291354
	LOSS [training: 5.198514366643145 | validation: 5.362313131407969]
	TIME [epoch: 10.3 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159917775148485		[learning rate: 0.0029065]
	Learning Rate: 0.00290649
	LOSS [training: 5.159917775148485 | validation: 5.229359190681264]
	TIME [epoch: 10.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.086306458474299		[learning rate: 0.0028995]
	Learning Rate: 0.00289945
	LOSS [training: 5.086306458474299 | validation: 5.304311585048236]
	TIME [epoch: 10.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.11260916757646		[learning rate: 0.0028924]
	Learning Rate: 0.00289243
	LOSS [training: 5.11260916757646 | validation: 5.262233249668984]
	TIME [epoch: 10.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.096127709370123		[learning rate: 0.0028854]
	Learning Rate: 0.00288543
	LOSS [training: 5.096127709370123 | validation: 5.4372494988893845]
	TIME [epoch: 10.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.238053560465845		[learning rate: 0.0028784]
	Learning Rate: 0.00287844
	LOSS [training: 5.238053560465845 | validation: 5.132059574679163]
	TIME [epoch: 10.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0691357567007955		[learning rate: 0.0028715]
	Learning Rate: 0.00287148
	LOSS [training: 5.0691357567007955 | validation: 5.49216724139723]
	TIME [epoch: 10.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.146785593840263		[learning rate: 0.0028645]
	Learning Rate: 0.00286453
	LOSS [training: 5.146785593840263 | validation: 5.280980251724977]
	TIME [epoch: 10.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.426364340354094		[learning rate: 0.0028576]
	Learning Rate: 0.00285759
	LOSS [training: 5.426364340354094 | validation: 5.308263999852584]
	TIME [epoch: 10.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.138538897746122		[learning rate: 0.0028507]
	Learning Rate: 0.00285067
	LOSS [training: 5.138538897746122 | validation: 5.340533039088492]
	TIME [epoch: 10.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177751644967818		[learning rate: 0.0028438]
	Learning Rate: 0.00284377
	LOSS [training: 5.177751644967818 | validation: 5.606742769005767]
	TIME [epoch: 10.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.416122865556881		[learning rate: 0.0028369]
	Learning Rate: 0.00283689
	LOSS [training: 5.416122865556881 | validation: 5.3285707085429275]
	TIME [epoch: 10.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3686659104362		[learning rate: 0.00283]
	Learning Rate: 0.00283002
	LOSS [training: 5.3686659104362 | validation: 5.808203410184872]
	TIME [epoch: 10.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.343202808350132		[learning rate: 0.0028232]
	Learning Rate: 0.00282317
	LOSS [training: 5.343202808350132 | validation: 5.541387535671535]
	TIME [epoch: 10.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.289986085505023		[learning rate: 0.0028163]
	Learning Rate: 0.00281633
	LOSS [training: 5.289986085505023 | validation: 5.337870413670918]
	TIME [epoch: 10.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.482605297791091		[learning rate: 0.0028095]
	Learning Rate: 0.00280952
	LOSS [training: 5.482605297791091 | validation: 5.6222202186131565]
	TIME [epoch: 10.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.457749670571533		[learning rate: 0.0028027]
	Learning Rate: 0.00280272
	LOSS [training: 5.457749670571533 | validation: 5.340657311209226]
	TIME [epoch: 10.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2133168814937445		[learning rate: 0.0027959]
	Learning Rate: 0.00279593
	LOSS [training: 5.2133168814937445 | validation: 5.426302624512602]
	TIME [epoch: 10.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.303106098759691		[learning rate: 0.0027892]
	Learning Rate: 0.00278916
	LOSS [training: 5.303106098759691 | validation: 5.2957798365610325]
	TIME [epoch: 10.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.19326774608088		[learning rate: 0.0027824]
	Learning Rate: 0.00278241
	LOSS [training: 5.19326774608088 | validation: 5.369593387092132]
	TIME [epoch: 10.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.263048322938007		[learning rate: 0.0027757]
	Learning Rate: 0.00277567
	LOSS [training: 5.263048322938007 | validation: 5.385935361581308]
	TIME [epoch: 10.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178021509670126		[learning rate: 0.002769]
	Learning Rate: 0.00276895
	LOSS [training: 5.178021509670126 | validation: 5.472378519534705]
	TIME [epoch: 10.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.249316385799771		[learning rate: 0.0027623]
	Learning Rate: 0.00276225
	LOSS [training: 5.249316385799771 | validation: 5.338289983770797]
	TIME [epoch: 10.3 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.314646310175747		[learning rate: 0.0027556]
	Learning Rate: 0.00275556
	LOSS [training: 5.314646310175747 | validation: 6.346826228249118]
	TIME [epoch: 10.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.544307269043336		[learning rate: 0.0027489]
	Learning Rate: 0.00274889
	LOSS [training: 5.544307269043336 | validation: 5.460450513302]
	TIME [epoch: 10.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.426902249154248		[learning rate: 0.0027422]
	Learning Rate: 0.00274224
	LOSS [training: 5.426902249154248 | validation: 5.586641348765759]
	TIME [epoch: 10.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.302776930358048		[learning rate: 0.0027356]
	Learning Rate: 0.0027356
	LOSS [training: 5.302776930358048 | validation: 5.35400550837764]
	TIME [epoch: 10.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25222878191965		[learning rate: 0.002729]
	Learning Rate: 0.00272898
	LOSS [training: 5.25222878191965 | validation: 5.382976871572834]
	TIME [epoch: 10.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.246380841102036		[learning rate: 0.0027224]
	Learning Rate: 0.00272237
	LOSS [training: 5.246380841102036 | validation: 5.472592123105919]
	TIME [epoch: 10.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.247110335932902		[learning rate: 0.0027158]
	Learning Rate: 0.00271578
	LOSS [training: 5.247110335932902 | validation: 5.386096376230954]
	TIME [epoch: 10.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.370593912541768		[learning rate: 0.0027092]
	Learning Rate: 0.00270921
	LOSS [training: 5.370593912541768 | validation: 5.3718633395935855]
	TIME [epoch: 10.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2609186346269805		[learning rate: 0.0027026]
	Learning Rate: 0.00270265
	LOSS [training: 5.2609186346269805 | validation: 5.313718193637438]
	TIME [epoch: 10.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.269790924245825		[learning rate: 0.0026961]
	Learning Rate: 0.00269611
	LOSS [training: 5.269790924245825 | validation: 5.413483174760144]
	TIME [epoch: 10.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.214295827597131		[learning rate: 0.0026896]
	Learning Rate: 0.00268958
	LOSS [training: 5.214295827597131 | validation: 5.419607678597977]
	TIME [epoch: 10.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223500723446173		[learning rate: 0.0026831]
	Learning Rate: 0.00268307
	LOSS [training: 5.223500723446173 | validation: 5.297978162421989]
	TIME [epoch: 10.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.295676967047379		[learning rate: 0.0026766]
	Learning Rate: 0.00267657
	LOSS [training: 5.295676967047379 | validation: 5.348606101376011]
	TIME [epoch: 10.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180561569900182		[learning rate: 0.0026701]
	Learning Rate: 0.00267009
	LOSS [training: 5.180561569900182 | validation: 5.441527016701813]
	TIME [epoch: 10.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.203561750308606		[learning rate: 0.0026636]
	Learning Rate: 0.00266363
	LOSS [training: 5.203561750308606 | validation: 5.3051589588851344]
	TIME [epoch: 10.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18770680557178		[learning rate: 0.0026572]
	Learning Rate: 0.00265718
	LOSS [training: 5.18770680557178 | validation: 5.384597866881843]
	TIME [epoch: 10.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2810040130702705		[learning rate: 0.0026507]
	Learning Rate: 0.00265075
	LOSS [training: 5.2810040130702705 | validation: 5.330862538201202]
	TIME [epoch: 10.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.204676199013475		[learning rate: 0.0026443]
	Learning Rate: 0.00264433
	LOSS [training: 5.204676199013475 | validation: 5.449799874050539]
	TIME [epoch: 10.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.157867564154702		[learning rate: 0.0026379]
	Learning Rate: 0.00263793
	LOSS [training: 5.157867564154702 | validation: 5.297393077849835]
	TIME [epoch: 10.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15702702046471		[learning rate: 0.0026315]
	Learning Rate: 0.00263154
	LOSS [training: 5.15702702046471 | validation: 5.357759020705471]
	TIME [epoch: 10.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.136687634629675		[learning rate: 0.0026252]
	Learning Rate: 0.00262517
	LOSS [training: 5.136687634629675 | validation: 5.215169835118825]
	TIME [epoch: 10.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.136402715253732		[learning rate: 0.0026188]
	Learning Rate: 0.00261882
	LOSS [training: 5.136402715253732 | validation: 5.291344045659533]
	TIME [epoch: 10.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.137790292791804		[learning rate: 0.0026125]
	Learning Rate: 0.00261248
	LOSS [training: 5.137790292791804 | validation: 5.336977778303285]
	TIME [epoch: 10.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.143153331070673		[learning rate: 0.0026062]
	Learning Rate: 0.00260615
	LOSS [training: 5.143153331070673 | validation: 5.697612168842442]
	TIME [epoch: 10.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.273984794898011		[learning rate: 0.0025998]
	Learning Rate: 0.00259984
	LOSS [training: 5.273984794898011 | validation: 5.253174249318595]
	TIME [epoch: 10.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.120361907277721		[learning rate: 0.0025936]
	Learning Rate: 0.00259355
	LOSS [training: 5.120361907277721 | validation: 5.258757102543568]
	TIME [epoch: 10.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160099907375022		[learning rate: 0.0025873]
	Learning Rate: 0.00258727
	LOSS [training: 5.160099907375022 | validation: 5.469662831592451]
	TIME [epoch: 10.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.185025192347116		[learning rate: 0.002581]
	Learning Rate: 0.00258101
	LOSS [training: 5.185025192347116 | validation: 5.294572811386813]
	TIME [epoch: 10.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.093488297837176		[learning rate: 0.0025748]
	Learning Rate: 0.00257476
	LOSS [training: 5.093488297837176 | validation: 5.490112754739523]
	TIME [epoch: 10.3 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.13943121698896		[learning rate: 0.0025685]
	Learning Rate: 0.00256853
	LOSS [training: 5.13943121698896 | validation: 5.2947947249744685]
	TIME [epoch: 10.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1928641176210375		[learning rate: 0.0025623]
	Learning Rate: 0.00256231
	LOSS [training: 5.1928641176210375 | validation: 5.279674802816717]
	TIME [epoch: 10.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.13950792622113		[learning rate: 0.0025561]
	Learning Rate: 0.00255611
	LOSS [training: 5.13950792622113 | validation: 5.488886853419246]
	TIME [epoch: 10.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.136066977306875		[learning rate: 0.0025499]
	Learning Rate: 0.00254992
	LOSS [training: 5.136066977306875 | validation: 5.282338128531478]
	TIME [epoch: 10.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.104426673899818		[learning rate: 0.0025437]
	Learning Rate: 0.00254375
	LOSS [training: 5.104426673899818 | validation: 5.3332563850299275]
	TIME [epoch: 10.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.108006226800332		[learning rate: 0.0025376]
	Learning Rate: 0.00253759
	LOSS [training: 5.108006226800332 | validation: 5.2696090092902805]
	TIME [epoch: 10.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163839417909832		[learning rate: 0.0025314]
	Learning Rate: 0.00253144
	LOSS [training: 5.163839417909832 | validation: 5.367860666344249]
	TIME [epoch: 10.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1966424539367955		[learning rate: 0.0025253]
	Learning Rate: 0.00252532
	LOSS [training: 5.1966424539367955 | validation: 5.27479037297497]
	TIME [epoch: 10.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1312725131273105		[learning rate: 0.0025192]
	Learning Rate: 0.0025192
	LOSS [training: 5.1312725131273105 | validation: 5.253585475151609]
	TIME [epoch: 10.3 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.138704743116321		[learning rate: 0.0025131]
	Learning Rate: 0.0025131
	LOSS [training: 5.138704743116321 | validation: 5.302563913087133]
	TIME [epoch: 10.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.124399503801307		[learning rate: 0.002507]
	Learning Rate: 0.00250702
	LOSS [training: 5.124399503801307 | validation: 5.339724369630301]
	TIME [epoch: 10.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.143580518344029		[learning rate: 0.002501]
	Learning Rate: 0.00250095
	LOSS [training: 5.143580518344029 | validation: 5.254731032107923]
	TIME [epoch: 10.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.121609858734516		[learning rate: 0.0024949]
	Learning Rate: 0.0024949
	LOSS [training: 5.121609858734516 | validation: 5.287934107552255]
	TIME [epoch: 10.3 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.100968667939769		[learning rate: 0.0024889]
	Learning Rate: 0.00248886
	LOSS [training: 5.100968667939769 | validation: 5.298346481417609]
	TIME [epoch: 10.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156472546197696		[learning rate: 0.0024828]
	Learning Rate: 0.00248283
	LOSS [training: 5.156472546197696 | validation: 5.40420347454879]
	TIME [epoch: 10.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.111441132828871		[learning rate: 0.0024768]
	Learning Rate: 0.00247682
	LOSS [training: 5.111441132828871 | validation: 5.240571478033766]
	TIME [epoch: 10.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.277068827096513		[learning rate: 0.0024708]
	Learning Rate: 0.00247083
	LOSS [training: 5.277068827096513 | validation: 5.249824911501371]
	TIME [epoch: 10.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.297689679790411		[learning rate: 0.0024648]
	Learning Rate: 0.00246484
	LOSS [training: 5.297689679790411 | validation: 5.671471744047425]
	TIME [epoch: 10.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.305407654742069		[learning rate: 0.0024589]
	Learning Rate: 0.00245888
	LOSS [training: 5.305407654742069 | validation: 5.347859487729916]
	TIME [epoch: 10.3 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25433431255384		[learning rate: 0.0024529]
	Learning Rate: 0.00245292
	LOSS [training: 5.25433431255384 | validation: 5.437724156448159]
	TIME [epoch: 10.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.219695635007927		[learning rate: 0.002447]
	Learning Rate: 0.00244699
	LOSS [training: 5.219695635007927 | validation: 5.304617285033784]
	TIME [epoch: 10.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.173890144202981		[learning rate: 0.0024411]
	Learning Rate: 0.00244106
	LOSS [training: 5.173890144202981 | validation: 5.389449905137512]
	TIME [epoch: 10.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.126993794943838		[learning rate: 0.0024352]
	Learning Rate: 0.00243515
	LOSS [training: 5.126993794943838 | validation: 5.290423962397029]
	TIME [epoch: 10.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181533823582152		[learning rate: 0.0024293]
	Learning Rate: 0.00242926
	LOSS [training: 5.181533823582152 | validation: 5.330836837405453]
	TIME [epoch: 10.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.094633779906791		[learning rate: 0.0024234]
	Learning Rate: 0.00242338
	LOSS [training: 5.094633779906791 | validation: 5.328472039791355]
	TIME [epoch: 10.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148595443909736		[learning rate: 0.0024175]
	Learning Rate: 0.00241751
	LOSS [training: 5.148595443909736 | validation: 5.676161745176602]
	TIME [epoch: 10.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.328468079047626		[learning rate: 0.0024117]
	Learning Rate: 0.00241166
	LOSS [training: 5.328468079047626 | validation: 5.629331692551394]
	TIME [epoch: 10.3 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.314150552573324		[learning rate: 0.0024058]
	Learning Rate: 0.00240582
	LOSS [training: 5.314150552573324 | validation: 5.29370490236674]
	TIME [epoch: 10.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.278523824025546		[learning rate: 0.0024]
	Learning Rate: 0.0024
	LOSS [training: 5.278523824025546 | validation: 5.794567013227274]
	TIME [epoch: 10.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.323349874956567		[learning rate: 0.0023942]
	Learning Rate: 0.00239419
	LOSS [training: 5.323349874956567 | validation: 5.391017425205269]
	TIME [epoch: 10.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.146359256560861		[learning rate: 0.0023884]
	Learning Rate: 0.00238839
	LOSS [training: 5.146359256560861 | validation: 5.332634992343274]
	TIME [epoch: 10.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165123361882837		[learning rate: 0.0023826]
	Learning Rate: 0.00238261
	LOSS [training: 5.165123361882837 | validation: 5.351891259802823]
	TIME [epoch: 10.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164769105699781		[learning rate: 0.0023768]
	Learning Rate: 0.00237684
	LOSS [training: 5.164769105699781 | validation: 5.276232037935544]
	TIME [epoch: 10.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.243998152146387		[learning rate: 0.0023711]
	Learning Rate: 0.00237109
	LOSS [training: 5.243998152146387 | validation: 5.8831872617589624]
	TIME [epoch: 10.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.447595088877256		[learning rate: 0.0023653]
	Learning Rate: 0.00236535
	LOSS [training: 5.447595088877256 | validation: 5.413791976126138]
	TIME [epoch: 10.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.276565449054659		[learning rate: 0.0023596]
	Learning Rate: 0.00235962
	LOSS [training: 5.276565449054659 | validation: 5.363896032517305]
	TIME [epoch: 10.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2381573969691795		[learning rate: 0.0023539]
	Learning Rate: 0.00235391
	LOSS [training: 5.2381573969691795 | validation: 5.346366184560907]
	TIME [epoch: 10.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.26653945070314		[learning rate: 0.0023482]
	Learning Rate: 0.00234821
	LOSS [training: 5.26653945070314 | validation: 5.630285843286636]
	TIME [epoch: 10.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.313481505704443		[learning rate: 0.0023425]
	Learning Rate: 0.00234252
	LOSS [training: 5.313481505704443 | validation: 5.4721515268671]
	TIME [epoch: 10.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.235476512731688		[learning rate: 0.0023369]
	Learning Rate: 0.00233685
	LOSS [training: 5.235476512731688 | validation: 5.6059515638485005]
	TIME [epoch: 10.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2880488844945255		[learning rate: 0.0023312]
	Learning Rate: 0.0023312
	LOSS [training: 5.2880488844945255 | validation: 5.59935160109537]
	TIME [epoch: 10.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.524047549429641		[learning rate: 0.0023256]
	Learning Rate: 0.00232555
	LOSS [training: 5.524047549429641 | validation: 5.73691190903372]
	TIME [epoch: 10.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4000306138117065		[learning rate: 0.0023199]
	Learning Rate: 0.00231992
	LOSS [training: 5.4000306138117065 | validation: 5.442210277608776]
	TIME [epoch: 10.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.248814437859762		[learning rate: 0.0023143]
	Learning Rate: 0.00231431
	LOSS [training: 5.248814437859762 | validation: 5.295500822768445]
	TIME [epoch: 10.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.22351961642411		[learning rate: 0.0023087]
	Learning Rate: 0.0023087
	LOSS [training: 5.22351961642411 | validation: 5.3124607827232015]
	TIME [epoch: 10.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.283129916192581		[learning rate: 0.0023031]
	Learning Rate: 0.00230312
	LOSS [training: 5.283129916192581 | validation: 5.347556155560402]
	TIME [epoch: 10.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25728178114115		[learning rate: 0.0022975]
	Learning Rate: 0.00229754
	LOSS [training: 5.25728178114115 | validation: 5.316449407202749]
	TIME [epoch: 10.3 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.318128504409771		[learning rate: 0.002292]
	Learning Rate: 0.00229198
	LOSS [training: 5.318128504409771 | validation: 5.727631014548107]
	TIME [epoch: 10.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.438711014025472		[learning rate: 0.0022864]
	Learning Rate: 0.00228643
	LOSS [training: 5.438711014025472 | validation: 5.3314073473235215]
	TIME [epoch: 10.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.332543237314003		[learning rate: 0.0022809]
	Learning Rate: 0.0022809
	LOSS [training: 5.332543237314003 | validation: 5.6302791388791915]
	TIME [epoch: 10.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.413736447433727		[learning rate: 0.0022754]
	Learning Rate: 0.00227537
	LOSS [training: 5.413736447433727 | validation: 5.3720273019586235]
	TIME [epoch: 10.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3384334478738396		[learning rate: 0.0022699]
	Learning Rate: 0.00226986
	LOSS [training: 5.3384334478738396 | validation: 5.518248701951779]
	TIME [epoch: 10.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.37316236635997		[learning rate: 0.0022644]
	Learning Rate: 0.00226437
	LOSS [training: 5.37316236635997 | validation: 5.3570940776765985]
	TIME [epoch: 10.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.345759030812561		[learning rate: 0.0022589]
	Learning Rate: 0.00225889
	LOSS [training: 5.345759030812561 | validation: 5.633296951156189]
	TIME [epoch: 10.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2580844276136665		[learning rate: 0.0022534]
	Learning Rate: 0.00225342
	LOSS [training: 5.2580844276136665 | validation: 5.344806955288359]
	TIME [epoch: 10.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.226593726780671		[learning rate: 0.002248]
	Learning Rate: 0.00224796
	LOSS [training: 5.226593726780671 | validation: 5.332872533953819]
	TIME [epoch: 10.3 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192040991656014		[learning rate: 0.0022425]
	Learning Rate: 0.00224252
	LOSS [training: 5.192040991656014 | validation: 5.33433367392199]
	TIME [epoch: 10.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234734374274535		[learning rate: 0.0022371]
	Learning Rate: 0.00223709
	LOSS [training: 5.234734374274535 | validation: 5.293943395617411]
	TIME [epoch: 10.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.222813912309268		[learning rate: 0.0022317]
	Learning Rate: 0.00223168
	LOSS [training: 5.222813912309268 | validation: 5.332683111416285]
	TIME [epoch: 10.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.210893571158211		[learning rate: 0.0022263]
	Learning Rate: 0.00222628
	LOSS [training: 5.210893571158211 | validation: 5.335428824659917]
	TIME [epoch: 10.3 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.277263986994334		[learning rate: 0.0022209]
	Learning Rate: 0.00222089
	LOSS [training: 5.277263986994334 | validation: 5.381442704483832]
	TIME [epoch: 10.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.301131075180637		[learning rate: 0.0022155]
	Learning Rate: 0.00221551
	LOSS [training: 5.301131075180637 | validation: 5.311411869005392]
	TIME [epoch: 10.3 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.279923981854394		[learning rate: 0.0022101]
	Learning Rate: 0.00221015
	LOSS [training: 5.279923981854394 | validation: 5.464669196603418]
	TIME [epoch: 10.3 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2147200684723325		[learning rate: 0.0022048]
	Learning Rate: 0.0022048
	LOSS [training: 5.2147200684723325 | validation: 5.33783807271528]
	TIME [epoch: 10.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.242337019684493		[learning rate: 0.0021995]
	Learning Rate: 0.00219946
	LOSS [training: 5.242337019684493 | validation: 5.374238814331798]
	TIME [epoch: 10.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.206232646290785		[learning rate: 0.0021941]
	Learning Rate: 0.00219413
	LOSS [training: 5.206232646290785 | validation: 5.61826383682019]
	TIME [epoch: 10.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241852915611396		[learning rate: 0.0021888]
	Learning Rate: 0.00218882
	LOSS [training: 5.241852915611396 | validation: 6.241380965084061]
	TIME [epoch: 10.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.554693159098558		[learning rate: 0.0021835]
	Learning Rate: 0.00218352
	LOSS [training: 5.554693159098558 | validation: 5.306026659020823]
	TIME [epoch: 10.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.240422888122107		[learning rate: 0.0021782]
	Learning Rate: 0.00217824
	LOSS [training: 5.240422888122107 | validation: 5.294722476121922]
	TIME [epoch: 10.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.272266107511625		[learning rate: 0.002173]
	Learning Rate: 0.00217296
	LOSS [training: 5.272266107511625 | validation: 5.326868744475442]
	TIME [epoch: 10.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.254621452002286		[learning rate: 0.0021677]
	Learning Rate: 0.0021677
	LOSS [training: 5.254621452002286 | validation: 5.597152026920425]
	TIME [epoch: 10.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.249678694871285		[learning rate: 0.0021625]
	Learning Rate: 0.00216246
	LOSS [training: 5.249678694871285 | validation: 5.768305872054557]
	TIME [epoch: 10.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.646041577372541		[learning rate: 0.0021572]
	Learning Rate: 0.00215722
	LOSS [training: 5.646041577372541 | validation: 5.426292488643542]
	TIME [epoch: 10.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.221286029283637		[learning rate: 0.002152]
	Learning Rate: 0.002152
	LOSS [training: 5.221286029283637 | validation: 5.351161097325664]
	TIME [epoch: 10.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.211052597869704		[learning rate: 0.0021468]
	Learning Rate: 0.00214679
	LOSS [training: 5.211052597869704 | validation: 5.3508522674312795]
	TIME [epoch: 10.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148700439283802		[learning rate: 0.0021416]
	Learning Rate: 0.00214159
	LOSS [training: 5.148700439283802 | validation: 5.332998909743658]
	TIME [epoch: 10.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.209631992026608		[learning rate: 0.0021364]
	Learning Rate: 0.00213641
	LOSS [training: 5.209631992026608 | validation: 5.278701340638036]
	TIME [epoch: 10.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.327745526430151		[learning rate: 0.0021312]
	Learning Rate: 0.00213124
	LOSS [training: 5.327745526430151 | validation: 5.40111340849942]
	TIME [epoch: 10.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.260641822492315		[learning rate: 0.0021261]
	Learning Rate: 0.00212608
	LOSS [training: 5.260641822492315 | validation: 5.348416732919212]
	TIME [epoch: 10.3 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1689666503231475		[learning rate: 0.0021209]
	Learning Rate: 0.00212093
	LOSS [training: 5.1689666503231475 | validation: 5.433226904589501]
	TIME [epoch: 10.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.149261225127631		[learning rate: 0.0021158]
	Learning Rate: 0.0021158
	LOSS [training: 5.149261225127631 | validation: 5.299051683048665]
	TIME [epoch: 10.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.20503855767002		[learning rate: 0.0021107]
	Learning Rate: 0.00211067
	LOSS [training: 5.20503855767002 | validation: 5.399649190424968]
	TIME [epoch: 10.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1690395485751965		[learning rate: 0.0021056]
	Learning Rate: 0.00210556
	LOSS [training: 5.1690395485751965 | validation: 5.310145310757807]
	TIME [epoch: 10.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.120492321625159		[learning rate: 0.0021005]
	Learning Rate: 0.00210047
	LOSS [training: 5.120492321625159 | validation: 5.634375955805756]
	TIME [epoch: 10.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.26043316158122		[learning rate: 0.0020954]
	Learning Rate: 0.00209538
	LOSS [training: 5.26043316158122 | validation: 5.557100393495245]
	TIME [epoch: 10.3 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155901087330092		[learning rate: 0.0020903]
	Learning Rate: 0.00209031
	LOSS [training: 5.155901087330092 | validation: 5.8176686705652765]
	TIME [epoch: 10.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.362518545643927		[learning rate: 0.0020852]
	Learning Rate: 0.00208525
	LOSS [training: 5.362518545643927 | validation: 5.303699418785128]
	TIME [epoch: 10.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154742526146352		[learning rate: 0.0020802]
	Learning Rate: 0.0020802
	LOSS [training: 5.154742526146352 | validation: 5.526082288056723]
	TIME [epoch: 10.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.279448168486997		[learning rate: 0.0020752]
	Learning Rate: 0.00207517
	LOSS [training: 5.279448168486997 | validation: 5.336286676929135]
	TIME [epoch: 10.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165978042926316		[learning rate: 0.0020701]
	Learning Rate: 0.00207014
	LOSS [training: 5.165978042926316 | validation: 5.345650273924303]
	TIME [epoch: 10.3 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14351757755722		[learning rate: 0.0020651]
	Learning Rate: 0.00206513
	LOSS [training: 5.14351757755722 | validation: 5.25971158329177]
	TIME [epoch: 10.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.13355788322688		[learning rate: 0.0020601]
	Learning Rate: 0.00206013
	LOSS [training: 5.13355788322688 | validation: 5.4343897913318795]
	TIME [epoch: 10.3 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179802039959803		[learning rate: 0.0020551]
	Learning Rate: 0.00205514
	LOSS [training: 5.179802039959803 | validation: 5.347383839732317]
	TIME [epoch: 10.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.256182086325329		[learning rate: 0.0020502]
	Learning Rate: 0.00205017
	LOSS [training: 5.256182086325329 | validation: 5.3952256052542324]
	TIME [epoch: 10.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.233078644365101		[learning rate: 0.0020452]
	Learning Rate: 0.0020452
	LOSS [training: 5.233078644365101 | validation: 5.4818889340759736]
	TIME [epoch: 10.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.268414786073069		[learning rate: 0.0020403]
	Learning Rate: 0.00204025
	LOSS [training: 5.268414786073069 | validation: 5.479243055601516]
	TIME [epoch: 10.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.20010176486389		[learning rate: 0.0020353]
	Learning Rate: 0.00203531
	LOSS [training: 5.20010176486389 | validation: 5.435543116595129]
	TIME [epoch: 10.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193726934309585		[learning rate: 0.0020304]
	Learning Rate: 0.00203039
	LOSS [training: 5.193726934309585 | validation: 5.621006475550301]
	TIME [epoch: 10.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.315879703257274		[learning rate: 0.0020255]
	Learning Rate: 0.00202547
	LOSS [training: 5.315879703257274 | validation: 5.346947339126448]
	TIME [epoch: 10.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.206559064421384		[learning rate: 0.0020206]
	Learning Rate: 0.00202057
	LOSS [training: 5.206559064421384 | validation: 5.477404981228663]
	TIME [epoch: 10.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2209698249520455		[learning rate: 0.0020157]
	Learning Rate: 0.00201568
	LOSS [training: 5.2209698249520455 | validation: 5.385181116256816]
	TIME [epoch: 10.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191313490635685		[learning rate: 0.0020108]
	Learning Rate: 0.0020108
	LOSS [training: 5.191313490635685 | validation: 5.532928880034899]
	TIME [epoch: 10.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.424426950772262		[learning rate: 0.0020059]
	Learning Rate: 0.00200593
	LOSS [training: 5.424426950772262 | validation: 5.489135951692703]
	TIME [epoch: 10.3 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.22852282240772		[learning rate: 0.0020011]
	Learning Rate: 0.00200107
	LOSS [training: 5.22852282240772 | validation: 5.421444000762153]
	TIME [epoch: 10.3 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.106381301011188		[learning rate: 0.0019962]
	Learning Rate: 0.00199623
	LOSS [training: 5.106381301011188 | validation: 5.294391914154207]
	TIME [epoch: 10.3 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.130580977283835		[learning rate: 0.0019914]
	Learning Rate: 0.0019914
	LOSS [training: 5.130580977283835 | validation: 5.340373343847744]
	TIME [epoch: 10.3 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.085269477522948		[learning rate: 0.0019866]
	Learning Rate: 0.00198658
	LOSS [training: 5.085269477522948 | validation: 5.578656645956129]
	TIME [epoch: 10.3 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.244029205725852		[learning rate: 0.0019818]
	Learning Rate: 0.00198177
	LOSS [training: 5.244029205725852 | validation: 5.280262879326683]
	TIME [epoch: 10.3 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1320307059779084		[learning rate: 0.001977]
	Learning Rate: 0.00197697
	LOSS [training: 5.1320307059779084 | validation: 5.578227750178151]
	TIME [epoch: 10.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176675772078033		[learning rate: 0.0019722]
	Learning Rate: 0.00197218
	LOSS [training: 5.176675772078033 | validation: 5.5507485656842]
	TIME [epoch: 10.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.257104960257832		[learning rate: 0.0019674]
	Learning Rate: 0.00196741
	LOSS [training: 5.257104960257832 | validation: 5.355395011437915]
	TIME [epoch: 10.3 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.244897266511048		[learning rate: 0.0019626]
	Learning Rate: 0.00196265
	LOSS [training: 5.244897266511048 | validation: 5.738382271033082]
	TIME [epoch: 10.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.43398360831372		[learning rate: 0.0019579]
	Learning Rate: 0.0019579
	LOSS [training: 5.43398360831372 | validation: 5.1956117243202815]
	TIME [epoch: 10.3 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0647812375985115		[learning rate: 0.0019532]
	Learning Rate: 0.00195316
	LOSS [training: 5.0647812375985115 | validation: 5.257061614854592]
	TIME [epoch: 10.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.023728143726376		[learning rate: 0.0019484]
	Learning Rate: 0.00194843
	LOSS [training: 5.023728143726376 | validation: 5.143468659237783]
	TIME [epoch: 10.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.089778079789528		[learning rate: 0.0019437]
	Learning Rate: 0.00194371
	LOSS [training: 5.089778079789528 | validation: 5.272975383580466]
	TIME [epoch: 10.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.088143163766689		[learning rate: 0.001939]
	Learning Rate: 0.00193901
	LOSS [training: 5.088143163766689 | validation: 5.22595123657123]
	TIME [epoch: 10.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.080236021353857		[learning rate: 0.0019343]
	Learning Rate: 0.00193431
	LOSS [training: 5.080236021353857 | validation: 5.176913831685938]
	TIME [epoch: 10.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.090589097301967		[learning rate: 0.0019296]
	Learning Rate: 0.00192963
	LOSS [training: 5.090589097301967 | validation: 5.309468529067866]
	TIME [epoch: 10.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.040867233112074		[learning rate: 0.001925]
	Learning Rate: 0.00192496
	LOSS [training: 5.040867233112074 | validation: 5.237802747201986]
	TIME [epoch: 10.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0331025998075605		[learning rate: 0.0019203]
	Learning Rate: 0.0019203
	LOSS [training: 5.0331025998075605 | validation: 5.358495290449707]
	TIME [epoch: 10.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.076360994378026		[learning rate: 0.0019156]
	Learning Rate: 0.00191565
	LOSS [training: 5.076360994378026 | validation: 5.315420220126169]
	TIME [epoch: 10.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.081943337133491		[learning rate: 0.001911]
	Learning Rate: 0.00191101
	LOSS [training: 5.081943337133491 | validation: 5.314132189008955]
	TIME [epoch: 10.3 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.036338649820315		[learning rate: 0.0019064]
	Learning Rate: 0.00190638
	LOSS [training: 5.036338649820315 | validation: 5.326393864752365]
	TIME [epoch: 10.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.054233610222628		[learning rate: 0.0019018]
	Learning Rate: 0.00190177
	LOSS [training: 5.054233610222628 | validation: 5.229532197136291]
	TIME [epoch: 10.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.066216978649598		[learning rate: 0.0018972]
	Learning Rate: 0.00189717
	LOSS [training: 5.066216978649598 | validation: 5.241274340511575]
	TIME [epoch: 10.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.078757187352116		[learning rate: 0.0018926]
	Learning Rate: 0.00189257
	LOSS [training: 5.078757187352116 | validation: 5.429635276289398]
	TIME [epoch: 10.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171156869594974		[learning rate: 0.001888]
	Learning Rate: 0.00188799
	LOSS [training: 5.171156869594974 | validation: 5.30852928572612]
	TIME [epoch: 10.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.108785644434329		[learning rate: 0.0018834]
	Learning Rate: 0.00188342
	LOSS [training: 5.108785644434329 | validation: 5.287598788466214]
	TIME [epoch: 10.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.228390826957427		[learning rate: 0.0018789]
	Learning Rate: 0.00187886
	LOSS [training: 5.228390826957427 | validation: 5.427565438751149]
	TIME [epoch: 10.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1814610535476815		[learning rate: 0.0018743]
	Learning Rate: 0.00187431
	LOSS [training: 5.1814610535476815 | validation: 5.304082024866089]
	TIME [epoch: 10.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.069997383571932		[learning rate: 0.0018698]
	Learning Rate: 0.00186978
	LOSS [training: 5.069997383571932 | validation: 5.283920930247157]
	TIME [epoch: 10.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.066122603741726		[learning rate: 0.0018652]
	Learning Rate: 0.00186525
	LOSS [training: 5.066122603741726 | validation: 5.195972691191766]
	TIME [epoch: 10.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.064551284765291		[learning rate: 0.0018607]
	Learning Rate: 0.00186073
	LOSS [training: 5.064551284765291 | validation: 5.185603674881715]
	TIME [epoch: 10.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.023161239075299		[learning rate: 0.0018562]
	Learning Rate: 0.00185623
	LOSS [training: 5.023161239075299 | validation: 5.324282114199317]
	TIME [epoch: 10.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.05082591207502		[learning rate: 0.0018517]
	Learning Rate: 0.00185174
	LOSS [training: 5.05082591207502 | validation: 5.2959153038629845]
	TIME [epoch: 10.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.202965972096864		[learning rate: 0.0018473]
	Learning Rate: 0.00184725
	LOSS [training: 5.202965972096864 | validation: 5.204871946205312]
	TIME [epoch: 10.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.102604148622639		[learning rate: 0.0018428]
	Learning Rate: 0.00184278
	LOSS [training: 5.102604148622639 | validation: 5.184222000427929]
	TIME [epoch: 10.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.06341555507895		[learning rate: 0.0018383]
	Learning Rate: 0.00183832
	LOSS [training: 5.06341555507895 | validation: 5.250793645494888]
	TIME [epoch: 10.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.075248023412316		[learning rate: 0.0018339]
	Learning Rate: 0.00183387
	LOSS [training: 5.075248023412316 | validation: 5.3242731376816055]
	TIME [epoch: 10.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1422155647198355		[learning rate: 0.0018294]
	Learning Rate: 0.00182943
	LOSS [training: 5.1422155647198355 | validation: 5.192671335617923]
	TIME [epoch: 10.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0364664605947835		[learning rate: 0.001825]
	Learning Rate: 0.001825
	LOSS [training: 5.0364664605947835 | validation: 5.164865468864438]
	TIME [epoch: 10.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0804541356857325		[learning rate: 0.0018206]
	Learning Rate: 0.00182058
	LOSS [training: 5.0804541356857325 | validation: 5.194434853092359]
	TIME [epoch: 10.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.069552230496465		[learning rate: 0.0018162]
	Learning Rate: 0.00181618
	LOSS [training: 5.069552230496465 | validation: 5.30414132183775]
	TIME [epoch: 10.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.082937566294325		[learning rate: 0.0018118]
	Learning Rate: 0.00181178
	LOSS [training: 5.082937566294325 | validation: 5.273208912284107]
	TIME [epoch: 10.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.077311220527041		[learning rate: 0.0018074]
	Learning Rate: 0.00180739
	LOSS [training: 5.077311220527041 | validation: 5.308649606844089]
	TIME [epoch: 10.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.114944524886128		[learning rate: 0.001803]
	Learning Rate: 0.00180302
	LOSS [training: 5.114944524886128 | validation: 5.202412727939908]
	TIME [epoch: 10.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.073920412414008		[learning rate: 0.0017987]
	Learning Rate: 0.00179865
	LOSS [training: 5.073920412414008 | validation: 5.378661330914752]
	TIME [epoch: 10.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.053500852022773		[learning rate: 0.0017943]
	Learning Rate: 0.0017943
	LOSS [training: 5.053500852022773 | validation: 5.242012032354272]
	TIME [epoch: 10.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.047942466515723		[learning rate: 0.00179]
	Learning Rate: 0.00178995
	LOSS [training: 5.047942466515723 | validation: 5.424687149399643]
	TIME [epoch: 10.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.049817904060189		[learning rate: 0.0017856]
	Learning Rate: 0.00178562
	LOSS [training: 5.049817904060189 | validation: 5.259698947565636]
	TIME [epoch: 10.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.076181969930884		[learning rate: 0.0017813]
	Learning Rate: 0.0017813
	LOSS [training: 5.076181969930884 | validation: 5.268204583397703]
	TIME [epoch: 10.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.067323405461731		[learning rate: 0.001777]
	Learning Rate: 0.00177699
	LOSS [training: 5.067323405461731 | validation: 5.1726157231129815]
	TIME [epoch: 10.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0466464992403335		[learning rate: 0.0017727]
	Learning Rate: 0.00177269
	LOSS [training: 5.0466464992403335 | validation: 5.142944735483759]
	TIME [epoch: 10.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.008019207042407		[learning rate: 0.0017684]
	Learning Rate: 0.00176839
	LOSS [training: 5.008019207042407 | validation: 5.303750493064995]
	TIME [epoch: 10.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.061503462594353		[learning rate: 0.0017641]
	Learning Rate: 0.00176411
	LOSS [training: 5.061503462594353 | validation: 5.127293495645961]
	TIME [epoch: 10.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.990569973438939		[learning rate: 0.0017598]
	Learning Rate: 0.00175984
	LOSS [training: 4.990569973438939 | validation: 5.1931019579367765]
	TIME [epoch: 10.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.008595735602159		[learning rate: 0.0017556]
	Learning Rate: 0.00175558
	LOSS [training: 5.008595735602159 | validation: 5.190577610195037]
	TIME [epoch: 10.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.040975287394101		[learning rate: 0.0017513]
	Learning Rate: 0.00175133
	LOSS [training: 5.040975287394101 | validation: 5.23324382670409]
	TIME [epoch: 10.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.040263645427271		[learning rate: 0.0017471]
	Learning Rate: 0.00174709
	LOSS [training: 5.040263645427271 | validation: 5.2431540301359485]
	TIME [epoch: 10.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.032813465129633		[learning rate: 0.0017429]
	Learning Rate: 0.00174286
	LOSS [training: 5.032813465129633 | validation: 5.162449962181853]
	TIME [epoch: 10.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.020363018757028		[learning rate: 0.0017386]
	Learning Rate: 0.00173864
	LOSS [training: 5.020363018757028 | validation: 5.12779305509386]
	TIME [epoch: 10.3 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.135592361971797		[learning rate: 0.0017344]
	Learning Rate: 0.00173443
	LOSS [training: 5.135592361971797 | validation: 5.182198617025327]
	TIME [epoch: 10.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.079154571033108		[learning rate: 0.0017302]
	Learning Rate: 0.00173024
	LOSS [training: 5.079154571033108 | validation: 5.132829105162366]
	TIME [epoch: 10.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.023207080400268		[learning rate: 0.001726]
	Learning Rate: 0.00172605
	LOSS [training: 5.023207080400268 | validation: 5.238318093891031]
	TIME [epoch: 10.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.983775200268768		[learning rate: 0.0017219]
	Learning Rate: 0.00172187
	LOSS [training: 4.983775200268768 | validation: 5.273381463756589]
	TIME [epoch: 10.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.053817536861627		[learning rate: 0.0017177]
	Learning Rate: 0.0017177
	LOSS [training: 5.053817536861627 | validation: 5.121514153837859]
	TIME [epoch: 10.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.010117609377742		[learning rate: 0.0017135]
	Learning Rate: 0.00171354
	LOSS [training: 5.010117609377742 | validation: 5.1338856734538005]
	TIME [epoch: 10.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.016821471570997		[learning rate: 0.0017094]
	Learning Rate: 0.00170939
	LOSS [training: 5.016821471570997 | validation: 5.138662081808675]
	TIME [epoch: 10.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.028263892770627		[learning rate: 0.0017053]
	Learning Rate: 0.00170526
	LOSS [training: 5.028263892770627 | validation: 5.1470946110161915]
	TIME [epoch: 10.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.051083355103321		[learning rate: 0.0017011]
	Learning Rate: 0.00170113
	LOSS [training: 5.051083355103321 | validation: 5.472006425933421]
	TIME [epoch: 10.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.115951298788111		[learning rate: 0.001697]
	Learning Rate: 0.00169701
	LOSS [training: 5.115951298788111 | validation: 5.3118848825832075]
	TIME [epoch: 10.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.011558644248018		[learning rate: 0.0016929]
	Learning Rate: 0.0016929
	LOSS [training: 5.011558644248018 | validation: 5.187680319341369]
	TIME [epoch: 10.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.030724513285817		[learning rate: 0.0016888]
	Learning Rate: 0.0016888
	LOSS [training: 5.030724513285817 | validation: 5.256751220644828]
	TIME [epoch: 10.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.99840482097312		[learning rate: 0.0016847]
	Learning Rate: 0.00168471
	LOSS [training: 4.99840482097312 | validation: 5.266432719798208]
	TIME [epoch: 10.3 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.013589840611316		[learning rate: 0.0016806]
	Learning Rate: 0.00168064
	LOSS [training: 5.013589840611316 | validation: 5.168900946471784]
	TIME [epoch: 10.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.099757246206425		[learning rate: 0.0016766]
	Learning Rate: 0.00167657
	LOSS [training: 5.099757246206425 | validation: 5.222572140980685]
	TIME [epoch: 10.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.08646544580955		[learning rate: 0.0016725]
	Learning Rate: 0.00167251
	LOSS [training: 5.08646544580955 | validation: 5.347192021127401]
	TIME [epoch: 10.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1157017260512845		[learning rate: 0.0016685]
	Learning Rate: 0.00166846
	LOSS [training: 5.1157017260512845 | validation: 5.16376803718005]
	TIME [epoch: 10.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.116833502214225		[learning rate: 0.0016644]
	Learning Rate: 0.00166442
	LOSS [training: 5.116833502214225 | validation: 5.225889145814413]
	TIME [epoch: 10.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.049367385216134		[learning rate: 0.0016604]
	Learning Rate: 0.00166039
	LOSS [training: 5.049367385216134 | validation: 5.153845342715847]
	TIME [epoch: 10.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163194533449607		[learning rate: 0.0016564]
	Learning Rate: 0.00165637
	LOSS [training: 5.163194533449607 | validation: 5.195263792159819]
	TIME [epoch: 10.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.198179933298397		[learning rate: 0.0016524]
	Learning Rate: 0.00165236
	LOSS [training: 5.198179933298397 | validation: 5.2656627442277015]
	TIME [epoch: 10.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.211168696542615		[learning rate: 0.0016484]
	Learning Rate: 0.00164836
	LOSS [training: 5.211168696542615 | validation: 5.249132526652411]
	TIME [epoch: 10.3 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.072937489318891		[learning rate: 0.0016444]
	Learning Rate: 0.00164437
	LOSS [training: 5.072937489318891 | validation: 5.303074084328556]
	TIME [epoch: 10.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.073923579084625		[learning rate: 0.0016404]
	Learning Rate: 0.00164039
	LOSS [training: 5.073923579084625 | validation: 5.126605045126772]
	TIME [epoch: 10.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.053079950199344		[learning rate: 0.0016364]
	Learning Rate: 0.00163642
	LOSS [training: 5.053079950199344 | validation: 5.339413205085703]
	TIME [epoch: 10.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.413336123738029		[learning rate: 0.0016325]
	Learning Rate: 0.00163246
	LOSS [training: 5.413336123738029 | validation: 5.606235370691049]
	TIME [epoch: 10.3 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.528591654254912		[learning rate: 0.0016285]
	Learning Rate: 0.00162851
	LOSS [training: 5.528591654254912 | validation: 5.348022293835607]
	TIME [epoch: 10.3 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2929383145171105		[learning rate: 0.0016246]
	Learning Rate: 0.00162456
	LOSS [training: 5.2929383145171105 | validation: 5.442369743114316]
	TIME [epoch: 10.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.31663168982552		[learning rate: 0.0016206]
	Learning Rate: 0.00162063
	LOSS [training: 5.31663168982552 | validation: 5.272656688028205]
	TIME [epoch: 10.3 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.283251566574981		[learning rate: 0.0016167]
	Learning Rate: 0.00161671
	LOSS [training: 5.283251566574981 | validation: 5.356205756619588]
	TIME [epoch: 10.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.355331889280836		[learning rate: 0.0016128]
	Learning Rate: 0.00161279
	LOSS [training: 5.355331889280836 | validation: 5.429187175995633]
	TIME [epoch: 10.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.350419335742087		[learning rate: 0.0016089]
	Learning Rate: 0.00160889
	LOSS [training: 5.350419335742087 | validation: 5.2770462703175225]
	TIME [epoch: 10.3 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.359167795485074		[learning rate: 0.001605]
	Learning Rate: 0.001605
	LOSS [training: 5.359167795485074 | validation: 5.435804610416942]
	TIME [epoch: 10.3 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.336602216590088		[learning rate: 0.0016011]
	Learning Rate: 0.00160111
	LOSS [training: 5.336602216590088 | validation: 5.351177617784898]
	TIME [epoch: 10.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3896875675179645		[learning rate: 0.0015972]
	Learning Rate: 0.00159723
	LOSS [training: 5.3896875675179645 | validation: 5.546897599260464]
	TIME [epoch: 10.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.501144869266777		[learning rate: 0.0015934]
	Learning Rate: 0.00159337
	LOSS [training: 5.501144869266777 | validation: 5.438911253110231]
	TIME [epoch: 10.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.405559289055073		[learning rate: 0.0015895]
	Learning Rate: 0.00158951
	LOSS [training: 5.405559289055073 | validation: 5.407285761294195]
	TIME [epoch: 10.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.391511210413685		[learning rate: 0.0015857]
	Learning Rate: 0.00158566
	LOSS [training: 5.391511210413685 | validation: 5.479950460796893]
	TIME [epoch: 10.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4077463464014714		[learning rate: 0.0015818]
	Learning Rate: 0.00158182
	LOSS [training: 5.4077463464014714 | validation: 5.462501951281338]
	TIME [epoch: 10.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.412997611089432		[learning rate: 0.001578]
	Learning Rate: 0.00157799
	LOSS [training: 5.412997611089432 | validation: 5.570103039608541]
	TIME [epoch: 10.3 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.404271535189435		[learning rate: 0.0015742]
	Learning Rate: 0.00157417
	LOSS [training: 5.404271535189435 | validation: 5.2734805095068875]
	TIME [epoch: 10.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.335939163176515		[learning rate: 0.0015704]
	Learning Rate: 0.00157036
	LOSS [training: 5.335939163176515 | validation: 5.432178854760896]
	TIME [epoch: 10.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.358512696332804		[learning rate: 0.0015666]
	Learning Rate: 0.00156656
	LOSS [training: 5.358512696332804 | validation: 5.296796048917504]
	TIME [epoch: 10.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.322512513088044		[learning rate: 0.0015628]
	Learning Rate: 0.00156277
	LOSS [training: 5.322512513088044 | validation: 5.458672754700008]
	TIME [epoch: 10.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.415430047484074		[learning rate: 0.001559]
	Learning Rate: 0.00155899
	LOSS [training: 5.415430047484074 | validation: 5.239471175055131]
	TIME [epoch: 10.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.29391735108418		[learning rate: 0.0015552]
	Learning Rate: 0.00155521
	LOSS [training: 5.29391735108418 | validation: 5.3685863869887065]
	TIME [epoch: 10.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.294011281876923		[learning rate: 0.0015514]
	Learning Rate: 0.00155145
	LOSS [training: 5.294011281876923 | validation: 5.227993432388381]
	TIME [epoch: 10.3 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.262127785454892		[learning rate: 0.0015477]
	Learning Rate: 0.00154769
	LOSS [training: 5.262127785454892 | validation: 5.22742661375544]
	TIME [epoch: 10.3 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.258602165427284		[learning rate: 0.0015439]
	Learning Rate: 0.00154394
	LOSS [training: 5.258602165427284 | validation: 5.301039050689091]
	TIME [epoch: 10.3 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.286790465602472		[learning rate: 0.0015402]
	Learning Rate: 0.00154021
	LOSS [training: 5.286790465602472 | validation: 5.224871829418304]
	TIME [epoch: 10.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241670840840806		[learning rate: 0.0015365]
	Learning Rate: 0.00153648
	LOSS [training: 5.241670840840806 | validation: 5.266764205192454]
	TIME [epoch: 10.3 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.200504058522245		[learning rate: 0.0015328]
	Learning Rate: 0.00153276
	LOSS [training: 5.200504058522245 | validation: 5.239312654601442]
	TIME [epoch: 10.3 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.212907457553317		[learning rate: 0.001529]
	Learning Rate: 0.00152905
	LOSS [training: 5.212907457553317 | validation: 5.216119836778862]
	TIME [epoch: 10.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.318471997192639		[learning rate: 0.0015253]
	Learning Rate: 0.00152535
	LOSS [training: 5.318471997192639 | validation: 5.145188733232651]
	TIME [epoch: 10.3 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.198495298931796		[learning rate: 0.0015217]
	Learning Rate: 0.00152165
	LOSS [training: 5.198495298931796 | validation: 5.28682027404884]
	TIME [epoch: 10.3 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.203623648119246		[learning rate: 0.001518]
	Learning Rate: 0.00151797
	LOSS [training: 5.203623648119246 | validation: 5.227916743656707]
	TIME [epoch: 10.3 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.28774494582642		[learning rate: 0.0015143]
	Learning Rate: 0.00151429
	LOSS [training: 5.28774494582642 | validation: 5.257676577299649]
	TIME [epoch: 10.3 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.260256771520359		[learning rate: 0.0015106]
	Learning Rate: 0.00151063
	LOSS [training: 5.260256771520359 | validation: 5.227836288188797]
	TIME [epoch: 10.3 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.330098842634631		[learning rate: 0.001507]
	Learning Rate: 0.00150697
	LOSS [training: 5.330098842634631 | validation: 5.199408957698019]
	TIME [epoch: 10.3 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.290785338696826		[learning rate: 0.0015033]
	Learning Rate: 0.00150332
	LOSS [training: 5.290785338696826 | validation: 5.39265057936798]
	TIME [epoch: 10.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.284995835605569		[learning rate: 0.0014997]
	Learning Rate: 0.00149968
	LOSS [training: 5.284995835605569 | validation: 5.220033493368369]
	TIME [epoch: 10.3 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.269111985000849		[learning rate: 0.0014961]
	Learning Rate: 0.00149605
	LOSS [training: 5.269111985000849 | validation: 5.165950531117025]
	TIME [epoch: 10.3 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.243380959474183		[learning rate: 0.0014924]
	Learning Rate: 0.00149243
	LOSS [training: 5.243380959474183 | validation: 5.193825568044065]
	TIME [epoch: 10.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.225330436425461		[learning rate: 0.0014888]
	Learning Rate: 0.00148882
	LOSS [training: 5.225330436425461 | validation: 5.18058526867448]
	TIME [epoch: 10.3 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.262744450811245		[learning rate: 0.0014852]
	Learning Rate: 0.00148522
	LOSS [training: 5.262744450811245 | validation: 5.225935118652423]
	TIME [epoch: 10.3 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.263604718002216		[learning rate: 0.0014816]
	Learning Rate: 0.00148162
	LOSS [training: 5.263604718002216 | validation: 5.188618973841151]
	TIME [epoch: 10.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.34593713395502		[learning rate: 0.001478]
	Learning Rate: 0.00147803
	LOSS [training: 5.34593713395502 | validation: 5.282011478384379]
	TIME [epoch: 10.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.225891852596795		[learning rate: 0.0014745]
	Learning Rate: 0.00147446
	LOSS [training: 5.225891852596795 | validation: 5.172962903317806]
	TIME [epoch: 10.3 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.254316731813493		[learning rate: 0.0014709]
	Learning Rate: 0.00147089
	LOSS [training: 5.254316731813493 | validation: 5.2571537124396786]
	TIME [epoch: 10.3 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.259204520056551		[learning rate: 0.0014673]
	Learning Rate: 0.00146732
	LOSS [training: 5.259204520056551 | validation: 5.228381499320493]
	TIME [epoch: 10.3 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.229959099831038		[learning rate: 0.0014638]
	Learning Rate: 0.00146377
	LOSS [training: 5.229959099831038 | validation: 5.223389215519514]
	TIME [epoch: 10.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234529467129472		[learning rate: 0.0014602]
	Learning Rate: 0.00146023
	LOSS [training: 5.234529467129472 | validation: 5.307015747896903]
	TIME [epoch: 10.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.261932238608997		[learning rate: 0.0014567]
	Learning Rate: 0.00145669
	LOSS [training: 5.261932238608997 | validation: 5.2060753576034156]
	TIME [epoch: 10.3 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.20850554890839		[learning rate: 0.0014532]
	Learning Rate: 0.00145317
	LOSS [training: 5.20850554890839 | validation: 5.161052378120634]
	TIME [epoch: 10.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2693232199716435		[learning rate: 0.0014497]
	Learning Rate: 0.00144965
	LOSS [training: 5.2693232199716435 | validation: 5.164244210937023]
	TIME [epoch: 10.3 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182812094733258		[learning rate: 0.0014461]
	Learning Rate: 0.00144614
	LOSS [training: 5.182812094733258 | validation: 5.213815426451695]
	TIME [epoch: 10.3 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181343657200533		[learning rate: 0.0014426]
	Learning Rate: 0.00144264
	LOSS [training: 5.181343657200533 | validation: 5.181482354116944]
	TIME [epoch: 10.3 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175134081545908		[learning rate: 0.0014391]
	Learning Rate: 0.00143915
	LOSS [training: 5.175134081545908 | validation: 5.272201925498443]
	TIME [epoch: 10.3 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.17585964336017		[learning rate: 0.0014357]
	Learning Rate: 0.00143566
	LOSS [training: 5.17585964336017 | validation: 5.317736809019569]
	TIME [epoch: 10.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.253055486836629		[learning rate: 0.0014322]
	Learning Rate: 0.00143219
	LOSS [training: 5.253055486836629 | validation: 5.273535837491479]
	TIME [epoch: 10.3 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.199793148849313		[learning rate: 0.0014287]
	Learning Rate: 0.00142872
	LOSS [training: 5.199793148849313 | validation: 5.4178122859410065]
	TIME [epoch: 10.3 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.250942569024133		[learning rate: 0.0014253]
	Learning Rate: 0.00142526
	LOSS [training: 5.250942569024133 | validation: 5.172180865338991]
	TIME [epoch: 10.3 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172897464886095		[learning rate: 0.0014218]
	Learning Rate: 0.00142181
	LOSS [training: 5.172897464886095 | validation: 5.191714991184438]
	TIME [epoch: 10.3 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193386620156035		[learning rate: 0.0014184]
	Learning Rate: 0.00141837
	LOSS [training: 5.193386620156035 | validation: 5.22955437422392]
	TIME [epoch: 10.3 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.245929077924521		[learning rate: 0.0014149]
	Learning Rate: 0.00141494
	LOSS [training: 5.245929077924521 | validation: 5.167028207834135]
	TIME [epoch: 10.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181977148693124		[learning rate: 0.0014115]
	Learning Rate: 0.00141151
	LOSS [training: 5.181977148693124 | validation: 5.2037353859711635]
	TIME [epoch: 10.3 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164571476010517		[learning rate: 0.0014081]
	Learning Rate: 0.00140809
	LOSS [training: 5.164571476010517 | validation: 5.2484213437129155]
	TIME [epoch: 10.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241646325301755		[learning rate: 0.0014047]
	Learning Rate: 0.00140469
	LOSS [training: 5.241646325301755 | validation: 5.220660984901346]
	TIME [epoch: 10.3 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.200126628130223		[learning rate: 0.0014013]
	Learning Rate: 0.00140128
	LOSS [training: 5.200126628130223 | validation: 5.173403050544884]
	TIME [epoch: 10.3 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.220049188677588		[learning rate: 0.0013979]
	Learning Rate: 0.00139789
	LOSS [training: 5.220049188677588 | validation: 5.487701577546115]
	TIME [epoch: 10.3 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.258239303690181		[learning rate: 0.0013945]
	Learning Rate: 0.00139451
	LOSS [training: 5.258239303690181 | validation: 5.167084957062534]
	TIME [epoch: 10.3 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.261725694934481		[learning rate: 0.0013911]
	Learning Rate: 0.00139113
	LOSS [training: 5.261725694934481 | validation: 5.18447220847317]
	TIME [epoch: 10.3 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180399908531217		[learning rate: 0.0013878]
	Learning Rate: 0.00138776
	LOSS [training: 5.180399908531217 | validation: 5.241215276900281]
	TIME [epoch: 10.3 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.188193227629461		[learning rate: 0.0013844]
	Learning Rate: 0.00138441
	LOSS [training: 5.188193227629461 | validation: 5.128853643037282]
	TIME [epoch: 10.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193981738629242		[learning rate: 0.0013811]
	Learning Rate: 0.00138105
	LOSS [training: 5.193981738629242 | validation: 5.178326065939591]
	TIME [epoch: 10.3 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190096227639678		[learning rate: 0.0013777]
	Learning Rate: 0.00137771
	LOSS [training: 5.190096227639678 | validation: 5.20252026740184]
	TIME [epoch: 10.3 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16336919721383		[learning rate: 0.0013744]
	Learning Rate: 0.00137437
	LOSS [training: 5.16336919721383 | validation: 5.217904840868396]
	TIME [epoch: 10.3 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.198920026067617		[learning rate: 0.001371]
	Learning Rate: 0.00137105
	LOSS [training: 5.198920026067617 | validation: 5.1885422245028945]
	TIME [epoch: 10.3 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.17434329735263		[learning rate: 0.0013677]
	Learning Rate: 0.00136773
	LOSS [training: 5.17434329735263 | validation: 5.182863592706526]
	TIME [epoch: 10.3 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.169041590945212		[learning rate: 0.0013644]
	Learning Rate: 0.00136442
	LOSS [training: 5.169041590945212 | validation: 5.100359004813816]
	TIME [epoch: 10.3 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.101576103375349		[learning rate: 0.0013611]
	Learning Rate: 0.00136111
	LOSS [training: 5.101576103375349 | validation: 5.117484712073308]
	TIME [epoch: 10.3 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.099936375006334		[learning rate: 0.0013578]
	Learning Rate: 0.00135782
	LOSS [training: 5.099936375006334 | validation: 5.150833512949612]
	TIME [epoch: 10.3 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160466535140129		[learning rate: 0.0013545]
	Learning Rate: 0.00135453
	LOSS [training: 5.160466535140129 | validation: 5.170173874273536]
	TIME [epoch: 10.3 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.121503867347729		[learning rate: 0.0013513]
	Learning Rate: 0.00135125
	LOSS [training: 5.121503867347729 | validation: 5.2268914877041315]
	TIME [epoch: 10.3 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.107970564155019		[learning rate: 0.001348]
	Learning Rate: 0.00134798
	LOSS [training: 5.107970564155019 | validation: 5.215953908344313]
	TIME [epoch: 10.3 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1548650213649765		[learning rate: 0.0013447]
	Learning Rate: 0.00134472
	LOSS [training: 5.1548650213649765 | validation: 5.115190094291867]
	TIME [epoch: 10.3 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.187955921114611		[learning rate: 0.0013415]
	Learning Rate: 0.00134146
	LOSS [training: 5.187955921114611 | validation: 5.127265524252634]
	TIME [epoch: 10.3 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.135243752261488		[learning rate: 0.0013382]
	Learning Rate: 0.00133822
	LOSS [training: 5.135243752261488 | validation: 5.144680693065982]
	TIME [epoch: 10.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132037223108695		[learning rate: 0.001335]
	Learning Rate: 0.00133498
	LOSS [training: 5.132037223108695 | validation: 5.151823785553212]
	TIME [epoch: 10.3 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170553575386899		[learning rate: 0.0013317]
	Learning Rate: 0.00133174
	LOSS [training: 5.170553575386899 | validation: 5.163821092868038]
	TIME [epoch: 10.3 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1533580455953985		[learning rate: 0.0013285]
	Learning Rate: 0.00132852
	LOSS [training: 5.1533580455953985 | validation: 5.1316110129948065]
	TIME [epoch: 10.3 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.114480007217059		[learning rate: 0.0013253]
	Learning Rate: 0.0013253
	LOSS [training: 5.114480007217059 | validation: 5.39991961698968]
	TIME [epoch: 10.3 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175978123894306		[learning rate: 0.0013221]
	Learning Rate: 0.0013221
	LOSS [training: 5.175978123894306 | validation: 5.201493419348437]
	TIME [epoch: 10.3 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178152497640781		[learning rate: 0.0013189]
	Learning Rate: 0.0013189
	LOSS [training: 5.178152497640781 | validation: 5.125650970811931]
	TIME [epoch: 10.3 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.137228105682075		[learning rate: 0.0013157]
	Learning Rate: 0.0013157
	LOSS [training: 5.137228105682075 | validation: 5.15173931156523]
	TIME [epoch: 10.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.206438549462833		[learning rate: 0.0013125]
	Learning Rate: 0.00131252
	LOSS [training: 5.206438549462833 | validation: 5.268099644997355]
	TIME [epoch: 10.3 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2232216882984		[learning rate: 0.0013093]
	Learning Rate: 0.00130934
	LOSS [training: 5.2232216882984 | validation: 5.1594824907826276]
	TIME [epoch: 10.3 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2829891561472255		[learning rate: 0.0013062]
	Learning Rate: 0.00130617
	LOSS [training: 5.2829891561472255 | validation: 5.1859039295805065]
	TIME [epoch: 10.3 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.287281425482353		[learning rate: 0.001303]
	Learning Rate: 0.00130301
	LOSS [training: 5.287281425482353 | validation: 5.410782009049414]
	TIME [epoch: 10.3 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.259114469430438		[learning rate: 0.0012999]
	Learning Rate: 0.00129985
	LOSS [training: 5.259114469430438 | validation: 5.159254330744148]
	TIME [epoch: 10.3 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.220664818963241		[learning rate: 0.0012967]
	Learning Rate: 0.00129671
	LOSS [training: 5.220664818963241 | validation: 5.303406138228724]
	TIME [epoch: 10.3 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.237290267754316		[learning rate: 0.0012936]
	Learning Rate: 0.00129357
	LOSS [training: 5.237290267754316 | validation: 5.253622841928425]
	TIME [epoch: 10.3 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.334958603687042		[learning rate: 0.0012904]
	Learning Rate: 0.00129044
	LOSS [training: 5.334958603687042 | validation: 5.286507909871134]
	TIME [epoch: 10.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.340730805185834		[learning rate: 0.0012873]
	Learning Rate: 0.00128731
	LOSS [training: 5.340730805185834 | validation: 5.250083238027876]
	TIME [epoch: 10.3 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.368453880548996		[learning rate: 0.0012842]
	Learning Rate: 0.0012842
	LOSS [training: 5.368453880548996 | validation: 5.318683957074412]
	TIME [epoch: 10.3 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3035814051606955		[learning rate: 0.0012811]
	Learning Rate: 0.00128109
	LOSS [training: 5.3035814051606955 | validation: 5.328842929381283]
	TIME [epoch: 10.3 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.31269141685244		[learning rate: 0.001278]
	Learning Rate: 0.00127799
	LOSS [training: 5.31269141685244 | validation: 5.346949955430758]
	TIME [epoch: 10.3 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.331624588674623		[learning rate: 0.0012749]
	Learning Rate: 0.00127489
	LOSS [training: 5.331624588674623 | validation: 5.312923230631068]
	TIME [epoch: 10.3 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.272248856946144		[learning rate: 0.0012718]
	Learning Rate: 0.00127181
	LOSS [training: 5.272248856946144 | validation: 5.187580064115945]
	TIME [epoch: 10.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.270676209451517		[learning rate: 0.0012687]
	Learning Rate: 0.00126873
	LOSS [training: 5.270676209451517 | validation: 5.188143808391701]
	TIME [epoch: 10.3 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.302848057813166		[learning rate: 0.0012657]
	Learning Rate: 0.00126566
	LOSS [training: 5.302848057813166 | validation: 5.211003621767571]
	TIME [epoch: 10.3 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.250131045475897		[learning rate: 0.0012626]
	Learning Rate: 0.00126259
	LOSS [training: 5.250131045475897 | validation: 5.198522839414955]
	TIME [epoch: 10.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.222262920351289		[learning rate: 0.0012595]
	Learning Rate: 0.00125954
	LOSS [training: 5.222262920351289 | validation: 5.244601394230374]
	TIME [epoch: 10.3 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.218837288499907		[learning rate: 0.0012565]
	Learning Rate: 0.00125649
	LOSS [training: 5.218837288499907 | validation: 5.197829517166826]
	TIME [epoch: 10.3 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.206655767221563		[learning rate: 0.0012534]
	Learning Rate: 0.00125344
	LOSS [training: 5.206655767221563 | validation: 5.179032828454597]
	TIME [epoch: 10.3 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.24416027139323		[learning rate: 0.0012504]
	Learning Rate: 0.00125041
	LOSS [training: 5.24416027139323 | validation: 5.22044513989227]
	TIME [epoch: 10.3 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.22259774474436		[learning rate: 0.0012474]
	Learning Rate: 0.00124738
	LOSS [training: 5.22259774474436 | validation: 5.251849565558936]
	TIME [epoch: 10.3 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.207119770318382		[learning rate: 0.0012444]
	Learning Rate: 0.00124436
	LOSS [training: 5.207119770318382 | validation: 5.22206927427108]
	TIME [epoch: 10.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1530888196403755		[learning rate: 0.0012414]
	Learning Rate: 0.00124135
	LOSS [training: 5.1530888196403755 | validation: 5.1671768464480445]
	TIME [epoch: 10.3 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.19537177221985		[learning rate: 0.0012383]
	Learning Rate: 0.00123835
	LOSS [training: 5.19537177221985 | validation: 5.133457017981032]
	TIME [epoch: 10.3 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163522330388624		[learning rate: 0.0012353]
	Learning Rate: 0.00123535
	LOSS [training: 5.163522330388624 | validation: 5.138581867351721]
	TIME [epoch: 10.3 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163452597115369		[learning rate: 0.0012324]
	Learning Rate: 0.00123236
	LOSS [training: 5.163452597115369 | validation: 5.228487177136343]
	TIME [epoch: 10.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.200847104909478		[learning rate: 0.0012294]
	Learning Rate: 0.00122937
	LOSS [training: 5.200847104909478 | validation: 5.136154876159657]
	TIME [epoch: 10.3 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.058309487309205		[learning rate: 0.0012264]
	Learning Rate: 0.0012264
	LOSS [training: 5.058309487309205 | validation: 5.119524794347586]
	TIME [epoch: 10.3 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.063361504786558		[learning rate: 0.0012234]
	Learning Rate: 0.00122343
	LOSS [training: 5.063361504786558 | validation: 5.162351212729662]
	TIME [epoch: 10.3 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.113468119448312		[learning rate: 0.0012205]
	Learning Rate: 0.00122047
	LOSS [training: 5.113468119448312 | validation: 5.1066698562323]
	TIME [epoch: 10.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.08386691950717		[learning rate: 0.0012175]
	Learning Rate: 0.00121751
	LOSS [training: 5.08386691950717 | validation: 5.129696621940354]
	TIME [epoch: 10.3 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.220579856818787		[learning rate: 0.0012146]
	Learning Rate: 0.00121457
	LOSS [training: 5.220579856818787 | validation: 5.190467172745949]
	TIME [epoch: 10.3 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180698529756163		[learning rate: 0.0012116]
	Learning Rate: 0.00121163
	LOSS [training: 5.180698529756163 | validation: 5.210509826966345]
	TIME [epoch: 10.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1808514561014665		[learning rate: 0.0012087]
	Learning Rate: 0.00120869
	LOSS [training: 5.1808514561014665 | validation: 5.15512900609132]
	TIME [epoch: 10.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.125617344427602		[learning rate: 0.0012058]
	Learning Rate: 0.00120577
	LOSS [training: 5.125617344427602 | validation: 5.145028850512644]
	TIME [epoch: 10.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.117507167093508		[learning rate: 0.0012028]
	Learning Rate: 0.00120285
	LOSS [training: 5.117507167093508 | validation: 5.146348750934311]
	TIME [epoch: 10.3 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.085345859809818		[learning rate: 0.0011999]
	Learning Rate: 0.00119994
	LOSS [training: 5.085345859809818 | validation: 5.2596201897096195]
	TIME [epoch: 10.3 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.105844341666034		[learning rate: 0.001197]
	Learning Rate: 0.00119703
	LOSS [training: 5.105844341666034 | validation: 5.1340661131764005]
	TIME [epoch: 10.3 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.121126919029803		[learning rate: 0.0011941]
	Learning Rate: 0.00119413
	LOSS [training: 5.121126919029803 | validation: 5.120979281523297]
	TIME [epoch: 10.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.092675762046454		[learning rate: 0.0011912]
	Learning Rate: 0.00119124
	LOSS [training: 5.092675762046454 | validation: 5.130833151930993]
	TIME [epoch: 10.3 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.117188321958688		[learning rate: 0.0011884]
	Learning Rate: 0.00118836
	LOSS [training: 5.117188321958688 | validation: 5.153002004799044]
	TIME [epoch: 10.3 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.10513505223999		[learning rate: 0.0011855]
	Learning Rate: 0.00118548
	LOSS [training: 5.10513505223999 | validation: 5.1222778697868225]
	TIME [epoch: 10.3 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.069477604069485		[learning rate: 0.0011826]
	Learning Rate: 0.00118261
	LOSS [training: 5.069477604069485 | validation: 5.155065881531282]
	TIME [epoch: 10.3 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.088818170698035		[learning rate: 0.0011797]
	Learning Rate: 0.00117975
	LOSS [training: 5.088818170698035 | validation: 5.126341716096686]
	TIME [epoch: 10.3 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.140629669942719		[learning rate: 0.0011769]
	Learning Rate: 0.00117689
	LOSS [training: 5.140629669942719 | validation: 5.111531190345254]
	TIME [epoch: 10.3 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0758299427772755		[learning rate: 0.001174]
	Learning Rate: 0.00117404
	LOSS [training: 5.0758299427772755 | validation: 5.217188330868662]
	TIME [epoch: 10.3 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.107979508790356		[learning rate: 0.0011712]
	Learning Rate: 0.0011712
	LOSS [training: 5.107979508790356 | validation: 5.132472115559508]
	TIME [epoch: 10.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.087279690324065		[learning rate: 0.0011684]
	Learning Rate: 0.00116837
	LOSS [training: 5.087279690324065 | validation: 5.210890111665233]
	TIME [epoch: 10.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.110428363164273		[learning rate: 0.0011655]
	Learning Rate: 0.00116554
	LOSS [training: 5.110428363164273 | validation: 5.188186495951499]
	TIME [epoch: 10.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.113677897350953		[learning rate: 0.0011627]
	Learning Rate: 0.00116272
	LOSS [training: 5.113677897350953 | validation: 5.125637281112421]
	TIME [epoch: 10.3 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181265219017372		[learning rate: 0.0011599]
	Learning Rate: 0.0011599
	LOSS [training: 5.181265219017372 | validation: 5.145204558236426]
	TIME [epoch: 10.3 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.140195054042897		[learning rate: 0.0011571]
	Learning Rate: 0.00115709
	LOSS [training: 5.140195054042897 | validation: 5.161000364571504]
	TIME [epoch: 10.3 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1114107981579355		[learning rate: 0.0011543]
	Learning Rate: 0.00115429
	LOSS [training: 5.1114107981579355 | validation: 5.16526733821691]
	TIME [epoch: 10.3 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.110822807947875		[learning rate: 0.0011515]
	Learning Rate: 0.0011515
	LOSS [training: 5.110822807947875 | validation: 5.116568205080259]
	TIME [epoch: 10.3 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.085857494049806		[learning rate: 0.0011487]
	Learning Rate: 0.00114871
	LOSS [training: 5.085857494049806 | validation: 5.208722215799396]
	TIME [epoch: 10.3 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.129979034956612		[learning rate: 0.0011459]
	Learning Rate: 0.00114593
	LOSS [training: 5.129979034956612 | validation: 5.126733337100607]
	TIME [epoch: 10.3 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.093977972506375		[learning rate: 0.0011432]
	Learning Rate: 0.00114316
	LOSS [training: 5.093977972506375 | validation: 5.148024320794425]
	TIME [epoch: 10.3 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.079724205515243		[learning rate: 0.0011404]
	Learning Rate: 0.00114039
	LOSS [training: 5.079724205515243 | validation: 5.290952319003854]
	TIME [epoch: 10.3 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.08848827980212		[learning rate: 0.0011376]
	Learning Rate: 0.00113763
	LOSS [training: 5.08848827980212 | validation: 5.090367687605702]
	TIME [epoch: 10.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.090403102424354		[learning rate: 0.0011349]
	Learning Rate: 0.00113487
	LOSS [training: 5.090403102424354 | validation: 5.0972705831028255]
	TIME [epoch: 10.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.097189343570635		[learning rate: 0.0011321]
	Learning Rate: 0.00113213
	LOSS [training: 5.097189343570635 | validation: 5.111419496885815]
	TIME [epoch: 10.3 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0531898273420754		[learning rate: 0.0011294]
	Learning Rate: 0.00112939
	LOSS [training: 5.0531898273420754 | validation: 5.10424771086125]
	TIME [epoch: 10.3 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.002957082907088		[learning rate: 0.0011267]
	Learning Rate: 0.00112665
	LOSS [training: 5.002957082907088 | validation: 5.111310497465298]
	TIME [epoch: 10.3 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.98282241206494		[learning rate: 0.0011239]
	Learning Rate: 0.00112392
	LOSS [training: 4.98282241206494 | validation: 5.093619192980093]
	TIME [epoch: 10.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.970469533220657		[learning rate: 0.0011212]
	Learning Rate: 0.0011212
	LOSS [training: 4.970469533220657 | validation: 5.129878431428516]
	TIME [epoch: 10.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.001314961305958		[learning rate: 0.0011185]
	Learning Rate: 0.00111849
	LOSS [training: 5.001314961305958 | validation: 5.117761115360373]
	TIME [epoch: 10.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.010288687562804		[learning rate: 0.0011158]
	Learning Rate: 0.00111578
	LOSS [training: 5.010288687562804 | validation: 5.129407762860392]
	TIME [epoch: 10.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.973479693800828		[learning rate: 0.0011131]
	Learning Rate: 0.00111308
	LOSS [training: 4.973479693800828 | validation: 5.083053226494229]
	TIME [epoch: 10.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.962050936799192		[learning rate: 0.0011104]
	Learning Rate: 0.00111039
	LOSS [training: 4.962050936799192 | validation: 5.236161126311275]
	TIME [epoch: 10.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.05957585372273		[learning rate: 0.0011077]
	Learning Rate: 0.0011077
	LOSS [training: 5.05957585372273 | validation: 5.341051325027451]
	TIME [epoch: 10.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.043337688889015		[learning rate: 0.001105]
	Learning Rate: 0.00110502
	LOSS [training: 5.043337688889015 | validation: 5.124515089850917]
	TIME [epoch: 10.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.011532651216042		[learning rate: 0.0011023]
	Learning Rate: 0.00110234
	LOSS [training: 5.011532651216042 | validation: 5.123847423064766]
	TIME [epoch: 10.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.958912743821825		[learning rate: 0.0010997]
	Learning Rate: 0.00109967
	LOSS [training: 4.958912743821825 | validation: 5.061957314725305]
	TIME [epoch: 10.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.968282245346804		[learning rate: 0.001097]
	Learning Rate: 0.00109701
	LOSS [training: 4.968282245346804 | validation: 5.2078457851894955]
	TIME [epoch: 10.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.98369414362271		[learning rate: 0.0010944]
	Learning Rate: 0.00109435
	LOSS [training: 4.98369414362271 | validation: 5.097046891231444]
	TIME [epoch: 10.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.961969754500316		[learning rate: 0.0010917]
	Learning Rate: 0.00109171
	LOSS [training: 4.961969754500316 | validation: 5.180218154606262]
	TIME [epoch: 10.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.023766710950783		[learning rate: 0.0010891]
	Learning Rate: 0.00108906
	LOSS [training: 5.023766710950783 | validation: 5.120773218392096]
	TIME [epoch: 10.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.999366271617687		[learning rate: 0.0010864]
	Learning Rate: 0.00108643
	LOSS [training: 4.999366271617687 | validation: 5.218067124767573]
	TIME [epoch: 10.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.092805184643646		[learning rate: 0.0010838]
	Learning Rate: 0.0010838
	LOSS [training: 5.092805184643646 | validation: 5.173260409172317]
	TIME [epoch: 10.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.994893165234389		[learning rate: 0.0010812]
	Learning Rate: 0.00108117
	LOSS [training: 4.994893165234389 | validation: 5.211271371355231]
	TIME [epoch: 10.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.064195964432479		[learning rate: 0.0010786]
	Learning Rate: 0.00107855
	LOSS [training: 5.064195964432479 | validation: 5.105727344516449]
	TIME [epoch: 10.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9783689454279285		[learning rate: 0.0010759]
	Learning Rate: 0.00107594
	LOSS [training: 4.9783689454279285 | validation: 5.097124127619365]
	TIME [epoch: 10.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9755196160906205		[learning rate: 0.0010733]
	Learning Rate: 0.00107334
	LOSS [training: 4.9755196160906205 | validation: 5.0750596012379265]
	TIME [epoch: 10.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.034766032326591		[learning rate: 0.0010707]
	Learning Rate: 0.00107074
	LOSS [training: 5.034766032326591 | validation: 5.139647493042387]
	TIME [epoch: 10.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.007960203329844		[learning rate: 0.0010681]
	Learning Rate: 0.00106815
	LOSS [training: 5.007960203329844 | validation: 5.2101761411691845]
	TIME [epoch: 10.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.028992002936794		[learning rate: 0.0010656]
	Learning Rate: 0.00106556
	LOSS [training: 5.028992002936794 | validation: 5.115980714229992]
	TIME [epoch: 10.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.009590591493393		[learning rate: 0.001063]
	Learning Rate: 0.00106298
	LOSS [training: 5.009590591493393 | validation: 5.0821260197604]
	TIME [epoch: 10.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.995733199771719		[learning rate: 0.0010604]
	Learning Rate: 0.00106041
	LOSS [training: 4.995733199771719 | validation: 5.106427349928078]
	TIME [epoch: 10.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.125102133478368		[learning rate: 0.0010578]
	Learning Rate: 0.00105784
	LOSS [training: 5.125102133478368 | validation: 5.195934046209917]
	TIME [epoch: 10.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1042319426902365		[learning rate: 0.0010553]
	Learning Rate: 0.00105528
	LOSS [training: 5.1042319426902365 | validation: 5.26771808893304]
	TIME [epoch: 10.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.088395934536889		[learning rate: 0.0010527]
	Learning Rate: 0.00105273
	LOSS [training: 5.088395934536889 | validation: 5.093114010459582]
	TIME [epoch: 10.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.069812850612371		[learning rate: 0.0010502]
	Learning Rate: 0.00105018
	LOSS [training: 5.069812850612371 | validation: 5.151423039485048]
	TIME [epoch: 10.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0769564118438435		[learning rate: 0.0010476]
	Learning Rate: 0.00104764
	LOSS [training: 5.0769564118438435 | validation: 5.169095617849843]
	TIME [epoch: 10.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.060209654142976		[learning rate: 0.0010451]
	Learning Rate: 0.0010451
	LOSS [training: 5.060209654142976 | validation: 5.106904656231155]
	TIME [epoch: 10.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.031994600744708		[learning rate: 0.0010426]
	Learning Rate: 0.00104257
	LOSS [training: 5.031994600744708 | validation: 5.08620164390024]
	TIME [epoch: 10.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.004182660006146		[learning rate: 0.00104]
	Learning Rate: 0.00104005
	LOSS [training: 5.004182660006146 | validation: 5.096042044909392]
	TIME [epoch: 10.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.042089639280515		[learning rate: 0.0010375]
	Learning Rate: 0.00103753
	LOSS [training: 5.042089639280515 | validation: 5.17811180114365]
	TIME [epoch: 10.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.01347648894887		[learning rate: 0.001035]
	Learning Rate: 0.00103502
	LOSS [training: 5.01347648894887 | validation: 5.086102290841814]
	TIME [epoch: 10.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9795261273237115		[learning rate: 0.0010325]
	Learning Rate: 0.00103251
	LOSS [training: 4.9795261273237115 | validation: 5.242447432620178]
	TIME [epoch: 10.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.077359940538756		[learning rate: 0.00103]
	Learning Rate: 0.00103001
	LOSS [training: 5.077359940538756 | validation: 5.191957320524641]
	TIME [epoch: 10.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.030744890146119		[learning rate: 0.0010275]
	Learning Rate: 0.00102752
	LOSS [training: 5.030744890146119 | validation: 5.088721824585341]
	TIME [epoch: 10.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.014749361450443		[learning rate: 0.001025]
	Learning Rate: 0.00102503
	LOSS [training: 5.014749361450443 | validation: 5.095751173240616]
	TIME [epoch: 10.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.988286623286892		[learning rate: 0.0010225]
	Learning Rate: 0.00102255
	LOSS [training: 4.988286623286892 | validation: 5.086081560956273]
	TIME [epoch: 10.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.015157759150724		[learning rate: 0.0010201]
	Learning Rate: 0.00102007
	LOSS [training: 5.015157759150724 | validation: 5.091149569909122]
	TIME [epoch: 10.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.002725871276981		[learning rate: 0.0010176]
	Learning Rate: 0.0010176
	LOSS [training: 5.002725871276981 | validation: 5.092419827457402]
	TIME [epoch: 10.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.997416967300833		[learning rate: 0.0010151]
	Learning Rate: 0.00101514
	LOSS [training: 4.997416967300833 | validation: 5.3392834046775715]
	TIME [epoch: 10.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.06065127602198		[learning rate: 0.0010127]
	Learning Rate: 0.00101268
	LOSS [training: 5.06065127602198 | validation: 5.232725488688418]
	TIME [epoch: 10.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.067240718066431		[learning rate: 0.0010102]
	Learning Rate: 0.00101023
	LOSS [training: 5.067240718066431 | validation: 5.153377270092656]
	TIME [epoch: 10.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9555652825805625		[learning rate: 0.0010078]
	Learning Rate: 0.00100779
	LOSS [training: 4.9555652825805625 | validation: 5.12610048243142]
	TIME [epoch: 10.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.986342015919635		[learning rate: 0.0010053]
	Learning Rate: 0.00100535
	LOSS [training: 4.986342015919635 | validation: 5.134730566438011]
	TIME [epoch: 10.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.971092082150115		[learning rate: 0.0010029]
	Learning Rate: 0.00100291
	LOSS [training: 4.971092082150115 | validation: 5.100848635970595]
	TIME [epoch: 10.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0020991715624605		[learning rate: 0.0010005]
	Learning Rate: 0.00100048
	LOSS [training: 5.0020991715624605 | validation: 5.1811431149780365]
	TIME [epoch: 10.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.982209019942113		[learning rate: 0.00099806]
	Learning Rate: 0.000998063
	LOSS [training: 4.982209019942113 | validation: 5.150112753274422]
	TIME [epoch: 10.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.046886023851726		[learning rate: 0.00099565]
	Learning Rate: 0.000995647
	LOSS [training: 5.046886023851726 | validation: 5.394455905765288]
	TIME [epoch: 10.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165484474471157		[learning rate: 0.00099324]
	Learning Rate: 0.000993237
	LOSS [training: 5.165484474471157 | validation: 5.30569488089777]
	TIME [epoch: 10.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0335818470155		[learning rate: 0.00099083]
	Learning Rate: 0.000990832
	LOSS [training: 5.0335818470155 | validation: 5.14775301901232]
	TIME [epoch: 10.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.959492027294745		[learning rate: 0.00098843]
	Learning Rate: 0.000988433
	LOSS [training: 4.959492027294745 | validation: 5.2034074002968795]
	TIME [epoch: 10.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.944361698250339		[learning rate: 0.00098604]
	Learning Rate: 0.00098604
	LOSS [training: 4.944361698250339 | validation: 5.1140100796925445]
	TIME [epoch: 10.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.011860799461502		[learning rate: 0.00098365]
	Learning Rate: 0.000983653
	LOSS [training: 5.011860799461502 | validation: 5.1932192356796145]
	TIME [epoch: 10.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.021439741246081		[learning rate: 0.00098127]
	Learning Rate: 0.000981272
	LOSS [training: 5.021439741246081 | validation: 5.108332298600868]
	TIME [epoch: 10.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.92522200798443		[learning rate: 0.0009789]
	Learning Rate: 0.000978897
	LOSS [training: 4.92522200798443 | validation: 5.096661455973403]
	TIME [epoch: 10.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.950514937386085		[learning rate: 0.00097653]
	Learning Rate: 0.000976527
	LOSS [training: 4.950514937386085 | validation: 5.156091672729799]
	TIME [epoch: 10.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.97474348487848		[learning rate: 0.00097416]
	Learning Rate: 0.000974163
	LOSS [training: 4.97474348487848 | validation: 5.150143394420594]
	TIME [epoch: 10.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.925734487162232		[learning rate: 0.0009718]
	Learning Rate: 0.000971805
	LOSS [training: 4.925734487162232 | validation: 5.066427767721225]
	TIME [epoch: 10.3 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8939921572281175		[learning rate: 0.00096945]
	Learning Rate: 0.000969452
	LOSS [training: 4.8939921572281175 | validation: 5.075212103349177]
	TIME [epoch: 10.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.015217893016993		[learning rate: 0.00096711]
	Learning Rate: 0.000967105
	LOSS [training: 5.015217893016993 | validation: 5.173771376966991]
	TIME [epoch: 10.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1458806591867745		[learning rate: 0.00096476]
	Learning Rate: 0.000964764
	LOSS [training: 5.1458806591867745 | validation: 5.419568199060076]
	TIME [epoch: 10.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.245167770473469		[learning rate: 0.00096243]
	Learning Rate: 0.000962428
	LOSS [training: 5.245167770473469 | validation: 5.251838083189282]
	TIME [epoch: 10.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.035695728154433		[learning rate: 0.0009601]
	Learning Rate: 0.000960098
	LOSS [training: 5.035695728154433 | validation: 5.1764046004652275]
	TIME [epoch: 10.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.049571055027673		[learning rate: 0.00095777]
	Learning Rate: 0.000957774
	LOSS [training: 5.049571055027673 | validation: 5.2613907599103085]
	TIME [epoch: 10.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.054372715502027		[learning rate: 0.00095546]
	Learning Rate: 0.000955456
	LOSS [training: 5.054372715502027 | validation: 5.172781667770497]
	TIME [epoch: 10.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.026566159566745		[learning rate: 0.00095314]
	Learning Rate: 0.000953143
	LOSS [training: 5.026566159566745 | validation: 5.190469196884326]
	TIME [epoch: 10.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.023753359505401		[learning rate: 0.00095084]
	Learning Rate: 0.000950835
	LOSS [training: 5.023753359505401 | validation: 5.142469529084256]
	TIME [epoch: 10.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175840695943172		[learning rate: 0.00094853]
	Learning Rate: 0.000948533
	LOSS [training: 5.175840695943172 | validation: 5.375365555676941]
	TIME [epoch: 10.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.228744050164881		[learning rate: 0.00094624]
	Learning Rate: 0.000946237
	LOSS [training: 5.228744050164881 | validation: 5.3058012748373855]
	TIME [epoch: 10.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.102753337887382		[learning rate: 0.00094395]
	Learning Rate: 0.000943946
	LOSS [training: 5.102753337887382 | validation: 5.258895952265427]
	TIME [epoch: 10.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0955371458381915		[learning rate: 0.00094166]
	Learning Rate: 0.000941661
	LOSS [training: 5.0955371458381915 | validation: 5.367297099156559]
	TIME [epoch: 10.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.296798784291102		[learning rate: 0.00093938]
	Learning Rate: 0.000939382
	LOSS [training: 5.296798784291102 | validation: 5.523485210311366]
	TIME [epoch: 10.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.390552144360338		[learning rate: 0.00093711]
	Learning Rate: 0.000937108
	LOSS [training: 5.390552144360338 | validation: 5.5901573438107945]
	TIME [epoch: 10.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.496861740505992		[learning rate: 0.00093484]
	Learning Rate: 0.000934839
	LOSS [training: 5.496861740505992 | validation: 5.592020685405275]
	TIME [epoch: 10.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4098367992924254		[learning rate: 0.00093258]
	Learning Rate: 0.000932576
	LOSS [training: 5.4098367992924254 | validation: 5.47303380487663]
	TIME [epoch: 10.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.425119937477876		[learning rate: 0.00093032]
	Learning Rate: 0.000930318
	LOSS [training: 5.425119937477876 | validation: 5.503200792587944]
	TIME [epoch: 10.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4979195427710055		[learning rate: 0.00092807]
	Learning Rate: 0.000928066
	LOSS [training: 5.4979195427710055 | validation: 5.5287650847589065]
	TIME [epoch: 10.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.477517552894669		[learning rate: 0.00092582]
	Learning Rate: 0.00092582
	LOSS [training: 5.477517552894669 | validation: 5.54427876398632]
	TIME [epoch: 10.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.465213372986039		[learning rate: 0.00092358]
	Learning Rate: 0.000923578
	LOSS [training: 5.465213372986039 | validation: 5.492421999200858]
	TIME [epoch: 10.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.450007704854052		[learning rate: 0.00092134]
	Learning Rate: 0.000921342
	LOSS [training: 5.450007704854052 | validation: 5.485991922098617]
	TIME [epoch: 10.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4414668583824355		[learning rate: 0.00091911]
	Learning Rate: 0.000919112
	LOSS [training: 5.4414668583824355 | validation: 5.485502320643636]
	TIME [epoch: 10.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.467097259488538		[learning rate: 0.00091689]
	Learning Rate: 0.000916887
	LOSS [training: 5.467097259488538 | validation: 5.5284607475895085]
	TIME [epoch: 10.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.452010591334268		[learning rate: 0.00091467]
	Learning Rate: 0.000914667
	LOSS [training: 5.452010591334268 | validation: 5.495404329437365]
	TIME [epoch: 10.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5055804417378456		[learning rate: 0.00091245]
	Learning Rate: 0.000912453
	LOSS [training: 5.5055804417378456 | validation: 5.4925616529810055]
	TIME [epoch: 10.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.448426674214511		[learning rate: 0.00091024]
	Learning Rate: 0.000910244
	LOSS [training: 5.448426674214511 | validation: 5.501563278864646]
	TIME [epoch: 10.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.459380755252705		[learning rate: 0.00090804]
	Learning Rate: 0.000908041
	LOSS [training: 5.459380755252705 | validation: 5.496546539663002]
	TIME [epoch: 10.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.411157243255497		[learning rate: 0.00090584]
	Learning Rate: 0.000905843
	LOSS [training: 5.411157243255497 | validation: 5.462129744000618]
	TIME [epoch: 10.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.409363542125355		[learning rate: 0.00090365]
	Learning Rate: 0.000903649
	LOSS [training: 5.409363542125355 | validation: 5.480124795003004]
	TIME [epoch: 10.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4565428512576375		[learning rate: 0.00090146]
	Learning Rate: 0.000901462
	LOSS [training: 5.4565428512576375 | validation: 5.548193592373361]
	TIME [epoch: 10.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.469620211546126		[learning rate: 0.00089928]
	Learning Rate: 0.00089928
	LOSS [training: 5.469620211546126 | validation: 5.4496345670779265]
	TIME [epoch: 10.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.358425779062422		[learning rate: 0.0008971]
	Learning Rate: 0.000897103
	LOSS [training: 5.358425779062422 | validation: 5.416047182196105]
	TIME [epoch: 10.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.333575288381171		[learning rate: 0.00089493]
	Learning Rate: 0.000894931
	LOSS [training: 5.333575288381171 | validation: 5.423627410675959]
	TIME [epoch: 10.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234158909973436		[learning rate: 0.00089276]
	Learning Rate: 0.000892764
	LOSS [training: 5.234158909973436 | validation: 5.305534121416822]
	TIME [epoch: 10.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18921992659026		[learning rate: 0.0008906]
	Learning Rate: 0.000890603
	LOSS [training: 5.18921992659026 | validation: 5.333282245789434]
	TIME [epoch: 10.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.146631932544826		[learning rate: 0.00088845]
	Learning Rate: 0.000888447
	LOSS [training: 5.146631932544826 | validation: 5.289747050818053]
	TIME [epoch: 10.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.138989163451919		[learning rate: 0.0008863]
	Learning Rate: 0.000886296
	LOSS [training: 5.138989163451919 | validation: 5.330709592128165]
	TIME [epoch: 10.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160902874571614		[learning rate: 0.00088415]
	Learning Rate: 0.000884151
	LOSS [training: 5.160902874571614 | validation: 5.371733029954705]
	TIME [epoch: 10.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.215514435478113		[learning rate: 0.00088201]
	Learning Rate: 0.00088201
	LOSS [training: 5.215514435478113 | validation: 5.405510908760156]
	TIME [epoch: 10.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.299631971983514		[learning rate: 0.00087988]
	Learning Rate: 0.000879875
	LOSS [training: 5.299631971983514 | validation: 5.348414936224964]
	TIME [epoch: 10.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.264352342226212		[learning rate: 0.00087775]
	Learning Rate: 0.000877745
	LOSS [training: 5.264352342226212 | validation: 5.379209319023992]
	TIME [epoch: 10.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.337428947955056		[learning rate: 0.00087562]
	Learning Rate: 0.00087562
	LOSS [training: 5.337428947955056 | validation: 5.433993504965793]
	TIME [epoch: 10.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.449873237351303		[learning rate: 0.0008735]
	Learning Rate: 0.000873501
	LOSS [training: 5.449873237351303 | validation: 5.527465458877386]
	TIME [epoch: 10.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.476821491131574		[learning rate: 0.00087139]
	Learning Rate: 0.000871386
	LOSS [training: 5.476821491131574 | validation: 5.5814283771435935]
	TIME [epoch: 10.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4661136820273475		[learning rate: 0.00086928]
	Learning Rate: 0.000869277
	LOSS [training: 5.4661136820273475 | validation: 5.655727374234084]
	TIME [epoch: 10.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.60970523069745		[learning rate: 0.00086717]
	Learning Rate: 0.000867172
	LOSS [training: 5.60970523069745 | validation: 5.684574140596075]
	TIME [epoch: 10.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.600871736369979		[learning rate: 0.00086507]
	Learning Rate: 0.000865073
	LOSS [training: 5.600871736369979 | validation: 5.540196361979149]
	TIME [epoch: 10.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.551603973012148		[learning rate: 0.00086298]
	Learning Rate: 0.000862979
	LOSS [training: 5.551603973012148 | validation: 5.4630291053002615]
	TIME [epoch: 10.3 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.563667469020972		[learning rate: 0.00086089]
	Learning Rate: 0.000860889
	LOSS [training: 5.563667469020972 | validation: 5.536791378874293]
	TIME [epoch: 10.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.590112003566473		[learning rate: 0.00085881]
	Learning Rate: 0.000858805
	LOSS [training: 5.590112003566473 | validation: 5.48567761375967]
	TIME [epoch: 10.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.62708333037813		[learning rate: 0.00085673]
	Learning Rate: 0.000856726
	LOSS [training: 5.62708333037813 | validation: 5.5516649400177265]
	TIME [epoch: 10.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.557494096830163		[learning rate: 0.00085465]
	Learning Rate: 0.000854652
	LOSS [training: 5.557494096830163 | validation: 5.570942967014287]
	TIME [epoch: 10.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.582540783503038		[learning rate: 0.00085258]
	Learning Rate: 0.000852583
	LOSS [training: 5.582540783503038 | validation: 5.521728794234994]
	TIME [epoch: 10.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.616241291762911		[learning rate: 0.00085052]
	Learning Rate: 0.000850519
	LOSS [training: 5.616241291762911 | validation: 5.533780261306655]
	TIME [epoch: 10.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.57762271048023		[learning rate: 0.00084846]
	Learning Rate: 0.00084846
	LOSS [training: 5.57762271048023 | validation: 5.512527239499134]
	TIME [epoch: 10.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.567433530168292		[learning rate: 0.00084641]
	Learning Rate: 0.000846406
	LOSS [training: 5.567433530168292 | validation: 5.503559179106441]
	TIME [epoch: 10.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.548697534593866		[learning rate: 0.00084436]
	Learning Rate: 0.000844357
	LOSS [training: 5.548697534593866 | validation: 5.511710491882775]
	TIME [epoch: 10.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.622018722989582		[learning rate: 0.00084231]
	Learning Rate: 0.000842313
	LOSS [training: 5.622018722989582 | validation: 5.572802595500347]
	TIME [epoch: 10.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.634288980591353		[learning rate: 0.00084027]
	Learning Rate: 0.000840274
	LOSS [training: 5.634288980591353 | validation: 5.485438965605215]
	TIME [epoch: 10.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.579431491131148		[learning rate: 0.00083824]
	Learning Rate: 0.00083824
	LOSS [training: 5.579431491131148 | validation: 5.485588064364732]
	TIME [epoch: 10.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.486419150905778		[learning rate: 0.00083621]
	Learning Rate: 0.000836211
	LOSS [training: 5.486419150905778 | validation: 5.474246142806654]
	TIME [epoch: 10.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.442583149120849		[learning rate: 0.00083419]
	Learning Rate: 0.000834187
	LOSS [training: 5.442583149120849 | validation: 5.456870646801205]
	TIME [epoch: 10.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.387507379908465		[learning rate: 0.00083217]
	Learning Rate: 0.000832167
	LOSS [training: 5.387507379908465 | validation: 5.43102156205011]
	TIME [epoch: 10.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.35714277552383		[learning rate: 0.00083015]
	Learning Rate: 0.000830152
	LOSS [training: 5.35714277552383 | validation: 5.411047774448064]
	TIME [epoch: 10.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.33813063284788		[learning rate: 0.00082814]
	Learning Rate: 0.000828143
	LOSS [training: 5.33813063284788 | validation: 5.3536535874770115]
	TIME [epoch: 10.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.328381265789423		[learning rate: 0.00082614]
	Learning Rate: 0.000826138
	LOSS [training: 5.328381265789423 | validation: 5.5049172097279255]
	TIME [epoch: 10.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.428429702272231		[learning rate: 0.00082414]
	Learning Rate: 0.000824138
	LOSS [training: 5.428429702272231 | validation: 5.46058754528058]
	TIME [epoch: 10.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4587326617017515		[learning rate: 0.00082214]
	Learning Rate: 0.000822143
	LOSS [training: 5.4587326617017515 | validation: 5.590579225561348]
	TIME [epoch: 10.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.547690144368491		[learning rate: 0.00082015]
	Learning Rate: 0.000820153
	LOSS [training: 5.547690144368491 | validation: 5.552492820103482]
	TIME [epoch: 10.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.551520939433405		[learning rate: 0.00081817]
	Learning Rate: 0.000818167
	LOSS [training: 5.551520939433405 | validation: 5.467861878422002]
	TIME [epoch: 10.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.512410675903693		[learning rate: 0.00081619]
	Learning Rate: 0.000816187
	LOSS [training: 5.512410675903693 | validation: 5.517739875331037]
	TIME [epoch: 10.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5572900748184235		[learning rate: 0.00081421]
	Learning Rate: 0.000814211
	LOSS [training: 5.5572900748184235 | validation: 5.496290568479774]
	TIME [epoch: 10.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.552732850820126		[learning rate: 0.00081224]
	Learning Rate: 0.00081224
	LOSS [training: 5.552732850820126 | validation: 5.500318392347849]
	TIME [epoch: 10.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.506370131310918		[learning rate: 0.00081027]
	Learning Rate: 0.000810273
	LOSS [training: 5.506370131310918 | validation: 5.490036653046477]
	TIME [epoch: 10.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.588422835296944		[learning rate: 0.00080831]
	Learning Rate: 0.000808312
	LOSS [training: 5.588422835296944 | validation: 5.480523666599004]
	TIME [epoch: 10.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5568086801973635		[learning rate: 0.00080636]
	Learning Rate: 0.000806355
	LOSS [training: 5.5568086801973635 | validation: 5.5884675350508966]
	TIME [epoch: 10.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.598905412535772		[learning rate: 0.0008044]
	Learning Rate: 0.000804403
	LOSS [training: 5.598905412535772 | validation: 5.580960310040946]
	TIME [epoch: 10.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.587221596652915		[learning rate: 0.00080246]
	Learning Rate: 0.000802456
	LOSS [training: 5.587221596652915 | validation: 5.498275729278443]
	TIME [epoch: 10.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.528202990455664		[learning rate: 0.00080051]
	Learning Rate: 0.000800513
	LOSS [training: 5.528202990455664 | validation: 5.501397007850616]
	TIME [epoch: 10.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.559513881363882		[learning rate: 0.00079858]
	Learning Rate: 0.000798575
	LOSS [training: 5.559513881363882 | validation: 5.46686478344012]
	TIME [epoch: 10.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.522469104653598		[learning rate: 0.00079664]
	Learning Rate: 0.000796642
	LOSS [training: 5.522469104653598 | validation: 5.545828151735118]
	TIME [epoch: 10.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.630012285367577		[learning rate: 0.00079471]
	Learning Rate: 0.000794713
	LOSS [training: 5.630012285367577 | validation: 5.438541848469672]
	TIME [epoch: 10.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.503275097659818		[learning rate: 0.00079279]
	Learning Rate: 0.00079279
	LOSS [training: 5.503275097659818 | validation: 5.4448094573475565]
	TIME [epoch: 10.3 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5087650246278645		[learning rate: 0.00079087]
	Learning Rate: 0.00079087
	LOSS [training: 5.5087650246278645 | validation: 5.433032742819989]
	TIME [epoch: 10.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.488586411523701		[learning rate: 0.00078896]
	Learning Rate: 0.000788956
	LOSS [training: 5.488586411523701 | validation: 5.447074118837976]
	TIME [epoch: 10.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.586877878892978		[learning rate: 0.00078705]
	Learning Rate: 0.000787046
	LOSS [training: 5.586877878892978 | validation: 5.473882513933809]
	TIME [epoch: 10.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.546975825362456		[learning rate: 0.00078514]
	Learning Rate: 0.000785141
	LOSS [training: 5.546975825362456 | validation: 5.427270324871918]
	TIME [epoch: 10.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.441404031737859		[learning rate: 0.00078324]
	Learning Rate: 0.00078324
	LOSS [training: 5.441404031737859 | validation: 5.429823769236746]
	TIME [epoch: 10.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.446369438816377		[learning rate: 0.00078134]
	Learning Rate: 0.000781344
	LOSS [training: 5.446369438816377 | validation: 5.455370787740289]
	TIME [epoch: 10.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.587892327525376		[learning rate: 0.00077945]
	Learning Rate: 0.000779452
	LOSS [training: 5.587892327525376 | validation: 5.5169487825390915]
	TIME [epoch: 10.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.609501250858488		[learning rate: 0.00077757]
	Learning Rate: 0.000777565
	LOSS [training: 5.609501250858488 | validation: 5.454848260754879]
	TIME [epoch: 10.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.632033416443013		[learning rate: 0.00077568]
	Learning Rate: 0.000775683
	LOSS [training: 5.632033416443013 | validation: 5.547680455394977]
	TIME [epoch: 10.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5965041190374745		[learning rate: 0.00077381]
	Learning Rate: 0.000773805
	LOSS [training: 5.5965041190374745 | validation: 5.508867248784927]
	TIME [epoch: 10.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.544400653293743		[learning rate: 0.00077193]
	Learning Rate: 0.000771932
	LOSS [training: 5.544400653293743 | validation: 5.480775633192398]
	TIME [epoch: 10.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.611821146268651		[learning rate: 0.00077006]
	Learning Rate: 0.000770063
	LOSS [training: 5.611821146268651 | validation: 5.436404227869986]
	TIME [epoch: 10.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.582863903053495		[learning rate: 0.0007682]
	Learning Rate: 0.000768199
	LOSS [training: 5.582863903053495 | validation: 5.453215539968356]
	TIME [epoch: 10.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.593186570359061		[learning rate: 0.00076634]
	Learning Rate: 0.000766339
	LOSS [training: 5.593186570359061 | validation: 5.45704411652998]
	TIME [epoch: 10.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.594370068858695		[learning rate: 0.00076448]
	Learning Rate: 0.000764484
	LOSS [training: 5.594370068858695 | validation: 5.477519031919046]
	TIME [epoch: 10.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.552591354462622		[learning rate: 0.00076263]
	Learning Rate: 0.000762633
	LOSS [training: 5.552591354462622 | validation: 5.440029297683295]
	TIME [epoch: 10.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.629944439474855		[learning rate: 0.00076079]
	Learning Rate: 0.000760787
	LOSS [training: 5.629944439474855 | validation: 5.482095929526113]
	TIME [epoch: 10.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.670999103625379		[learning rate: 0.00075895]
	Learning Rate: 0.000758945
	LOSS [training: 5.670999103625379 | validation: 5.491577327752305]
	TIME [epoch: 10.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.602884881681592		[learning rate: 0.00075711]
	Learning Rate: 0.000757108
	LOSS [training: 5.602884881681592 | validation: 5.45882652907982]
	TIME [epoch: 10.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.638399637916711		[learning rate: 0.00075528]
	Learning Rate: 0.000755275
	LOSS [training: 5.638399637916711 | validation: 5.480119786985243]
	TIME [epoch: 10.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.658964342185889		[learning rate: 0.00075345]
	Learning Rate: 0.000753447
	LOSS [training: 5.658964342185889 | validation: 5.453298468569297]
	TIME [epoch: 10.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.604559538615921		[learning rate: 0.00075162]
	Learning Rate: 0.000751623
	LOSS [training: 5.604559538615921 | validation: 5.476080970464486]
	TIME [epoch: 10.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.610288298615172		[learning rate: 0.0007498]
	Learning Rate: 0.000749803
	LOSS [training: 5.610288298615172 | validation: 5.484754887973415]
	TIME [epoch: 10.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.569626907771843		[learning rate: 0.00074799]
	Learning Rate: 0.000747988
	LOSS [training: 5.569626907771843 | validation: 5.442456372363471]
	TIME [epoch: 10.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.605014230290686		[learning rate: 0.00074618]
	Learning Rate: 0.000746177
	LOSS [training: 5.605014230290686 | validation: 5.423084631219276]
	TIME [epoch: 10.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.596369779164464		[learning rate: 0.00074437]
	Learning Rate: 0.000744371
	LOSS [training: 5.596369779164464 | validation: 5.448382545915611]
	TIME [epoch: 10.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.605262481467255		[learning rate: 0.00074257]
	Learning Rate: 0.000742569
	LOSS [training: 5.605262481467255 | validation: 5.43603810215483]
	TIME [epoch: 10.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6078111141112625		[learning rate: 0.00074077]
	Learning Rate: 0.000740771
	LOSS [training: 5.6078111141112625 | validation: 5.459339355535258]
	TIME [epoch: 10.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6540497513621455		[learning rate: 0.00073898]
	Learning Rate: 0.000738978
	LOSS [training: 5.6540497513621455 | validation: 5.501464758064521]
	TIME [epoch: 10.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.581958590695206		[learning rate: 0.00073719]
	Learning Rate: 0.000737189
	LOSS [training: 5.581958590695206 | validation: 5.437078407484221]
	TIME [epoch: 10.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.597821132891262		[learning rate: 0.0007354]
	Learning Rate: 0.000735405
	LOSS [training: 5.597821132891262 | validation: 5.448861967956724]
	TIME [epoch: 10.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.455168279220301		[learning rate: 0.00073362]
	Learning Rate: 0.000733624
	LOSS [training: 5.455168279220301 | validation: 5.412341847430087]
	TIME [epoch: 10.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.508034001806755		[learning rate: 0.00073185]
	Learning Rate: 0.000731848
	LOSS [training: 5.508034001806755 | validation: 5.427486950720832]
	TIME [epoch: 10.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.472317955417878		[learning rate: 0.00073008]
	Learning Rate: 0.000730077
	LOSS [training: 5.472317955417878 | validation: 5.607378201534855]
	TIME [epoch: 10.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.441263628583221		[learning rate: 0.00072831]
	Learning Rate: 0.000728309
	LOSS [training: 5.441263628583221 | validation: 5.427784889436218]
	TIME [epoch: 10.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.502044419718237		[learning rate: 0.00072655]
	Learning Rate: 0.000726546
	LOSS [training: 5.502044419718237 | validation: 5.397103398908]
	TIME [epoch: 10.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.572857697517601		[learning rate: 0.00072479]
	Learning Rate: 0.000724787
	LOSS [training: 5.572857697517601 | validation: 5.4859293957593405]
	TIME [epoch: 10.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.599616958652607		[learning rate: 0.00072303]
	Learning Rate: 0.000723033
	LOSS [training: 5.599616958652607 | validation: 5.440199035480293]
	TIME [epoch: 10.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.521058426095931		[learning rate: 0.00072128]
	Learning Rate: 0.000721282
	LOSS [training: 5.521058426095931 | validation: 5.449793035172499]
	TIME [epoch: 10.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.574880581787349		[learning rate: 0.00071954]
	Learning Rate: 0.000719536
	LOSS [training: 5.574880581787349 | validation: 5.4208199764632985]
	TIME [epoch: 10.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.522763133134236		[learning rate: 0.00071779]
	Learning Rate: 0.000717794
	LOSS [training: 5.522763133134236 | validation: 5.441434886534796]
	TIME [epoch: 10.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.553704268413343		[learning rate: 0.00071606]
	Learning Rate: 0.000716057
	LOSS [training: 5.553704268413343 | validation: 5.4256452054520174]
	TIME [epoch: 10.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.57222426253289		[learning rate: 0.00071432]
	Learning Rate: 0.000714323
	LOSS [training: 5.57222426253289 | validation: 5.477301779686876]
	TIME [epoch: 10.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.581375132657891		[learning rate: 0.00071259]
	Learning Rate: 0.000712594
	LOSS [training: 5.581375132657891 | validation: 5.435552006830426]
	TIME [epoch: 10.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.510257886398615		[learning rate: 0.00071087]
	Learning Rate: 0.000710869
	LOSS [training: 5.510257886398615 | validation: 5.438780321125157]
	TIME [epoch: 10.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.531415658290538		[learning rate: 0.00070915]
	Learning Rate: 0.000709148
	LOSS [training: 5.531415658290538 | validation: 5.340209856540469]
	TIME [epoch: 10.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.512575380841013		[learning rate: 0.00070743]
	Learning Rate: 0.000707431
	LOSS [training: 5.512575380841013 | validation: 5.420293957095291]
	TIME [epoch: 10.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.548063288979277		[learning rate: 0.00070572]
	Learning Rate: 0.000705719
	LOSS [training: 5.548063288979277 | validation: 5.47599876806398]
	TIME [epoch: 10.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.528785255729805		[learning rate: 0.00070401]
	Learning Rate: 0.00070401
	LOSS [training: 5.528785255729805 | validation: 5.414677314410344]
	TIME [epoch: 10.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.520774719634121		[learning rate: 0.00070231]
	Learning Rate: 0.000702306
	LOSS [training: 5.520774719634121 | validation: 5.430729078996531]
	TIME [epoch: 10.3 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.468989964669191		[learning rate: 0.00070061]
	Learning Rate: 0.000700606
	LOSS [training: 5.468989964669191 | validation: 5.438780277007193]
	TIME [epoch: 10.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.464204387315618		[learning rate: 0.00069891]
	Learning Rate: 0.00069891
	LOSS [training: 5.464204387315618 | validation: 5.41897404780676]
	TIME [epoch: 10.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.460104423498061		[learning rate: 0.00069722]
	Learning Rate: 0.000697218
	LOSS [training: 5.460104423498061 | validation: 5.450583528677643]
	TIME [epoch: 10.3 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.50565209278025		[learning rate: 0.00069553]
	Learning Rate: 0.00069553
	LOSS [training: 5.50565209278025 | validation: 5.429713624674017]
	TIME [epoch: 10.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.446638922914997		[learning rate: 0.00069385]
	Learning Rate: 0.000693846
	LOSS [training: 5.446638922914997 | validation: 5.4292168947876505]
	TIME [epoch: 10.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.477600932928707		[learning rate: 0.00069217]
	Learning Rate: 0.000692166
	LOSS [training: 5.477600932928707 | validation: 5.431292252416973]
	TIME [epoch: 10.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4426178182334715		[learning rate: 0.00069049]
	Learning Rate: 0.000690491
	LOSS [training: 5.4426178182334715 | validation: 5.404602883454685]
	TIME [epoch: 10.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4939716372048455		[learning rate: 0.00068882]
	Learning Rate: 0.000688819
	LOSS [training: 5.4939716372048455 | validation: 5.406522883862217]
	TIME [epoch: 10.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4534946966897655		[learning rate: 0.00068715]
	Learning Rate: 0.000687152
	LOSS [training: 5.4534946966897655 | validation: 5.409644447607125]
	TIME [epoch: 10.3 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.414316542803983		[learning rate: 0.00068549]
	Learning Rate: 0.000685488
	LOSS [training: 5.414316542803983 | validation: 5.419089475667961]
	TIME [epoch: 10.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.433144403799085		[learning rate: 0.00068383]
	Learning Rate: 0.000683829
	LOSS [training: 5.433144403799085 | validation: 5.349502054513569]
	TIME [epoch: 10.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4456836682707905		[learning rate: 0.00068217]
	Learning Rate: 0.000682173
	LOSS [training: 5.4456836682707905 | validation: 5.392844375563884]
	TIME [epoch: 10.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.377467946013821		[learning rate: 0.00068052]
	Learning Rate: 0.000680522
	LOSS [training: 5.377467946013821 | validation: 5.429830561940835]
	TIME [epoch: 10.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.426942343844312		[learning rate: 0.00067887]
	Learning Rate: 0.000678874
	LOSS [training: 5.426942343844312 | validation: 5.421990708170419]
	TIME [epoch: 10.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3617300282759475		[learning rate: 0.00067723]
	Learning Rate: 0.000677231
	LOSS [training: 5.3617300282759475 | validation: 5.329674828416544]
	TIME [epoch: 10.3 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.428025800881976		[learning rate: 0.00067559]
	Learning Rate: 0.000675592
	LOSS [training: 5.428025800881976 | validation: 5.3598241787700704]
	TIME [epoch: 10.3 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.378054309401383		[learning rate: 0.00067396]
	Learning Rate: 0.000673956
	LOSS [training: 5.378054309401383 | validation: 5.397137052028943]
	TIME [epoch: 10.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.383250166535227		[learning rate: 0.00067232]
	Learning Rate: 0.000672325
	LOSS [training: 5.383250166535227 | validation: 5.4407723613625825]
	TIME [epoch: 10.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4100759737021376		[learning rate: 0.0006707]
	Learning Rate: 0.000670697
	LOSS [training: 5.4100759737021376 | validation: 5.3397569222366865]
	TIME [epoch: 10.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.344378275836787		[learning rate: 0.00066907]
	Learning Rate: 0.000669073
	LOSS [training: 5.344378275836787 | validation: 5.396847247488859]
	TIME [epoch: 10.3 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.394497510155704		[learning rate: 0.00066745]
	Learning Rate: 0.000667454
	LOSS [training: 5.394497510155704 | validation: 5.439697033114075]
	TIME [epoch: 10.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.386100793600404		[learning rate: 0.00066584]
	Learning Rate: 0.000665838
	LOSS [training: 5.386100793600404 | validation: 5.408050091489811]
	TIME [epoch: 10.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.441360256287333		[learning rate: 0.00066423]
	Learning Rate: 0.000664226
	LOSS [training: 5.441360256287333 | validation: 5.4145046769598695]
	TIME [epoch: 10.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4729736013845285		[learning rate: 0.00066262]
	Learning Rate: 0.000662618
	LOSS [training: 5.4729736013845285 | validation: 5.424703668317197]
	TIME [epoch: 10.3 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.445742596680315		[learning rate: 0.00066101]
	Learning Rate: 0.000661014
	LOSS [training: 5.445742596680315 | validation: 5.429735098240392]
	TIME [epoch: 10.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.419847187568122		[learning rate: 0.00065941]
	Learning Rate: 0.000659414
	LOSS [training: 5.419847187568122 | validation: 5.38094500014131]
	TIME [epoch: 10.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.45397002770093		[learning rate: 0.00065782]
	Learning Rate: 0.000657817
	LOSS [training: 5.45397002770093 | validation: 5.39660408578215]
	TIME [epoch: 10.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.472187429305447		[learning rate: 0.00065622]
	Learning Rate: 0.000656225
	LOSS [training: 5.472187429305447 | validation: 5.369974089748679]
	TIME [epoch: 10.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.518969424008209		[learning rate: 0.00065464]
	Learning Rate: 0.000654636
	LOSS [training: 5.518969424008209 | validation: 5.396252116030257]
	TIME [epoch: 10.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.46350537630863		[learning rate: 0.00065305]
	Learning Rate: 0.000653051
	LOSS [training: 5.46350537630863 | validation: 5.439457730683871]
	TIME [epoch: 10.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.459035881203681		[learning rate: 0.00065147]
	Learning Rate: 0.000651471
	LOSS [training: 5.459035881203681 | validation: 5.437524005620385]
	TIME [epoch: 10.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.454116596016102		[learning rate: 0.00064989]
	Learning Rate: 0.000649893
	LOSS [training: 5.454116596016102 | validation: 5.443726780835332]
	TIME [epoch: 10.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.43642482115483		[learning rate: 0.00064832]
	Learning Rate: 0.00064832
	LOSS [training: 5.43642482115483 | validation: 5.495338057090561]
	TIME [epoch: 10.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.499672146742671		[learning rate: 0.00064675]
	Learning Rate: 0.000646751
	LOSS [training: 5.499672146742671 | validation: 5.407396474748528]
	TIME [epoch: 10.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.468852401216737		[learning rate: 0.00064519]
	Learning Rate: 0.000645185
	LOSS [training: 5.468852401216737 | validation: 5.422559842059566]
	TIME [epoch: 10.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4489051843684955		[learning rate: 0.00064362]
	Learning Rate: 0.000643623
	LOSS [training: 5.4489051843684955 | validation: 5.416440364299395]
	TIME [epoch: 10.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.433717127292157		[learning rate: 0.00064206]
	Learning Rate: 0.000642065
	LOSS [training: 5.433717127292157 | validation: 5.413077335802956]
	TIME [epoch: 10.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.452553935557232		[learning rate: 0.00064051]
	Learning Rate: 0.000640511
	LOSS [training: 5.452553935557232 | validation: 5.409267199289149]
	TIME [epoch: 10.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.502346617384736		[learning rate: 0.00063896]
	Learning Rate: 0.00063896
	LOSS [training: 5.502346617384736 | validation: 5.414370492331686]
	TIME [epoch: 10.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.423848593259881		[learning rate: 0.00063741]
	Learning Rate: 0.000637413
	LOSS [training: 5.423848593259881 | validation: 5.413800945049047]
	TIME [epoch: 10.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4561105535607926		[learning rate: 0.00063587]
	Learning Rate: 0.00063587
	LOSS [training: 5.4561105535607926 | validation: 5.41621795921133]
	TIME [epoch: 10.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.453755073268815		[learning rate: 0.00063433]
	Learning Rate: 0.000634331
	LOSS [training: 5.453755073268815 | validation: 5.450676770052057]
	TIME [epoch: 10.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.431872767587077		[learning rate: 0.0006328]
	Learning Rate: 0.000632795
	LOSS [training: 5.431872767587077 | validation: 5.4747694548473955]
	TIME [epoch: 10.3 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.421975786673725		[learning rate: 0.00063126]
	Learning Rate: 0.000631263
	LOSS [training: 5.421975786673725 | validation: 5.393720790815044]
	TIME [epoch: 10.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.415700112095248		[learning rate: 0.00062974]
	Learning Rate: 0.000629735
	LOSS [training: 5.415700112095248 | validation: 5.328675730644203]
	TIME [epoch: 10.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3600567135370785		[learning rate: 0.00062821]
	Learning Rate: 0.00062821
	LOSS [training: 5.3600567135370785 | validation: 5.320234962793519]
	TIME [epoch: 10.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.421403076654119		[learning rate: 0.00062669]
	Learning Rate: 0.00062669
	LOSS [training: 5.421403076654119 | validation: 5.418285836207202]
	TIME [epoch: 10.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.398401832712704		[learning rate: 0.00062517]
	Learning Rate: 0.000625173
	LOSS [training: 5.398401832712704 | validation: 5.337139822814307]
	TIME [epoch: 10.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3838517390480805		[learning rate: 0.00062366]
	Learning Rate: 0.000623659
	LOSS [training: 5.3838517390480805 | validation: 5.552608131571293]
	TIME [epoch: 10.3 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.371782417933453		[learning rate: 0.00062215]
	Learning Rate: 0.000622149
	LOSS [training: 5.371782417933453 | validation: 5.395967589876217]
	TIME [epoch: 10.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.415798196874151		[learning rate: 0.00062064]
	Learning Rate: 0.000620643
	LOSS [training: 5.415798196874151 | validation: 5.379562462689346]
	TIME [epoch: 10.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4418246425601975		[learning rate: 0.00061914]
	Learning Rate: 0.000619141
	LOSS [training: 5.4418246425601975 | validation: 5.402523046865997]
	TIME [epoch: 10.3 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.393413916984256		[learning rate: 0.00061764]
	Learning Rate: 0.000617642
	LOSS [training: 5.393413916984256 | validation: 5.524901669435906]
	TIME [epoch: 10.3 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.421240359533278		[learning rate: 0.00061615]
	Learning Rate: 0.000616147
	LOSS [training: 5.421240359533278 | validation: 5.4150731393513745]
	TIME [epoch: 10.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.442546955772005		[learning rate: 0.00061466]
	Learning Rate: 0.000614655
	LOSS [training: 5.442546955772005 | validation: 5.337934075847049]
	TIME [epoch: 10.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.391949400874674		[learning rate: 0.00061317]
	Learning Rate: 0.000613167
	LOSS [training: 5.391949400874674 | validation: 5.334099280040927]
	TIME [epoch: 10.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.339699859809391		[learning rate: 0.00061168]
	Learning Rate: 0.000611683
	LOSS [training: 5.339699859809391 | validation: 5.322239135968111]
	TIME [epoch: 10.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.324353155338554		[learning rate: 0.0006102]
	Learning Rate: 0.000610202
	LOSS [training: 5.324353155338554 | validation: 5.352542812410932]
	TIME [epoch: 10.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.347972459227984		[learning rate: 0.00060872]
	Learning Rate: 0.000608725
	LOSS [training: 5.347972459227984 | validation: 5.36886634373195]
	TIME [epoch: 10.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.382293103416777		[learning rate: 0.00060725]
	Learning Rate: 0.000607251
	LOSS [training: 5.382293103416777 | validation: 5.336170890026465]
	TIME [epoch: 10.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.264646845469407		[learning rate: 0.00060578]
	Learning Rate: 0.000605781
	LOSS [training: 5.264646845469407 | validation: 5.327205118357553]
	TIME [epoch: 10.3 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.283726712858622		[learning rate: 0.00060431]
	Learning Rate: 0.000604315
	LOSS [training: 5.283726712858622 | validation: 5.331003808377589]
	TIME [epoch: 10.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.337177005637035		[learning rate: 0.00060285]
	Learning Rate: 0.000602852
	LOSS [training: 5.337177005637035 | validation: 5.348874444297937]
	TIME [epoch: 10.3 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.300768311250846		[learning rate: 0.00060139]
	Learning Rate: 0.000601392
	LOSS [training: 5.300768311250846 | validation: 5.3206290183557465]
	TIME [epoch: 10.3 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.260188824749153		[learning rate: 0.00059994]
	Learning Rate: 0.000599936
	LOSS [training: 5.260188824749153 | validation: 5.329741690623284]
	TIME [epoch: 10.3 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.273443221197337		[learning rate: 0.00059848]
	Learning Rate: 0.000598484
	LOSS [training: 5.273443221197337 | validation: 5.317288940330191]
	TIME [epoch: 10.3 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.326397288829176		[learning rate: 0.00059704]
	Learning Rate: 0.000597035
	LOSS [training: 5.326397288829176 | validation: 5.332895008708248]
	TIME [epoch: 10.3 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3124140060223		[learning rate: 0.00059559]
	Learning Rate: 0.00059559
	LOSS [training: 5.3124140060223 | validation: 5.348343634989772]
	TIME [epoch: 10.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3166787061345335		[learning rate: 0.00059415]
	Learning Rate: 0.000594148
	LOSS [training: 5.3166787061345335 | validation: 5.323511622547229]
	TIME [epoch: 10.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.312715786689509		[learning rate: 0.00059271]
	Learning Rate: 0.00059271
	LOSS [training: 5.312715786689509 | validation: 5.3161358427822805]
	TIME [epoch: 10.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.290932663285223		[learning rate: 0.00059128]
	Learning Rate: 0.000591275
	LOSS [training: 5.290932663285223 | validation: 5.32931632359087]
	TIME [epoch: 10.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3032291561459575		[learning rate: 0.00058984]
	Learning Rate: 0.000589844
	LOSS [training: 5.3032291561459575 | validation: 5.3143363728972215]
	TIME [epoch: 10.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.358690187983049		[learning rate: 0.00058842]
	Learning Rate: 0.000588416
	LOSS [training: 5.358690187983049 | validation: 5.35636972342114]
	TIME [epoch: 10.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.269735725721526		[learning rate: 0.00058699]
	Learning Rate: 0.000586991
	LOSS [training: 5.269735725721526 | validation: 5.3526369252327894]
	TIME [epoch: 10.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.283748330040787		[learning rate: 0.00058557]
	Learning Rate: 0.00058557
	LOSS [training: 5.283748330040787 | validation: 5.329007778639177]
	TIME [epoch: 10.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.286224064996988		[learning rate: 0.00058415]
	Learning Rate: 0.000584153
	LOSS [training: 5.286224064996988 | validation: 5.295217092913893]
	TIME [epoch: 10.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.312254368671859		[learning rate: 0.00058274]
	Learning Rate: 0.000582738
	LOSS [training: 5.312254368671859 | validation: 5.345919006028331]
	TIME [epoch: 10.3 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.300140375528184		[learning rate: 0.00058133]
	Learning Rate: 0.000581328
	LOSS [training: 5.300140375528184 | validation: 5.3135373995379895]
	TIME [epoch: 10.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.282145483352008		[learning rate: 0.00057992]
	Learning Rate: 0.00057992
	LOSS [training: 5.282145483352008 | validation: 5.319476636656652]
	TIME [epoch: 10.3 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.264204863345558		[learning rate: 0.00057852]
	Learning Rate: 0.000578517
	LOSS [training: 5.264204863345558 | validation: 5.32476212966541]
	TIME [epoch: 10.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.276152927993056		[learning rate: 0.00057712]
	Learning Rate: 0.000577116
	LOSS [training: 5.276152927993056 | validation: 5.3331144627961]
	TIME [epoch: 10.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.247139795391698		[learning rate: 0.00057572]
	Learning Rate: 0.000575719
	LOSS [training: 5.247139795391698 | validation: 5.318041008250873]
	TIME [epoch: 10.3 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.260128000538938		[learning rate: 0.00057433]
	Learning Rate: 0.000574325
	LOSS [training: 5.260128000538938 | validation: 5.338675002270505]
	TIME [epoch: 10.3 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2708562225411715		[learning rate: 0.00057293]
	Learning Rate: 0.000572935
	LOSS [training: 5.2708562225411715 | validation: 5.3175869912238936]
	TIME [epoch: 10.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.321782735515754		[learning rate: 0.00057155]
	Learning Rate: 0.000571548
	LOSS [training: 5.321782735515754 | validation: 5.360599619392737]
	TIME [epoch: 10.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.295752469229199		[learning rate: 0.00057016]
	Learning Rate: 0.000570164
	LOSS [training: 5.295752469229199 | validation: 5.338154666941409]
	TIME [epoch: 10.3 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.296575061979048		[learning rate: 0.00056878]
	Learning Rate: 0.000568784
	LOSS [training: 5.296575061979048 | validation: 5.341815765367278]
	TIME [epoch: 10.3 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.266281758368477		[learning rate: 0.00056741]
	Learning Rate: 0.000567407
	LOSS [training: 5.266281758368477 | validation: 5.331757602239178]
	TIME [epoch: 10.3 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.261640881463479		[learning rate: 0.00056603]
	Learning Rate: 0.000566033
	LOSS [training: 5.261640881463479 | validation: 5.361993611641379]
	TIME [epoch: 10.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2791329592547855		[learning rate: 0.00056466]
	Learning Rate: 0.000564663
	LOSS [training: 5.2791329592547855 | validation: 5.298872567666231]
	TIME [epoch: 10.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.278217883238208		[learning rate: 0.0005633]
	Learning Rate: 0.000563296
	LOSS [training: 5.278217883238208 | validation: 5.320085324311002]
	TIME [epoch: 10.3 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.274359088983038		[learning rate: 0.00056193]
	Learning Rate: 0.000561933
	LOSS [training: 5.274359088983038 | validation: 5.341439244112634]
	TIME [epoch: 10.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.257978172951326		[learning rate: 0.00056057]
	Learning Rate: 0.000560572
	LOSS [training: 5.257978172951326 | validation: 5.304977476525842]
	TIME [epoch: 10.3 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3031858566314485		[learning rate: 0.00055922]
	Learning Rate: 0.000559215
	LOSS [training: 5.3031858566314485 | validation: 5.387730563347084]
	TIME [epoch: 10.3 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.258066273033771		[learning rate: 0.00055786]
	Learning Rate: 0.000557861
	LOSS [training: 5.258066273033771 | validation: 5.346651890566549]
	TIME [epoch: 10.3 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.287182733352017		[learning rate: 0.00055651]
	Learning Rate: 0.000556511
	LOSS [training: 5.287182733352017 | validation: 5.318660913614553]
	TIME [epoch: 10.3 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.266106989314554		[learning rate: 0.00055516]
	Learning Rate: 0.000555164
	LOSS [training: 5.266106989314554 | validation: 5.332267005398889]
	TIME [epoch: 10.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2739187727017445		[learning rate: 0.00055382]
	Learning Rate: 0.00055382
	LOSS [training: 5.2739187727017445 | validation: 5.339990954724113]
	TIME [epoch: 10.3 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.235230750090773		[learning rate: 0.00055248]
	Learning Rate: 0.000552479
	LOSS [training: 5.235230750090773 | validation: 5.328852812944468]
	TIME [epoch: 10.3 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2693722715846		[learning rate: 0.00055114]
	Learning Rate: 0.000551142
	LOSS [training: 5.2693722715846 | validation: 5.349256305849545]
	TIME [epoch: 10.3 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.249171430441486		[learning rate: 0.00054981]
	Learning Rate: 0.000549807
	LOSS [training: 5.249171430441486 | validation: 5.30646898172339]
	TIME [epoch: 10.3 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.26246261319044		[learning rate: 0.00054848]
	Learning Rate: 0.000548476
	LOSS [training: 5.26246261319044 | validation: 5.303169923878276]
	TIME [epoch: 10.3 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.283226647454147		[learning rate: 0.00054715]
	Learning Rate: 0.000547149
	LOSS [training: 5.283226647454147 | validation: 5.301345321190148]
	TIME [epoch: 10.3 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.279417996810522		[learning rate: 0.00054582]
	Learning Rate: 0.000545824
	LOSS [training: 5.279417996810522 | validation: 5.322832457170401]
	TIME [epoch: 10.3 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241598371001764		[learning rate: 0.0005445]
	Learning Rate: 0.000544503
	LOSS [training: 5.241598371001764 | validation: 5.309324009737595]
	TIME [epoch: 10.3 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.246392808829529		[learning rate: 0.00054318]
	Learning Rate: 0.000543185
	LOSS [training: 5.246392808829529 | validation: 5.340918160634585]
	TIME [epoch: 10.3 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2479865793304965		[learning rate: 0.00054187]
	Learning Rate: 0.000541869
	LOSS [training: 5.2479865793304965 | validation: 5.296992620225016]
	TIME [epoch: 10.3 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223612020989152		[learning rate: 0.00054056]
	Learning Rate: 0.000540558
	LOSS [training: 5.223612020989152 | validation: 5.3082334327213765]
	TIME [epoch: 10.3 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2347633742993		[learning rate: 0.00053925]
	Learning Rate: 0.000539249
	LOSS [training: 5.2347633742993 | validation: 5.32467424891736]
	TIME [epoch: 10.3 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2693015468222075		[learning rate: 0.00053794]
	Learning Rate: 0.000537944
	LOSS [training: 5.2693015468222075 | validation: 5.317518464987677]
	TIME [epoch: 10.3 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25870223916332		[learning rate: 0.00053664]
	Learning Rate: 0.000536641
	LOSS [training: 5.25870223916332 | validation: 5.303894096235922]
	TIME [epoch: 10.3 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.239267881416531		[learning rate: 0.00053534]
	Learning Rate: 0.000535342
	LOSS [training: 5.239267881416531 | validation: 5.303379544696811]
	TIME [epoch: 10.3 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25981746701585		[learning rate: 0.00053405]
	Learning Rate: 0.000534046
	LOSS [training: 5.25981746701585 | validation: 5.314807714288504]
	TIME [epoch: 10.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.247298446266444		[learning rate: 0.00053275]
	Learning Rate: 0.000532754
	LOSS [training: 5.247298446266444 | validation: 5.310945419943311]
	TIME [epoch: 10.3 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.226069634708217		[learning rate: 0.00053146]
	Learning Rate: 0.000531464
	LOSS [training: 5.226069634708217 | validation: 5.338899978786644]
	TIME [epoch: 10.3 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.272252919041629		[learning rate: 0.00053018]
	Learning Rate: 0.000530177
	LOSS [training: 5.272252919041629 | validation: 5.332414615430244]
	TIME [epoch: 10.3 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241178528433935		[learning rate: 0.00052889]
	Learning Rate: 0.000528894
	LOSS [training: 5.241178528433935 | validation: 5.31482665533887]
	TIME [epoch: 10.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.245916250563514		[learning rate: 0.00052761]
	Learning Rate: 0.000527613
	LOSS [training: 5.245916250563514 | validation: 5.30600632613086]
	TIME [epoch: 10.3 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.23970309972042		[learning rate: 0.00052634]
	Learning Rate: 0.000526336
	LOSS [training: 5.23970309972042 | validation: 5.30752188226098]
	TIME [epoch: 10.3 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.24491335929946		[learning rate: 0.00052506]
	Learning Rate: 0.000525062
	LOSS [training: 5.24491335929946 | validation: 5.299405667066593]
	TIME [epoch: 10.3 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.253946153488881		[learning rate: 0.00052379]
	Learning Rate: 0.000523791
	LOSS [training: 5.253946153488881 | validation: 5.3073079225249975]
	TIME [epoch: 10.3 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234005838317915		[learning rate: 0.00052252]
	Learning Rate: 0.000522523
	LOSS [training: 5.234005838317915 | validation: 5.310851767141247]
	TIME [epoch: 10.3 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241391342452298		[learning rate: 0.00052126]
	Learning Rate: 0.000521258
	LOSS [training: 5.241391342452298 | validation: 5.3259347518494815]
	TIME [epoch: 10.3 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.287420903830026		[learning rate: 0.00052]
	Learning Rate: 0.000519996
	LOSS [training: 5.287420903830026 | validation: 5.315127446216581]
	TIME [epoch: 10.3 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2490429343402765		[learning rate: 0.00051874]
	Learning Rate: 0.000518737
	LOSS [training: 5.2490429343402765 | validation: 5.314023031055428]
	TIME [epoch: 10.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2649867114189455		[learning rate: 0.00051748]
	Learning Rate: 0.000517481
	LOSS [training: 5.2649867114189455 | validation: 5.352649882164256]
	TIME [epoch: 10.3 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.261303323532521		[learning rate: 0.00051623]
	Learning Rate: 0.000516229
	LOSS [training: 5.261303323532521 | validation: 5.338055315611877]
	TIME [epoch: 10.3 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.316192809389854		[learning rate: 0.00051498]
	Learning Rate: 0.000514979
	LOSS [training: 5.316192809389854 | validation: 5.31191322801895]
	TIME [epoch: 10.3 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.307333214627867		[learning rate: 0.00051373]
	Learning Rate: 0.000513732
	LOSS [training: 5.307333214627867 | validation: 5.338573363220667]
	TIME [epoch: 10.3 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.301182726879941		[learning rate: 0.00051249]
	Learning Rate: 0.000512489
	LOSS [training: 5.301182726879941 | validation: 5.3143835317955315]
	TIME [epoch: 10.3 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.323963706257128		[learning rate: 0.00051125]
	Learning Rate: 0.000511248
	LOSS [training: 5.323963706257128 | validation: 5.33286755841595]
	TIME [epoch: 10.3 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.332395654912089		[learning rate: 0.00051001]
	Learning Rate: 0.00051001
	LOSS [training: 5.332395654912089 | validation: 5.332605156413715]
	TIME [epoch: 10.3 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.359154338264961		[learning rate: 0.00050878]
	Learning Rate: 0.000508776
	LOSS [training: 5.359154338264961 | validation: 5.341598071044818]
	TIME [epoch: 10.3 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.326793424453821		[learning rate: 0.00050754]
	Learning Rate: 0.000507544
	LOSS [training: 5.326793424453821 | validation: 5.3910025551643175]
	TIME [epoch: 10.3 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.308524605986971		[learning rate: 0.00050632]
	Learning Rate: 0.000506315
	LOSS [training: 5.308524605986971 | validation: 5.38627453178705]
	TIME [epoch: 10.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.362447763064606		[learning rate: 0.00050509]
	Learning Rate: 0.00050509
	LOSS [training: 5.362447763064606 | validation: 5.388932862630127]
	TIME [epoch: 10.3 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3806467794959385		[learning rate: 0.00050387]
	Learning Rate: 0.000503867
	LOSS [training: 5.3806467794959385 | validation: 5.353747574846919]
	TIME [epoch: 10.3 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.330091114977835		[learning rate: 0.00050265]
	Learning Rate: 0.000502647
	LOSS [training: 5.330091114977835 | validation: 5.315704999843913]
	TIME [epoch: 10.3 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.365376384655436		[learning rate: 0.00050143]
	Learning Rate: 0.00050143
	LOSS [training: 5.365376384655436 | validation: 5.324632597382695]
	TIME [epoch: 10.3 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.335252521681094		[learning rate: 0.00050022]
	Learning Rate: 0.000500216
	LOSS [training: 5.335252521681094 | validation: 5.386337142885233]
	TIME [epoch: 10.3 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.338840654272937		[learning rate: 0.00049901]
	Learning Rate: 0.000499005
	LOSS [training: 5.338840654272937 | validation: 5.348952948484673]
	TIME [epoch: 10.3 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.379851301032874		[learning rate: 0.0004978]
	Learning Rate: 0.000497797
	LOSS [training: 5.379851301032874 | validation: 5.312255282209403]
	TIME [epoch: 10.3 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.293920836176939		[learning rate: 0.00049659]
	Learning Rate: 0.000496592
	LOSS [training: 5.293920836176939 | validation: 5.338349513758701]
	TIME [epoch: 10.3 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2991244201327445		[learning rate: 0.00049539]
	Learning Rate: 0.00049539
	LOSS [training: 5.2991244201327445 | validation: 5.311789072554865]
	TIME [epoch: 10.3 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.308512829357041		[learning rate: 0.00049419]
	Learning Rate: 0.000494191
	LOSS [training: 5.308512829357041 | validation: 5.31368163649097]
	TIME [epoch: 10.3 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2824782421627265		[learning rate: 0.00049299]
	Learning Rate: 0.000492995
	LOSS [training: 5.2824782421627265 | validation: 5.308572877523472]
	TIME [epoch: 10.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.320095321427138		[learning rate: 0.0004918]
	Learning Rate: 0.000491801
	LOSS [training: 5.320095321427138 | validation: 5.391384349594368]
	TIME [epoch: 10.3 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.310560846513403		[learning rate: 0.00049061]
	Learning Rate: 0.00049061
	LOSS [training: 5.310560846513403 | validation: 5.368488601871436]
	TIME [epoch: 10.3 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.350210633948169		[learning rate: 0.00048942]
	Learning Rate: 0.000489423
	LOSS [training: 5.350210633948169 | validation: 5.3624847512210065]
	TIME [epoch: 10.3 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.283428359858475		[learning rate: 0.00048824]
	Learning Rate: 0.000488238
	LOSS [training: 5.283428359858475 | validation: 5.367410606047975]
	TIME [epoch: 10.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.303212249369892		[learning rate: 0.00048706]
	Learning Rate: 0.000487056
	LOSS [training: 5.303212249369892 | validation: 5.298039111680789]
	TIME [epoch: 10.3 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.287722505140026		[learning rate: 0.00048588]
	Learning Rate: 0.000485877
	LOSS [training: 5.287722505140026 | validation: 5.335938345509492]
	TIME [epoch: 10.3 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2847862350318024		[learning rate: 0.0004847]
	Learning Rate: 0.000484701
	LOSS [training: 5.2847862350318024 | validation: 5.312569959953017]
	TIME [epoch: 10.3 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.288617159099435		[learning rate: 0.00048353]
	Learning Rate: 0.000483527
	LOSS [training: 5.288617159099435 | validation: 5.344460790121644]
	TIME [epoch: 10.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.267523888352471		[learning rate: 0.00048236]
	Learning Rate: 0.000482357
	LOSS [training: 5.267523888352471 | validation: 5.35479316834041]
	TIME [epoch: 10.3 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.316159692260973		[learning rate: 0.00048119]
	Learning Rate: 0.000481189
	LOSS [training: 5.316159692260973 | validation: 5.316653079140791]
	TIME [epoch: 10.3 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30149011199261		[learning rate: 0.00048002]
	Learning Rate: 0.000480024
	LOSS [training: 5.30149011199261 | validation: 5.304241827948878]
	TIME [epoch: 10.3 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.271963613807237		[learning rate: 0.00047886]
	Learning Rate: 0.000478862
	LOSS [training: 5.271963613807237 | validation: 5.327819764892218]
	TIME [epoch: 10.3 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.232002152758282		[learning rate: 0.0004777]
	Learning Rate: 0.000477703
	LOSS [training: 5.232002152758282 | validation: 5.339707474252459]
	TIME [epoch: 10.3 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.285121507765015		[learning rate: 0.00047655]
	Learning Rate: 0.000476546
	LOSS [training: 5.285121507765015 | validation: 5.309262436523674]
	TIME [epoch: 10.3 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.255472013429711		[learning rate: 0.00047539]
	Learning Rate: 0.000475393
	LOSS [training: 5.255472013429711 | validation: 5.323491474401078]
	TIME [epoch: 10.3 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.328111110374195		[learning rate: 0.00047424]
	Learning Rate: 0.000474242
	LOSS [training: 5.328111110374195 | validation: 5.306978299839872]
	TIME [epoch: 10.3 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.243354235036344		[learning rate: 0.00047309]
	Learning Rate: 0.000473094
	LOSS [training: 5.243354235036344 | validation: 5.3355741931796095]
	TIME [epoch: 10.3 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.227230140116245		[learning rate: 0.00047195]
	Learning Rate: 0.000471949
	LOSS [training: 5.227230140116245 | validation: 5.314069953843582]
	TIME [epoch: 10.3 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.226242228066367		[learning rate: 0.00047081]
	Learning Rate: 0.000470806
	LOSS [training: 5.226242228066367 | validation: 5.298445417401174]
	TIME [epoch: 10.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.296724363469201		[learning rate: 0.00046967]
	Learning Rate: 0.000469666
	LOSS [training: 5.296724363469201 | validation: 5.329556349615001]
	TIME [epoch: 10.3 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.269758867676781		[learning rate: 0.00046853]
	Learning Rate: 0.000468529
	LOSS [training: 5.269758867676781 | validation: 5.331408417040925]
	TIME [epoch: 10.3 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.248847409265679		[learning rate: 0.0004674]
	Learning Rate: 0.000467395
	LOSS [training: 5.248847409265679 | validation: 5.314416987577886]
	TIME [epoch: 10.3 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.316693953496426		[learning rate: 0.00046626]
	Learning Rate: 0.000466264
	LOSS [training: 5.316693953496426 | validation: 5.325793754429241]
	TIME [epoch: 10.3 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.246403215616947		[learning rate: 0.00046513]
	Learning Rate: 0.000465135
	LOSS [training: 5.246403215616947 | validation: 5.318606879840521]
	TIME [epoch: 10.3 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2263493556130936		[learning rate: 0.00046401]
	Learning Rate: 0.000464009
	LOSS [training: 5.2263493556130936 | validation: 5.29831809877157]
	TIME [epoch: 10.3 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.251737110076458		[learning rate: 0.00046289]
	Learning Rate: 0.000462886
	LOSS [training: 5.251737110076458 | validation: 5.399762375598458]
	TIME [epoch: 10.3 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.259291890145278		[learning rate: 0.00046177]
	Learning Rate: 0.000461765
	LOSS [training: 5.259291890145278 | validation: 5.3598194744882]
	TIME [epoch: 10.3 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.242590142312647		[learning rate: 0.00046065]
	Learning Rate: 0.000460647
	LOSS [training: 5.242590142312647 | validation: 5.311664838074664]
	TIME [epoch: 10.3 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.251852807303146		[learning rate: 0.00045953]
	Learning Rate: 0.000459532
	LOSS [training: 5.251852807303146 | validation: 5.291575378696849]
	TIME [epoch: 10.3 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.246022427206183		[learning rate: 0.00045842]
	Learning Rate: 0.00045842
	LOSS [training: 5.246022427206183 | validation: 5.328454840237106]
	TIME [epoch: 10.3 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.268161778568898		[learning rate: 0.00045731]
	Learning Rate: 0.00045731
	LOSS [training: 5.268161778568898 | validation: 5.30438274006149]
	TIME [epoch: 10.3 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.224090015597232		[learning rate: 0.0004562]
	Learning Rate: 0.000456203
	LOSS [training: 5.224090015597232 | validation: 5.316113091345552]
	TIME [epoch: 10.3 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.232447221577506		[learning rate: 0.0004551]
	Learning Rate: 0.000455098
	LOSS [training: 5.232447221577506 | validation: 5.318439062146992]
	TIME [epoch: 10.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.220700461945016		[learning rate: 0.000454]
	Learning Rate: 0.000453997
	LOSS [training: 5.220700461945016 | validation: 5.334438134574368]
	TIME [epoch: 10.3 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.239743840529597		[learning rate: 0.0004529]
	Learning Rate: 0.000452898
	LOSS [training: 5.239743840529597 | validation: 5.335616721398967]
	TIME [epoch: 10.3 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.222899535506059		[learning rate: 0.0004518]
	Learning Rate: 0.000451801
	LOSS [training: 5.222899535506059 | validation: 5.312921068644914]
	TIME [epoch: 10.3 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.279063094383331		[learning rate: 0.00045071]
	Learning Rate: 0.000450708
	LOSS [training: 5.279063094383331 | validation: 5.328181600256167]
	TIME [epoch: 10.3 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.233835686781756		[learning rate: 0.00044962]
	Learning Rate: 0.000449616
	LOSS [training: 5.233835686781756 | validation: 5.308311240535444]
	TIME [epoch: 10.3 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.251442357983865		[learning rate: 0.00044853]
	Learning Rate: 0.000448528
	LOSS [training: 5.251442357983865 | validation: 5.34541204730285]
	TIME [epoch: 10.3 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.259004291094347		[learning rate: 0.00044744]
	Learning Rate: 0.000447442
	LOSS [training: 5.259004291094347 | validation: 5.277753555981759]
	TIME [epoch: 10.3 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2234731042270965		[learning rate: 0.00044636]
	Learning Rate: 0.000446359
	LOSS [training: 5.2234731042270965 | validation: 5.355634178608255]
	TIME [epoch: 10.3 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.261494374462243		[learning rate: 0.00044528]
	Learning Rate: 0.000445278
	LOSS [training: 5.261494374462243 | validation: 5.35593428483295]
	TIME [epoch: 10.3 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.263695260771277		[learning rate: 0.0004442]
	Learning Rate: 0.0004442
	LOSS [training: 5.263695260771277 | validation: 5.309216615743988]
	TIME [epoch: 10.3 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.205352251232672		[learning rate: 0.00044313]
	Learning Rate: 0.000443125
	LOSS [training: 5.205352251232672 | validation: 5.32651835401136]
	TIME [epoch: 10.3 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.203750827227998		[learning rate: 0.00044205]
	Learning Rate: 0.000442052
	LOSS [training: 5.203750827227998 | validation: 5.329227185270863]
	TIME [epoch: 10.3 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2234578793865545		[learning rate: 0.00044098]
	Learning Rate: 0.000440982
	LOSS [training: 5.2234578793865545 | validation: 5.307941251700184]
	TIME [epoch: 10.3 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.237636516377958		[learning rate: 0.00043991]
	Learning Rate: 0.000439915
	LOSS [training: 5.237636516377958 | validation: 5.2970346987763515]
	TIME [epoch: 10.3 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.216554882582307		[learning rate: 0.00043885]
	Learning Rate: 0.00043885
	LOSS [training: 5.216554882582307 | validation: 5.298063801427909]
	TIME [epoch: 10.3 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.229301452413933		[learning rate: 0.00043779]
	Learning Rate: 0.000437787
	LOSS [training: 5.229301452413933 | validation: 5.298597585021935]
	TIME [epoch: 10.3 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.19742525318644		[learning rate: 0.00043673]
	Learning Rate: 0.000436727
	LOSS [training: 5.19742525318644 | validation: 5.304271531236832]
	TIME [epoch: 10.3 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.229420353969006		[learning rate: 0.00043567]
	Learning Rate: 0.00043567
	LOSS [training: 5.229420353969006 | validation: 5.308647613289099]
	TIME [epoch: 10.3 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.206957756562222		[learning rate: 0.00043462]
	Learning Rate: 0.000434616
	LOSS [training: 5.206957756562222 | validation: 5.3367832459419065]
	TIME [epoch: 10.3 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.265867781517679		[learning rate: 0.00043356]
	Learning Rate: 0.000433563
	LOSS [training: 5.265867781517679 | validation: 5.322231816824195]
	TIME [epoch: 10.3 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.249779628643713		[learning rate: 0.00043251]
	Learning Rate: 0.000432514
	LOSS [training: 5.249779628643713 | validation: 5.316944134163866]
	TIME [epoch: 10.3 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234529513551656		[learning rate: 0.00043147]
	Learning Rate: 0.000431467
	LOSS [training: 5.234529513551656 | validation: 5.333383917833848]
	TIME [epoch: 10.3 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.251261310091101		[learning rate: 0.00043042]
	Learning Rate: 0.000430422
	LOSS [training: 5.251261310091101 | validation: 5.291172180618387]
	TIME [epoch: 10.3 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.202331702999205		[learning rate: 0.00042938]
	Learning Rate: 0.00042938
	LOSS [training: 5.202331702999205 | validation: 5.322222334155858]
	TIME [epoch: 10.3 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.250745459467927		[learning rate: 0.00042834]
	Learning Rate: 0.000428341
	LOSS [training: 5.250745459467927 | validation: 5.298384399729628]
	TIME [epoch: 10.3 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.263595484786075		[learning rate: 0.0004273]
	Learning Rate: 0.000427304
	LOSS [training: 5.263595484786075 | validation: 5.301017093789032]
	TIME [epoch: 10.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2155452857184486		[learning rate: 0.00042627]
	Learning Rate: 0.000426269
	LOSS [training: 5.2155452857184486 | validation: 5.282488662353484]
	TIME [epoch: 10.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.230861739791058		[learning rate: 0.00042524]
	Learning Rate: 0.000425238
	LOSS [training: 5.230861739791058 | validation: 5.312503789872022]
	TIME [epoch: 10.3 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1932119591672485		[learning rate: 0.00042421]
	Learning Rate: 0.000424208
	LOSS [training: 5.1932119591672485 | validation: 5.301625222725631]
	TIME [epoch: 10.3 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.22150863998104		[learning rate: 0.00042318]
	Learning Rate: 0.000423181
	LOSS [training: 5.22150863998104 | validation: 5.302256187749381]
	TIME [epoch: 10.3 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.232362212638259		[learning rate: 0.00042216]
	Learning Rate: 0.000422157
	LOSS [training: 5.232362212638259 | validation: 5.333320544891714]
	TIME [epoch: 10.3 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.243684541619237		[learning rate: 0.00042113]
	Learning Rate: 0.000421135
	LOSS [training: 5.243684541619237 | validation: 5.344604527363361]
	TIME [epoch: 10.3 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2726483636363355		[learning rate: 0.00042012]
	Learning Rate: 0.000420115
	LOSS [training: 5.2726483636363355 | validation: 5.325377814052401]
	TIME [epoch: 10.3 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.208470668696817		[learning rate: 0.0004191]
	Learning Rate: 0.000419098
	LOSS [training: 5.208470668696817 | validation: 5.304941292520177]
	TIME [epoch: 10.3 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.24630710714717		[learning rate: 0.00041808]
	Learning Rate: 0.000418084
	LOSS [training: 5.24630710714717 | validation: 5.34832492681137]
	TIME [epoch: 10.3 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.232995459606871		[learning rate: 0.00041707]
	Learning Rate: 0.000417072
	LOSS [training: 5.232995459606871 | validation: 5.317056420922988]
	TIME [epoch: 10.3 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2319450280916655		[learning rate: 0.00041606]
	Learning Rate: 0.000416062
	LOSS [training: 5.2319450280916655 | validation: 5.308053712627385]
	TIME [epoch: 10.3 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.204469828496689		[learning rate: 0.00041505]
	Learning Rate: 0.000415055
	LOSS [training: 5.204469828496689 | validation: 5.305132115297406]
	TIME [epoch: 10.3 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.188977118339304		[learning rate: 0.00041405]
	Learning Rate: 0.00041405
	LOSS [training: 5.188977118339304 | validation: 5.3142603209446255]
	TIME [epoch: 10.3 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2013758772491645		[learning rate: 0.00041305]
	Learning Rate: 0.000413048
	LOSS [training: 5.2013758772491645 | validation: 5.289801482931099]
	TIME [epoch: 10.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234974815335724		[learning rate: 0.00041205]
	Learning Rate: 0.000412048
	LOSS [training: 5.234974815335724 | validation: 5.305913585730798]
	TIME [epoch: 10.3 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.225145069292664		[learning rate: 0.00041105]
	Learning Rate: 0.00041105
	LOSS [training: 5.225145069292664 | validation: 5.289931809703334]
	TIME [epoch: 10.3 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.232891202834013		[learning rate: 0.00041005]
	Learning Rate: 0.000410055
	LOSS [training: 5.232891202834013 | validation: 5.320828268072756]
	TIME [epoch: 10.3 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.208122260148836		[learning rate: 0.00040906]
	Learning Rate: 0.000409062
	LOSS [training: 5.208122260148836 | validation: 5.291583400160995]
	TIME [epoch: 10.3 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.20276482289388		[learning rate: 0.00040807]
	Learning Rate: 0.000408072
	LOSS [training: 5.20276482289388 | validation: 5.293439498835369]
	TIME [epoch: 10.3 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.212909451194639		[learning rate: 0.00040708]
	Learning Rate: 0.000407084
	LOSS [training: 5.212909451194639 | validation: 5.304743541831454]
	TIME [epoch: 10.3 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2054634750932784		[learning rate: 0.0004061]
	Learning Rate: 0.000406099
	LOSS [training: 5.2054634750932784 | validation: 5.298243330010202]
	TIME [epoch: 10.3 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.211498446859162		[learning rate: 0.00040512]
	Learning Rate: 0.000405116
	LOSS [training: 5.211498446859162 | validation: 5.337526398885956]
	TIME [epoch: 10.3 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.21877767737461		[learning rate: 0.00040413]
	Learning Rate: 0.000404135
	LOSS [training: 5.21877767737461 | validation: 5.327003362566801]
	TIME [epoch: 10.3 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.214648676157603		[learning rate: 0.00040316]
	Learning Rate: 0.000403157
	LOSS [training: 5.214648676157603 | validation: 5.324044400079316]
	TIME [epoch: 10.3 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.254709385878489		[learning rate: 0.00040218]
	Learning Rate: 0.00040218
	LOSS [training: 5.254709385878489 | validation: 5.298779614111232]
	TIME [epoch: 10.3 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.204592981298617		[learning rate: 0.00040121]
	Learning Rate: 0.000401207
	LOSS [training: 5.204592981298617 | validation: 5.325373437287412]
	TIME [epoch: 10.3 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16494984631713		[learning rate: 0.00040024]
	Learning Rate: 0.000400236
	LOSS [training: 5.16494984631713 | validation: 5.303428755302081]
	TIME [epoch: 10.3 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191151955653928		[learning rate: 0.00039927]
	Learning Rate: 0.000399267
	LOSS [training: 5.191151955653928 | validation: 5.375103570305387]
	TIME [epoch: 10.3 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.236555161171881		[learning rate: 0.0003983]
	Learning Rate: 0.0003983
	LOSS [training: 5.236555161171881 | validation: 5.303423183541904]
	TIME [epoch: 10.3 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223487229652221		[learning rate: 0.00039734]
	Learning Rate: 0.000397336
	LOSS [training: 5.223487229652221 | validation: 5.3027816047730685]
	TIME [epoch: 10.3 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192556704196648		[learning rate: 0.00039637]
	Learning Rate: 0.000396374
	LOSS [training: 5.192556704196648 | validation: 5.2909825799145755]
	TIME [epoch: 10.3 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.231182885731139		[learning rate: 0.00039541]
	Learning Rate: 0.000395415
	LOSS [training: 5.231182885731139 | validation: 5.296101725722401]
	TIME [epoch: 10.3 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.210156050629189		[learning rate: 0.00039446]
	Learning Rate: 0.000394457
	LOSS [training: 5.210156050629189 | validation: 5.311319105690672]
	TIME [epoch: 10.3 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197978864090713		[learning rate: 0.0003935]
	Learning Rate: 0.000393502
	LOSS [training: 5.197978864090713 | validation: 5.307302160337533]
	TIME [epoch: 10.3 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.205861341673038		[learning rate: 0.00039255]
	Learning Rate: 0.00039255
	LOSS [training: 5.205861341673038 | validation: 5.288357706632528]
	TIME [epoch: 10.3 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148179693365812		[learning rate: 0.0003916]
	Learning Rate: 0.000391599
	LOSS [training: 5.148179693365812 | validation: 5.279832049691383]
	TIME [epoch: 10.3 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160235540515385		[learning rate: 0.00039065]
	Learning Rate: 0.000390651
	LOSS [training: 5.160235540515385 | validation: 5.290235883238961]
	TIME [epoch: 10.3 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197646490786357		[learning rate: 0.00038971]
	Learning Rate: 0.000389706
	LOSS [training: 5.197646490786357 | validation: 5.295517950293459]
	TIME [epoch: 10.3 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.162140033495762		[learning rate: 0.00038876]
	Learning Rate: 0.000388762
	LOSS [training: 5.162140033495762 | validation: 5.307470546765552]
	TIME [epoch: 10.3 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1819093803124705		[learning rate: 0.00038782]
	Learning Rate: 0.000387821
	LOSS [training: 5.1819093803124705 | validation: 5.280127805916679]
	TIME [epoch: 10.3 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184437642535696		[learning rate: 0.00038688]
	Learning Rate: 0.000386882
	LOSS [training: 5.184437642535696 | validation: 5.294171882614946]
	TIME [epoch: 10.3 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.199694705777452		[learning rate: 0.00038595]
	Learning Rate: 0.000385946
	LOSS [training: 5.199694705777452 | validation: 5.27382399107449]
	TIME [epoch: 10.3 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.146947171847989		[learning rate: 0.00038501]
	Learning Rate: 0.000385011
	LOSS [training: 5.146947171847989 | validation: 5.313237211571717]
	TIME [epoch: 10.3 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.149328430594869		[learning rate: 0.00038408]
	Learning Rate: 0.000384079
	LOSS [training: 5.149328430594869 | validation: 5.290602166078047]
	TIME [epoch: 10.3 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180666982678097		[learning rate: 0.00038315]
	Learning Rate: 0.00038315
	LOSS [training: 5.180666982678097 | validation: 5.342056125585629]
	TIME [epoch: 10.3 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192277817568747		[learning rate: 0.00038222]
	Learning Rate: 0.000382222
	LOSS [training: 5.192277817568747 | validation: 5.32984545188158]
	TIME [epoch: 10.3 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.208218508301402		[learning rate: 0.0003813]
	Learning Rate: 0.000381297
	LOSS [training: 5.208218508301402 | validation: 5.2892413915461685]
	TIME [epoch: 10.3 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.201404329851718		[learning rate: 0.00038037]
	Learning Rate: 0.000380374
	LOSS [training: 5.201404329851718 | validation: 5.316330621899864]
	TIME [epoch: 10.3 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164942835033764		[learning rate: 0.00037945]
	Learning Rate: 0.000379453
	LOSS [training: 5.164942835033764 | validation: 5.297341681467656]
	TIME [epoch: 10.3 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.21513214835592		[learning rate: 0.00037853]
	Learning Rate: 0.000378534
	LOSS [training: 5.21513214835592 | validation: 5.329382250823785]
	TIME [epoch: 10.3 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192681811878623		[learning rate: 0.00037762]
	Learning Rate: 0.000377618
	LOSS [training: 5.192681811878623 | validation: 5.29094577084006]
	TIME [epoch: 10.3 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191277176301885		[learning rate: 0.0003767]
	Learning Rate: 0.000376704
	LOSS [training: 5.191277176301885 | validation: 5.29456422774851]
	TIME [epoch: 10.3 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181734773112528		[learning rate: 0.00037579]
	Learning Rate: 0.000375792
	LOSS [training: 5.181734773112528 | validation: 5.277203376379722]
	TIME [epoch: 10.3 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16714292596559		[learning rate: 0.00037488]
	Learning Rate: 0.000374882
	LOSS [training: 5.16714292596559 | validation: 5.292257717719109]
	TIME [epoch: 10.3 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.186137518507861		[learning rate: 0.00037397]
	Learning Rate: 0.000373975
	LOSS [training: 5.186137518507861 | validation: 5.300460801808823]
	TIME [epoch: 10.3 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.174839838707636		[learning rate: 0.00037307]
	Learning Rate: 0.000373069
	LOSS [training: 5.174839838707636 | validation: 5.296111660383014]
	TIME [epoch: 10.3 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176152202756125		[learning rate: 0.00037217]
	Learning Rate: 0.000372166
	LOSS [training: 5.176152202756125 | validation: 5.3093740918791195]
	TIME [epoch: 10.3 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155359233357534		[learning rate: 0.00037127]
	Learning Rate: 0.000371265
	LOSS [training: 5.155359233357534 | validation: 5.285553816183282]
	TIME [epoch: 10.3 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1847261705282115		[learning rate: 0.00037037]
	Learning Rate: 0.000370366
	LOSS [training: 5.1847261705282115 | validation: 5.310246177046543]
	TIME [epoch: 10.3 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.169664177953001		[learning rate: 0.00036947]
	Learning Rate: 0.00036947
	LOSS [training: 5.169664177953001 | validation: 5.30172849036699]
	TIME [epoch: 10.3 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153671080349232		[learning rate: 0.00036858]
	Learning Rate: 0.000368575
	LOSS [training: 5.153671080349232 | validation: 5.333567158754296]
	TIME [epoch: 10.3 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.196927726881569		[learning rate: 0.00036768]
	Learning Rate: 0.000367683
	LOSS [training: 5.196927726881569 | validation: 5.293512908213656]
	TIME [epoch: 10.3 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.189636624958057		[learning rate: 0.00036679]
	Learning Rate: 0.000366793
	LOSS [training: 5.189636624958057 | validation: 5.305073085871435]
	TIME [epoch: 10.3 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.157373561737907		[learning rate: 0.00036591]
	Learning Rate: 0.000365905
	LOSS [training: 5.157373561737907 | validation: 5.2931010788104915]
	TIME [epoch: 10.3 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190880339754538		[learning rate: 0.00036502]
	Learning Rate: 0.000365019
	LOSS [training: 5.190880339754538 | validation: 5.289394845079019]
	TIME [epoch: 10.3 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184038877442374		[learning rate: 0.00036414]
	Learning Rate: 0.000364136
	LOSS [training: 5.184038877442374 | validation: 5.306163039892165]
	TIME [epoch: 10.3 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.194231237728683		[learning rate: 0.00036325]
	Learning Rate: 0.000363254
	LOSS [training: 5.194231237728683 | validation: 5.304282481563109]
	TIME [epoch: 10.3 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154684863109699		[learning rate: 0.00036237]
	Learning Rate: 0.000362375
	LOSS [training: 5.154684863109699 | validation: 5.300830669516211]
	TIME [epoch: 10.3 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.185865888415465		[learning rate: 0.0003615]
	Learning Rate: 0.000361497
	LOSS [training: 5.185865888415465 | validation: 5.30518205370056]
	TIME [epoch: 10.3 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172725587467292		[learning rate: 0.00036062]
	Learning Rate: 0.000360622
	LOSS [training: 5.172725587467292 | validation: 5.295332922654886]
	TIME [epoch: 10.3 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.20514227023268		[learning rate: 0.00035975]
	Learning Rate: 0.000359749
	LOSS [training: 5.20514227023268 | validation: 5.289643757838148]
	TIME [epoch: 10.3 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192482845676584		[learning rate: 0.00035888]
	Learning Rate: 0.000358878
	LOSS [training: 5.192482845676584 | validation: 5.277698330905172]
	TIME [epoch: 10.3 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158005076279345		[learning rate: 0.00035801]
	Learning Rate: 0.00035801
	LOSS [training: 5.158005076279345 | validation: 5.307646823957705]
	TIME [epoch: 10.3 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160207742623214		[learning rate: 0.00035714]
	Learning Rate: 0.000357143
	LOSS [training: 5.160207742623214 | validation: 5.274600251476303]
	TIME [epoch: 10.3 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.17961516893264		[learning rate: 0.00035628]
	Learning Rate: 0.000356278
	LOSS [training: 5.17961516893264 | validation: 5.277830656567137]
	TIME [epoch: 10.3 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.201433145436896		[learning rate: 0.00035542]
	Learning Rate: 0.000355416
	LOSS [training: 5.201433145436896 | validation: 5.28719045506538]
	TIME [epoch: 10.3 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164987726454102		[learning rate: 0.00035456]
	Learning Rate: 0.000354555
	LOSS [training: 5.164987726454102 | validation: 5.278207324104695]
	TIME [epoch: 10.3 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167383635104534		[learning rate: 0.0003537]
	Learning Rate: 0.000353697
	LOSS [training: 5.167383635104534 | validation: 5.30839228663837]
	TIME [epoch: 10.3 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192375826484442		[learning rate: 0.00035284]
	Learning Rate: 0.000352841
	LOSS [training: 5.192375826484442 | validation: 5.3013070235360304]
	TIME [epoch: 10.3 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.174215057125403		[learning rate: 0.00035199]
	Learning Rate: 0.000351987
	LOSS [training: 5.174215057125403 | validation: 5.299622583534265]
	TIME [epoch: 10.3 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178449628027204		[learning rate: 0.00035113]
	Learning Rate: 0.000351135
	LOSS [training: 5.178449628027204 | validation: 5.276730309040655]
	TIME [epoch: 10.3 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165459795064577		[learning rate: 0.00035028]
	Learning Rate: 0.000350285
	LOSS [training: 5.165459795064577 | validation: 5.309876974048157]
	TIME [epoch: 10.3 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154429704171032		[learning rate: 0.00034944]
	Learning Rate: 0.000349437
	LOSS [training: 5.154429704171032 | validation: 5.306910908313774]
	TIME [epoch: 10.3 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171481087379526		[learning rate: 0.00034859]
	Learning Rate: 0.000348591
	LOSS [training: 5.171481087379526 | validation: 5.292578985040004]
	TIME [epoch: 10.3 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.142324202697013		[learning rate: 0.00034775]
	Learning Rate: 0.000347747
	LOSS [training: 5.142324202697013 | validation: 5.307562469187472]
	TIME [epoch: 10.3 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.186768681147896		[learning rate: 0.0003469]
	Learning Rate: 0.000346905
	LOSS [training: 5.186768681147896 | validation: 5.30444794545329]
	TIME [epoch: 10.3 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.217511117863241		[learning rate: 0.00034607]
	Learning Rate: 0.000346065
	LOSS [training: 5.217511117863241 | validation: 5.349185886536474]
	TIME [epoch: 10.3 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153101649430513		[learning rate: 0.00034523]
	Learning Rate: 0.000345227
	LOSS [training: 5.153101649430513 | validation: 5.295359173116249]
	TIME [epoch: 10.3 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156289470144911		[learning rate: 0.00034439]
	Learning Rate: 0.000344392
	LOSS [training: 5.156289470144911 | validation: 5.296237780837309]
	TIME [epoch: 10.3 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177433245071468		[learning rate: 0.00034356]
	Learning Rate: 0.000343558
	LOSS [training: 5.177433245071468 | validation: 5.311406509027006]
	TIME [epoch: 10.3 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1817537500069		[learning rate: 0.00034273]
	Learning Rate: 0.000342726
	LOSS [training: 5.1817537500069 | validation: 5.307826501220383]
	TIME [epoch: 10.3 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191961376669788		[learning rate: 0.0003419]
	Learning Rate: 0.000341897
	LOSS [training: 5.191961376669788 | validation: 5.298543307069444]
	TIME [epoch: 10.3 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163108242429901		[learning rate: 0.00034107]
	Learning Rate: 0.000341069
	LOSS [training: 5.163108242429901 | validation: 5.305313143989214]
	TIME [epoch: 10.3 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.168430395237484		[learning rate: 0.00034024]
	Learning Rate: 0.000340243
	LOSS [training: 5.168430395237484 | validation: 5.27894262842312]
	TIME [epoch: 10.3 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166697988646256		[learning rate: 0.00033942]
	Learning Rate: 0.00033942
	LOSS [training: 5.166697988646256 | validation: 5.295526868427198]
	TIME [epoch: 10.3 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176351934210197		[learning rate: 0.0003386]
	Learning Rate: 0.000338598
	LOSS [training: 5.176351934210197 | validation: 5.311318562886773]
	TIME [epoch: 10.3 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163043075714285		[learning rate: 0.00033778]
	Learning Rate: 0.000337778
	LOSS [training: 5.163043075714285 | validation: 5.293504455162938]
	TIME [epoch: 10.3 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172417057948139		[learning rate: 0.00033696]
	Learning Rate: 0.00033696
	LOSS [training: 5.172417057948139 | validation: 5.307649332634477]
	TIME [epoch: 10.3 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.185396459159682		[learning rate: 0.00033614]
	Learning Rate: 0.000336145
	LOSS [training: 5.185396459159682 | validation: 5.296998523117013]
	TIME [epoch: 10.3 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163438077041769		[learning rate: 0.00033533]
	Learning Rate: 0.000335331
	LOSS [training: 5.163438077041769 | validation: 5.3261827500144685]
	TIME [epoch: 10.3 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.157446494491567		[learning rate: 0.00033452]
	Learning Rate: 0.000334519
	LOSS [training: 5.157446494491567 | validation: 5.292978752130696]
	TIME [epoch: 10.3 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1857917082230385		[learning rate: 0.00033371]
	Learning Rate: 0.000333709
	LOSS [training: 5.1857917082230385 | validation: 5.34468186638534]
	TIME [epoch: 10.3 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223122536533211		[learning rate: 0.0003329]
	Learning Rate: 0.000332902
	LOSS [training: 5.223122536533211 | validation: 5.288141907183096]
	TIME [epoch: 10.3 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172637379331603		[learning rate: 0.0003321]
	Learning Rate: 0.000332096
	LOSS [training: 5.172637379331603 | validation: 5.286191978780903]
	TIME [epoch: 10.3 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176806177090457		[learning rate: 0.00033129]
	Learning Rate: 0.000331292
	LOSS [training: 5.176806177090457 | validation: 5.321370813367724]
	TIME [epoch: 10.3 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178465831108914		[learning rate: 0.00033049]
	Learning Rate: 0.00033049
	LOSS [training: 5.178465831108914 | validation: 5.292560603491949]
	TIME [epoch: 10.3 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184532068904953		[learning rate: 0.00032969]
	Learning Rate: 0.00032969
	LOSS [training: 5.184532068904953 | validation: 5.338122276143306]
	TIME [epoch: 10.3 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.161375334844886		[learning rate: 0.00032889]
	Learning Rate: 0.000328891
	LOSS [training: 5.161375334844886 | validation: 5.2860388156006755]
	TIME [epoch: 10.3 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181043970761936		[learning rate: 0.0003281]
	Learning Rate: 0.000328095
	LOSS [training: 5.181043970761936 | validation: 5.307452555570467]
	TIME [epoch: 10.3 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.21234012211212		[learning rate: 0.0003273]
	Learning Rate: 0.000327301
	LOSS [training: 5.21234012211212 | validation: 5.319162165086932]
	TIME [epoch: 10.3 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16270977062796		[learning rate: 0.00032651]
	Learning Rate: 0.000326509
	LOSS [training: 5.16270977062796 | validation: 5.28832493749629]
	TIME [epoch: 10.3 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.189592688572978		[learning rate: 0.00032572]
	Learning Rate: 0.000325718
	LOSS [training: 5.189592688572978 | validation: 5.290396907038842]
	TIME [epoch: 10.3 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.146845942460709		[learning rate: 0.00032493]
	Learning Rate: 0.00032493
	LOSS [training: 5.146845942460709 | validation: 5.279857628899705]
	TIME [epoch: 10.3 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180446813117707		[learning rate: 0.00032414]
	Learning Rate: 0.000324143
	LOSS [training: 5.180446813117707 | validation: 5.333053280140639]
	TIME [epoch: 10.3 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.162842958163433		[learning rate: 0.00032336]
	Learning Rate: 0.000323358
	LOSS [training: 5.162842958163433 | validation: 5.303415781710489]
	TIME [epoch: 10.3 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164822586575185		[learning rate: 0.00032258]
	Learning Rate: 0.000322576
	LOSS [training: 5.164822586575185 | validation: 5.2888033802300445]
	TIME [epoch: 10.3 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190118645612797		[learning rate: 0.00032179]
	Learning Rate: 0.000321795
	LOSS [training: 5.190118645612797 | validation: 5.296750914933933]
	TIME [epoch: 10.3 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.216497705928774		[learning rate: 0.00032102]
	Learning Rate: 0.000321016
	LOSS [training: 5.216497705928774 | validation: 5.309948784350362]
	TIME [epoch: 10.3 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175151690049074		[learning rate: 0.00032024]
	Learning Rate: 0.000320239
	LOSS [training: 5.175151690049074 | validation: 5.2990548769216]
	TIME [epoch: 10.3 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.141766449935984		[learning rate: 0.00031946]
	Learning Rate: 0.000319463
	LOSS [training: 5.141766449935984 | validation: 5.288380693041076]
	TIME [epoch: 10.3 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183446291466014		[learning rate: 0.00031869]
	Learning Rate: 0.00031869
	LOSS [training: 5.183446291466014 | validation: 5.284549416977711]
	TIME [epoch: 10.3 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16216809961645		[learning rate: 0.00031792]
	Learning Rate: 0.000317918
	LOSS [training: 5.16216809961645 | validation: 5.289329886578913]
	TIME [epoch: 10.3 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.199610711407443		[learning rate: 0.00031715]
	Learning Rate: 0.000317149
	LOSS [training: 5.199610711407443 | validation: 5.286218005551423]
	TIME [epoch: 10.3 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.195131396497946		[learning rate: 0.00031638]
	Learning Rate: 0.000316381
	LOSS [training: 5.195131396497946 | validation: 5.299734742652361]
	TIME [epoch: 10.3 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2051435789222005		[learning rate: 0.00031562]
	Learning Rate: 0.000315615
	LOSS [training: 5.2051435789222005 | validation: 5.288504318194625]
	TIME [epoch: 10.3 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18611585934972		[learning rate: 0.00031485]
	Learning Rate: 0.000314851
	LOSS [training: 5.18611585934972 | validation: 5.302280951310162]
	TIME [epoch: 10.3 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.208400494848872		[learning rate: 0.00031409]
	Learning Rate: 0.000314089
	LOSS [training: 5.208400494848872 | validation: 5.335723503671859]
	TIME [epoch: 10.3 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1725573219021275		[learning rate: 0.00031333]
	Learning Rate: 0.000313329
	LOSS [training: 5.1725573219021275 | validation: 5.29942198473168]
	TIME [epoch: 10.3 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163971585488883		[learning rate: 0.00031257]
	Learning Rate: 0.00031257
	LOSS [training: 5.163971585488883 | validation: 5.305697053039093]
	TIME [epoch: 10.3 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148063183031719		[learning rate: 0.00031181]
	Learning Rate: 0.000311813
	LOSS [training: 5.148063183031719 | validation: 5.300056497690272]
	TIME [epoch: 10.3 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.145249421736842		[learning rate: 0.00031106]
	Learning Rate: 0.000311058
	LOSS [training: 5.145249421736842 | validation: 5.287404146012995]
	TIME [epoch: 10.3 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148202808391372		[learning rate: 0.00031031]
	Learning Rate: 0.000310305
	LOSS [training: 5.148202808391372 | validation: 5.298111100272954]
	TIME [epoch: 10.3 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154372178962073		[learning rate: 0.00030955]
	Learning Rate: 0.000309554
	LOSS [training: 5.154372178962073 | validation: 5.284155270524882]
	TIME [epoch: 10.3 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.152931732549096		[learning rate: 0.0003088]
	Learning Rate: 0.000308805
	LOSS [training: 5.152931732549096 | validation: 5.280281598757212]
	TIME [epoch: 10.3 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182734047767173		[learning rate: 0.00030806]
	Learning Rate: 0.000308057
	LOSS [training: 5.182734047767173 | validation: 5.2937872394059164]
	TIME [epoch: 10.3 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155429499277695		[learning rate: 0.00030731]
	Learning Rate: 0.000307312
	LOSS [training: 5.155429499277695 | validation: 5.2826019362380565]
	TIME [epoch: 10.3 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.157380985028822		[learning rate: 0.00030657]
	Learning Rate: 0.000306568
	LOSS [training: 5.157380985028822 | validation: 5.299766463519318]
	TIME [epoch: 10.3 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172081567836367		[learning rate: 0.00030583]
	Learning Rate: 0.000305826
	LOSS [training: 5.172081567836367 | validation: 5.313658130274107]
	TIME [epoch: 10.3 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164725708743001		[learning rate: 0.00030509]
	Learning Rate: 0.000305085
	LOSS [training: 5.164725708743001 | validation: 5.297173584927853]
	TIME [epoch: 10.3 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.196882934488729		[learning rate: 0.00030435]
	Learning Rate: 0.000304347
	LOSS [training: 5.196882934488729 | validation: 5.276232215161376]
	TIME [epoch: 10.3 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1765196736870065		[learning rate: 0.00030361]
	Learning Rate: 0.00030361
	LOSS [training: 5.1765196736870065 | validation: 5.305378783143151]
	TIME [epoch: 10.3 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167290424739982		[learning rate: 0.00030287]
	Learning Rate: 0.000302875
	LOSS [training: 5.167290424739982 | validation: 5.373546727291342]
	TIME [epoch: 10.3 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.194976476775742		[learning rate: 0.00030214]
	Learning Rate: 0.000302142
	LOSS [training: 5.194976476775742 | validation: 5.293925596879468]
	TIME [epoch: 10.3 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197402346130459		[learning rate: 0.00030141]
	Learning Rate: 0.00030141
	LOSS [training: 5.197402346130459 | validation: 5.282724695425575]
	TIME [epoch: 10.3 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.198055761368792		[learning rate: 0.00030068]
	Learning Rate: 0.000300681
	LOSS [training: 5.198055761368792 | validation: 5.3082658498514546]
	TIME [epoch: 10.3 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.196565352013319		[learning rate: 0.00029995]
	Learning Rate: 0.000299953
	LOSS [training: 5.196565352013319 | validation: 5.285295226141558]
	TIME [epoch: 10.3 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171178341039843		[learning rate: 0.00029923]
	Learning Rate: 0.000299226
	LOSS [training: 5.171178341039843 | validation: 5.309482834213464]
	TIME [epoch: 10.3 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1677002034105355		[learning rate: 0.0002985]
	Learning Rate: 0.000298502
	LOSS [training: 5.1677002034105355 | validation: 5.307225842607012]
	TIME [epoch: 10.3 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172827866166474		[learning rate: 0.00029778]
	Learning Rate: 0.000297779
	LOSS [training: 5.172827866166474 | validation: 5.322187515270015]
	TIME [epoch: 10.3 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158171758599581		[learning rate: 0.00029706]
	Learning Rate: 0.000297059
	LOSS [training: 5.158171758599581 | validation: 5.3078646118165205]
	TIME [epoch: 10.3 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184869511718453		[learning rate: 0.00029634]
	Learning Rate: 0.000296339
	LOSS [training: 5.184869511718453 | validation: 5.288304539857912]
	TIME [epoch: 10.3 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1706221477009615		[learning rate: 0.00029562]
	Learning Rate: 0.000295622
	LOSS [training: 5.1706221477009615 | validation: 5.329085156893045]
	TIME [epoch: 10.3 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.224317524423817		[learning rate: 0.00029491]
	Learning Rate: 0.000294906
	LOSS [training: 5.224317524423817 | validation: 5.290750669357757]
	TIME [epoch: 10.3 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.189998269579801		[learning rate: 0.00029419]
	Learning Rate: 0.000294193
	LOSS [training: 5.189998269579801 | validation: 5.29536820631664]
	TIME [epoch: 10.3 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.226872602153499		[learning rate: 0.00029348]
	Learning Rate: 0.00029348
	LOSS [training: 5.226872602153499 | validation: 5.309315773391679]
	TIME [epoch: 10.3 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171752634008648		[learning rate: 0.00029277]
	Learning Rate: 0.00029277
	LOSS [training: 5.171752634008648 | validation: 5.290450447483992]
	TIME [epoch: 10.3 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.200945052072223		[learning rate: 0.00029206]
	Learning Rate: 0.000292061
	LOSS [training: 5.200945052072223 | validation: 5.31315202269381]
	TIME [epoch: 10.3 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164369618673385		[learning rate: 0.00029135]
	Learning Rate: 0.000291354
	LOSS [training: 5.164369618673385 | validation: 5.274292153182411]
	TIME [epoch: 10.3 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16640815047058		[learning rate: 0.00029065]
	Learning Rate: 0.000290649
	LOSS [training: 5.16640815047058 | validation: 5.291794051389553]
	TIME [epoch: 10.3 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180792997011214		[learning rate: 0.00028995]
	Learning Rate: 0.000289945
	LOSS [training: 5.180792997011214 | validation: 5.311252436749156]
	TIME [epoch: 10.3 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.187770413940043		[learning rate: 0.00028924]
	Learning Rate: 0.000289243
	LOSS [training: 5.187770413940043 | validation: 5.2884031784899035]
	TIME [epoch: 10.3 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179251431571231		[learning rate: 0.00028854]
	Learning Rate: 0.000288543
	LOSS [training: 5.179251431571231 | validation: 5.321554615988134]
	TIME [epoch: 10.3 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.230716782815992		[learning rate: 0.00028784]
	Learning Rate: 0.000287844
	LOSS [training: 5.230716782815992 | validation: 5.303872743710433]
	TIME [epoch: 10.3 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183185117156014		[learning rate: 0.00028715]
	Learning Rate: 0.000287148
	LOSS [training: 5.183185117156014 | validation: 5.283275717771325]
	TIME [epoch: 10.3 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184964475434057		[learning rate: 0.00028645]
	Learning Rate: 0.000286453
	LOSS [training: 5.184964475434057 | validation: 5.309385579211952]
	TIME [epoch: 10.3 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163192782903884		[learning rate: 0.00028576]
	Learning Rate: 0.000285759
	LOSS [training: 5.163192782903884 | validation: 5.306301028038049]
	TIME [epoch: 10.3 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197873839295897		[learning rate: 0.00028507]
	Learning Rate: 0.000285067
	LOSS [training: 5.197873839295897 | validation: 5.293841830143963]
	TIME [epoch: 10.3 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.187905983714097		[learning rate: 0.00028438]
	Learning Rate: 0.000284377
	LOSS [training: 5.187905983714097 | validation: 5.280155970032773]
	TIME [epoch: 10.3 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.222726127263059		[learning rate: 0.00028369]
	Learning Rate: 0.000283689
	LOSS [training: 5.222726127263059 | validation: 5.304254343867613]
	TIME [epoch: 10.3 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.202936161604578		[learning rate: 0.000283]
	Learning Rate: 0.000283002
	LOSS [training: 5.202936161604578 | validation: 5.290471833937882]
	TIME [epoch: 10.3 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.186201664735097		[learning rate: 0.00028232]
	Learning Rate: 0.000282317
	LOSS [training: 5.186201664735097 | validation: 5.294809443749414]
	TIME [epoch: 10.3 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1786789957269885		[learning rate: 0.00028163]
	Learning Rate: 0.000281633
	LOSS [training: 5.1786789957269885 | validation: 5.283872410760987]
	TIME [epoch: 10.3 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172068331459411		[learning rate: 0.00028095]
	Learning Rate: 0.000280952
	LOSS [training: 5.172068331459411 | validation: 5.289737922811222]
	TIME [epoch: 10.3 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183457071798188		[learning rate: 0.00028027]
	Learning Rate: 0.000280272
	LOSS [training: 5.183457071798188 | validation: 5.289564090361123]
	TIME [epoch: 10.3 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175602681697778		[learning rate: 0.00027959]
	Learning Rate: 0.000279593
	LOSS [training: 5.175602681697778 | validation: 5.306182142001611]
	TIME [epoch: 10.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183479703727189		[learning rate: 0.00027892]
	Learning Rate: 0.000278916
	LOSS [training: 5.183479703727189 | validation: 5.313747075723483]
	TIME [epoch: 10.3 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190785912470113		[learning rate: 0.00027824]
	Learning Rate: 0.000278241
	LOSS [training: 5.190785912470113 | validation: 5.295692081140141]
	TIME [epoch: 10.3 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.210572228343671		[learning rate: 0.00027757]
	Learning Rate: 0.000277567
	LOSS [training: 5.210572228343671 | validation: 5.29971059584404]
	TIME [epoch: 10.3 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159397807438575		[learning rate: 0.0002769]
	Learning Rate: 0.000276895
	LOSS [training: 5.159397807438575 | validation: 5.299198897572905]
	TIME [epoch: 10.3 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.162931066326027		[learning rate: 0.00027623]
	Learning Rate: 0.000276225
	LOSS [training: 5.162931066326027 | validation: 5.306995618296692]
	TIME [epoch: 10.3 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.195095024916122		[learning rate: 0.00027556]
	Learning Rate: 0.000275556
	LOSS [training: 5.195095024916122 | validation: 5.285698573411757]
	TIME [epoch: 10.3 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.143190662891357		[learning rate: 0.00027489]
	Learning Rate: 0.000274889
	LOSS [training: 5.143190662891357 | validation: 5.315153751211484]
	TIME [epoch: 10.3 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184557035736692		[learning rate: 0.00027422]
	Learning Rate: 0.000274224
	LOSS [training: 5.184557035736692 | validation: 5.288167115135876]
	TIME [epoch: 10.3 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1940948056266425		[learning rate: 0.00027356]
	Learning Rate: 0.00027356
	LOSS [training: 5.1940948056266425 | validation: 5.292229599310749]
	TIME [epoch: 10.3 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1641623969477575		[learning rate: 0.0002729]
	Learning Rate: 0.000272898
	LOSS [training: 5.1641623969477575 | validation: 5.291837168618245]
	TIME [epoch: 10.3 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.187584399763287		[learning rate: 0.00027224]
	Learning Rate: 0.000272237
	LOSS [training: 5.187584399763287 | validation: 5.329931771443053]
	TIME [epoch: 10.3 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191282946550267		[learning rate: 0.00027158]
	Learning Rate: 0.000271578
	LOSS [training: 5.191282946550267 | validation: 5.295069012747647]
	TIME [epoch: 10.3 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181789937369596		[learning rate: 0.00027092]
	Learning Rate: 0.000270921
	LOSS [training: 5.181789937369596 | validation: 5.296405017655288]
	TIME [epoch: 10.3 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.169098713418469		[learning rate: 0.00027026]
	Learning Rate: 0.000270265
	LOSS [training: 5.169098713418469 | validation: 5.312783399438201]
	TIME [epoch: 10.3 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183128003497172		[learning rate: 0.00026961]
	Learning Rate: 0.000269611
	LOSS [training: 5.183128003497172 | validation: 5.295438997941646]
	TIME [epoch: 10.3 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182494669976697		[learning rate: 0.00026896]
	Learning Rate: 0.000268958
	LOSS [training: 5.182494669976697 | validation: 5.293562253771865]
	TIME [epoch: 10.3 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16114768031216		[learning rate: 0.00026831]
	Learning Rate: 0.000268307
	LOSS [training: 5.16114768031216 | validation: 5.296581005519441]
	TIME [epoch: 10.3 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1658396615909865		[learning rate: 0.00026766]
	Learning Rate: 0.000267657
	LOSS [training: 5.1658396615909865 | validation: 5.287378477668326]
	TIME [epoch: 10.3 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15346029762783		[learning rate: 0.00026701]
	Learning Rate: 0.000267009
	LOSS [training: 5.15346029762783 | validation: 5.285552701466846]
	TIME [epoch: 10.3 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175030606612204		[learning rate: 0.00026636]
	Learning Rate: 0.000266363
	LOSS [training: 5.175030606612204 | validation: 5.291549764970939]
	TIME [epoch: 10.3 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172468729796734		[learning rate: 0.00026572]
	Learning Rate: 0.000265718
	LOSS [training: 5.172468729796734 | validation: 5.308527705284625]
	TIME [epoch: 10.3 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.205229358611538		[learning rate: 0.00026507]
	Learning Rate: 0.000265075
	LOSS [training: 5.205229358611538 | validation: 5.290872361126486]
	TIME [epoch: 10.3 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184688968461925		[learning rate: 0.00026443]
	Learning Rate: 0.000264433
	LOSS [training: 5.184688968461925 | validation: 5.297686468749056]
	TIME [epoch: 10.3 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181239372841383		[learning rate: 0.00026379]
	Learning Rate: 0.000263793
	LOSS [training: 5.181239372841383 | validation: 5.28480120459185]
	TIME [epoch: 10.3 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154755025990218		[learning rate: 0.00026315]
	Learning Rate: 0.000263154
	LOSS [training: 5.154755025990218 | validation: 5.290150658498313]
	TIME [epoch: 10.3 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1776088697860505		[learning rate: 0.00026252]
	Learning Rate: 0.000262517
	LOSS [training: 5.1776088697860505 | validation: 5.278827998127588]
	TIME [epoch: 10.3 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1639440560158185		[learning rate: 0.00026188]
	Learning Rate: 0.000261882
	LOSS [training: 5.1639440560158185 | validation: 5.296450994370166]
	TIME [epoch: 10.3 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1795439341380085		[learning rate: 0.00026125]
	Learning Rate: 0.000261248
	LOSS [training: 5.1795439341380085 | validation: 5.30095502195266]
	TIME [epoch: 10.3 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.207862815017823		[learning rate: 0.00026062]
	Learning Rate: 0.000260615
	LOSS [training: 5.207862815017823 | validation: 5.294534620321099]
	TIME [epoch: 10.3 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164930683336739		[learning rate: 0.00025998]
	Learning Rate: 0.000259984
	LOSS [training: 5.164930683336739 | validation: 5.2864853113717425]
	TIME [epoch: 10.3 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167773447380535		[learning rate: 0.00025936]
	Learning Rate: 0.000259355
	LOSS [training: 5.167773447380535 | validation: 5.2761781447659555]
	TIME [epoch: 10.3 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.187351925728035		[learning rate: 0.00025873]
	Learning Rate: 0.000258727
	LOSS [training: 5.187351925728035 | validation: 5.290055599146842]
	TIME [epoch: 10.3 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18253085031372		[learning rate: 0.0002581]
	Learning Rate: 0.000258101
	LOSS [training: 5.18253085031372 | validation: 5.282405019754695]
	TIME [epoch: 10.3 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184238583332386		[learning rate: 0.00025748]
	Learning Rate: 0.000257476
	LOSS [training: 5.184238583332386 | validation: 5.286674965868183]
	TIME [epoch: 10.3 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.216257989529707		[learning rate: 0.00025685]
	Learning Rate: 0.000256853
	LOSS [training: 5.216257989529707 | validation: 5.300945655152955]
	TIME [epoch: 10.3 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1869881940925335		[learning rate: 0.00025623]
	Learning Rate: 0.000256231
	LOSS [training: 5.1869881940925335 | validation: 5.299398752195047]
	TIME [epoch: 10.3 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192640038961083		[learning rate: 0.00025561]
	Learning Rate: 0.000255611
	LOSS [training: 5.192640038961083 | validation: 5.2952003133307235]
	TIME [epoch: 10.3 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190746959999155		[learning rate: 0.00025499]
	Learning Rate: 0.000254992
	LOSS [training: 5.190746959999155 | validation: 5.285208912237415]
	TIME [epoch: 10.3 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2102453910075		[learning rate: 0.00025437]
	Learning Rate: 0.000254375
	LOSS [training: 5.2102453910075 | validation: 5.307947260043631]
	TIME [epoch: 10.3 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.220007742898304		[learning rate: 0.00025376]
	Learning Rate: 0.000253759
	LOSS [training: 5.220007742898304 | validation: 5.321514634930683]
	TIME [epoch: 10.3 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176268721028109		[learning rate: 0.00025314]
	Learning Rate: 0.000253144
	LOSS [training: 5.176268721028109 | validation: 5.314124968315229]
	TIME [epoch: 10.3 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.195404070443085		[learning rate: 0.00025253]
	Learning Rate: 0.000252532
	LOSS [training: 5.195404070443085 | validation: 5.290986449967281]
	TIME [epoch: 10.3 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1771124749487		[learning rate: 0.00025192]
	Learning Rate: 0.00025192
	LOSS [training: 5.1771124749487 | validation: 5.294180408083173]
	TIME [epoch: 10.3 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167587833883133		[learning rate: 0.00025131]
	Learning Rate: 0.00025131
	LOSS [training: 5.167587833883133 | validation: 5.361988740852503]
	TIME [epoch: 10.3 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.209261112834524		[learning rate: 0.0002507]
	Learning Rate: 0.000250702
	LOSS [training: 5.209261112834524 | validation: 5.291201420497032]
	TIME [epoch: 10.3 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.141589237134367		[learning rate: 0.0002501]
	Learning Rate: 0.000250095
	LOSS [training: 5.141589237134367 | validation: 5.272834262054665]
	TIME [epoch: 10.3 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15631216625685		[learning rate: 0.00024949]
	Learning Rate: 0.00024949
	LOSS [training: 5.15631216625685 | validation: 5.296362899535636]
	TIME [epoch: 10.3 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153126765961247		[learning rate: 0.00024889]
	Learning Rate: 0.000248886
	LOSS [training: 5.153126765961247 | validation: 5.291537170153714]
	TIME [epoch: 10.3 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151397660054722		[learning rate: 0.00024828]
	Learning Rate: 0.000248283
	LOSS [training: 5.151397660054722 | validation: 5.288021459176888]
	TIME [epoch: 10.3 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179445910326603		[learning rate: 0.00024768]
	Learning Rate: 0.000247682
	LOSS [training: 5.179445910326603 | validation: 5.2979829334304895]
	TIME [epoch: 10.3 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182824983094292		[learning rate: 0.00024708]
	Learning Rate: 0.000247083
	LOSS [training: 5.182824983094292 | validation: 5.293523113852483]
	TIME [epoch: 10.3 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14664106268757		[learning rate: 0.00024648]
	Learning Rate: 0.000246484
	LOSS [training: 5.14664106268757 | validation: 5.288528526844726]
	TIME [epoch: 10.3 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1770687958355195		[learning rate: 0.00024589]
	Learning Rate: 0.000245888
	LOSS [training: 5.1770687958355195 | validation: 5.285191613881047]
	TIME [epoch: 10.3 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.141943815534003		[learning rate: 0.00024529]
	Learning Rate: 0.000245292
	LOSS [training: 5.141943815534003 | validation: 5.2933950727298065]
	TIME [epoch: 10.3 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.147604632984295		[learning rate: 0.0002447]
	Learning Rate: 0.000244699
	LOSS [training: 5.147604632984295 | validation: 5.2928558768703216]
	TIME [epoch: 10.3 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172679340140968		[learning rate: 0.00024411]
	Learning Rate: 0.000244106
	LOSS [training: 5.172679340140968 | validation: 5.297655475166145]
	TIME [epoch: 10.3 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191579667168218		[learning rate: 0.00024352]
	Learning Rate: 0.000243515
	LOSS [training: 5.191579667168218 | validation: 5.294219300077027]
	TIME [epoch: 10.3 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166594746894841		[learning rate: 0.00024293]
	Learning Rate: 0.000242926
	LOSS [training: 5.166594746894841 | validation: 5.295521175486255]
	TIME [epoch: 10.3 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177846275124847		[learning rate: 0.00024234]
	Learning Rate: 0.000242338
	LOSS [training: 5.177846275124847 | validation: 5.277652808453849]
	TIME [epoch: 10.3 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184784206106918		[learning rate: 0.00024175]
	Learning Rate: 0.000241751
	LOSS [training: 5.184784206106918 | validation: 5.304763461332884]
	TIME [epoch: 10.3 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.145980662944493		[learning rate: 0.00024117]
	Learning Rate: 0.000241166
	LOSS [training: 5.145980662944493 | validation: 5.3104089714250815]
	TIME [epoch: 10.3 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156694223734096		[learning rate: 0.00024058]
	Learning Rate: 0.000240582
	LOSS [training: 5.156694223734096 | validation: 5.295136598585972]
	TIME [epoch: 10.3 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155949661895095		[learning rate: 0.00024]
	Learning Rate: 0.00024
	LOSS [training: 5.155949661895095 | validation: 5.280333540967072]
	TIME [epoch: 10.3 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170734728191785		[learning rate: 0.00023942]
	Learning Rate: 0.000239419
	LOSS [training: 5.170734728191785 | validation: 5.311714003078472]
	TIME [epoch: 10.3 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166270501698837		[learning rate: 0.00023884]
	Learning Rate: 0.000238839
	LOSS [training: 5.166270501698837 | validation: 5.286671910289881]
	TIME [epoch: 10.3 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151131172055796		[learning rate: 0.00023826]
	Learning Rate: 0.000238261
	LOSS [training: 5.151131172055796 | validation: 5.282687662281685]
	TIME [epoch: 10.3 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16683747065578		[learning rate: 0.00023768]
	Learning Rate: 0.000237684
	LOSS [training: 5.16683747065578 | validation: 5.296498976676345]
	TIME [epoch: 10.3 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.168708554608948		[learning rate: 0.00023711]
	Learning Rate: 0.000237109
	LOSS [training: 5.168708554608948 | validation: 5.2744753012408]
	TIME [epoch: 10.3 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.149146513527291		[learning rate: 0.00023653]
	Learning Rate: 0.000236535
	LOSS [training: 5.149146513527291 | validation: 5.269510217201236]
	TIME [epoch: 10.3 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.150375878204344		[learning rate: 0.00023596]
	Learning Rate: 0.000235962
	LOSS [training: 5.150375878204344 | validation: 5.287295606601573]
	TIME [epoch: 10.3 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1518282199584835		[learning rate: 0.00023539]
	Learning Rate: 0.000235391
	LOSS [training: 5.1518282199584835 | validation: 5.274212980218504]
	TIME [epoch: 10.3 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.189610971072482		[learning rate: 0.00023482]
	Learning Rate: 0.000234821
	LOSS [training: 5.189610971072482 | validation: 5.279514474486573]
	TIME [epoch: 10.3 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.130781911590549		[learning rate: 0.00023425]
	Learning Rate: 0.000234252
	LOSS [training: 5.130781911590549 | validation: 5.30633419189737]
	TIME [epoch: 10.3 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179314626154134		[learning rate: 0.00023369]
	Learning Rate: 0.000233685
	LOSS [training: 5.179314626154134 | validation: 5.27408519528396]
	TIME [epoch: 10.3 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166367497650899		[learning rate: 0.00023312]
	Learning Rate: 0.00023312
	LOSS [training: 5.166367497650899 | validation: 5.275960894406231]
	TIME [epoch: 10.3 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153936680140449		[learning rate: 0.00023256]
	Learning Rate: 0.000232555
	LOSS [training: 5.153936680140449 | validation: 5.284050691841919]
	TIME [epoch: 10.3 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151760746063992		[learning rate: 0.00023199]
	Learning Rate: 0.000231992
	LOSS [training: 5.151760746063992 | validation: 5.269931963882357]
	TIME [epoch: 10.3 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.134884218942517		[learning rate: 0.00023143]
	Learning Rate: 0.000231431
	LOSS [training: 5.134884218942517 | validation: 5.286172234809574]
	TIME [epoch: 10.3 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165052542903957		[learning rate: 0.00023087]
	Learning Rate: 0.00023087
	LOSS [training: 5.165052542903957 | validation: 5.28753640626724]
	TIME [epoch: 10.3 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.131407965303923		[learning rate: 0.00023031]
	Learning Rate: 0.000230312
	LOSS [training: 5.131407965303923 | validation: 5.297388277737354]
	TIME [epoch: 10.3 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.147763233512163		[learning rate: 0.00022975]
	Learning Rate: 0.000229754
	LOSS [training: 5.147763233512163 | validation: 5.279661081285406]
	TIME [epoch: 10.3 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.147021754025853		[learning rate: 0.0002292]
	Learning Rate: 0.000229198
	LOSS [training: 5.147021754025853 | validation: 5.301294013435625]
	TIME [epoch: 10.3 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148111166704733		[learning rate: 0.00022864]
	Learning Rate: 0.000228643
	LOSS [training: 5.148111166704733 | validation: 5.2824543872754735]
	TIME [epoch: 10.3 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154886798670461		[learning rate: 0.00022809]
	Learning Rate: 0.000228089
	LOSS [training: 5.154886798670461 | validation: 5.289215176568951]
	TIME [epoch: 10.3 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1594489184700345		[learning rate: 0.00022754]
	Learning Rate: 0.000227537
	LOSS [training: 5.1594489184700345 | validation: 5.2865457519395465]
	TIME [epoch: 10.3 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170734154711756		[learning rate: 0.00022699]
	Learning Rate: 0.000226987
	LOSS [training: 5.170734154711756 | validation: 5.305093507508748]
	TIME [epoch: 10.3 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.134932462123897		[learning rate: 0.00022644]
	Learning Rate: 0.000226437
	LOSS [training: 5.134932462123897 | validation: 5.279219342821248]
	TIME [epoch: 10.3 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.149357595215258		[learning rate: 0.00022589]
	Learning Rate: 0.000225889
	LOSS [training: 5.149357595215258 | validation: 5.27509252954389]
	TIME [epoch: 10.3 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156755961312198		[learning rate: 0.00022534]
	Learning Rate: 0.000225342
	LOSS [training: 5.156755961312198 | validation: 5.296703991739851]
	TIME [epoch: 10.3 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.150306195906721		[learning rate: 0.0002248]
	Learning Rate: 0.000224796
	LOSS [training: 5.150306195906721 | validation: 5.287353547927531]
	TIME [epoch: 10.3 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.136621666393141		[learning rate: 0.00022425]
	Learning Rate: 0.000224252
	LOSS [training: 5.136621666393141 | validation: 5.283629104865279]
	TIME [epoch: 10.3 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.131547892222		[learning rate: 0.00022371]
	Learning Rate: 0.000223709
	LOSS [training: 5.131547892222 | validation: 5.286116452667605]
	TIME [epoch: 10.3 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14319553971305		[learning rate: 0.00022317]
	Learning Rate: 0.000223168
	LOSS [training: 5.14319553971305 | validation: 5.286744949162693]
	TIME [epoch: 10.3 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.144492439659952		[learning rate: 0.00022263]
	Learning Rate: 0.000222628
	LOSS [training: 5.144492439659952 | validation: 5.292463167440444]
	TIME [epoch: 10.3 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.13547763596217		[learning rate: 0.00022209]
	Learning Rate: 0.000222089
	LOSS [training: 5.13547763596217 | validation: 5.288107653037695]
	TIME [epoch: 10.3 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164421208238748		[learning rate: 0.00022155]
	Learning Rate: 0.000221551
	LOSS [training: 5.164421208238748 | validation: 5.29370496215008]
	TIME [epoch: 10.3 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15387544644106		[learning rate: 0.00022101]
	Learning Rate: 0.000221015
	LOSS [training: 5.15387544644106 | validation: 5.296814011514167]
	TIME [epoch: 10.3 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.140901321063395		[learning rate: 0.00022048]
	Learning Rate: 0.00022048
	LOSS [training: 5.140901321063395 | validation: 5.288947580314301]
	TIME [epoch: 10.3 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184225893994108		[learning rate: 0.00021995]
	Learning Rate: 0.000219946
	LOSS [training: 5.184225893994108 | validation: 5.294355469602013]
	TIME [epoch: 10.3 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.17194104655698		[learning rate: 0.00021941]
	Learning Rate: 0.000219413
	LOSS [training: 5.17194104655698 | validation: 5.2774888811095435]
	TIME [epoch: 10.3 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.195808368101682		[learning rate: 0.00021888]
	Learning Rate: 0.000218882
	LOSS [training: 5.195808368101682 | validation: 5.293541348137302]
	TIME [epoch: 10.3 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.162245078257851		[learning rate: 0.00021835]
	Learning Rate: 0.000218352
	LOSS [training: 5.162245078257851 | validation: 5.293544072076688]
	TIME [epoch: 10.3 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16868802291035		[learning rate: 0.00021782]
	Learning Rate: 0.000217824
	LOSS [training: 5.16868802291035 | validation: 5.296526371743296]
	TIME [epoch: 10.3 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167406294458202		[learning rate: 0.0002173]
	Learning Rate: 0.000217296
	LOSS [training: 5.167406294458202 | validation: 5.275619267973979]
	TIME [epoch: 10.3 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14164316499931		[learning rate: 0.00021677]
	Learning Rate: 0.00021677
	LOSS [training: 5.14164316499931 | validation: 5.285260094806762]
	TIME [epoch: 10.3 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182920011753729		[learning rate: 0.00021625]
	Learning Rate: 0.000216246
	LOSS [training: 5.182920011753729 | validation: 5.282208726062562]
	TIME [epoch: 10.3 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1688871407405586		[learning rate: 0.00021572]
	Learning Rate: 0.000215722
	LOSS [training: 5.1688871407405586 | validation: 5.303541597162844]
	TIME [epoch: 10.3 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.21199730480426		[learning rate: 0.0002152]
	Learning Rate: 0.0002152
	LOSS [training: 5.21199730480426 | validation: 5.298912307604623]
	TIME [epoch: 10.3 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.152579821290995		[learning rate: 0.00021468]
	Learning Rate: 0.000214679
	LOSS [training: 5.152579821290995 | validation: 5.273657452316735]
	TIME [epoch: 10.3 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183744669604116		[learning rate: 0.00021416]
	Learning Rate: 0.000214159
	LOSS [training: 5.183744669604116 | validation: 5.280307064342241]
	TIME [epoch: 10.3 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177795852610625		[learning rate: 0.00021364]
	Learning Rate: 0.000213641
	LOSS [training: 5.177795852610625 | validation: 5.294631335098934]
	TIME [epoch: 10.3 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1729639004546		[learning rate: 0.00021312]
	Learning Rate: 0.000213124
	LOSS [training: 5.1729639004546 | validation: 5.297683940604911]
	TIME [epoch: 10.3 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14951458542197		[learning rate: 0.00021261]
	Learning Rate: 0.000212608
	LOSS [training: 5.14951458542197 | validation: 5.290521392766732]
	TIME [epoch: 10.3 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153253263343876		[learning rate: 0.00021209]
	Learning Rate: 0.000212093
	LOSS [training: 5.153253263343876 | validation: 5.2957375304706105]
	TIME [epoch: 10.3 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191574621457521		[learning rate: 0.00021158]
	Learning Rate: 0.00021158
	LOSS [training: 5.191574621457521 | validation: 5.292110465445794]
	TIME [epoch: 10.3 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16126434919098		[learning rate: 0.00021107]
	Learning Rate: 0.000211067
	LOSS [training: 5.16126434919098 | validation: 5.297606434157429]
	TIME [epoch: 10.3 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.149973574404008		[learning rate: 0.00021056]
	Learning Rate: 0.000210556
	LOSS [training: 5.149973574404008 | validation: 5.289916872566614]
	TIME [epoch: 10.3 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155610429133445		[learning rate: 0.00021005]
	Learning Rate: 0.000210047
	LOSS [training: 5.155610429133445 | validation: 5.288874505478641]
	TIME [epoch: 10.3 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167757788679738		[learning rate: 0.00020954]
	Learning Rate: 0.000209538
	LOSS [training: 5.167757788679738 | validation: 5.317761296371731]
	TIME [epoch: 10.3 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154047445505527		[learning rate: 0.00020903]
	Learning Rate: 0.000209031
	LOSS [training: 5.154047445505527 | validation: 5.306831818706091]
	TIME [epoch: 10.3 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.187379573528348		[learning rate: 0.00020852]
	Learning Rate: 0.000208525
	LOSS [training: 5.187379573528348 | validation: 5.307389371776963]
	TIME [epoch: 10.3 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1568286836449335		[learning rate: 0.00020802]
	Learning Rate: 0.00020802
	LOSS [training: 5.1568286836449335 | validation: 5.282801191832639]
	TIME [epoch: 10.3 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165639209852669		[learning rate: 0.00020752]
	Learning Rate: 0.000207516
	LOSS [training: 5.165639209852669 | validation: 5.29420227814828]
	TIME [epoch: 10.3 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15469843033428		[learning rate: 0.00020701]
	Learning Rate: 0.000207014
	LOSS [training: 5.15469843033428 | validation: 5.296241653409886]
	TIME [epoch: 10.3 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.147308266789441		[learning rate: 0.00020651]
	Learning Rate: 0.000206513
	LOSS [training: 5.147308266789441 | validation: 5.282070551633886]
	TIME [epoch: 10.3 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153970705226493		[learning rate: 0.00020601]
	Learning Rate: 0.000206013
	LOSS [training: 5.153970705226493 | validation: 5.2794554773312745]
	TIME [epoch: 10.3 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158188291184442		[learning rate: 0.00020551]
	Learning Rate: 0.000205514
	LOSS [training: 5.158188291184442 | validation: 5.295642269412018]
	TIME [epoch: 10.3 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164765430640487		[learning rate: 0.00020502]
	Learning Rate: 0.000205017
	LOSS [training: 5.164765430640487 | validation: 5.277128161558457]
	TIME [epoch: 10.3 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151775531354472		[learning rate: 0.00020452]
	Learning Rate: 0.000204521
	LOSS [training: 5.151775531354472 | validation: 5.284710534719582]
	TIME [epoch: 10.3 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172523518789154		[learning rate: 0.00020403]
	Learning Rate: 0.000204025
	LOSS [training: 5.172523518789154 | validation: 5.295519807524771]
	TIME [epoch: 10.3 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163824122975951		[learning rate: 0.00020353]
	Learning Rate: 0.000203531
	LOSS [training: 5.163824122975951 | validation: 5.274109183978936]
	TIME [epoch: 10.3 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1785919041655095		[learning rate: 0.00020304]
	Learning Rate: 0.000203039
	LOSS [training: 5.1785919041655095 | validation: 5.285236306712537]
	TIME [epoch: 10.3 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.145221655109516		[learning rate: 0.00020255]
	Learning Rate: 0.000202547
	LOSS [training: 5.145221655109516 | validation: 5.294927017038827]
	TIME [epoch: 10.3 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.152942861402618		[learning rate: 0.00020206]
	Learning Rate: 0.000202057
	LOSS [training: 5.152942861402618 | validation: 5.272821502062997]
	TIME [epoch: 10.3 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155943456462819		[learning rate: 0.00020157]
	Learning Rate: 0.000201568
	LOSS [training: 5.155943456462819 | validation: 5.279734757969169]
	TIME [epoch: 10.3 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14274076397241		[learning rate: 0.00020108]
	Learning Rate: 0.00020108
	LOSS [training: 5.14274076397241 | validation: 5.283447132872358]
	TIME [epoch: 10.3 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155373797878829		[learning rate: 0.00020059]
	Learning Rate: 0.000200593
	LOSS [training: 5.155373797878829 | validation: 5.289279054147613]
	TIME [epoch: 10.3 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.186180709302121		[learning rate: 0.00020011]
	Learning Rate: 0.000200107
	LOSS [training: 5.186180709302121 | validation: 5.29661358128299]
	TIME [epoch: 10.3 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16416046804037		[learning rate: 0.00019962]
	Learning Rate: 0.000199623
	LOSS [training: 5.16416046804037 | validation: 5.289954081895112]
	TIME [epoch: 10.3 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172148559602886		[learning rate: 0.00019914]
	Learning Rate: 0.00019914
	LOSS [training: 5.172148559602886 | validation: 5.3009229861799865]
	TIME [epoch: 10.3 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178335680092127		[learning rate: 0.00019866]
	Learning Rate: 0.000198658
	LOSS [training: 5.178335680092127 | validation: 5.280008898508204]
	TIME [epoch: 10.3 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.147296248202488		[learning rate: 0.00019818]
	Learning Rate: 0.000198177
	LOSS [training: 5.147296248202488 | validation: 5.290509996493365]
	TIME [epoch: 10.3 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.203309073021407		[learning rate: 0.0001977]
	Learning Rate: 0.000197697
	LOSS [training: 5.203309073021407 | validation: 5.291634991629585]
	TIME [epoch: 10.3 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177007684434199		[learning rate: 0.00019722]
	Learning Rate: 0.000197218
	LOSS [training: 5.177007684434199 | validation: 5.28752790442446]
	TIME [epoch: 10.3 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180992571870449		[learning rate: 0.00019674]
	Learning Rate: 0.000196741
	LOSS [training: 5.180992571870449 | validation: 5.25368059339487]
	TIME [epoch: 10.3 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192642151865749		[learning rate: 0.00019626]
	Learning Rate: 0.000196265
	LOSS [training: 5.192642151865749 | validation: 5.281781396764349]
	TIME [epoch: 10.3 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171532248777851		[learning rate: 0.00019579]
	Learning Rate: 0.00019579
	LOSS [training: 5.171532248777851 | validation: 5.284970724016359]
	TIME [epoch: 10.3 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1768792521951426		[learning rate: 0.00019532]
	Learning Rate: 0.000195316
	LOSS [training: 5.1768792521951426 | validation: 5.3205167465704255]
	TIME [epoch: 10.3 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.196921094028587		[learning rate: 0.00019484]
	Learning Rate: 0.000194843
	LOSS [training: 5.196921094028587 | validation: 5.310162405148569]
	TIME [epoch: 10.3 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2129620248777595		[learning rate: 0.00019437]
	Learning Rate: 0.000194371
	LOSS [training: 5.2129620248777595 | validation: 5.3038577312557385]
	TIME [epoch: 10.3 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159001839621044		[learning rate: 0.0001939]
	Learning Rate: 0.000193901
	LOSS [training: 5.159001839621044 | validation: 5.286047584242906]
	TIME [epoch: 10.3 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178783754317875		[learning rate: 0.00019343]
	Learning Rate: 0.000193431
	LOSS [training: 5.178783754317875 | validation: 5.263678490473256]
	TIME [epoch: 10.3 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181829520677598		[learning rate: 0.00019296]
	Learning Rate: 0.000192963
	LOSS [training: 5.181829520677598 | validation: 5.300517767665313]
	TIME [epoch: 10.3 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179279846717816		[learning rate: 0.0001925]
	Learning Rate: 0.000192496
	LOSS [training: 5.179279846717816 | validation: 5.303602653480399]
	TIME [epoch: 10.3 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.185152731845145		[learning rate: 0.00019203]
	Learning Rate: 0.00019203
	LOSS [training: 5.185152731845145 | validation: 5.298549214440402]
	TIME [epoch: 10.3 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.174806257178162		[learning rate: 0.00019156]
	Learning Rate: 0.000191565
	LOSS [training: 5.174806257178162 | validation: 5.287675565169681]
	TIME [epoch: 10.3 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184719050454276		[learning rate: 0.0001911]
	Learning Rate: 0.000191101
	LOSS [training: 5.184719050454276 | validation: 5.278728043361037]
	TIME [epoch: 10.3 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.186065605578038		[learning rate: 0.00019064]
	Learning Rate: 0.000190638
	LOSS [training: 5.186065605578038 | validation: 5.304112866045998]
	TIME [epoch: 10.3 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172594568706809		[learning rate: 0.00019018]
	Learning Rate: 0.000190177
	LOSS [training: 5.172594568706809 | validation: 5.291004064961363]
	TIME [epoch: 10.3 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165061879066983		[learning rate: 0.00018972]
	Learning Rate: 0.000189717
	LOSS [training: 5.165061879066983 | validation: 5.3064191558569425]
	TIME [epoch: 10.3 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1805063961662245		[learning rate: 0.00018926]
	Learning Rate: 0.000189257
	LOSS [training: 5.1805063961662245 | validation: 5.286936892815679]
	TIME [epoch: 10.3 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.173735406905638		[learning rate: 0.0001888]
	Learning Rate: 0.000188799
	LOSS [training: 5.173735406905638 | validation: 5.27800852976128]
	TIME [epoch: 10.3 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164460207493136		[learning rate: 0.00018834]
	Learning Rate: 0.000188342
	LOSS [training: 5.164460207493136 | validation: 5.28653791590835]
	TIME [epoch: 10.3 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.211670874713505		[learning rate: 0.00018789]
	Learning Rate: 0.000187886
	LOSS [training: 5.211670874713505 | validation: 5.3007098064828435]
	TIME [epoch: 10.3 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1668716678147035		[learning rate: 0.00018743]
	Learning Rate: 0.000187431
	LOSS [training: 5.1668716678147035 | validation: 5.29841533137284]
	TIME [epoch: 10.3 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154312326680783		[learning rate: 0.00018698]
	Learning Rate: 0.000186978
	LOSS [training: 5.154312326680783 | validation: 5.2919809371154205]
	TIME [epoch: 10.3 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1636062946986545		[learning rate: 0.00018652]
	Learning Rate: 0.000186525
	LOSS [training: 5.1636062946986545 | validation: 5.278710586532363]
	TIME [epoch: 10.3 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.20239713421728		[learning rate: 0.00018607]
	Learning Rate: 0.000186073
	LOSS [training: 5.20239713421728 | validation: 5.290508400519939]
	TIME [epoch: 10.3 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.174634499547649		[learning rate: 0.00018562]
	Learning Rate: 0.000185623
	LOSS [training: 5.174634499547649 | validation: 5.310665398301766]
	TIME [epoch: 10.3 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1731851691808455		[learning rate: 0.00018517]
	Learning Rate: 0.000185174
	LOSS [training: 5.1731851691808455 | validation: 5.307478910927222]
	TIME [epoch: 10.3 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1688005153800605		[learning rate: 0.00018473]
	Learning Rate: 0.000184725
	LOSS [training: 5.1688005153800605 | validation: 5.285299657414294]
	TIME [epoch: 10.3 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151492492456755		[learning rate: 0.00018428]
	Learning Rate: 0.000184278
	LOSS [training: 5.151492492456755 | validation: 5.281975444588119]
	TIME [epoch: 10.3 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132470076109175		[learning rate: 0.00018383]
	Learning Rate: 0.000183832
	LOSS [training: 5.132470076109175 | validation: 5.295086727255307]
	TIME [epoch: 10.3 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148321766204594		[learning rate: 0.00018339]
	Learning Rate: 0.000183387
	LOSS [training: 5.148321766204594 | validation: 5.286090249169805]
	TIME [epoch: 10.3 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170897015491722		[learning rate: 0.00018294]
	Learning Rate: 0.000182943
	LOSS [training: 5.170897015491722 | validation: 5.2673611940418805]
	TIME [epoch: 10.3 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154028464806796		[learning rate: 0.0001825]
	Learning Rate: 0.0001825
	LOSS [training: 5.154028464806796 | validation: 5.275508607239897]
	TIME [epoch: 10.3 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.169827753822339		[learning rate: 0.00018206]
	Learning Rate: 0.000182058
	LOSS [training: 5.169827753822339 | validation: 5.278872528515028]
	TIME [epoch: 10.3 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.161591696123337		[learning rate: 0.00018162]
	Learning Rate: 0.000181618
	LOSS [training: 5.161591696123337 | validation: 5.285015250403136]
	TIME [epoch: 10.3 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.161914327440256		[learning rate: 0.00018118]
	Learning Rate: 0.000181178
	LOSS [training: 5.161914327440256 | validation: 5.278216893176522]
	TIME [epoch: 10.3 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191790633673663		[learning rate: 0.00018074]
	Learning Rate: 0.000180739
	LOSS [training: 5.191790633673663 | validation: 5.2893073200282785]
	TIME [epoch: 10.3 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165041778887055		[learning rate: 0.0001803]
	Learning Rate: 0.000180302
	LOSS [training: 5.165041778887055 | validation: 5.293453690816551]
	TIME [epoch: 10.3 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15104060952775		[learning rate: 0.00017987]
	Learning Rate: 0.000179865
	LOSS [training: 5.15104060952775 | validation: 5.273366872844502]
	TIME [epoch: 10.3 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158140146639786		[learning rate: 0.00017943]
	Learning Rate: 0.00017943
	LOSS [training: 5.158140146639786 | validation: 5.306441405086989]
	TIME [epoch: 10.3 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163113845012816		[learning rate: 0.000179]
	Learning Rate: 0.000178996
	LOSS [training: 5.163113845012816 | validation: 5.284234974718207]
	TIME [epoch: 10.3 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.147030987321817		[learning rate: 0.00017856]
	Learning Rate: 0.000178562
	LOSS [training: 5.147030987321817 | validation: 5.292460545196332]
	TIME [epoch: 10.3 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.168963330756253		[learning rate: 0.00017813]
	Learning Rate: 0.00017813
	LOSS [training: 5.168963330756253 | validation: 5.289416278465857]
	TIME [epoch: 10.3 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160383462145917		[learning rate: 0.0001777]
	Learning Rate: 0.000177699
	LOSS [training: 5.160383462145917 | validation: 5.296208565006473]
	TIME [epoch: 10.3 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178786883628627		[learning rate: 0.00017727]
	Learning Rate: 0.000177268
	LOSS [training: 5.178786883628627 | validation: 5.277691273652411]
	TIME [epoch: 10.3 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1658010966670345		[learning rate: 0.00017684]
	Learning Rate: 0.000176839
	LOSS [training: 5.1658010966670345 | validation: 5.300235088077994]
	TIME [epoch: 10.3 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18618013882939		[learning rate: 0.00017641]
	Learning Rate: 0.000176411
	LOSS [training: 5.18618013882939 | validation: 5.295114846295669]
	TIME [epoch: 10.3 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172683032233633		[learning rate: 0.00017598]
	Learning Rate: 0.000175984
	LOSS [training: 5.172683032233633 | validation: 5.285645420377555]
	TIME [epoch: 10.3 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179977346901272		[learning rate: 0.00017556]
	Learning Rate: 0.000175558
	LOSS [training: 5.179977346901272 | validation: 5.294005353813325]
	TIME [epoch: 10.3 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.142056901315908		[learning rate: 0.00017513]
	Learning Rate: 0.000175133
	LOSS [training: 5.142056901315908 | validation: 5.270860716801072]
	TIME [epoch: 10.3 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.169446758428395		[learning rate: 0.00017471]
	Learning Rate: 0.000174709
	LOSS [training: 5.169446758428395 | validation: 5.288645398512249]
	TIME [epoch: 10.3 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1877645777823265		[learning rate: 0.00017429]
	Learning Rate: 0.000174286
	LOSS [training: 5.1877645777823265 | validation: 5.291908225662663]
	TIME [epoch: 10.3 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16842571962132		[learning rate: 0.00017386]
	Learning Rate: 0.000173864
	LOSS [training: 5.16842571962132 | validation: 5.279390312971173]
	TIME [epoch: 10.3 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163700520612771		[learning rate: 0.00017344]
	Learning Rate: 0.000173443
	LOSS [training: 5.163700520612771 | validation: 5.268599705499147]
	TIME [epoch: 10.3 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132071964844396		[learning rate: 0.00017302]
	Learning Rate: 0.000173024
	LOSS [training: 5.132071964844396 | validation: 5.290427734845799]
	TIME [epoch: 10.3 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1518337916565455		[learning rate: 0.0001726]
	Learning Rate: 0.000172605
	LOSS [training: 5.1518337916565455 | validation: 5.278889453096063]
	TIME [epoch: 10.3 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182432614542254		[learning rate: 0.00017219]
	Learning Rate: 0.000172187
	LOSS [training: 5.182432614542254 | validation: 5.307571425308804]
	TIME [epoch: 10.3 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.141436138930892		[learning rate: 0.00017177]
	Learning Rate: 0.00017177
	LOSS [training: 5.141436138930892 | validation: 5.292326100835287]
	TIME [epoch: 10.3 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15887680281807		[learning rate: 0.00017135]
	Learning Rate: 0.000171354
	LOSS [training: 5.15887680281807 | validation: 5.284766526401219]
	TIME [epoch: 10.3 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.138349199432235		[learning rate: 0.00017094]
	Learning Rate: 0.000170939
	LOSS [training: 5.138349199432235 | validation: 5.284836719957578]
	TIME [epoch: 10.3 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.141431195713038		[learning rate: 0.00017053]
	Learning Rate: 0.000170526
	LOSS [training: 5.141431195713038 | validation: 5.29722440332176]
	TIME [epoch: 10.3 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.154337235975525		[learning rate: 0.00017011]
	Learning Rate: 0.000170113
	LOSS [training: 5.154337235975525 | validation: 5.288239077166069]
	TIME [epoch: 10.3 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151891619609981		[learning rate: 0.0001697]
	Learning Rate: 0.000169701
	LOSS [training: 5.151891619609981 | validation: 5.297052681459059]
	TIME [epoch: 10.3 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163268436833985		[learning rate: 0.00016929]
	Learning Rate: 0.00016929
	LOSS [training: 5.163268436833985 | validation: 5.284686047967719]
	TIME [epoch: 10.3 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159759934341869		[learning rate: 0.00016888]
	Learning Rate: 0.00016888
	LOSS [training: 5.159759934341869 | validation: 5.304123373616835]
	TIME [epoch: 10.3 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158889215506293		[learning rate: 0.00016847]
	Learning Rate: 0.000168471
	LOSS [training: 5.158889215506293 | validation: 5.28752760926978]
	TIME [epoch: 10.3 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151388637867484		[learning rate: 0.00016806]
	Learning Rate: 0.000168064
	LOSS [training: 5.151388637867484 | validation: 5.296940680485888]
	TIME [epoch: 10.3 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.143305586424235		[learning rate: 0.00016766]
	Learning Rate: 0.000167657
	LOSS [training: 5.143305586424235 | validation: 5.282731480979841]
	TIME [epoch: 10.3 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178533171279733		[learning rate: 0.00016725]
	Learning Rate: 0.000167251
	LOSS [training: 5.178533171279733 | validation: 5.2803293783714205]
	TIME [epoch: 10.3 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179500084028394		[learning rate: 0.00016685]
	Learning Rate: 0.000166846
	LOSS [training: 5.179500084028394 | validation: 5.288340619765627]
	TIME [epoch: 10.3 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151775670945039		[learning rate: 0.00016644]
	Learning Rate: 0.000166442
	LOSS [training: 5.151775670945039 | validation: 5.281718412214257]
	TIME [epoch: 10.3 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1364173956595645		[learning rate: 0.00016604]
	Learning Rate: 0.000166039
	LOSS [training: 5.1364173956595645 | validation: 5.306229124576167]
	TIME [epoch: 10.3 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.125368308336165		[learning rate: 0.00016564]
	Learning Rate: 0.000165637
	LOSS [training: 5.125368308336165 | validation: 5.286264093653485]
	TIME [epoch: 10.3 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1624825586959		[learning rate: 0.00016524]
	Learning Rate: 0.000165236
	LOSS [training: 5.1624825586959 | validation: 5.272199713280613]
	TIME [epoch: 10.3 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.135173576158494		[learning rate: 0.00016484]
	Learning Rate: 0.000164836
	LOSS [training: 5.135173576158494 | validation: 5.286196047568]
	TIME [epoch: 10.3 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.140168402130752		[learning rate: 0.00016444]
	Learning Rate: 0.000164437
	LOSS [training: 5.140168402130752 | validation: 5.293871420869364]
	TIME [epoch: 10.3 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.141459894581318		[learning rate: 0.00016404]
	Learning Rate: 0.000164039
	LOSS [training: 5.141459894581318 | validation: 5.28020082222063]
	TIME [epoch: 10.3 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.144132780658401		[learning rate: 0.00016364]
	Learning Rate: 0.000163642
	LOSS [training: 5.144132780658401 | validation: 5.279097612342383]
	TIME [epoch: 10.3 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1621921762582454		[learning rate: 0.00016325]
	Learning Rate: 0.000163246
	LOSS [training: 5.1621921762582454 | validation: 5.311923366528898]
	TIME [epoch: 10.3 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.189891724639106		[learning rate: 0.00016285]
	Learning Rate: 0.000162851
	LOSS [training: 5.189891724639106 | validation: 5.277333333263573]
	TIME [epoch: 10.3 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159296628910086		[learning rate: 0.00016246]
	Learning Rate: 0.000162456
	LOSS [training: 5.159296628910086 | validation: 5.280634705383502]
	TIME [epoch: 10.3 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1757250942217805		[learning rate: 0.00016206]
	Learning Rate: 0.000162063
	LOSS [training: 5.1757250942217805 | validation: 5.310994159762165]
	TIME [epoch: 10.3 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176678773803816		[learning rate: 0.00016167]
	Learning Rate: 0.000161671
	LOSS [training: 5.176678773803816 | validation: 5.282523016961758]
	TIME [epoch: 10.3 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1435794342102845		[learning rate: 0.00016128]
	Learning Rate: 0.000161279
	LOSS [training: 5.1435794342102845 | validation: 5.277864444401116]
	TIME [epoch: 10.3 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176443728533528		[learning rate: 0.00016089]
	Learning Rate: 0.000160889
	LOSS [training: 5.176443728533528 | validation: 5.30973229652119]
	TIME [epoch: 10.3 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.19710701843256		[learning rate: 0.0001605]
	Learning Rate: 0.000160499
	LOSS [training: 5.19710701843256 | validation: 5.291127583003753]
	TIME [epoch: 10.3 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176508048673904		[learning rate: 0.00016011]
	Learning Rate: 0.000160111
	LOSS [training: 5.176508048673904 | validation: 5.292892735474233]
	TIME [epoch: 10.3 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15716703493328		[learning rate: 0.00015972]
	Learning Rate: 0.000159723
	LOSS [training: 5.15716703493328 | validation: 5.280439930780676]
	TIME [epoch: 10.3 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151448249637102		[learning rate: 0.00015934]
	Learning Rate: 0.000159337
	LOSS [training: 5.151448249637102 | validation: 5.305844519798259]
	TIME [epoch: 10.3 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1774349184093555		[learning rate: 0.00015895]
	Learning Rate: 0.000158951
	LOSS [training: 5.1774349184093555 | validation: 5.2947062906255615]
	TIME [epoch: 10.3 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181692111439365		[learning rate: 0.00015857]
	Learning Rate: 0.000158566
	LOSS [training: 5.181692111439365 | validation: 5.282330855512422]
	TIME [epoch: 10.3 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175611010876934		[learning rate: 0.00015818]
	Learning Rate: 0.000158182
	LOSS [training: 5.175611010876934 | validation: 5.288560617289213]
	TIME [epoch: 10.3 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.186602326507684		[learning rate: 0.0001578]
	Learning Rate: 0.000157799
	LOSS [training: 5.186602326507684 | validation: 5.305725518563021]
	TIME [epoch: 10.3 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.161699636319681		[learning rate: 0.00015742]
	Learning Rate: 0.000157417
	LOSS [training: 5.161699636319681 | validation: 5.285137159581025]
	TIME [epoch: 10.3 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16248979452631		[learning rate: 0.00015704]
	Learning Rate: 0.000157036
	LOSS [training: 5.16248979452631 | validation: 5.277179154537935]
	TIME [epoch: 10.3 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153944238353064		[learning rate: 0.00015666]
	Learning Rate: 0.000156656
	LOSS [training: 5.153944238353064 | validation: 5.282176320312326]
	TIME [epoch: 10.3 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.169514856984435		[learning rate: 0.00015628]
	Learning Rate: 0.000156277
	LOSS [training: 5.169514856984435 | validation: 5.312701895074679]
	TIME [epoch: 10.3 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1621718478830045		[learning rate: 0.0001559]
	Learning Rate: 0.000155899
	LOSS [training: 5.1621718478830045 | validation: 5.281922089143321]
	TIME [epoch: 10.3 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158496096208031		[learning rate: 0.00015552]
	Learning Rate: 0.000155521
	LOSS [training: 5.158496096208031 | validation: 5.2980771448490405]
	TIME [epoch: 10.3 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153677441359302		[learning rate: 0.00015514]
	Learning Rate: 0.000155145
	LOSS [training: 5.153677441359302 | validation: 5.27676146634596]
	TIME [epoch: 10.3 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177617244414245		[learning rate: 0.00015477]
	Learning Rate: 0.000154769
	LOSS [training: 5.177617244414245 | validation: 5.264299009665981]
	TIME [epoch: 10.3 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166644903835687		[learning rate: 0.00015439]
	Learning Rate: 0.000154394
	LOSS [training: 5.166644903835687 | validation: 5.275166115453608]
	TIME [epoch: 10.3 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1793492882683925		[learning rate: 0.00015402]
	Learning Rate: 0.000154021
	LOSS [training: 5.1793492882683925 | validation: 5.302985371027741]
	TIME [epoch: 10.3 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160695637149591		[learning rate: 0.00015365]
	Learning Rate: 0.000153648
	LOSS [training: 5.160695637149591 | validation: 5.287034046517156]
	TIME [epoch: 10.3 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148514878286926		[learning rate: 0.00015328]
	Learning Rate: 0.000153276
	LOSS [training: 5.148514878286926 | validation: 5.292988343075329]
	TIME [epoch: 10.3 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165014490987813		[learning rate: 0.0001529]
	Learning Rate: 0.000152905
	LOSS [training: 5.165014490987813 | validation: 5.296258789664362]
	TIME [epoch: 10.3 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184331774712377		[learning rate: 0.00015253]
	Learning Rate: 0.000152535
	LOSS [training: 5.184331774712377 | validation: 5.305157471030038]
	TIME [epoch: 10.3 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180990290480127		[learning rate: 0.00015217]
	Learning Rate: 0.000152165
	LOSS [training: 5.180990290480127 | validation: 5.292067377545959]
	TIME [epoch: 10.3 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156573661013048		[learning rate: 0.0001518]
	Learning Rate: 0.000151797
	LOSS [training: 5.156573661013048 | validation: 5.278327555303412]
	TIME [epoch: 10.3 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.185234199152345		[learning rate: 0.00015143]
	Learning Rate: 0.00015143
	LOSS [training: 5.185234199152345 | validation: 5.295871072133653]
	TIME [epoch: 10.3 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172481828654749		[learning rate: 0.00015106]
	Learning Rate: 0.000151063
	LOSS [training: 5.172481828654749 | validation: 5.287688869209084]
	TIME [epoch: 10.3 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166362377276011		[learning rate: 0.0001507]
	Learning Rate: 0.000150697
	LOSS [training: 5.166362377276011 | validation: 5.286218767738803]
	TIME [epoch: 10.3 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.173853832646391		[learning rate: 0.00015033]
	Learning Rate: 0.000150332
	LOSS [training: 5.173853832646391 | validation: 5.274412776323889]
	TIME [epoch: 10.3 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1486086224304675		[learning rate: 0.00014997]
	Learning Rate: 0.000149968
	LOSS [training: 5.1486086224304675 | validation: 5.273177782379296]
	TIME [epoch: 10.3 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177544674753909		[learning rate: 0.00014961]
	Learning Rate: 0.000149605
	LOSS [training: 5.177544674753909 | validation: 5.293426644379844]
	TIME [epoch: 10.3 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.209174767917366		[learning rate: 0.00014924]
	Learning Rate: 0.000149243
	LOSS [training: 5.209174767917366 | validation: 5.302512400394717]
	TIME [epoch: 10.3 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.19250994365931		[learning rate: 0.00014888]
	Learning Rate: 0.000148882
	LOSS [training: 5.19250994365931 | validation: 5.292413617520651]
	TIME [epoch: 10.3 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184695846400086		[learning rate: 0.00014852]
	Learning Rate: 0.000148522
	LOSS [training: 5.184695846400086 | validation: 5.280648829395045]
	TIME [epoch: 10.3 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.128247080648473		[learning rate: 0.00014816]
	Learning Rate: 0.000148162
	LOSS [training: 5.128247080648473 | validation: 5.28777939792187]
	TIME [epoch: 10.3 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1572900994230615		[learning rate: 0.0001478]
	Learning Rate: 0.000147803
	LOSS [training: 5.1572900994230615 | validation: 5.284868419559657]
	TIME [epoch: 10.3 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179706601341193		[learning rate: 0.00014745]
	Learning Rate: 0.000147446
	LOSS [training: 5.179706601341193 | validation: 5.289890119433448]
	TIME [epoch: 10.3 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159333072468306		[learning rate: 0.00014709]
	Learning Rate: 0.000147089
	LOSS [training: 5.159333072468306 | validation: 5.28904076945352]
	TIME [epoch: 10.3 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164682861305911		[learning rate: 0.00014673]
	Learning Rate: 0.000146732
	LOSS [training: 5.164682861305911 | validation: 5.294759497131545]
	TIME [epoch: 10.3 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.124058176756845		[learning rate: 0.00014638]
	Learning Rate: 0.000146377
	LOSS [training: 5.124058176756845 | validation: 5.277149568594835]
	TIME [epoch: 10.3 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.152100474638779		[learning rate: 0.00014602]
	Learning Rate: 0.000146023
	LOSS [training: 5.152100474638779 | validation: 5.296921030018878]
	TIME [epoch: 10.3 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166187748740205		[learning rate: 0.00014567]
	Learning Rate: 0.000145669
	LOSS [training: 5.166187748740205 | validation: 5.281020260914069]
	TIME [epoch: 10.3 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.13670173937047		[learning rate: 0.00014532]
	Learning Rate: 0.000145317
	LOSS [training: 5.13670173937047 | validation: 5.272204729178184]
	TIME [epoch: 10.3 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148420395442832		[learning rate: 0.00014497]
	Learning Rate: 0.000144965
	LOSS [training: 5.148420395442832 | validation: 5.2905583623597705]
	TIME [epoch: 10.3 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.152544511853895		[learning rate: 0.00014461]
	Learning Rate: 0.000144614
	LOSS [training: 5.152544511853895 | validation: 5.283784641923299]
	TIME [epoch: 10.3 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193907515435376		[learning rate: 0.00014426]
	Learning Rate: 0.000144264
	LOSS [training: 5.193907515435376 | validation: 5.28604622666713]
	TIME [epoch: 10.3 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.168730628974062		[learning rate: 0.00014391]
	Learning Rate: 0.000143915
	LOSS [training: 5.168730628974062 | validation: 5.315152276515362]
	TIME [epoch: 10.3 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.161691940157278		[learning rate: 0.00014357]
	Learning Rate: 0.000143566
	LOSS [training: 5.161691940157278 | validation: 5.298637565085025]
	TIME [epoch: 10.3 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158537977979352		[learning rate: 0.00014322]
	Learning Rate: 0.000143219
	LOSS [training: 5.158537977979352 | validation: 5.285863831742159]
	TIME [epoch: 10.3 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.153392621362157		[learning rate: 0.00014287]
	Learning Rate: 0.000142872
	LOSS [training: 5.153392621362157 | validation: 5.271659773935662]
	TIME [epoch: 10.3 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.155439956467103		[learning rate: 0.00014253]
	Learning Rate: 0.000142526
	LOSS [training: 5.155439956467103 | validation: 5.307087758001535]
	TIME [epoch: 10.3 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171682817883621		[learning rate: 0.00014218]
	Learning Rate: 0.000142181
	LOSS [training: 5.171682817883621 | validation: 5.3048019041186745]
	TIME [epoch: 10.3 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178170323798135		[learning rate: 0.00014184]
	Learning Rate: 0.000141837
	LOSS [training: 5.178170323798135 | validation: 5.296199451261803]
	TIME [epoch: 10.3 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184862656231989		[learning rate: 0.00014149]
	Learning Rate: 0.000141494
	LOSS [training: 5.184862656231989 | validation: 5.296420770600162]
	TIME [epoch: 10.3 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.157704025041319		[learning rate: 0.00014115]
	Learning Rate: 0.000141151
	LOSS [training: 5.157704025041319 | validation: 5.284455238587551]
	TIME [epoch: 10.3 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15680652076908		[learning rate: 0.00014081]
	Learning Rate: 0.000140809
	LOSS [training: 5.15680652076908 | validation: 5.287533695850358]
	TIME [epoch: 10.3 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.168064544572042		[learning rate: 0.00014047]
	Learning Rate: 0.000140468
	LOSS [training: 5.168064544572042 | validation: 5.272812815169242]
	TIME [epoch: 10.3 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.161593816851332		[learning rate: 0.00014013]
	Learning Rate: 0.000140128
	LOSS [training: 5.161593816851332 | validation: 5.286449191182144]
	TIME [epoch: 10.3 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.113431588219013		[learning rate: 0.00013979]
	Learning Rate: 0.000139789
	LOSS [training: 5.113431588219013 | validation: 5.275804531889422]
	TIME [epoch: 10.3 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.142020262282104		[learning rate: 0.00013945]
	Learning Rate: 0.000139451
	LOSS [training: 5.142020262282104 | validation: 5.291262481187413]
	TIME [epoch: 10.3 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164187306859951		[learning rate: 0.00013911]
	Learning Rate: 0.000139113
	LOSS [training: 5.164187306859951 | validation: 5.281800377579762]
	TIME [epoch: 10.3 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.146956684851018		[learning rate: 0.00013878]
	Learning Rate: 0.000138776
	LOSS [training: 5.146956684851018 | validation: 5.289361057046408]
	TIME [epoch: 10.3 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159277430175475		[learning rate: 0.00013844]
	Learning Rate: 0.000138441
	LOSS [training: 5.159277430175475 | validation: 5.286615085502249]
	TIME [epoch: 10.3 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.134142744644162		[learning rate: 0.00013811]
	Learning Rate: 0.000138105
	LOSS [training: 5.134142744644162 | validation: 5.289494443805531]
	TIME [epoch: 10.3 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.144873585672947		[learning rate: 0.00013777]
	Learning Rate: 0.000137771
	LOSS [training: 5.144873585672947 | validation: 5.28872910158784]
	TIME [epoch: 10.3 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1471602748070335		[learning rate: 0.00013744]
	Learning Rate: 0.000137437
	LOSS [training: 5.1471602748070335 | validation: 5.267264702153437]
	TIME [epoch: 10.3 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1491432561949155		[learning rate: 0.0001371]
	Learning Rate: 0.000137105
	LOSS [training: 5.1491432561949155 | validation: 5.289029255966433]
	TIME [epoch: 10.3 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132864730973543		[learning rate: 0.00013677]
	Learning Rate: 0.000136773
	LOSS [training: 5.132864730973543 | validation: 5.286780030486457]
	TIME [epoch: 10.3 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.134888426428764		[learning rate: 0.00013644]
	Learning Rate: 0.000136442
	LOSS [training: 5.134888426428764 | validation: 5.289257304014554]
	TIME [epoch: 10.3 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1293660711255145		[learning rate: 0.00013611]
	Learning Rate: 0.000136111
	LOSS [training: 5.1293660711255145 | validation: 5.277493602885857]
	TIME [epoch: 10.3 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.130798525095953		[learning rate: 0.00013578]
	Learning Rate: 0.000135782
	LOSS [training: 5.130798525095953 | validation: 5.287396567717263]
	TIME [epoch: 10.3 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132707926149391		[learning rate: 0.00013545]
	Learning Rate: 0.000135453
	LOSS [training: 5.132707926149391 | validation: 5.278057196206019]
	TIME [epoch: 10.3 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180084943912977		[learning rate: 0.00013513]
	Learning Rate: 0.000135125
	LOSS [training: 5.180084943912977 | validation: 5.269183163834354]
	TIME [epoch: 10.3 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.130683113727184		[learning rate: 0.0001348]
	Learning Rate: 0.000134798
	LOSS [training: 5.130683113727184 | validation: 5.277052675362499]
	TIME [epoch: 10.3 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132402731002763		[learning rate: 0.00013447]
	Learning Rate: 0.000134472
	LOSS [training: 5.132402731002763 | validation: 5.282062677482659]
	TIME [epoch: 10.3 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.145502528349791		[learning rate: 0.00013415]
	Learning Rate: 0.000134146
	LOSS [training: 5.145502528349791 | validation: 5.286115582662664]
	TIME [epoch: 10.3 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132402721070981		[learning rate: 0.00013382]
	Learning Rate: 0.000133822
	LOSS [training: 5.132402721070981 | validation: 5.280253088361542]
	TIME [epoch: 10.3 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14407370488995		[learning rate: 0.0001335]
	Learning Rate: 0.000133498
	LOSS [training: 5.14407370488995 | validation: 5.290290745076756]
	TIME [epoch: 10.3 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.143699819500413		[learning rate: 0.00013317]
	Learning Rate: 0.000133175
	LOSS [training: 5.143699819500413 | validation: 5.290087260181675]
	TIME [epoch: 10.3 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151996170407267		[learning rate: 0.00013285]
	Learning Rate: 0.000132852
	LOSS [training: 5.151996170407267 | validation: 5.273985173993839]
	TIME [epoch: 10.3 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148621604233767		[learning rate: 0.00013253]
	Learning Rate: 0.00013253
	LOSS [training: 5.148621604233767 | validation: 5.285488931933803]
	TIME [epoch: 10.3 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.137955183442891		[learning rate: 0.00013221]
	Learning Rate: 0.00013221
	LOSS [training: 5.137955183442891 | validation: 5.287189455448406]
	TIME [epoch: 10.3 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1367299774537365		[learning rate: 0.00013189]
	Learning Rate: 0.00013189
	LOSS [training: 5.1367299774537365 | validation: 5.283502455238192]
	TIME [epoch: 10.3 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160466523176732		[learning rate: 0.00013157]
	Learning Rate: 0.00013157
	LOSS [training: 5.160466523176732 | validation: 5.289119013371913]
	TIME [epoch: 10.3 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.142454267711804		[learning rate: 0.00013125]
	Learning Rate: 0.000131252
	LOSS [training: 5.142454267711804 | validation: 5.3073649663297395]
	TIME [epoch: 10.3 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1536482649137865		[learning rate: 0.00013093]
	Learning Rate: 0.000130934
	LOSS [training: 5.1536482649137865 | validation: 5.277259851570015]
	TIME [epoch: 10.3 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.140625211416125		[learning rate: 0.00013062]
	Learning Rate: 0.000130617
	LOSS [training: 5.140625211416125 | validation: 5.260070323461208]
	TIME [epoch: 10.3 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.138008203093387		[learning rate: 0.0001303]
	Learning Rate: 0.000130301
	LOSS [training: 5.138008203093387 | validation: 5.272842461627971]
	TIME [epoch: 10.3 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.129072161382192		[learning rate: 0.00012999]
	Learning Rate: 0.000129985
	LOSS [training: 5.129072161382192 | validation: 5.252775432294189]
	TIME [epoch: 10.3 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156758558043135		[learning rate: 0.00012967]
	Learning Rate: 0.000129671
	LOSS [training: 5.156758558043135 | validation: 5.276172608600818]
	TIME [epoch: 10.3 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170327629139176		[learning rate: 0.00012936]
	Learning Rate: 0.000129357
	LOSS [training: 5.170327629139176 | validation: 5.302451459158682]
	TIME [epoch: 10.3 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190911951613879		[learning rate: 0.00012904]
	Learning Rate: 0.000129044
	LOSS [training: 5.190911951613879 | validation: 5.27420664742641]
	TIME [epoch: 10.3 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.132379709051536		[learning rate: 0.00012873]
	Learning Rate: 0.000128731
	LOSS [training: 5.132379709051536 | validation: 5.287804433754557]
	TIME [epoch: 10.3 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.131554792585639		[learning rate: 0.00012842]
	Learning Rate: 0.00012842
	LOSS [training: 5.131554792585639 | validation: 5.28287527975306]
	TIME [epoch: 10.3 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177783259199681		[learning rate: 0.00012811]
	Learning Rate: 0.000128109
	LOSS [training: 5.177783259199681 | validation: 5.2747004925194485]
	TIME [epoch: 10.3 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171534040136367		[learning rate: 0.0001278]
	Learning Rate: 0.000127799
	LOSS [training: 5.171534040136367 | validation: 5.3038654135043695]
	TIME [epoch: 10.3 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.150991521964493		[learning rate: 0.00012749]
	Learning Rate: 0.000127489
	LOSS [training: 5.150991521964493 | validation: 5.286934498126975]
	TIME [epoch: 10.3 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160064725483128		[learning rate: 0.00012718]
	Learning Rate: 0.000127181
	LOSS [training: 5.160064725483128 | validation: 5.286464206494759]
	TIME [epoch: 10.3 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.141376163758801		[learning rate: 0.00012687]
	Learning Rate: 0.000126873
	LOSS [training: 5.141376163758801 | validation: 5.308319312129878]
	TIME [epoch: 10.3 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175450070631424		[learning rate: 0.00012657]
	Learning Rate: 0.000126566
	LOSS [training: 5.175450070631424 | validation: 5.276125663600295]
	TIME [epoch: 10.3 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151354048808669		[learning rate: 0.00012626]
	Learning Rate: 0.000126259
	LOSS [training: 5.151354048808669 | validation: 5.278050852292507]
	TIME [epoch: 10.3 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.166994438252323		[learning rate: 0.00012595]
	Learning Rate: 0.000125954
	LOSS [training: 5.166994438252323 | validation: 5.283999403217799]
	TIME [epoch: 10.3 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.185958534692114		[learning rate: 0.00012565]
	Learning Rate: 0.000125649
	LOSS [training: 5.185958534692114 | validation: 5.297156256857051]
	TIME [epoch: 10.3 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.187959447539852		[learning rate: 0.00012534]
	Learning Rate: 0.000125345
	LOSS [training: 5.187959447539852 | validation: 5.280983591051771]
	TIME [epoch: 10.3 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156047256431701		[learning rate: 0.00012504]
	Learning Rate: 0.000125041
	LOSS [training: 5.156047256431701 | validation: 5.296637234995952]
	TIME [epoch: 10.3 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191317874437104		[learning rate: 0.00012474]
	Learning Rate: 0.000124738
	LOSS [training: 5.191317874437104 | validation: 5.2817300825113875]
	TIME [epoch: 10.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165255972957304		[learning rate: 0.00012444]
	Learning Rate: 0.000124436
	LOSS [training: 5.165255972957304 | validation: 5.29162123611331]
	TIME [epoch: 10.3 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15893900679688		[learning rate: 0.00012414]
	Learning Rate: 0.000124135
	LOSS [training: 5.15893900679688 | validation: 5.28732481668961]
	TIME [epoch: 10.3 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.138772325852321		[learning rate: 0.00012383]
	Learning Rate: 0.000123835
	LOSS [training: 5.138772325852321 | validation: 5.290046936906868]
	TIME [epoch: 10.3 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.174487256231441		[learning rate: 0.00012353]
	Learning Rate: 0.000123535
	LOSS [training: 5.174487256231441 | validation: 5.291095313280335]
	TIME [epoch: 10.3 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.196621574695956		[learning rate: 0.00012324]
	Learning Rate: 0.000123236
	LOSS [training: 5.196621574695956 | validation: 5.2830510195806015]
	TIME [epoch: 10.3 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183968914292265		[learning rate: 0.00012294]
	Learning Rate: 0.000122937
	LOSS [training: 5.183968914292265 | validation: 5.282852291085334]
	TIME [epoch: 10.3 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171047025967753		[learning rate: 0.00012264]
	Learning Rate: 0.00012264
	LOSS [training: 5.171047025967753 | validation: 5.289309432974778]
	TIME [epoch: 10.3 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158342398846775		[learning rate: 0.00012234]
	Learning Rate: 0.000122343
	LOSS [training: 5.158342398846775 | validation: 5.291810018276199]
	TIME [epoch: 10.3 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.152105601866029		[learning rate: 0.00012205]
	Learning Rate: 0.000122047
	LOSS [training: 5.152105601866029 | validation: 5.31546371788405]
	TIME [epoch: 10.3 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176004282055201		[learning rate: 0.00012175]
	Learning Rate: 0.000121751
	LOSS [training: 5.176004282055201 | validation: 5.292493762926366]
	TIME [epoch: 10.3 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156601326634606		[learning rate: 0.00012146]
	Learning Rate: 0.000121457
	LOSS [training: 5.156601326634606 | validation: 5.295475079529772]
	TIME [epoch: 10.3 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1471380534685585		[learning rate: 0.00012116]
	Learning Rate: 0.000121163
	LOSS [training: 5.1471380534685585 | validation: 5.28135050153596]
	TIME [epoch: 10.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172926627887906		[learning rate: 0.00012087]
	Learning Rate: 0.000120869
	LOSS [training: 5.172926627887906 | validation: 5.30013290466165]
	TIME [epoch: 10.3 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.168343726273601		[learning rate: 0.00012058]
	Learning Rate: 0.000120577
	LOSS [training: 5.168343726273601 | validation: 5.289358322736375]
	TIME [epoch: 10.3 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158589004684387		[learning rate: 0.00012028]
	Learning Rate: 0.000120285
	LOSS [training: 5.158589004684387 | validation: 5.272756533147206]
	TIME [epoch: 10.3 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175708922428883		[learning rate: 0.00011999]
	Learning Rate: 0.000119994
	LOSS [training: 5.175708922428883 | validation: 5.27325689613529]
	TIME [epoch: 10.3 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190586868210206		[learning rate: 0.0001197]
	Learning Rate: 0.000119703
	LOSS [training: 5.190586868210206 | validation: 5.295245747031603]
	TIME [epoch: 10.3 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175560871531731		[learning rate: 0.00011941]
	Learning Rate: 0.000119413
	LOSS [training: 5.175560871531731 | validation: 5.283933415225013]
	TIME [epoch: 10.3 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182760819812609		[learning rate: 0.00011912]
	Learning Rate: 0.000119124
	LOSS [training: 5.182760819812609 | validation: 5.282189484917981]
	TIME [epoch: 10.3 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.195488029394736		[learning rate: 0.00011884]
	Learning Rate: 0.000118836
	LOSS [training: 5.195488029394736 | validation: 5.285630620755922]
	TIME [epoch: 10.3 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15667306363401		[learning rate: 0.00011855]
	Learning Rate: 0.000118548
	LOSS [training: 5.15667306363401 | validation: 5.290088406680854]
	TIME [epoch: 10.3 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.158271735686783		[learning rate: 0.00011826]
	Learning Rate: 0.000118261
	LOSS [training: 5.158271735686783 | validation: 5.286194054668983]
	TIME [epoch: 10.3 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178665566793056		[learning rate: 0.00011797]
	Learning Rate: 0.000117975
	LOSS [training: 5.178665566793056 | validation: 5.2892984035143344]
	TIME [epoch: 10.3 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.199530720084148		[learning rate: 0.00011769]
	Learning Rate: 0.000117689
	LOSS [training: 5.199530720084148 | validation: 5.296592886123579]
	TIME [epoch: 10.3 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181755189927401		[learning rate: 0.0001174]
	Learning Rate: 0.000117404
	LOSS [training: 5.181755189927401 | validation: 5.28886833071705]
	TIME [epoch: 10.3 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.177940804660264		[learning rate: 0.00011712]
	Learning Rate: 0.00011712
	LOSS [training: 5.177940804660264 | validation: 5.291048773271227]
	TIME [epoch: 10.3 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1742258421783145		[learning rate: 0.00011684]
	Learning Rate: 0.000116837
	LOSS [training: 5.1742258421783145 | validation: 5.287214127205457]
	TIME [epoch: 10.3 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190679894828778		[learning rate: 0.00011655]
	Learning Rate: 0.000116554
	LOSS [training: 5.190679894828778 | validation: 5.2847833983077]
	TIME [epoch: 10.3 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.185664894569109		[learning rate: 0.00011627]
	Learning Rate: 0.000116272
	LOSS [training: 5.185664894569109 | validation: 5.276679541257343]
	TIME [epoch: 10.3 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179041229932141		[learning rate: 0.00011599]
	Learning Rate: 0.00011599
	LOSS [training: 5.179041229932141 | validation: 5.278332532926785]
	TIME [epoch: 10.3 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.162500390036286		[learning rate: 0.00011571]
	Learning Rate: 0.000115709
	LOSS [training: 5.162500390036286 | validation: 5.291492335906055]
	TIME [epoch: 10.3 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.195209853933867		[learning rate: 0.00011543]
	Learning Rate: 0.000115429
	LOSS [training: 5.195209853933867 | validation: 5.295793447038354]
	TIME [epoch: 10.3 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.188583049900835		[learning rate: 0.00011515]
	Learning Rate: 0.00011515
	LOSS [training: 5.188583049900835 | validation: 5.296441094538399]
	TIME [epoch: 10.3 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178583758779		[learning rate: 0.00011487]
	Learning Rate: 0.000114871
	LOSS [training: 5.178583758779 | validation: 5.295255245186347]
	TIME [epoch: 10.3 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.172276979027235		[learning rate: 0.00011459]
	Learning Rate: 0.000114593
	LOSS [training: 5.172276979027235 | validation: 5.287312744017385]
	TIME [epoch: 10.3 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183634563641738		[learning rate: 0.00011432]
	Learning Rate: 0.000114316
	LOSS [training: 5.183634563641738 | validation: 5.291847655296297]
	TIME [epoch: 10.3 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.198147691994551		[learning rate: 0.00011404]
	Learning Rate: 0.000114039
	LOSS [training: 5.198147691994551 | validation: 5.294432017789693]
	TIME [epoch: 10.3 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.174176068889225		[learning rate: 0.00011376]
	Learning Rate: 0.000113763
	LOSS [training: 5.174176068889225 | validation: 5.2996712005162765]
	TIME [epoch: 10.3 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183909235182577		[learning rate: 0.00011349]
	Learning Rate: 0.000113487
	LOSS [training: 5.183909235182577 | validation: 5.289718045133568]
	TIME [epoch: 10.3 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.179171741368637		[learning rate: 0.00011321]
	Learning Rate: 0.000113213
	LOSS [training: 5.179171741368637 | validation: 5.311456673003149]
	TIME [epoch: 10.3 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18879205779204		[learning rate: 0.00011294]
	Learning Rate: 0.000112939
	LOSS [training: 5.18879205779204 | validation: 5.293858430457144]
	TIME [epoch: 10.3 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197035008400142		[learning rate: 0.00011267]
	Learning Rate: 0.000112665
	LOSS [training: 5.197035008400142 | validation: 5.289525747637181]
	TIME [epoch: 10.3 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191969320582305		[learning rate: 0.00011239]
	Learning Rate: 0.000112392
	LOSS [training: 5.191969320582305 | validation: 5.297613811687482]
	TIME [epoch: 10.3 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.215859244859425		[learning rate: 0.00011212]
	Learning Rate: 0.00011212
	LOSS [training: 5.215859244859425 | validation: 5.278713894538654]
	TIME [epoch: 10.3 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175917321775309		[learning rate: 0.00011185]
	Learning Rate: 0.000111849
	LOSS [training: 5.175917321775309 | validation: 5.30157124199402]
	TIME [epoch: 10.3 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178875707555361		[learning rate: 0.00011158]
	Learning Rate: 0.000111578
	LOSS [training: 5.178875707555361 | validation: 5.283299491518956]
	TIME [epoch: 10.3 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.208264049266563		[learning rate: 0.00011131]
	Learning Rate: 0.000111308
	LOSS [training: 5.208264049266563 | validation: 5.29726512012959]
	TIME [epoch: 10.3 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1572541932334435		[learning rate: 0.00011104]
	Learning Rate: 0.000111039
	LOSS [training: 5.1572541932334435 | validation: 5.291067079465624]
	TIME [epoch: 10.3 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.188617612143977		[learning rate: 0.00011077]
	Learning Rate: 0.00011077
	LOSS [training: 5.188617612143977 | validation: 5.282877253336307]
	TIME [epoch: 10.3 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178016625935214		[learning rate: 0.0001105]
	Learning Rate: 0.000110502
	LOSS [training: 5.178016625935214 | validation: 5.3000854022298585]
	TIME [epoch: 10.3 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163031505598402		[learning rate: 0.00011023]
	Learning Rate: 0.000110234
	LOSS [training: 5.163031505598402 | validation: 5.290909180540057]
	TIME [epoch: 10.3 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190236407624232		[learning rate: 0.00010997]
	Learning Rate: 0.000109967
	LOSS [training: 5.190236407624232 | validation: 5.286300491431952]
	TIME [epoch: 10.3 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.151563918235892		[learning rate: 0.0001097]
	Learning Rate: 0.000109701
	LOSS [training: 5.151563918235892 | validation: 5.294753755696447]
	TIME [epoch: 10.3 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.211121191870065		[learning rate: 0.00010944]
	Learning Rate: 0.000109435
	LOSS [training: 5.211121191870065 | validation: 5.2952068588888235]
	TIME [epoch: 10.3 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.19407498201517		[learning rate: 0.00010917]
	Learning Rate: 0.00010917
	LOSS [training: 5.19407498201517 | validation: 5.283873460173415]
	TIME [epoch: 10.3 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.163692169509528		[learning rate: 0.00010891]
	Learning Rate: 0.000108906
	LOSS [training: 5.163692169509528 | validation: 5.289105843615239]
	TIME [epoch: 10.3 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184007000827808		[learning rate: 0.00010864]
	Learning Rate: 0.000108643
	LOSS [training: 5.184007000827808 | validation: 5.291218926672501]
	TIME [epoch: 10.3 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18991623043881		[learning rate: 0.00010838]
	Learning Rate: 0.00010838
	LOSS [training: 5.18991623043881 | validation: 5.304412019220356]
	TIME [epoch: 10.3 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.212409234937107		[learning rate: 0.00010812]
	Learning Rate: 0.000108117
	LOSS [training: 5.212409234937107 | validation: 5.296905346299013]
	TIME [epoch: 10.3 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.20035785372107		[learning rate: 0.00010786]
	Learning Rate: 0.000107855
	LOSS [training: 5.20035785372107 | validation: 5.290263893152241]
	TIME [epoch: 10.3 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191905413265337		[learning rate: 0.00010759]
	Learning Rate: 0.000107594
	LOSS [training: 5.191905413265337 | validation: 5.279485594349421]
	TIME [epoch: 10.3 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171013515747346		[learning rate: 0.00010733]
	Learning Rate: 0.000107334
	LOSS [training: 5.171013515747346 | validation: 5.278914633886585]
	TIME [epoch: 10.3 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.175503221981055		[learning rate: 0.00010707]
	Learning Rate: 0.000107074
	LOSS [training: 5.175503221981055 | validation: 5.278219757545062]
	TIME [epoch: 10.3 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1720073981939105		[learning rate: 0.00010681]
	Learning Rate: 0.000106815
	LOSS [training: 5.1720073981939105 | validation: 5.2869373514007725]
	TIME [epoch: 10.3 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.195815618574619		[learning rate: 0.00010656]
	Learning Rate: 0.000106556
	LOSS [training: 5.195815618574619 | validation: 5.291410322192424]
	TIME [epoch: 10.3 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.160612273429761		[learning rate: 0.0001063]
	Learning Rate: 0.000106298
	LOSS [training: 5.160612273429761 | validation: 5.28394288594939]
	TIME [epoch: 10.3 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.185247385094269		[learning rate: 0.00010604]
	Learning Rate: 0.000106041
	LOSS [training: 5.185247385094269 | validation: 5.2890689528725305]
	TIME [epoch: 10.3 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.190700064923047		[learning rate: 0.00010578]
	Learning Rate: 0.000105784
	LOSS [training: 5.190700064923047 | validation: 5.311205821821256]
	TIME [epoch: 10.3 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.228466448409511		[learning rate: 0.00010553]
	Learning Rate: 0.000105528
	LOSS [training: 5.228466448409511 | validation: 5.312792810739941]
	TIME [epoch: 10.3 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.17294071272976		[learning rate: 0.00010527]
	Learning Rate: 0.000105273
	LOSS [training: 5.17294071272976 | validation: 5.286990491141453]
	TIME [epoch: 10.3 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.204867614113935		[learning rate: 0.00010502]
	Learning Rate: 0.000105018
	LOSS [training: 5.204867614113935 | validation: 5.30771690733091]
	TIME [epoch: 10.3 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.188732612535774		[learning rate: 0.00010476]
	Learning Rate: 0.000104764
	LOSS [training: 5.188732612535774 | validation: 5.304332083628224]
	TIME [epoch: 10.3 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183325356828521		[learning rate: 0.00010451]
	Learning Rate: 0.00010451
	LOSS [training: 5.183325356828521 | validation: 5.293914021562584]
	TIME [epoch: 10.3 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.195437467547786		[learning rate: 0.00010426]
	Learning Rate: 0.000104257
	LOSS [training: 5.195437467547786 | validation: 5.2836534246566025]
	TIME [epoch: 10.3 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.219742537421264		[learning rate: 0.000104]
	Learning Rate: 0.000104005
	LOSS [training: 5.219742537421264 | validation: 5.297758154328206]
	TIME [epoch: 10.3 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193257391318109		[learning rate: 0.00010375]
	Learning Rate: 0.000103753
	LOSS [training: 5.193257391318109 | validation: 5.291149668146854]
	TIME [epoch: 10.3 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193468062875517		[learning rate: 0.0001035]
	Learning Rate: 0.000103502
	LOSS [training: 5.193468062875517 | validation: 5.2913643820684095]
	TIME [epoch: 10.3 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1652220845152526		[learning rate: 0.00010325]
	Learning Rate: 0.000103251
	LOSS [training: 5.1652220845152526 | validation: 5.286143174491789]
	TIME [epoch: 10.3 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15999702260442		[learning rate: 0.000103]
	Learning Rate: 0.000103001
	LOSS [training: 5.15999702260442 | validation: 5.295990167076723]
	TIME [epoch: 10.3 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.191712436997328		[learning rate: 0.00010275]
	Learning Rate: 0.000102752
	LOSS [training: 5.191712436997328 | validation: 5.290624864013578]
	TIME [epoch: 10.3 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170251267760369		[learning rate: 0.0001025]
	Learning Rate: 0.000102503
	LOSS [training: 5.170251267760369 | validation: 5.28490558923316]
	TIME [epoch: 10.3 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197659172423657		[learning rate: 0.00010225]
	Learning Rate: 0.000102255
	LOSS [training: 5.197659172423657 | validation: 5.295052257631381]
	TIME [epoch: 10.3 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193234721004736		[learning rate: 0.00010201]
	Learning Rate: 0.000102007
	LOSS [training: 5.193234721004736 | validation: 5.2840733450171475]
	TIME [epoch: 10.3 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178447436808755		[learning rate: 0.00010176]
	Learning Rate: 0.00010176
	LOSS [training: 5.178447436808755 | validation: 5.269697787070956]
	TIME [epoch: 10.3 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1612422722649836		[learning rate: 0.00010151]
	Learning Rate: 0.000101514
	LOSS [training: 5.1612422722649836 | validation: 5.304433923442242]
	TIME [epoch: 10.3 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1886268111249665		[learning rate: 0.00010127]
	Learning Rate: 0.000101268
	LOSS [training: 5.1886268111249665 | validation: 5.27918827668327]
	TIME [epoch: 10.3 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1936594821415465		[learning rate: 0.00010102]
	Learning Rate: 0.000101023
	LOSS [training: 5.1936594821415465 | validation: 5.298216393493649]
	TIME [epoch: 10.3 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164212306636219		[learning rate: 0.00010078]
	Learning Rate: 0.000100779
	LOSS [training: 5.164212306636219 | validation: 5.288856276391544]
	TIME [epoch: 10.3 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.149543725528829		[learning rate: 0.00010053]
	Learning Rate: 0.000100535
	LOSS [training: 5.149543725528829 | validation: 5.286239986545298]
	TIME [epoch: 10.3 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.18088778243069		[learning rate: 0.00010029]
	Learning Rate: 0.000100291
	LOSS [training: 5.18088778243069 | validation: 5.289497020866163]
	TIME [epoch: 10.3 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1781888939815035		[learning rate: 0.00010005]
	Learning Rate: 0.000100048
	LOSS [training: 5.1781888939815035 | validation: 5.285143269683954]
	TIME [epoch: 10.3 sec]
Finished training in 20734.884 seconds.
