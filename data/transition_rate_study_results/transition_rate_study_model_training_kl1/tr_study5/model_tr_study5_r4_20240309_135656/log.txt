Args:
Namespace(name='model_tr_study5', outdir='out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4', training_data='data/transition_rate_studies/tr_study5/tr_study5_training/r4', validation_data='data/transition_rate_studies/tr_study5/tr_study5_validation/r4', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=500, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.2, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1462526

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.937437281518077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.937437281518077 | validation: 10.625712300844603]
	TIME [epoch: 113 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.856554841661989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.856554841661989 | validation: 10.148963751533804]
	TIME [epoch: 25.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.972798386475642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.972798386475642 | validation: 8.77301075033469]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.213645102065065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.213645102065065 | validation: 9.442459782829744]
	TIME [epoch: 25 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.95735200451458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.95735200451458 | validation: 7.7970946239420424]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.977151456078398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.977151456078398 | validation: 7.534278900296308]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.922111807480024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.922111807480024 | validation: 7.738531052682513]
	TIME [epoch: 25 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.667134703092957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.667134703092957 | validation: 7.272592280849358]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.3493670947986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.3493670947986 | validation: 6.948195938942797]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.4426008545729525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.4426008545729525 | validation: 7.019355780335687]
	TIME [epoch: 25 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.494992265982715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.494992265982715 | validation: 7.1896580070431995]
	TIME [epoch: 25 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.252294468652076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.252294468652076 | validation: 6.9299134199345325]
	TIME [epoch: 24.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.121799778886944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.121799778886944 | validation: 7.122309783849789]
	TIME [epoch: 25 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.194518523375508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.194518523375508 | validation: 6.871097661001663]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.175020715764358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.175020715764358 | validation: 6.894257654395239]
	TIME [epoch: 25 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.127759430568409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.127759430568409 | validation: 6.95866683493959]
	TIME [epoch: 25 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.062306418997068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.062306418997068 | validation: 6.7093145657544095]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.230752756978849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.230752756978849 | validation: 7.082477702378985]
	TIME [epoch: 25 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.98957014206341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.98957014206341 | validation: 6.6903610852944375]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.741754769749701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.741754769749701 | validation: 6.4568702580571395]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.554992120151787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.554992120151787 | validation: 6.3115896685327595]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.321089954784524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.321089954784524 | validation: 6.1750727047714715]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.574341176919181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.574341176919181 | validation: 6.403235050530126]
	TIME [epoch: 25 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.244304120081622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.244304120081622 | validation: 6.159229471023235]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.9827654247636275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.9827654247636275 | validation: 6.213538058378506]
	TIME [epoch: 25 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.970923990665971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.970923990665971 | validation: 5.924132558384354]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.826844685021029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.826844685021029 | validation: 6.2293535580504935]
	TIME [epoch: 25 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.061448964206677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.061448964206677 | validation: 6.083929121282002]
	TIME [epoch: 25 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.198778718947081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.198778718947081 | validation: 6.587274866579245]
	TIME [epoch: 25 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.0847418642576905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0847418642576905 | validation: 5.87955091309248]
	TIME [epoch: 25.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.82605297305933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.82605297305933 | validation: 5.863159566198334]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.100565015530719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.100565015530719 | validation: 6.085180482205893]
	TIME [epoch: 25 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.782715009292349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.782715009292349 | validation: 5.878487905991615]
	TIME [epoch: 25 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.805990145942952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.805990145942952 | validation: 5.981250514067288]
	TIME [epoch: 25 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.738934572456632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.738934572456632 | validation: 5.785717866619962]
	TIME [epoch: 24.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.641629780633734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.641629780633734 | validation: 5.817433717967019]
	TIME [epoch: 25 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.643696862786503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.643696862786503 | validation: 5.748683904659417]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.887369016823657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.887369016823657 | validation: 5.58776729245939]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.750060737738787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.750060737738787 | validation: 5.882104332180672]
	TIME [epoch: 25 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.757618452153538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.757618452153538 | validation: 5.733604658359764]
	TIME [epoch: 25 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.719606472996547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.719606472996547 | validation: 6.026723004465084]
	TIME [epoch: 25 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.517814461406531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.517814461406531 | validation: 5.512841541994021]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.248546478885871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.248546478885871 | validation: 5.397830989509869]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.058461260731093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.058461260731093 | validation: 5.263168221342353]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.068077156277681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.068077156277681 | validation: 6.6304818222718085]
	TIME [epoch: 25 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.717608161439979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.717608161439979 | validation: 5.310550258390521]
	TIME [epoch: 25 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.239551497133187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.239551497133187 | validation: 5.147127040303517]
	TIME [epoch: 24.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9944711182275405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9944711182275405 | validation: 4.812663025369594]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.509366844062025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.509366844062025 | validation: 6.504091787755079]
	TIME [epoch: 25 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.144308789039301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.144308789039301 | validation: 5.206803741083518]
	TIME [epoch: 25 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.723324679212911		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 4.723324679212911 | validation: 5.5157191779104275]
	TIME [epoch: 25 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.371689003266376		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 5.371689003266376 | validation: 5.562746199616881]
	TIME [epoch: 25 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.227942120296375		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 5.227942120296375 | validation: 5.035228867426469]
	TIME [epoch: 24.9 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.135167148923621		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 5.135167148923621 | validation: 5.519037807088007]
	TIME [epoch: 25 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.849846161739278		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 4.849846161739278 | validation: 4.317965171761657]
	TIME [epoch: 25 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r4_20240309_135656/states/model_tr_study5_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.437148142066856		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 6.437148142066856 | validation: 5.867893487869187]
	TIME [epoch: 25 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.177063873406441		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 5.177063873406441 | validation: 5.004879985412082]
	TIME [epoch: 25 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.844296906388522		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 4.844296906388522 | validation: 5.380571196394885]
	TIME [epoch: 25 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.054681546241698		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 5.054681546241698 | validation: 4.984601063443376]
	TIME [epoch: 25 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.906604533820844		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 4.906604533820844 | validation: 5.515248027059852]
	TIME [epoch: 25 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.346330411080278		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 5.346330411080278 | validation: 5.254104404732252]
	TIME [epoch: 25 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.612791573711176		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 4.612791573711176 | validation: 4.941864773862996]
	TIME [epoch: 24.9 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.510508990090759		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 5.510508990090759 | validation: 4.899864104861158]
	TIME [epoch: 25 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.826870308595632		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 4.826870308595632 | validation: 4.812820595538055]
	TIME [epoch: 25 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.041990316810851		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 6.041990316810851 | validation: 4.9183220910370995]
	TIME [epoch: 25 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4911663170915785		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 4.4911663170915785 | validation: 4.934764886212415]
	TIME [epoch: 25 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.985943291484021		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 4.985943291484021 | validation: 4.481295351009235]
	TIME [epoch: 25 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8167924400574815		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 4.8167924400574815 | validation: 5.1022568676201505]
	TIME [epoch: 24.9 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.918899278515303		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 4.918899278515303 | validation: 5.600673463239428]
	TIME [epoch: 25 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.815976117777588		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 4.815976117777588 | validation: 4.766868225124122]
	TIME [epoch: 25 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7557409644581465		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 4.7557409644581465 | validation: 4.9689492112581135]
	TIME [epoch: 24.9 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.66584880187608		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 4.66584880187608 | validation: 5.036212403386272]
	TIME [epoch: 25 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3619743419693044		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 5.3619743419693044 | validation: 5.006159001170953]
	TIME [epoch: 25 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.661361557217194		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 4.661361557217194 | validation: 6.498295557557574]
	TIME [epoch: 24.9 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.317171179742044		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 8.317171179742044 | validation: 9.425856124308982]
	TIME [epoch: 25 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.713820273812749		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 9.713820273812749 | validation: 9.2870485176892]
	TIME [epoch: 25 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.55221059063754		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 9.55221059063754 | validation: 9.16298082762828]
	TIME [epoch: 25 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.381836384841085		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 9.381836384841085 | validation: 8.918635315276203]
	TIME [epoch: 24.9 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.061967117806528		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 9.061967117806528 | validation: 8.477996843322435]
	TIME [epoch: 25 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.568317132810886		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 8.568317132810886 | validation: 9.067228986627471]
	TIME [epoch: 24.9 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.782977867833146		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 9.782977867833146 | validation: 11.352850391854528]
	TIME [epoch: 24.9 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.903642344241147		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 10.903642344241147 | validation: 8.890229082811764]
	TIME [epoch: 25 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.00470671217301		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 9.00470671217301 | validation: 8.936320973834295]
	TIME [epoch: 24.9 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.319145343082678		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 9.319145343082678 | validation: 8.896340508379485]
	TIME [epoch: 25 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.276156887431009		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 9.276156887431009 | validation: 8.708898788913757]
	TIME [epoch: 25 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.779652499855198		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 8.779652499855198 | validation: 7.5464265051924615]
	TIME [epoch: 24.9 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.25945308922582		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 7.25945308922582 | validation: 10.012184584848661]
	TIME [epoch: 25 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.06257411956056		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 9.06257411956056 | validation: 10.421646559081143]
	TIME [epoch: 25 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.569534332075188		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 9.569534332075188 | validation: 11.307026160975395]
	TIME [epoch: 24.9 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.07645841780019		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 10.07645841780019 | validation: 11.222213335939905]
	TIME [epoch: 24.9 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.241544147006026		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 9.241544147006026 | validation: 8.842138085869461]
	TIME [epoch: 25 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.565988900560352		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 9.565988900560352 | validation: 9.28586767286883]
	TIME [epoch: 24.9 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.811401810883101		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 7.811401810883101 | validation: 10.606152663532724]
	TIME [epoch: 24.9 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.894017400510261		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 10.894017400510261 | validation: 9.313801465756738]
	TIME [epoch: 25 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.867125502781655		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 7.867125502781655 | validation: 7.575375813688088]
	TIME [epoch: 24.9 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.413783066341448		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 7.413783066341448 | validation: 8.416768394050049]
	TIME [epoch: 24.9 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.399117781804154		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 7.399117781804154 | validation: 7.586519618548204]
	TIME [epoch: 24.9 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.164615322701068		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 7.164615322701068 | validation: 7.9944380329335525]
	TIME [epoch: 24.9 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.090900452198849		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 7.090900452198849 | validation: 7.311980853074065]
	TIME [epoch: 24.9 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.281401670112029		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 7.281401670112029 | validation: 8.159688082160972]
	TIME [epoch: 25 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.294532294533607		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 8.294532294533607 | validation: 10.863836661361425]
	TIME [epoch: 24.9 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.107810632950482		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 11.107810632950482 | validation: 10.210029954529611]
	TIME [epoch: 24.9 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.896547740432236		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 9.896547740432236 | validation: 9.167958474392233]
	TIME [epoch: 24.9 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.540407968610769		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 9.540407968610769 | validation: 9.229395646896265]
	TIME [epoch: 24.9 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.22216614804949		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 9.22216614804949 | validation: 8.706804293862378]
	TIME [epoch: 24.9 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.793350245688035		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 8.793350245688035 | validation: 8.446706347284115]
	TIME [epoch: 25 sec]
EPOCH 107/2000:
	Training over batches...
ERROR:
!!! UPDATED MODEL HAS NAN VALUES IN PHI.W[0] !!!
