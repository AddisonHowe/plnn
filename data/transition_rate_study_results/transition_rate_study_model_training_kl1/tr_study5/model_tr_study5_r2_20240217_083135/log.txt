Args:
Namespace(name='model_tr_study5', outdir='out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2', training_data='data/transition_rate_studies/tr_study5/tr_study5_training/r2', validation_data='data/transition_rate_studies/tr_study5/tr_study5_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=500, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2209008047

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.584943317068266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.584943317068266 | validation: 11.38763243629272]
	TIME [epoch: 49.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.40775013206662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.40775013206662 | validation: 10.56307138212083]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.657724474417627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.657724474417627 | validation: 10.95359394618622]
	TIME [epoch: 10.3 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.43872710809094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.43872710809094 | validation: 9.485991911109817]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.929094706760281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.929094706760281 | validation: 9.189698858566151]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.704726642356707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.704726642356707 | validation: 9.236781245193491]
	TIME [epoch: 10.3 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.256700497222704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.256700497222704 | validation: 8.869483038650122]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.267930075357766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.267930075357766 | validation: 7.875960395896645]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.746322422693293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.746322422693293 | validation: 8.410603894213235]
	TIME [epoch: 10.3 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.148562573069881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.148562573069881 | validation: 7.373332089330159]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.987890757762609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.987890757762609 | validation: 7.560808812621084]
	TIME [epoch: 10.3 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.250076590332171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.250076590332171 | validation: 6.305820589896371]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.84319953315367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.84319953315367 | validation: 6.210444032370383]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.756942069937693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.756942069937693 | validation: 5.710576587596964]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.524600954017936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.524600954017936 | validation: 6.954906730577551]
	TIME [epoch: 10.3 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.696230525731832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.696230525731832 | validation: 6.157980850624437]
	TIME [epoch: 10.3 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.284528251671491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.284528251671491 | validation: 6.32893344982904]
	TIME [epoch: 10.3 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.325556165684867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.325556165684867 | validation: 6.186250257680846]
	TIME [epoch: 10.3 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.398558967384682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.398558967384682 | validation: 5.72826871825883]
	TIME [epoch: 10.3 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.115655696367198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.115655696367198 | validation: 5.635801911787479]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.202221752755201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.202221752755201 | validation: 5.202523591201382]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.314104085238547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.314104085238547 | validation: 6.060995836355355]
	TIME [epoch: 10.3 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.162913515130364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.162913515130364 | validation: 4.824185984668942]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.901747323680633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.901747323680633 | validation: 4.6843203018433135]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.78300024949857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.78300024949857 | validation: 4.6715004087727765]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.183491058477366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.183491058477366 | validation: 5.218084289255511]
	TIME [epoch: 10.3 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.793695523162617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.793695523162617 | validation: 5.134031608743592]
	TIME [epoch: 10.3 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.670020949488749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.670020949488749 | validation: 4.956054978344303]
	TIME [epoch: 10.3 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.767623369299163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.767623369299163 | validation: 6.835201661838578]
	TIME [epoch: 10.4 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.212557200062571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.212557200062571 | validation: 4.898365173816152]
	TIME [epoch: 10.3 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.814114873309322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.814114873309322 | validation: 4.829635739973416]
	TIME [epoch: 10.3 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.542201340143514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.542201340143514 | validation: 5.311656799994325]
	TIME [epoch: 10.3 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7378571229565045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7378571229565045 | validation: 4.843957205209085]
	TIME [epoch: 10.3 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.570103469173537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.570103469173537 | validation: 4.349258531002508]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.657511460381936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.657511460381936 | validation: 4.466737695162234]
	TIME [epoch: 10.3 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.393680847247287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.393680847247287 | validation: 4.7055919638616395]
	TIME [epoch: 10.3 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.514337161672694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.514337161672694 | validation: 4.677870094838519]
	TIME [epoch: 10.3 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.434183159689065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.434183159689065 | validation: 4.315973105065845]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.321856264347865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.321856264347865 | validation: 4.936026558355175]
	TIME [epoch: 10.3 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.550673034851672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.550673034851672 | validation: 4.496183014888131]
	TIME [epoch: 10.3 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.425769197381329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.425769197381329 | validation: 4.568261892910804]
	TIME [epoch: 10.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.305403758556634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.305403758556634 | validation: 4.225713980136613]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234977030350725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.234977030350725 | validation: 7.519475421397388]
	TIME [epoch: 10.3 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.077248509217078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.077248509217078 | validation: 5.4129890147632205]
	TIME [epoch: 10.3 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.621349904883639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.621349904883639 | validation: 4.439025246149373]
	TIME [epoch: 10.3 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1015410630726254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1015410630726254 | validation: 4.115914378942436]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.112746019533103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.112746019533103 | validation: 4.1157017629741715]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.106782184414122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.106782184414122 | validation: 5.481335719314102]
	TIME [epoch: 10.3 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.540541048260149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.540541048260149 | validation: 4.119667910727324]
	TIME [epoch: 10.3 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.319793408695391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.319793408695391 | validation: 4.350225590008222]
	TIME [epoch: 10.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.124484752439574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.124484752439574 | validation: 3.963924441935713]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.118453700987103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.118453700987103 | validation: 4.046296188324069]
	TIME [epoch: 10.3 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.074407168922766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.074407168922766 | validation: 4.224380866497444]
	TIME [epoch: 10.3 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.996149644724867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.996149644724867 | validation: 4.016769049007442]
	TIME [epoch: 10.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.076867917336867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.076867917336867 | validation: 4.458183489423122]
	TIME [epoch: 10.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.107371608379387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.107371608379387 | validation: 4.349087606905917]
	TIME [epoch: 10.3 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3737474650043815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3737474650043815 | validation: 5.195551241661059]
	TIME [epoch: 10.3 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.232274501048089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.232274501048089 | validation: 3.9906218213797637]
	TIME [epoch: 10.3 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9232756546119285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9232756546119285 | validation: 4.006336107531472]
	TIME [epoch: 10.3 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.198319896465419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.198319896465419 | validation: 3.9499840714859857]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.09401023891731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.09401023891731 | validation: 3.961972964957072]
	TIME [epoch: 10.3 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.92526090082217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.92526090082217 | validation: 4.8695521873471845]
	TIME [epoch: 10.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.020473352101075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.020473352101075 | validation: 4.326989251712558]
	TIME [epoch: 10.3 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5982374835974404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5982374835974404 | validation: 4.136475565817188]
	TIME [epoch: 10.3 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.050404721336918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.050404721336918 | validation: 3.4131631669630145]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4218717099785265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4218717099785265 | validation: 2.9581351223643497]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.490638572693194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.490638572693194 | validation: 3.87619082918464]
	TIME [epoch: 10.3 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8852714656352703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8852714656352703 | validation: 3.439252799124507]
	TIME [epoch: 10.3 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3626510390147275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3626510390147275 | validation: 3.864571291008208]
	TIME [epoch: 10.3 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.346374283651442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.346374283651442 | validation: 3.5113696328225923]
	TIME [epoch: 10.3 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.487508187792939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.487508187792939 | validation: 4.62422814060799]
	TIME [epoch: 10.3 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8853414322719226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8853414322719226 | validation: 3.885367764314793]
	TIME [epoch: 10.3 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4488910944837676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4488910944837676 | validation: 3.2928029301211668]
	TIME [epoch: 10.3 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1876947244216955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1876947244216955 | validation: 3.3956223437199053]
	TIME [epoch: 10.3 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.749640971830664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.749640971830664 | validation: 3.9437897131846693]
	TIME [epoch: 10.3 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.768977429018612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.768977429018612 | validation: 3.7040295129185137]
	TIME [epoch: 10.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.18119323301993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.18119323301993 | validation: 3.6799058347218545]
	TIME [epoch: 10.3 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9671027545268687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9671027545268687 | validation: 3.4746914460734697]
	TIME [epoch: 10.3 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.81178467466592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.81178467466592 | validation: 2.9378851785434046]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25195539430162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.25195539430162 | validation: 5.091163682181398]
	TIME [epoch: 10.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.317165661226768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.317165661226768 | validation: 3.3227693133490788]
	TIME [epoch: 10.3 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.773354668522832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.773354668522832 | validation: 7.656880570824837]
	TIME [epoch: 10.3 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.241325545465809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.241325545465809 | validation: 3.975065215428856]
	TIME [epoch: 10.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.332303161086285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.332303161086285 | validation: 3.454028532907923]
	TIME [epoch: 10.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.698494246699351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.698494246699351 | validation: 3.3866177582549093]
	TIME [epoch: 10.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.307309032800748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.307309032800748 | validation: 3.114880918311294]
	TIME [epoch: 10.3 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1766169754292624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1766169754292624 | validation: 3.436181193636928]
	TIME [epoch: 10.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.546212848592044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.546212848592044 | validation: 3.0245271713922874]
	TIME [epoch: 10.3 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0188028585705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0188028585705 | validation: 2.7061928253579697]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2565798734094002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2565798734094002 | validation: 3.3220191542429856]
	TIME [epoch: 10.3 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.136565454542405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.136565454542405 | validation: 3.2981414240683775]
	TIME [epoch: 10.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3406854919151754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3406854919151754 | validation: 2.7898266019341906]
	TIME [epoch: 10.3 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.725895110293659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.725895110293659 | validation: 2.9455844418084025]
	TIME [epoch: 10.3 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1679284104793566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1679284104793566 | validation: 2.6134753438576435]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9297614606473323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9297614606473323 | validation: 4.354904934174874]
	TIME [epoch: 10.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9696335563534277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9696335563534277 | validation: 2.9972723665453556]
	TIME [epoch: 10.3 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.727586950111157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.727586950111157 | validation: 2.4501223784398283]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.79651068767794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.79651068767794 | validation: 2.6730466757196747]
	TIME [epoch: 10.3 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.103749585294215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.103749585294215 | validation: 2.358774022052723]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3042504525915435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3042504525915435 | validation: 2.5973650631664866]
	TIME [epoch: 10.3 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.071110188175626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.071110188175626 | validation: 2.734781392031269]
	TIME [epoch: 10.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.799560141201071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.799560141201071 | validation: 2.5519870777803657]
	TIME [epoch: 10.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6465796873322325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6465796873322325 | validation: 5.880095139810751]
	TIME [epoch: 10.3 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9923481688683227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9923481688683227 | validation: 2.6200484938965025]
	TIME [epoch: 10.3 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6670477967333754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6670477967333754 | validation: 4.15823323456433]
	TIME [epoch: 10.3 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.097234585932488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.097234585932488 | validation: 2.2721977705610037]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.722474712264865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.722474712264865 | validation: 2.214967508100592]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.916445717451367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.916445717451367 | validation: 3.013359304135798]
	TIME [epoch: 10.3 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.707396102371075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.707396102371075 | validation: 2.752319194280297]
	TIME [epoch: 10.3 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.499370018854404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.499370018854404 | validation: 2.417573803875047]
	TIME [epoch: 10.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7141982348873417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7141982348873417 | validation: 1.941564804863746]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.175563057881097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.175563057881097 | validation: 2.6593784631853095]
	TIME [epoch: 10.3 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.421983082221881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.421983082221881 | validation: 2.137772276620151]
	TIME [epoch: 10.3 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9601865236231006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9601865236231006 | validation: 2.0775774907226983]
	TIME [epoch: 10.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.540043906658661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.540043906658661 | validation: 3.013128700072347]
	TIME [epoch: 10.4 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977775320955229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.977775320955229 | validation: 2.262280530539525]
	TIME [epoch: 10.3 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3667469626440036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3667469626440036 | validation: 2.617702148293589]
	TIME [epoch: 10.3 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8270740820826084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8270740820826084 | validation: 2.4968744124518363]
	TIME [epoch: 10.3 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9925520139175004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9925520139175004 | validation: 2.620795157571338]
	TIME [epoch: 10.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9295408903305167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9295408903305167 | validation: 4.078873692570115]
	TIME [epoch: 10.3 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3271692607385575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3271692607385575 | validation: 1.8870402179107872]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4142543929068916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4142543929068916 | validation: 3.289926315999181]
	TIME [epoch: 10.3 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.448917027013189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.448917027013189 | validation: 3.2329863174029243]
	TIME [epoch: 10.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973705591432687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.973705591432687 | validation: 2.4187628833240264]
	TIME [epoch: 10.3 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4472831784115625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4472831784115625 | validation: 2.2740279195585034]
	TIME [epoch: 10.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3029616701289246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3029616701289246 | validation: 2.4678171451709705]
	TIME [epoch: 10.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.004754517184978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.004754517184978 | validation: 2.5295662401262393]
	TIME [epoch: 10.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.213615336583345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.213615336583345 | validation: 2.2416126324680175]
	TIME [epoch: 10.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3469432411525113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3469432411525113 | validation: 1.9872042251512678]
	TIME [epoch: 10.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3310405963478074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3310405963478074 | validation: 2.2647678330569274]
	TIME [epoch: 10.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5327597496324707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5327597496324707 | validation: 2.7574208623212337]
	TIME [epoch: 10.3 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.712260972694756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.712260972694756 | validation: 2.6041222726724245]
	TIME [epoch: 10.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.696537728285437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.696537728285437 | validation: 2.4339154786658255]
	TIME [epoch: 10.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5026283392716273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5026283392716273 | validation: 1.9016992650380533]
	TIME [epoch: 10.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8513308637551638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8513308637551638 | validation: 3.2148828485717686]
	TIME [epoch: 10.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.675583618458459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.675583618458459 | validation: 1.740725797629085]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8946713782634887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8946713782634887 | validation: 2.5290710737815907]
	TIME [epoch: 10.3 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0661710718435726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0661710718435726 | validation: 1.4728406278631299]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8251300941590944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8251300941590944 | validation: 2.1332734932146376]
	TIME [epoch: 10.3 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.775750030601709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.775750030601709 | validation: 1.8426665981868757]
	TIME [epoch: 10.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9551192851442862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9551192851442862 | validation: 1.717591416503065]
	TIME [epoch: 10.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7088558356226091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7088558356226091 | validation: 1.8055228284346478]
	TIME [epoch: 10.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8783307956270356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8783307956270356 | validation: 3.0313281715571847]
	TIME [epoch: 10.3 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5647734293163653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5647734293163653 | validation: 1.86965413965268]
	TIME [epoch: 10.3 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9534122812008268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9534122812008268 | validation: 2.0402218953231728]
	TIME [epoch: 10.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0336081866509743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0336081866509743 | validation: 2.03540960671792]
	TIME [epoch: 10.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.161952559995095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.161952559995095 | validation: 1.461911210751041]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.036177550648424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.036177550648424 | validation: 1.8537453289282908]
	TIME [epoch: 10.3 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8318222665875894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8318222665875894 | validation: 1.8910840442199293]
	TIME [epoch: 10.3 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1185103029604386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1185103029604386 | validation: 1.5806187164097696]
	TIME [epoch: 10.3 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0979715461265567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0979715461265567 | validation: 1.6948511802729067]
	TIME [epoch: 10.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0976628856648327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0976628856648327 | validation: 2.031021475507098]
	TIME [epoch: 10.3 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.857667295737072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.857667295737072 | validation: 1.6155139235204565]
	TIME [epoch: 10.3 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9731328791805578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9731328791805578 | validation: 2.583978182284558]
	TIME [epoch: 10.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6960528501556857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6960528501556857 | validation: 1.6715840708814977]
	TIME [epoch: 10.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7712312865937438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7712312865937438 | validation: 1.6671840742633532]
	TIME [epoch: 10.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0509555673827378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0509555673827378 | validation: 1.5620274968190422]
	TIME [epoch: 10.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.227349540913035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.227349540913035 | validation: 1.51530549439934]
	TIME [epoch: 10.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.524214566774419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.524214566774419 | validation: 1.9814372606677797]
	TIME [epoch: 10.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9933558428148594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9933558428148594 | validation: 1.54961390524235]
	TIME [epoch: 10.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9050376324886424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9050376324886424 | validation: 1.9401087697123225]
	TIME [epoch: 10.3 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6957386164070627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6957386164070627 | validation: 2.2802899273523383]
	TIME [epoch: 10.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9024661997827632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9024661997827632 | validation: 2.47389747704644]
	TIME [epoch: 10.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.398170748754824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.398170748754824 | validation: 1.8533368792202516]
	TIME [epoch: 10.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4627233820688015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4627233820688015 | validation: 2.086300372577199]
	TIME [epoch: 10.3 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8908161600549729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8908161600549729 | validation: 2.1015587162604197]
	TIME [epoch: 10.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9245511380766533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9245511380766533 | validation: 1.43123589746402]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9069913288688631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9069913288688631 | validation: 2.67247138985158]
	TIME [epoch: 10.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1844979323947307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1844979323947307 | validation: 3.1768150588844652]
	TIME [epoch: 10.3 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7377964621442494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7377964621442494 | validation: 1.6544693214773238]
	TIME [epoch: 10.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9113188428439518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9113188428439518 | validation: 1.9560006330165658]
	TIME [epoch: 10.3 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3006774331987194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3006774331987194 | validation: 1.8783140228761186]
	TIME [epoch: 10.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0771643150610712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0771643150610712 | validation: 1.9802895881000564]
	TIME [epoch: 10.3 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2041864360149823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2041864360149823 | validation: 2.2846919741557072]
	TIME [epoch: 10.3 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.111124071801648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.111124071801648 | validation: 1.8340357328060242]
	TIME [epoch: 10.3 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9912048072480624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9912048072480624 | validation: 2.022160433720705]
	TIME [epoch: 10.3 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0113692019499494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0113692019499494 | validation: 1.7437835555431849]
	TIME [epoch: 10.3 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288235331239273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.288235331239273 | validation: 1.6766883861764998]
	TIME [epoch: 10.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3411614643780663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3411614643780663 | validation: 2.5876973959047755]
	TIME [epoch: 10.3 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.336787586468374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.336787586468374 | validation: 1.9167859286862472]
	TIME [epoch: 10.3 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5348277792612124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5348277792612124 | validation: 5.321892221952347]
	TIME [epoch: 10.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0781670748019785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0781670748019785 | validation: 1.8266145960153706]
	TIME [epoch: 10.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5380256206018172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5380256206018172 | validation: 6.328034692491457]
	TIME [epoch: 10.3 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.918530255944522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.918530255944522 | validation: 2.575686952169937]
	TIME [epoch: 10.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5500682290216075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5500682290216075 | validation: 2.639470298020002]
	TIME [epoch: 10.3 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4689772461584107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4689772461584107 | validation: 1.921034566502511]
	TIME [epoch: 10.3 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4242304423292698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4242304423292698 | validation: 1.560780674361235]
	TIME [epoch: 10.3 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.833501354392534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.833501354392534 | validation: 2.027131992641653]
	TIME [epoch: 10.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.680083888218255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.680083888218255 | validation: 2.2805702461280988]
	TIME [epoch: 10.3 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4819117995870443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4819117995870443 | validation: 1.6936056485510167]
	TIME [epoch: 10.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5161897956404795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5161897956404795 | validation: 2.283781816034985]
	TIME [epoch: 10.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.416948008686721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.416948008686721 | validation: 2.2192367578542904]
	TIME [epoch: 10.3 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4375527287459917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4375527287459917 | validation: 1.8996916087948044]
	TIME [epoch: 10.3 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.324155904419082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.324155904419082 | validation: 2.1469061189463936]
	TIME [epoch: 10.3 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4182311192245707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4182311192245707 | validation: 2.0428618750261793]
	TIME [epoch: 10.3 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.398634301593884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.398634301593884 | validation: 2.1741399916405233]
	TIME [epoch: 10.3 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4606761051371446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4606761051371446 | validation: 2.1049646082924443]
	TIME [epoch: 10.3 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.298640861610275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.298640861610275 | validation: 2.1731676189727054]
	TIME [epoch: 10.3 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6608431277094144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6608431277094144 | validation: 2.2935622349718847]
	TIME [epoch: 10.3 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3893252366804325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3893252366804325 | validation: 2.014725729393626]
	TIME [epoch: 10.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.275180990725773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.275180990725773 | validation: 1.8036293851696719]
	TIME [epoch: 10.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7881373764243778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7881373764243778 | validation: 2.3896280224081585]
	TIME [epoch: 10.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248820036111132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.248820036111132 | validation: 1.6642717524780568]
	TIME [epoch: 10.3 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.304310193648713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.304310193648713 | validation: 1.7364529381725882]
	TIME [epoch: 10.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1592467697195317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1592467697195317 | validation: 1.4693611178066408]
	TIME [epoch: 10.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6167355508233485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6167355508233485 | validation: 1.4060817474212783]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9445433692174556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9445433692174556 | validation: 1.8207497859799555]
	TIME [epoch: 10.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.899806939825893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.899806939825893 | validation: 1.5396594989613082]
	TIME [epoch: 10.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1606258317052336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1606258317052336 | validation: 1.4769483884852657]
	TIME [epoch: 10.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5658365648027277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5658365648027277 | validation: 1.5800386057789302]
	TIME [epoch: 10.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.494186194840828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.494186194840828 | validation: 1.2703042777617999]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2827499885568059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2827499885568059 | validation: 1.3681484816853162]
	TIME [epoch: 10.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5283222193653614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5283222193653614 | validation: 1.1991901721215132]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8192196048862441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8192196048862441 | validation: 1.5869407467682977]
	TIME [epoch: 10.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.046239111150207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.046239111150207 | validation: 1.2790010463956083]
	TIME [epoch: 10.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7639106660478814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7639106660478814 | validation: 1.9400301333939536]
	TIME [epoch: 10.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5838799434522182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5838799434522182 | validation: 1.3998600071505007]
	TIME [epoch: 10.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5485816812795623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5485816812795623 | validation: 1.814051426175799]
	TIME [epoch: 10.3 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8716124694340721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8716124694340721 | validation: 2.3562149382528124]
	TIME [epoch: 10.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9537706135407902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9537706135407902 | validation: 1.968917534883783]
	TIME [epoch: 10.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6390131004542021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6390131004542021 | validation: 1.4681028655933406]
	TIME [epoch: 10.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5205795418639247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5205795418639247 | validation: 1.4376561627337043]
	TIME [epoch: 10.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.193194596523586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.193194596523586 | validation: 2.1383229492771196]
	TIME [epoch: 10.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.230585368188655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.230585368188655 | validation: 2.222808936480321]
	TIME [epoch: 10.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0213255042491243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0213255042491243 | validation: 1.5558553013877816]
	TIME [epoch: 10.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0868698888901345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0868698888901345 | validation: 1.7602821221866813]
	TIME [epoch: 10.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7044272444202138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7044272444202138 | validation: 4.845499732503092]
	TIME [epoch: 10.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9433445978763544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9433445978763544 | validation: 1.6333936453161386]
	TIME [epoch: 10.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8396442686783963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8396442686783963 | validation: 2.3325171605094033]
	TIME [epoch: 10.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4329437814512316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4329437814512316 | validation: 2.057278658016484]
	TIME [epoch: 10.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.223585794296352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.223585794296352 | validation: 1.8035405201610133]
	TIME [epoch: 10.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.264578097414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.264578097414 | validation: 1.415310470173843]
	TIME [epoch: 10.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.47694353253089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.47694353253089 | validation: 1.4291646413488344]
	TIME [epoch: 10.3 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5948358038144137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5948358038144137 | validation: 1.5890944032084509]
	TIME [epoch: 10.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7111555021495868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7111555021495868 | validation: 1.9311587107844952]
	TIME [epoch: 10.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.929448303102168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.929448303102168 | validation: 1.3502803337515081]
	TIME [epoch: 10.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4489523735751288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4489523735751288 | validation: 1.3122776788673225]
	TIME [epoch: 10.3 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.680022429723157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.680022429723157 | validation: 1.306701707624836]
	TIME [epoch: 10.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7344787759733933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7344787759733933 | validation: 1.1604521415575988]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6548943940787768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6548943940787768 | validation: 3.2413957343917406]
	TIME [epoch: 10.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8962289002305313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8962289002305313 | validation: 2.0095507937453556]
	TIME [epoch: 10.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.142598765423965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.142598765423965 | validation: 1.8333656838398662]
	TIME [epoch: 10.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.901425382838159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.901425382838159 | validation: 1.788216502664977]
	TIME [epoch: 10.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6875483190604244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6875483190604244 | validation: 1.8125949258429157]
	TIME [epoch: 10.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6108575424787346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6108575424787346 | validation: 1.6399932088847322]
	TIME [epoch: 10.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.66031069391323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.66031069391323 | validation: 1.5839747364987142]
	TIME [epoch: 10.3 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6664696258554983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6664696258554983 | validation: 1.6017010492764714]
	TIME [epoch: 10.3 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.908633661567362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.908633661567362 | validation: 2.2679019906920987]
	TIME [epoch: 10.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6053044244949186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6053044244949186 | validation: 2.733959046916153]
	TIME [epoch: 10.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7855357232887195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7855357232887195 | validation: 1.8268816891046111]
	TIME [epoch: 10.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6254661853669774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6254661853669774 | validation: 1.5606544669863456]
	TIME [epoch: 10.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.503428680608316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.503428680608316 | validation: 1.9421959799388928]
	TIME [epoch: 10.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.103699170867882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.103699170867882 | validation: 9.895346260214872]
	TIME [epoch: 10.3 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.62396959859569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.62396959859569 | validation: 6.379582739026655]
	TIME [epoch: 10.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.137086057962554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.137086057962554 | validation: 2.638956989457697]
	TIME [epoch: 10.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.322153390186586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.322153390186586 | validation: 2.68065655798113]
	TIME [epoch: 10.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0458069449912415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0458069449912415 | validation: 1.4676943725744644]
	TIME [epoch: 10.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7199314797190959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7199314797190959 | validation: 1.9081058847437413]
	TIME [epoch: 10.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1031041975897944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1031041975897944 | validation: 1.7010608665693292]
	TIME [epoch: 10.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5734583539595348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5734583539595348 | validation: 1.431519750133565]
	TIME [epoch: 10.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4858191392254518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4858191392254518 | validation: 1.3774076711995284]
	TIME [epoch: 10.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.682516927158968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.682516927158968 | validation: 1.3051969102648968]
	TIME [epoch: 10.3 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.452270987793583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.452270987793583 | validation: 2.2446474048647658]
	TIME [epoch: 10.3 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0174421125010005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0174421125010005 | validation: 1.843476782339062]
	TIME [epoch: 10.3 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0743619131158755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0743619131158755 | validation: 1.4511289434127412]
	TIME [epoch: 10.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.55723698555954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.55723698555954 | validation: 1.4468967551775935]
	TIME [epoch: 10.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.615776125985218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.615776125985218 | validation: 1.507705062010876]
	TIME [epoch: 10.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6814445716261848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6814445716261848 | validation: 1.2057034586461512]
	TIME [epoch: 10.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6463283901691628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6463283901691628 | validation: 1.44080363349511]
	TIME [epoch: 10.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6466475220286942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6466475220286942 | validation: 1.6664472056727087]
	TIME [epoch: 10.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5947464219523564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5947464219523564 | validation: 1.664662351509329]
	TIME [epoch: 10.3 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.84346777158578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.84346777158578 | validation: 1.5668583119618884]
	TIME [epoch: 10.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6627696591085812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6627696591085812 | validation: 1.8023408905193896]
	TIME [epoch: 10.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4730945211635125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4730945211635125 | validation: 1.5737092431291375]
	TIME [epoch: 10.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.001919964954349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.001919964954349 | validation: 1.1457409611216245]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9995479523015391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9995479523015391 | validation: 1.7678131337278244]
	TIME [epoch: 10.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.796777912101539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.796777912101539 | validation: 1.501924062642352]
	TIME [epoch: 10.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6779216998618995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6779216998618995 | validation: 1.5682072232736766]
	TIME [epoch: 10.3 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4705059804117453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4705059804117453 | validation: 1.3630780803303895]
	TIME [epoch: 10.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3341099936644196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3341099936644196 | validation: 1.2798578014171311]
	TIME [epoch: 10.3 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7399041561762032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7399041561762032 | validation: 1.376956456954727]
	TIME [epoch: 10.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4791012782065356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4791012782065356 | validation: 1.7346884531287026]
	TIME [epoch: 10.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6310278146682464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6310278146682464 | validation: 1.1980365629905054]
	TIME [epoch: 10.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5813248651451492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5813248651451492 | validation: 1.2286419258632786]
	TIME [epoch: 10.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8694000219723617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8694000219723617 | validation: 1.1377883952359944]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5039787307550978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5039787307550978 | validation: 1.291581945981474]
	TIME [epoch: 10.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7295586808491936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7295586808491936 | validation: 2.3145143880206134]
	TIME [epoch: 10.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6351393847612765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6351393847612765 | validation: 1.4949470386046337]
	TIME [epoch: 10.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5217191417904905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5217191417904905 | validation: 1.4559430853762405]
	TIME [epoch: 10.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.450202641730868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.450202641730868 | validation: 1.3796146369146312]
	TIME [epoch: 10.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.342994827973774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.342994827973774 | validation: 0.9997787787809561]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2713313770417365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2713313770417365 | validation: 1.448140441339436]
	TIME [epoch: 10.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8174044928803674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8174044928803674 | validation: 2.0142455804435975]
	TIME [epoch: 10.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.975509835989054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.975509835989054 | validation: 1.551122561766605]
	TIME [epoch: 10.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.557568653840048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.557568653840048 | validation: 1.4076043101424636]
	TIME [epoch: 10.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8407422411577308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8407422411577308 | validation: 1.7281843585502372]
	TIME [epoch: 10.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6533229324806835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6533229324806835 | validation: 1.7886887237305515]
	TIME [epoch: 10.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5703898298552876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5703898298552876 | validation: 1.356909242545437]
	TIME [epoch: 10.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7856812272914389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7856812272914389 | validation: 2.915608673997673]
	TIME [epoch: 10.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.679047328942002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.679047328942002 | validation: 1.6667943730177297]
	TIME [epoch: 10.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7547436407237231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7547436407237231 | validation: 1.772521910337637]
	TIME [epoch: 10.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9350179500115956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9350179500115956 | validation: 1.5837849909112505]
	TIME [epoch: 10.3 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8203234599282467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8203234599282467 | validation: 1.7285940879492967]
	TIME [epoch: 10.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8474565992156151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8474565992156151 | validation: 1.2559067273397475]
	TIME [epoch: 10.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5508722314843537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5508722314843537 | validation: 1.4163833158758998]
	TIME [epoch: 10.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4437668295257116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4437668295257116 | validation: 1.1900319777291017]
	TIME [epoch: 10.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5070915793866009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5070915793866009 | validation: 1.470839093707654]
	TIME [epoch: 10.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8300494370782032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8300494370782032 | validation: 1.6487515356411035]
	TIME [epoch: 10.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4198680152882788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4198680152882788 | validation: 1.3417106548292992]
	TIME [epoch: 10.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5376070823567223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5376070823567223 | validation: 1.6140841044877439]
	TIME [epoch: 10.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9991886952314863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9991886952314863 | validation: 1.6099634343892746]
	TIME [epoch: 10.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9927345565559702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9927345565559702 | validation: 1.3728737034199554]
	TIME [epoch: 10.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8725759415128613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8725759415128613 | validation: 1.5766256525570546]
	TIME [epoch: 10.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4204451717675148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4204451717675148 | validation: 1.5214270792936833]
	TIME [epoch: 10.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.841323369417155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.841323369417155 | validation: 1.6276920481827168]
	TIME [epoch: 10.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.882002793039295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.882002793039295 | validation: 1.4902511732753407]
	TIME [epoch: 10.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3761849716181234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3761849716181234 | validation: 1.9264104226736982]
	TIME [epoch: 10.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0675853420588703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0675853420588703 | validation: 1.8977524570366142]
	TIME [epoch: 10.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2165866286316893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2165866286316893 | validation: 3.800313338657442]
	TIME [epoch: 10.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.610290632433214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.610290632433214 | validation: 1.8603272214309141]
	TIME [epoch: 10.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8556630425578937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8556630425578937 | validation: 2.2377724117109237]
	TIME [epoch: 10.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.454088880864849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.454088880864849 | validation: 2.433729354354387]
	TIME [epoch: 10.3 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2756765401748646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2756765401748646 | validation: 2.2930307590824226]
	TIME [epoch: 10.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7742265494547547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7742265494547547 | validation: 5.265654707009116]
	TIME [epoch: 10.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.297269337075258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.297269337075258 | validation: 3.039260600672942]
	TIME [epoch: 10.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4756783766030015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4756783766030015 | validation: 1.2893676122195508]
	TIME [epoch: 10.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.332876946287293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.332876946287293 | validation: 1.4780435198201263]
	TIME [epoch: 10.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6533251083009577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6533251083009577 | validation: 1.6287721396962362]
	TIME [epoch: 10.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4036990796890227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4036990796890227 | validation: 1.534997491179693]
	TIME [epoch: 10.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.596028006824841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.596028006824841 | validation: 1.154989710301507]
	TIME [epoch: 10.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2841294941234105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2841294941234105 | validation: 1.8164336284909368]
	TIME [epoch: 10.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.488423985501046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.488423985501046 | validation: 1.7627978415050343]
	TIME [epoch: 10.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5932856770853856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5932856770853856 | validation: 1.2012353663023199]
	TIME [epoch: 10.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184885636864335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.184885636864335 | validation: 2.752458111650437]
	TIME [epoch: 10.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2123765060817493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2123765060817493 | validation: 1.342549584497936]
	TIME [epoch: 10.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5793596455057757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5793596455057757 | validation: 1.3480035251297668]
	TIME [epoch: 10.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.129971286618605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.129971286618605 | validation: 1.5308420884772442]
	TIME [epoch: 10.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6322253592928728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6322253592928728 | validation: 1.4821911064449773]
	TIME [epoch: 10.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8434003165468773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8434003165468773 | validation: 1.6350797251604037]
	TIME [epoch: 10.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6389248766265319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6389248766265319 | validation: 2.152429716649729]
	TIME [epoch: 10.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.918893633708296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.918893633708296 | validation: 2.520808843833292]
	TIME [epoch: 10.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8711329067998232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8711329067998232 | validation: 1.47211169032086]
	TIME [epoch: 10.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.758709994145099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.758709994145099 | validation: 1.5138259430833785]
	TIME [epoch: 10.3 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441963560573182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7441963560573182 | validation: 1.7925106527283043]
	TIME [epoch: 10.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8358242983091706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8358242983091706 | validation: 1.9107356711229893]
	TIME [epoch: 10.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.003477294815892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.003477294815892 | validation: 1.6769661909727844]
	TIME [epoch: 10.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.062367327066647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.062367327066647 | validation: 1.438181038896894]
	TIME [epoch: 10.3 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5687911595864032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5687911595864032 | validation: 1.5729104538957162]
	TIME [epoch: 10.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3146812680035977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3146812680035977 | validation: 1.708516673155991]
	TIME [epoch: 10.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.100980891286206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.100980891286206 | validation: 2.2096606000624712]
	TIME [epoch: 10.3 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.661582971669327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.661582971669327 | validation: 1.5397034748537561]
	TIME [epoch: 10.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.922801376694696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.922801376694696 | validation: 1.9125653565186076]
	TIME [epoch: 10.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6434997754093679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6434997754093679 | validation: 1.8654885609430398]
	TIME [epoch: 10.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.636390735023348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.636390735023348 | validation: 1.9522841026444218]
	TIME [epoch: 10.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.910535569606879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.910535569606879 | validation: 1.5776013548282057]
	TIME [epoch: 10.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8064884704225381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8064884704225381 | validation: 1.9991741554824378]
	TIME [epoch: 10.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.77011957788398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.77011957788398 | validation: 1.4728001339467875]
	TIME [epoch: 10.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6481781054992257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6481781054992257 | validation: 1.6884505586997802]
	TIME [epoch: 10.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9177332590192335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9177332590192335 | validation: 1.6884373373707109]
	TIME [epoch: 10.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9260452757017066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9260452757017066 | validation: 1.6030563677360812]
	TIME [epoch: 10.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6673456381157412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6673456381157412 | validation: 1.5445017344928045]
	TIME [epoch: 10.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7516653699349825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7516653699349825 | validation: 2.3112749993070953]
	TIME [epoch: 10.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9602232419085737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9602232419085737 | validation: 2.5733117888838324]
	TIME [epoch: 10.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8210697021286915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8210697021286915 | validation: 2.0483276625262268]
	TIME [epoch: 10.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.118282427492283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.118282427492283 | validation: 1.587035065627733]
	TIME [epoch: 10.3 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8436371402574518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8436371402574518 | validation: 2.1883672408142774]
	TIME [epoch: 10.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.800618319108588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.800618319108588 | validation: 1.7772630215105347]
	TIME [epoch: 10.3 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7294032633409828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7294032633409828 | validation: 1.537254006746894]
	TIME [epoch: 10.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.732982139032989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.732982139032989 | validation: 1.7363865425814116]
	TIME [epoch: 10.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.865995122218213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.865995122218213 | validation: 1.1972604983712822]
	TIME [epoch: 10.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263402841461908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.263402841461908 | validation: 1.8048077377445373]
	TIME [epoch: 10.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5634864669187845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5634864669187845 | validation: 1.6988210024217647]
	TIME [epoch: 10.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9096824819152254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9096824819152254 | validation: 2.775810847770218]
	TIME [epoch: 10.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0038338542714866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0038338542714866 | validation: 2.024951971667866]
	TIME [epoch: 10.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7858759638472965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7858759638472965 | validation: 1.296714584973621]
	TIME [epoch: 10.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.594639369674505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.594639369674505 | validation: 1.562548279708853]
	TIME [epoch: 10.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.865614665717913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.865614665717913 | validation: 1.5144960966933076]
	TIME [epoch: 10.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.831382150455007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.831382150455007 | validation: 1.62622201667877]
	TIME [epoch: 10.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9453612520807468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9453612520807468 | validation: 1.3888738947371673]
	TIME [epoch: 10.3 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.120036693675536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.120036693675536 | validation: 1.802084037050863]
	TIME [epoch: 10.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.226067097875442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.226067097875442 | validation: 2.953607625956479]
	TIME [epoch: 10.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.060059669712461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.060059669712461 | validation: 1.9189645562889446]
	TIME [epoch: 10.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2263020506555042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2263020506555042 | validation: 1.7656506350855552]
	TIME [epoch: 10.3 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9658121247053117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9658121247053117 | validation: 1.690722956662584]
	TIME [epoch: 10.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4922225598706165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4922225598706165 | validation: 1.8719974038751839]
	TIME [epoch: 10.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.229733042977447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.229733042977447 | validation: 2.0544427128334926]
	TIME [epoch: 10.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.039371552546096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.039371552546096 | validation: 1.7325255795245103]
	TIME [epoch: 10.3 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9682663179683586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9682663179683586 | validation: 1.8105568792846982]
	TIME [epoch: 10.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6515005512529126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6515005512529126 | validation: 1.4582215085885355]
	TIME [epoch: 10.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.42121767609959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.42121767609959 | validation: 1.5217193663672088]
	TIME [epoch: 10.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9606975158792774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9606975158792774 | validation: 1.938443123313073]
	TIME [epoch: 10.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.056886134397242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.056886134397242 | validation: 2.1267975759804982]
	TIME [epoch: 10.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7704534270731163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7704534270731163 | validation: 1.7302302104301372]
	TIME [epoch: 10.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7504670002511813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7504670002511813 | validation: 1.5398315335145127]
	TIME [epoch: 10.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5718199618414483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5718199618414483 | validation: 1.3501886484464316]
	TIME [epoch: 10.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.786153257186411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.786153257186411 | validation: 1.1824778280730552]
	TIME [epoch: 10.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.172019701409422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.172019701409422 | validation: 1.9094713924895843]
	TIME [epoch: 10.3 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4188891979854468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4188891979854468 | validation: 2.0465290933458897]
	TIME [epoch: 10.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0955757813745444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0955757813745444 | validation: 1.8006491611923188]
	TIME [epoch: 10.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1093969075687524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1093969075687524 | validation: 1.6973987339016983]
	TIME [epoch: 10.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.707947367006352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.707947367006352 | validation: 1.8372993555259678]
	TIME [epoch: 10.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.794921112718694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.794921112718694 | validation: 1.3122321255835845]
	TIME [epoch: 10.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6565116222563514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6565116222563514 | validation: 1.9496087932397876]
	TIME [epoch: 10.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3627869222303786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3627869222303786 | validation: 1.5634935275965083]
	TIME [epoch: 10.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7294693552121607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7294693552121607 | validation: 1.9501257808757435]
	TIME [epoch: 10.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9846970214065287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9846970214065287 | validation: 1.456118553584098]
	TIME [epoch: 10.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9732418490961101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9732418490961101 | validation: 2.2323412852024838]
	TIME [epoch: 10.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7827525700969162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7827525700969162 | validation: 1.3678515260490638]
	TIME [epoch: 10.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.603767547041977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.603767547041977 | validation: 1.590867463068517]
	TIME [epoch: 10.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0297165993779616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0297165993779616 | validation: 1.6487919257019608]
	TIME [epoch: 10.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1550206038123902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1550206038123902 | validation: 1.9720714056062054]
	TIME [epoch: 10.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1526954359584938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1526954359584938 | validation: 1.9546797687883983]
	TIME [epoch: 10.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1209476679290624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1209476679290624 | validation: 1.4507876566816844]
	TIME [epoch: 10.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7452984522054584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7452984522054584 | validation: 1.8329752564688908]
	TIME [epoch: 10.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.671556578373688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.671556578373688 | validation: 1.8781558373151546]
	TIME [epoch: 10.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8324868485453691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8324868485453691 | validation: 1.579065897191987]
	TIME [epoch: 10.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9357183749353972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9357183749353972 | validation: 2.011985274147308]
	TIME [epoch: 10.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.700447177177341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.700447177177341 | validation: 1.6945009273600897]
	TIME [epoch: 10.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.909268707318505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.909268707318505 | validation: 1.6040397425658577]
	TIME [epoch: 10.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6170938313641134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6170938313641134 | validation: 1.6351514758649754]
	TIME [epoch: 10.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6266420188927864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6266420188927864 | validation: 1.886779872372149]
	TIME [epoch: 10.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.092203671749378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.092203671749378 | validation: 1.95244967154631]
	TIME [epoch: 10.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9049873401097994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9049873401097994 | validation: 1.5650092597898861]
	TIME [epoch: 10.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8541719018618814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8541719018618814 | validation: 1.42776860607882]
	TIME [epoch: 10.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8169035768696156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8169035768696156 | validation: 1.6395193351124402]
	TIME [epoch: 10.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.857969982721544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.857969982721544 | validation: 1.8288445352626894]
	TIME [epoch: 10.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750408405740242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.750408405740242 | validation: 1.5975923033764332]
	TIME [epoch: 10.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7982980539721947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7982980539721947 | validation: 1.6204461767617862]
	TIME [epoch: 10.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425577649005788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7425577649005788 | validation: 1.443657214225034]
	TIME [epoch: 10.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7242435965395024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7242435965395024 | validation: 1.5412387842549473]
	TIME [epoch: 10.3 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1811079067455035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1811079067455035 | validation: 1.6058098451069145]
	TIME [epoch: 10.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1552214572401005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1552214572401005 | validation: 1.7963241269143624]
	TIME [epoch: 10.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.047366699414831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.047366699414831 | validation: 2.072536494032623]
	TIME [epoch: 10.3 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.262979091250367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.262979091250367 | validation: 4.728888895215949]
	TIME [epoch: 10.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.101174775966823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.101174775966823 | validation: 1.740065771170071]
	TIME [epoch: 10.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.010474086976118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.010474086976118 | validation: 3.8751063498516327]
	TIME [epoch: 10.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1256377567209315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1256377567209315 | validation: 2.6832119100911744]
	TIME [epoch: 10.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9157354076907147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9157354076907147 | validation: 2.8595368827406076]
	TIME [epoch: 10.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9846168629910634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9846168629910634 | validation: 2.9961558548708083]
	TIME [epoch: 10.3 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.706111541374875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.706111541374875 | validation: 2.2997777400473103]
	TIME [epoch: 10.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.224894620180831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.224894620180831 | validation: 1.9757053408899905]
	TIME [epoch: 10.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8336651373133912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8336651373133912 | validation: 1.6689343264043819]
	TIME [epoch: 10.3 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8883693803996628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8883693803996628 | validation: 2.5499568767905094]
	TIME [epoch: 10.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0243207954273243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0243207954273243 | validation: 1.7814839769852726]
	TIME [epoch: 10.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8984334793038466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8984334793038466 | validation: 1.8886264781625897]
	TIME [epoch: 10.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6327798716642663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6327798716642663 | validation: 2.386125827399646]
	TIME [epoch: 10.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2589436603897446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2589436603897446 | validation: 2.4447687463031706]
	TIME [epoch: 10.3 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5493813420886697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5493813420886697 | validation: 2.0495410380312653]
	TIME [epoch: 10.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.078706801678202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.078706801678202 | validation: 1.8190247143147087]
	TIME [epoch: 10.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0100761630246087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0100761630246087 | validation: 1.808995191312041]
	TIME [epoch: 10.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7670530320825222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7670530320825222 | validation: 1.813034779165996]
	TIME [epoch: 10.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7610010645033107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7610010645033107 | validation: 1.5656754348727049]
	TIME [epoch: 10.3 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.854176939436075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.854176939436075 | validation: 2.149042210848266]
	TIME [epoch: 10.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1341676553880964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1341676553880964 | validation: 1.9035087641561546]
	TIME [epoch: 10.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9700087978135996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9700087978135996 | validation: 3.417245317408082]
	TIME [epoch: 10.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2106710292696516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2106710292696516 | validation: 1.7494397759204898]
	TIME [epoch: 10.3 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.935288750596112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.935288750596112 | validation: 2.300120938212485]
	TIME [epoch: 10.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6211882559292987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6211882559292987 | validation: 1.76592654050536]
	TIME [epoch: 10.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18059319501409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.18059319501409 | validation: 2.161740465902325]
	TIME [epoch: 10.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.950901674829788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.950901674829788 | validation: 1.465703993687756]
	TIME [epoch: 10.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6992780316191396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6992780316191396 | validation: 1.9885414767049736]
	TIME [epoch: 10.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3021912570677943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3021912570677943 | validation: 1.8907182093674992]
	TIME [epoch: 10.3 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.141312232457338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.141312232457338 | validation: 2.0576640977043428]
	TIME [epoch: 10.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.906927896248466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.906927896248466 | validation: 1.305864263852701]
	TIME [epoch: 10.3 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4802523699627959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4802523699627959 | validation: 1.7721604416285937]
	TIME [epoch: 10.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.930537323499653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.930537323499653 | validation: 1.7868896108412975]
	TIME [epoch: 10.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.877500221173183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.877500221173183 | validation: 2.341912741064638]
	TIME [epoch: 10.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9973708405754707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9973708405754707 | validation: 1.9463382817594272]
	TIME [epoch: 10.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0059220615912743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0059220615912743 | validation: 1.848029565194127]
	TIME [epoch: 10.3 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.984412530457939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.984412530457939 | validation: 1.516630349847501]
	TIME [epoch: 10.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8421227760791585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8421227760791585 | validation: 1.7166081913685685]
	TIME [epoch: 10.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8443400391870206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8443400391870206 | validation: 1.9864915908077623]
	TIME [epoch: 10.3 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185461425316478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.185461425316478 | validation: 1.9351177713093746]
	TIME [epoch: 10.3 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.893368393624268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.893368393624268 | validation: 1.363277998727124]
	TIME [epoch: 10.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6190491754906855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6190491754906855 | validation: 1.9889261985372941]
	TIME [epoch: 10.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8291467540434685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8291467540434685 | validation: 1.6150456792908099]
	TIME [epoch: 10.3 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9547428996584302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9547428996584302 | validation: 1.7560054581333426]
	TIME [epoch: 10.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.63389725869316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.63389725869316 | validation: 1.656996923920213]
	TIME [epoch: 10.3 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.20933291549939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.20933291549939 | validation: 2.335773453583767]
	TIME [epoch: 10.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7030614428449726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7030614428449726 | validation: 1.6989401419534347]
	TIME [epoch: 10.3 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0024528491158926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0024528491158926 | validation: 2.1519936798752104]
	TIME [epoch: 10.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6499431326557499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6499431326557499 | validation: 2.381481624749029]
	TIME [epoch: 10.3 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.062643567822276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.062643567822276 | validation: 1.290730507279834]
	TIME [epoch: 10.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8797819380773593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8797819380773593 | validation: 2.3240366888421544]
	TIME [epoch: 10.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.345266143212821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.345266143212821 | validation: 3.041614102276925]
	TIME [epoch: 10.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4335513888210807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4335513888210807 | validation: 1.7562127499101017]
	TIME [epoch: 10.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5710335512107887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5710335512107887 | validation: 1.6266024165629847]
	TIME [epoch: 10.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.865078043288764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.865078043288764 | validation: 2.4100051803899794]
	TIME [epoch: 10.3 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3714049287945564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3714049287945564 | validation: 1.8222377531785705]
	TIME [epoch: 10.3 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.922098475890833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.922098475890833 | validation: 1.9832855581351723]
	TIME [epoch: 10.3 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7849452765654306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7849452765654306 | validation: 1.7160829394560353]
	TIME [epoch: 10.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5940358097711793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5940358097711793 | validation: 1.943434081956753]
	TIME [epoch: 10.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6738630898503986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6738630898503986 | validation: 2.0731495471679064]
	TIME [epoch: 10.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8770713419726213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8770713419726213 | validation: 1.6852577736346792]
	TIME [epoch: 10.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.709303052182019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.709303052182019 | validation: 2.218061971875921]
	TIME [epoch: 10.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.21658796170224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.21658796170224 | validation: 1.84436708889525]
	TIME [epoch: 10.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6257882881329553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6257882881329553 | validation: 2.3323827559459525]
	TIME [epoch: 10.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3197751706775733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3197751706775733 | validation: 2.2922097009162123]
	TIME [epoch: 10.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9083079733353767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9083079733353767 | validation: 1.2839163426252367]
	TIME [epoch: 10.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7694011560223117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7694011560223117 | validation: 2.7508572477400413]
	TIME [epoch: 10.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.163127865928534		[learning rate: 0.0099755]
	Learning Rate: 0.00997547
	LOSS [training: 2.163127865928534 | validation: 1.6582569402140195]
	TIME [epoch: 10.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6452316621614993		[learning rate: 0.0099449]
	Learning Rate: 0.00994489
	LOSS [training: 1.6452316621614993 | validation: 1.5918749285793143]
	TIME [epoch: 10.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.269163497051192		[learning rate: 0.0099144]
	Learning Rate: 0.0099144
	LOSS [training: 2.269163497051192 | validation: 1.4363114356732438]
	TIME [epoch: 10.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7739025982778098		[learning rate: 0.009884]
	Learning Rate: 0.00988401
	LOSS [training: 1.7739025982778098 | validation: 1.9871196184444848]
	TIME [epoch: 10.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5791449871732708		[learning rate: 0.0098537]
	Learning Rate: 0.00985371
	LOSS [training: 1.5791449871732708 | validation: 1.6198661220605517]
	TIME [epoch: 10.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5241117298857154		[learning rate: 0.0098235]
	Learning Rate: 0.00982351
	LOSS [training: 1.5241117298857154 | validation: 1.237072539655948]
	TIME [epoch: 10.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5795554161134084		[learning rate: 0.0097934]
	Learning Rate: 0.0097934
	LOSS [training: 1.5795554161134084 | validation: 1.9409082378462585]
	TIME [epoch: 10.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.120568402768039		[learning rate: 0.0097634]
	Learning Rate: 0.00976337
	LOSS [training: 2.120568402768039 | validation: 2.2107187994369824]
	TIME [epoch: 10.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.214516042014797		[learning rate: 0.0097334]
	Learning Rate: 0.00973345
	LOSS [training: 2.214516042014797 | validation: 1.9684969676963604]
	TIME [epoch: 10.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.692006224901654		[learning rate: 0.0097036]
	Learning Rate: 0.00970361
	LOSS [training: 2.692006224901654 | validation: 2.1396795390472705]
	TIME [epoch: 10.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9350640145837428		[learning rate: 0.0096739]
	Learning Rate: 0.00967386
	LOSS [training: 1.9350640145837428 | validation: 2.231982187712175]
	TIME [epoch: 10.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9578361577108567		[learning rate: 0.0096442]
	Learning Rate: 0.00964421
	LOSS [training: 1.9578361577108567 | validation: 1.920051223340755]
	TIME [epoch: 10.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.82689484387535		[learning rate: 0.0096146]
	Learning Rate: 0.00961465
	LOSS [training: 1.82689484387535 | validation: 1.6023706460663716]
	TIME [epoch: 10.3 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6132351842054864		[learning rate: 0.0095852]
	Learning Rate: 0.00958517
	LOSS [training: 1.6132351842054864 | validation: 1.354828549833096]
	TIME [epoch: 10.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.434533006838412		[learning rate: 0.0095558]
	Learning Rate: 0.00955579
	LOSS [training: 1.434533006838412 | validation: 1.2761604556000357]
	TIME [epoch: 10.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5195631706696515		[learning rate: 0.0095265]
	Learning Rate: 0.0095265
	LOSS [training: 1.5195631706696515 | validation: 2.3160341689374575]
	TIME [epoch: 10.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1668789927381367		[learning rate: 0.0094973]
	Learning Rate: 0.0094973
	LOSS [training: 2.1668789927381367 | validation: 1.539360413032652]
	TIME [epoch: 10.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7081112038856845		[learning rate: 0.0094682]
	Learning Rate: 0.00946818
	LOSS [training: 1.7081112038856845 | validation: 2.4718917010171855]
	TIME [epoch: 10.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6717272042176696		[learning rate: 0.0094392]
	Learning Rate: 0.00943916
	LOSS [training: 2.6717272042176696 | validation: 1.9061565162069765]
	TIME [epoch: 10.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5867556922928228		[learning rate: 0.0094102]
	Learning Rate: 0.00941022
	LOSS [training: 1.5867556922928228 | validation: 1.2677428685684158]
	TIME [epoch: 10.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3883286921992064		[learning rate: 0.0093814]
	Learning Rate: 0.00938138
	LOSS [training: 1.3883286921992064 | validation: 1.4813239223935932]
	TIME [epoch: 10.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4378705148543915		[learning rate: 0.0093526]
	Learning Rate: 0.00935262
	LOSS [training: 1.4378705148543915 | validation: 1.2450871529238243]
	TIME [epoch: 10.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3503277205852728		[learning rate: 0.009324]
	Learning Rate: 0.00932395
	LOSS [training: 1.3503277205852728 | validation: 1.4646465227402712]
	TIME [epoch: 10.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.343773138770443		[learning rate: 0.0092954]
	Learning Rate: 0.00929537
	LOSS [training: 1.343773138770443 | validation: 1.7637271070886311]
	TIME [epoch: 10.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1322215311740154		[learning rate: 0.0092669]
	Learning Rate: 0.00926687
	LOSS [training: 2.1322215311740154 | validation: 1.6348077634780815]
	TIME [epoch: 10.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8881420002949363		[learning rate: 0.0092385]
	Learning Rate: 0.00923847
	LOSS [training: 1.8881420002949363 | validation: 2.1813902759721358]
	TIME [epoch: 10.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6574654666659359		[learning rate: 0.0092101]
	Learning Rate: 0.00921015
	LOSS [training: 1.6574654666659359 | validation: 1.656514299087694]
	TIME [epoch: 10.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0507541774662528		[learning rate: 0.0091819]
	Learning Rate: 0.00918192
	LOSS [training: 3.0507541774662528 | validation: 5.204866418725219]
	TIME [epoch: 10.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4153282454818		[learning rate: 0.0091538]
	Learning Rate: 0.00915377
	LOSS [training: 3.4153282454818 | validation: 6.013936586770994]
	TIME [epoch: 10.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.672709879696539		[learning rate: 0.0091257]
	Learning Rate: 0.00912571
	LOSS [training: 4.672709879696539 | validation: 3.929562623752246]
	TIME [epoch: 10.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.120287149388291		[learning rate: 0.0090977]
	Learning Rate: 0.00909774
	LOSS [training: 4.120287149388291 | validation: 7.009567341271352]
	TIME [epoch: 10.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176625929969559		[learning rate: 0.0090698]
	Learning Rate: 0.00906985
	LOSS [training: 5.176625929969559 | validation: 2.6474821943565763]
	TIME [epoch: 10.3 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1392937379948185		[learning rate: 0.009042]
	Learning Rate: 0.00904205
	LOSS [training: 2.1392937379948185 | validation: 1.8683770013552987]
	TIME [epoch: 10.3 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5022770637087346		[learning rate: 0.0090143]
	Learning Rate: 0.00901433
	LOSS [training: 2.5022770637087346 | validation: 2.067979925100216]
	TIME [epoch: 10.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.355705158044693		[learning rate: 0.0089867]
	Learning Rate: 0.00898669
	LOSS [training: 2.355705158044693 | validation: 2.8397740188529785]
	TIME [epoch: 10.3 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.323199294340801		[learning rate: 0.0089591]
	Learning Rate: 0.00895915
	LOSS [training: 2.323199294340801 | validation: 1.7787863565797812]
	TIME [epoch: 10.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6794443530793977		[learning rate: 0.0089317]
	Learning Rate: 0.00893168
	LOSS [training: 1.6794443530793977 | validation: 1.8949996631026227]
	TIME [epoch: 10.3 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.122969234637497		[learning rate: 0.0089043]
	Learning Rate: 0.0089043
	LOSS [training: 2.122969234637497 | validation: 1.6636084494230021]
	TIME [epoch: 10.3 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.84250677093416		[learning rate: 0.008877]
	Learning Rate: 0.00887701
	LOSS [training: 1.84250677093416 | validation: 1.736914398724797]
	TIME [epoch: 10.3 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7890055668154894		[learning rate: 0.0088498]
	Learning Rate: 0.0088498
	LOSS [training: 1.7890055668154894 | validation: 1.4862140715029137]
	TIME [epoch: 10.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7915790754075218		[learning rate: 0.0088227]
	Learning Rate: 0.00882267
	LOSS [training: 1.7915790754075218 | validation: 1.7235071814223148]
	TIME [epoch: 10.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1529627408058154		[learning rate: 0.0087956]
	Learning Rate: 0.00879562
	LOSS [training: 2.1529627408058154 | validation: 2.0320695732161718]
	TIME [epoch: 10.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6856485365973661		[learning rate: 0.0087687]
	Learning Rate: 0.00876866
	LOSS [training: 1.6856485365973661 | validation: 1.5507649933120002]
	TIME [epoch: 10.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6553911183624463		[learning rate: 0.0087418]
	Learning Rate: 0.00874178
	LOSS [training: 1.6553911183624463 | validation: 1.9731360674671654]
	TIME [epoch: 10.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.664600371123915		[learning rate: 0.008715]
	Learning Rate: 0.00871499
	LOSS [training: 1.664600371123915 | validation: 1.44594514627568]
	TIME [epoch: 10.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.496452984303581		[learning rate: 0.0086883]
	Learning Rate: 0.00868827
	LOSS [training: 1.496452984303581 | validation: 1.3874982052235596]
	TIME [epoch: 10.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6394081189602119		[learning rate: 0.0086616]
	Learning Rate: 0.00866164
	LOSS [training: 1.6394081189602119 | validation: 1.6415776388919572]
	TIME [epoch: 10.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.491905908305507		[learning rate: 0.0086351]
	Learning Rate: 0.00863509
	LOSS [training: 1.491905908305507 | validation: 1.436267565357342]
	TIME [epoch: 10.3 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5353485662888793		[learning rate: 0.0086086]
	Learning Rate: 0.00860862
	LOSS [training: 1.5353485662888793 | validation: 1.3165707522465073]
	TIME [epoch: 10.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.422385959145642		[learning rate: 0.0085822]
	Learning Rate: 0.00858223
	LOSS [training: 1.422385959145642 | validation: 1.644399161400773]
	TIME [epoch: 10.3 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6901018779111858		[learning rate: 0.0085559]
	Learning Rate: 0.00855592
	LOSS [training: 1.6901018779111858 | validation: 1.7833317028865114]
	TIME [epoch: 10.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7732704434070787		[learning rate: 0.0085297]
	Learning Rate: 0.00852969
	LOSS [training: 1.7732704434070787 | validation: 1.3072166035552693]
	TIME [epoch: 10.3 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5846615560199528		[learning rate: 0.0085035]
	Learning Rate: 0.00850354
	LOSS [training: 1.5846615560199528 | validation: 1.2188526686629604]
	TIME [epoch: 10.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3960127176762747		[learning rate: 0.0084775]
	Learning Rate: 0.00847748
	LOSS [training: 1.3960127176762747 | validation: 2.13109987081441]
	TIME [epoch: 10.3 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6858441985325254		[learning rate: 0.0084515]
	Learning Rate: 0.00845149
	LOSS [training: 1.6858441985325254 | validation: 1.8652371311524787]
	TIME [epoch: 10.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8140568621241973		[learning rate: 0.0084256]
	Learning Rate: 0.00842558
	LOSS [training: 1.8140568621241973 | validation: 2.2223372004822233]
	TIME [epoch: 10.3 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8876065435359095		[learning rate: 0.0083998]
	Learning Rate: 0.00839976
	LOSS [training: 1.8876065435359095 | validation: 2.8966264605169227]
	TIME [epoch: 10.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.962370800252933		[learning rate: 0.008374]
	Learning Rate: 0.00837401
	LOSS [training: 1.962370800252933 | validation: 1.5154041816877535]
	TIME [epoch: 10.3 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.523819892694362		[learning rate: 0.0083483]
	Learning Rate: 0.00834834
	LOSS [training: 1.523819892694362 | validation: 1.8754572737762625]
	TIME [epoch: 10.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.554798189689781		[learning rate: 0.0083227]
	Learning Rate: 0.00832275
	LOSS [training: 1.554798189689781 | validation: 1.7016296022206516]
	TIME [epoch: 10.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4494066873492424		[learning rate: 0.0082972]
	Learning Rate: 0.00829723
	LOSS [training: 1.4494066873492424 | validation: 1.4060472207123027]
	TIME [epoch: 10.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.536659615719835		[learning rate: 0.0082718]
	Learning Rate: 0.0082718
	LOSS [training: 1.536659615719835 | validation: 1.234138971303596]
	TIME [epoch: 10.3 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.594560983259473		[learning rate: 0.0082464]
	Learning Rate: 0.00824644
	LOSS [training: 1.594560983259473 | validation: 1.44831161612085]
	TIME [epoch: 10.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5931660940775227		[learning rate: 0.0082212]
	Learning Rate: 0.00822116
	LOSS [training: 1.5931660940775227 | validation: 1.3277018320406506]
	TIME [epoch: 10.3 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.316757659166631		[learning rate: 0.008196]
	Learning Rate: 0.00819596
	LOSS [training: 1.316757659166631 | validation: 1.9256409827081749]
	TIME [epoch: 10.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6330019454093552		[learning rate: 0.0081708]
	Learning Rate: 0.00817084
	LOSS [training: 1.6330019454093552 | validation: 1.588518309973585]
	TIME [epoch: 10.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7324480119634575		[learning rate: 0.0081458]
	Learning Rate: 0.00814579
	LOSS [training: 1.7324480119634575 | validation: 1.7134227425274673]
	TIME [epoch: 10.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5710088692683604		[learning rate: 0.0081208]
	Learning Rate: 0.00812082
	LOSS [training: 1.5710088692683604 | validation: 1.225534702318686]
	TIME [epoch: 10.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6110931911819761		[learning rate: 0.0080959]
	Learning Rate: 0.00809593
	LOSS [training: 1.6110931911819761 | validation: 1.3320715114210573]
	TIME [epoch: 10.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5854208688690492		[learning rate: 0.0080711]
	Learning Rate: 0.00807111
	LOSS [training: 1.5854208688690492 | validation: 1.527994083290975]
	TIME [epoch: 10.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6886375520073993		[learning rate: 0.0080464]
	Learning Rate: 0.00804637
	LOSS [training: 1.6886375520073993 | validation: 1.906821977301412]
	TIME [epoch: 10.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7876643303576885		[learning rate: 0.0080217]
	Learning Rate: 0.0080217
	LOSS [training: 1.7876643303576885 | validation: 1.4372309704751618]
	TIME [epoch: 10.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4088146770736156		[learning rate: 0.0079971]
	Learning Rate: 0.00799711
	LOSS [training: 1.4088146770736156 | validation: 1.4540723114105416]
	TIME [epoch: 10.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4191696758525827		[learning rate: 0.0079726]
	Learning Rate: 0.0079726
	LOSS [training: 1.4191696758525827 | validation: 1.141548737796901]
	TIME [epoch: 10.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3439881870997021		[learning rate: 0.0079482]
	Learning Rate: 0.00794816
	LOSS [training: 1.3439881870997021 | validation: 1.5512548196698053]
	TIME [epoch: 10.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4245853807232955		[learning rate: 0.0079238]
	Learning Rate: 0.0079238
	LOSS [training: 1.4245853807232955 | validation: 1.4369470276751986]
	TIME [epoch: 10.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5363541719860578		[learning rate: 0.0078995]
	Learning Rate: 0.00789951
	LOSS [training: 1.5363541719860578 | validation: 1.4854834545293045]
	TIME [epoch: 10.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4680735071117668		[learning rate: 0.0078753]
	Learning Rate: 0.00787529
	LOSS [training: 1.4680735071117668 | validation: 2.3859568916933003]
	TIME [epoch: 10.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.914144513530306		[learning rate: 0.0078512]
	Learning Rate: 0.00785115
	LOSS [training: 1.914144513530306 | validation: 1.1932602300171222]
	TIME [epoch: 10.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4895071052132958		[learning rate: 0.0078271]
	Learning Rate: 0.00782708
	LOSS [training: 1.4895071052132958 | validation: 1.681177015374922]
	TIME [epoch: 10.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4075224147393395		[learning rate: 0.0078031]
	Learning Rate: 0.00780309
	LOSS [training: 1.4075224147393395 | validation: 1.3408761889535799]
	TIME [epoch: 10.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7537818756384826		[learning rate: 0.0077792]
	Learning Rate: 0.00777917
	LOSS [training: 1.7537818756384826 | validation: 2.450125186779032]
	TIME [epoch: 10.3 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7825140759572313		[learning rate: 0.0077553]
	Learning Rate: 0.00775532
	LOSS [training: 1.7825140759572313 | validation: 1.772301758045236]
	TIME [epoch: 10.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4360226530151128		[learning rate: 0.0077316]
	Learning Rate: 0.00773155
	LOSS [training: 1.4360226530151128 | validation: 1.210485746919613]
	TIME [epoch: 10.3 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3646761171121715		[learning rate: 0.0077079]
	Learning Rate: 0.00770785
	LOSS [training: 1.3646761171121715 | validation: 1.299194581677221]
	TIME [epoch: 10.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4233462700976645		[learning rate: 0.0076842]
	Learning Rate: 0.00768422
	LOSS [training: 1.4233462700976645 | validation: 1.005374589609154]
	TIME [epoch: 10.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.222149374532838		[learning rate: 0.0076607]
	Learning Rate: 0.00766067
	LOSS [training: 1.222149374532838 | validation: 1.5534892909231275]
	TIME [epoch: 10.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4863124100495249		[learning rate: 0.0076372]
	Learning Rate: 0.00763718
	LOSS [training: 1.4863124100495249 | validation: 0.9939962187874408]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5156480019616443		[learning rate: 0.0076138]
	Learning Rate: 0.00761377
	LOSS [training: 1.5156480019616443 | validation: 1.591174051057838]
	TIME [epoch: 10.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5225239317457842		[learning rate: 0.0075904]
	Learning Rate: 0.00759043
	LOSS [training: 1.5225239317457842 | validation: 1.7448269049860308]
	TIME [epoch: 10.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.39459774320244		[learning rate: 0.0075672]
	Learning Rate: 0.00756717
	LOSS [training: 1.39459774320244 | validation: 1.2475425086646525]
	TIME [epoch: 10.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2232817529939548		[learning rate: 0.007544]
	Learning Rate: 0.00754397
	LOSS [training: 1.2232817529939548 | validation: 1.6508134286650284]
	TIME [epoch: 10.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4141307473175337		[learning rate: 0.0075208]
	Learning Rate: 0.00752085
	LOSS [training: 1.4141307473175337 | validation: 1.1765396441524574]
	TIME [epoch: 10.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1728670657848972		[learning rate: 0.0074978]
	Learning Rate: 0.00749779
	LOSS [training: 1.1728670657848972 | validation: 1.0434451630182238]
	TIME [epoch: 10.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.432228985975613		[learning rate: 0.0074748]
	Learning Rate: 0.00747481
	LOSS [training: 1.432228985975613 | validation: 1.3997997662213322]
	TIME [epoch: 10.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.601234236008441		[learning rate: 0.0074519]
	Learning Rate: 0.00745189
	LOSS [training: 1.601234236008441 | validation: 1.3447801735418785]
	TIME [epoch: 10.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7295354852007114		[learning rate: 0.0074291]
	Learning Rate: 0.00742905
	LOSS [training: 1.7295354852007114 | validation: 1.508560655963071]
	TIME [epoch: 10.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.291064871609138		[learning rate: 0.0074063]
	Learning Rate: 0.00740628
	LOSS [training: 1.291064871609138 | validation: 1.8342408221351327]
	TIME [epoch: 10.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4257064920580813		[learning rate: 0.0073836]
	Learning Rate: 0.00738357
	LOSS [training: 1.4257064920580813 | validation: 2.2696960563046247]
	TIME [epoch: 10.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.831045217281888		[learning rate: 0.0073609]
	Learning Rate: 0.00736094
	LOSS [training: 1.831045217281888 | validation: 1.5365889581154852]
	TIME [epoch: 10.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.287129328960043		[learning rate: 0.0073384]
	Learning Rate: 0.00733838
	LOSS [training: 1.287129328960043 | validation: 1.271450166625324]
	TIME [epoch: 10.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8368364456389485		[learning rate: 0.0073159]
	Learning Rate: 0.00731588
	LOSS [training: 1.8368364456389485 | validation: 1.1987528748076917]
	TIME [epoch: 10.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4022042968277784		[learning rate: 0.0072935]
	Learning Rate: 0.00729345
	LOSS [training: 1.4022042968277784 | validation: 1.3966494509161345]
	TIME [epoch: 10.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5714460940775508		[learning rate: 0.0072711]
	Learning Rate: 0.0072711
	LOSS [training: 1.5714460940775508 | validation: 1.0121225908364724]
	TIME [epoch: 10.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1710671677694044		[learning rate: 0.0072488]
	Learning Rate: 0.00724881
	LOSS [training: 1.1710671677694044 | validation: 1.2122957953009967]
	TIME [epoch: 10.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2644220825929007		[learning rate: 0.0072266]
	Learning Rate: 0.00722659
	LOSS [training: 1.2644220825929007 | validation: 1.374078100781162]
	TIME [epoch: 10.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3833784731964296		[learning rate: 0.0072044]
	Learning Rate: 0.00720444
	LOSS [training: 1.3833784731964296 | validation: 1.4638583591963583]
	TIME [epoch: 10.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.723337636823701		[learning rate: 0.0071824]
	Learning Rate: 0.00718235
	LOSS [training: 1.723337636823701 | validation: 1.3094980631205266]
	TIME [epoch: 10.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2008899162355897		[learning rate: 0.0071603]
	Learning Rate: 0.00716033
	LOSS [training: 1.2008899162355897 | validation: 1.4347202472491989]
	TIME [epoch: 10.3 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3535055669989942		[learning rate: 0.0071384]
	Learning Rate: 0.00713838
	LOSS [training: 1.3535055669989942 | validation: 1.1905721942705931]
	TIME [epoch: 10.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.447063983343209		[learning rate: 0.0071165]
	Learning Rate: 0.0071165
	LOSS [training: 1.447063983343209 | validation: 1.164839672318173]
	TIME [epoch: 10.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1751920216336729		[learning rate: 0.0070947]
	Learning Rate: 0.00709469
	LOSS [training: 1.1751920216336729 | validation: 1.2655053449361178]
	TIME [epoch: 10.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3678424709393977		[learning rate: 0.0070729]
	Learning Rate: 0.00707294
	LOSS [training: 1.3678424709393977 | validation: 1.1005275332867532]
	TIME [epoch: 10.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2939248082186183		[learning rate: 0.0070513]
	Learning Rate: 0.00705126
	LOSS [training: 1.2939248082186183 | validation: 1.3691588643632426]
	TIME [epoch: 10.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2828805533067098		[learning rate: 0.0070296]
	Learning Rate: 0.00702964
	LOSS [training: 1.2828805533067098 | validation: 1.3464195956760556]
	TIME [epoch: 10.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.144096336546212		[learning rate: 0.0070081]
	Learning Rate: 0.00700809
	LOSS [training: 1.144096336546212 | validation: 1.17580421188054]
	TIME [epoch: 10.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0817180572191223		[learning rate: 0.0069866]
	Learning Rate: 0.00698661
	LOSS [training: 1.0817180572191223 | validation: 1.3820479026381305]
	TIME [epoch: 10.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.271664977839248		[learning rate: 0.0069652]
	Learning Rate: 0.0069652
	LOSS [training: 1.271664977839248 | validation: 1.1990739001894182]
	TIME [epoch: 10.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.129030768310922		[learning rate: 0.0069438]
	Learning Rate: 0.00694384
	LOSS [training: 1.129030768310922 | validation: 1.1348208246992086]
	TIME [epoch: 10.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.044137853530809		[learning rate: 0.0069226]
	Learning Rate: 0.00692256
	LOSS [training: 1.044137853530809 | validation: 1.1225682359662512]
	TIME [epoch: 10.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.144385849087076		[learning rate: 0.0069013]
	Learning Rate: 0.00690134
	LOSS [training: 1.144385849087076 | validation: 1.2214051276615947]
	TIME [epoch: 10.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.134820278776234		[learning rate: 0.0068802]
	Learning Rate: 0.00688018
	LOSS [training: 1.134820278776234 | validation: 0.9408049640133829]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1412490057617461		[learning rate: 0.0068591]
	Learning Rate: 0.00685909
	LOSS [training: 1.1412490057617461 | validation: 1.1897309733792327]
	TIME [epoch: 10.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2751623055590509		[learning rate: 0.0068381]
	Learning Rate: 0.00683807
	LOSS [training: 1.2751623055590509 | validation: 1.0965365789037622]
	TIME [epoch: 10.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9749092604930265		[learning rate: 0.0068171]
	Learning Rate: 0.0068171
	LOSS [training: 0.9749092604930265 | validation: 0.9097590370624241]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9258834691756336		[learning rate: 0.0067962]
	Learning Rate: 0.00679621
	LOSS [training: 0.9258834691756336 | validation: 1.0741396291228513]
	TIME [epoch: 10.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1265850653708296		[learning rate: 0.0067754]
	Learning Rate: 0.00677537
	LOSS [training: 1.1265850653708296 | validation: 1.1647341738571906]
	TIME [epoch: 10.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.349381074942356		[learning rate: 0.0067546]
	Learning Rate: 0.00675461
	LOSS [training: 1.349381074942356 | validation: 1.5249642439482287]
	TIME [epoch: 10.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.108807376561376		[learning rate: 0.0067339]
	Learning Rate: 0.0067339
	LOSS [training: 1.108807376561376 | validation: 1.3210650653734424]
	TIME [epoch: 10.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.17698386600805		[learning rate: 0.0067133]
	Learning Rate: 0.00671326
	LOSS [training: 1.17698386600805 | validation: 0.9711813812505906]
	TIME [epoch: 10.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.112635568360881		[learning rate: 0.0066927]
	Learning Rate: 0.00669268
	LOSS [training: 1.112635568360881 | validation: 1.3067804004983585]
	TIME [epoch: 10.3 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5744773653587087		[learning rate: 0.0066722]
	Learning Rate: 0.00667216
	LOSS [training: 1.5744773653587087 | validation: 1.0847117454515576]
	TIME [epoch: 10.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1206312099954168		[learning rate: 0.0066517]
	Learning Rate: 0.00665171
	LOSS [training: 1.1206312099954168 | validation: 1.0666625571050627]
	TIME [epoch: 10.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4968827826994662		[learning rate: 0.0066313]
	Learning Rate: 0.00663132
	LOSS [training: 1.4968827826994662 | validation: 2.216388058099316]
	TIME [epoch: 10.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.401016032664658		[learning rate: 0.006611]
	Learning Rate: 0.00661099
	LOSS [training: 1.401016032664658 | validation: 1.2083591157982247]
	TIME [epoch: 10.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1171339243686336		[learning rate: 0.0065907]
	Learning Rate: 0.00659073
	LOSS [training: 1.1171339243686336 | validation: 1.3062817353971912]
	TIME [epoch: 10.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.247012559793172		[learning rate: 0.0065705]
	Learning Rate: 0.00657052
	LOSS [training: 1.247012559793172 | validation: 1.25098371734834]
	TIME [epoch: 10.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9952814786994978		[learning rate: 0.0065504]
	Learning Rate: 0.00655038
	LOSS [training: 0.9952814786994978 | validation: 0.8038550073219151]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0644417458575708		[learning rate: 0.0065303]
	Learning Rate: 0.0065303
	LOSS [training: 1.0644417458575708 | validation: 1.0960442202717342]
	TIME [epoch: 10.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.171628004195149		[learning rate: 0.0065103]
	Learning Rate: 0.00651028
	LOSS [training: 1.171628004195149 | validation: 1.118230466215999]
	TIME [epoch: 10.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2055864595048535		[learning rate: 0.0064903]
	Learning Rate: 0.00649033
	LOSS [training: 1.2055864595048535 | validation: 0.8252352566632638]
	TIME [epoch: 10.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.041013964182111		[learning rate: 0.0064704]
	Learning Rate: 0.00647043
	LOSS [training: 1.041013964182111 | validation: 1.5791861468016495]
	TIME [epoch: 10.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4170238305070275		[learning rate: 0.0064506]
	Learning Rate: 0.0064506
	LOSS [training: 1.4170238305070275 | validation: 1.7902613007057055]
	TIME [epoch: 10.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4978528261817012		[learning rate: 0.0064308]
	Learning Rate: 0.00643082
	LOSS [training: 1.4978528261817012 | validation: 1.3257147835232048]
	TIME [epoch: 10.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4064249892821483		[learning rate: 0.0064111]
	Learning Rate: 0.00641111
	LOSS [training: 1.4064249892821483 | validation: 1.0453107288224741]
	TIME [epoch: 10.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.129206967863839		[learning rate: 0.0063915]
	Learning Rate: 0.00639146
	LOSS [training: 1.129206967863839 | validation: 1.6061484036712432]
	TIME [epoch: 10.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.39630097107032		[learning rate: 0.0063719]
	Learning Rate: 0.00637187
	LOSS [training: 1.39630097107032 | validation: 1.608356140332146]
	TIME [epoch: 10.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4466699327264299		[learning rate: 0.0063523]
	Learning Rate: 0.00635233
	LOSS [training: 1.4466699327264299 | validation: 1.1149401005692736]
	TIME [epoch: 10.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1325143223230711		[learning rate: 0.0063329]
	Learning Rate: 0.00633286
	LOSS [training: 1.1325143223230711 | validation: 1.1244198307487145]
	TIME [epoch: 10.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.066232406933496		[learning rate: 0.0063134]
	Learning Rate: 0.00631345
	LOSS [training: 1.066232406933496 | validation: 1.607986507060197]
	TIME [epoch: 10.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4949670845547633		[learning rate: 0.0062941]
	Learning Rate: 0.0062941
	LOSS [training: 1.4949670845547633 | validation: 1.0699090001287817]
	TIME [epoch: 10.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0921333436708904		[learning rate: 0.0062748]
	Learning Rate: 0.0062748
	LOSS [training: 1.0921333436708904 | validation: 1.9706011659663285]
	TIME [epoch: 10.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4314531717894763		[learning rate: 0.0062556]
	Learning Rate: 0.00625557
	LOSS [training: 1.4314531717894763 | validation: 1.339105147382539]
	TIME [epoch: 10.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2115879749940741		[learning rate: 0.0062364]
	Learning Rate: 0.00623639
	LOSS [training: 1.2115879749940741 | validation: 1.1593067327046436]
	TIME [epoch: 10.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.07739528502505		[learning rate: 0.0062173]
	Learning Rate: 0.00621727
	LOSS [training: 1.07739528502505 | validation: 1.0150094184567453]
	TIME [epoch: 10.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2221427172636468		[learning rate: 0.0061982]
	Learning Rate: 0.00619822
	LOSS [training: 1.2221427172636468 | validation: 1.295169394739517]
	TIME [epoch: 10.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2141403336037118		[learning rate: 0.0061792]
	Learning Rate: 0.00617922
	LOSS [training: 1.2141403336037118 | validation: 1.1451522983205298]
	TIME [epoch: 10.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0897684911760177		[learning rate: 0.0061603]
	Learning Rate: 0.00616027
	LOSS [training: 1.0897684911760177 | validation: 1.001424455061575]
	TIME [epoch: 10.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2047001304889375		[learning rate: 0.0061414]
	Learning Rate: 0.00614139
	LOSS [training: 1.2047001304889375 | validation: 1.6818780980781565]
	TIME [epoch: 10.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5020921537061853		[learning rate: 0.0061226]
	Learning Rate: 0.00612256
	LOSS [training: 1.5020921537061853 | validation: 1.444044554534726]
	TIME [epoch: 10.3 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0821392726157504		[learning rate: 0.0061038]
	Learning Rate: 0.0061038
	LOSS [training: 1.0821392726157504 | validation: 1.001365119225963]
	TIME [epoch: 10.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1155780062903795		[learning rate: 0.0060851]
	Learning Rate: 0.00608508
	LOSS [training: 1.1155780062903795 | validation: 1.2298282866431387]
	TIME [epoch: 10.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1952756213107691		[learning rate: 0.0060664]
	Learning Rate: 0.00606643
	LOSS [training: 1.1952756213107691 | validation: 1.2686630707222053]
	TIME [epoch: 10.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1382720046522556		[learning rate: 0.0060478]
	Learning Rate: 0.00604784
	LOSS [training: 1.1382720046522556 | validation: 1.5942250331655152]
	TIME [epoch: 10.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.391223871907864		[learning rate: 0.0060293]
	Learning Rate: 0.0060293
	LOSS [training: 1.391223871907864 | validation: 1.3968231989423519]
	TIME [epoch: 10.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3251440369541037		[learning rate: 0.0060108]
	Learning Rate: 0.00601081
	LOSS [training: 1.3251440369541037 | validation: 0.9512320106144324]
	TIME [epoch: 10.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0339533038806645		[learning rate: 0.0059924]
	Learning Rate: 0.00599239
	LOSS [training: 1.0339533038806645 | validation: 1.2309750411605838]
	TIME [epoch: 10.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0868876712289413		[learning rate: 0.005974]
	Learning Rate: 0.00597402
	LOSS [training: 1.0868876712289413 | validation: 1.0904838453025154]
	TIME [epoch: 10.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1693168183786589		[learning rate: 0.0059557]
	Learning Rate: 0.00595571
	LOSS [training: 1.1693168183786589 | validation: 1.10293744562713]
	TIME [epoch: 10.3 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1444887629515894		[learning rate: 0.0059375]
	Learning Rate: 0.00593745
	LOSS [training: 1.1444887629515894 | validation: 1.3014766229009116]
	TIME [epoch: 10.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2839826661815301		[learning rate: 0.0059192]
	Learning Rate: 0.00591925
	LOSS [training: 1.2839826661815301 | validation: 1.514213772019369]
	TIME [epoch: 10.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2228151637757967		[learning rate: 0.0059011]
	Learning Rate: 0.0059011
	LOSS [training: 1.2228151637757967 | validation: 1.1951953102105188]
	TIME [epoch: 10.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.289875225415798		[learning rate: 0.005883]
	Learning Rate: 0.00588302
	LOSS [training: 1.289875225415798 | validation: 1.270931554943896]
	TIME [epoch: 10.3 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.060411779610261		[learning rate: 0.005865]
	Learning Rate: 0.00586498
	LOSS [training: 1.060411779610261 | validation: 1.14067959315512]
	TIME [epoch: 10.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9648420422668977		[learning rate: 0.005847]
	Learning Rate: 0.005847
	LOSS [training: 0.9648420422668977 | validation: 0.9033696558567464]
	TIME [epoch: 10.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0555760377646743		[learning rate: 0.0058291]
	Learning Rate: 0.00582908
	LOSS [training: 1.0555760377646743 | validation: 0.9447543066276802]
	TIME [epoch: 10.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9437389993753669		[learning rate: 0.0058112]
	Learning Rate: 0.00581121
	LOSS [training: 0.9437389993753669 | validation: 1.0234549077597486]
	TIME [epoch: 10.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9434404567786701		[learning rate: 0.0057934]
	Learning Rate: 0.0057934
	LOSS [training: 0.9434404567786701 | validation: 1.1666355549199938]
	TIME [epoch: 10.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0562098423860191		[learning rate: 0.0057756]
	Learning Rate: 0.00577564
	LOSS [training: 1.0562098423860191 | validation: 0.9748643744328106]
	TIME [epoch: 10.3 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.927343227860745		[learning rate: 0.0057579]
	Learning Rate: 0.00575793
	LOSS [training: 0.927343227860745 | validation: 0.9557757842133141]
	TIME [epoch: 10.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.993790485498997		[learning rate: 0.0057403]
	Learning Rate: 0.00574028
	LOSS [training: 0.993790485498997 | validation: 0.9086387305998292]
	TIME [epoch: 10.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1029092568576768		[learning rate: 0.0057227]
	Learning Rate: 0.00572269
	LOSS [training: 1.1029092568576768 | validation: 1.3330843224783349]
	TIME [epoch: 10.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5862371432312963		[learning rate: 0.0057051]
	Learning Rate: 0.00570514
	LOSS [training: 1.5862371432312963 | validation: 1.5389537231236587]
	TIME [epoch: 10.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2069938462871312		[learning rate: 0.0056877]
	Learning Rate: 0.00568766
	LOSS [training: 1.2069938462871312 | validation: 1.070081376086589]
	TIME [epoch: 10.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.098865011365616		[learning rate: 0.0056702]
	Learning Rate: 0.00567022
	LOSS [training: 1.098865011365616 | validation: 0.8899522438487053]
	TIME [epoch: 10.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0953612004814541		[learning rate: 0.0056528]
	Learning Rate: 0.00565284
	LOSS [training: 1.0953612004814541 | validation: 1.1528154441542666]
	TIME [epoch: 10.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4029195080614072		[learning rate: 0.0056355]
	Learning Rate: 0.00563551
	LOSS [training: 1.4029195080614072 | validation: 1.4390381214410208]
	TIME [epoch: 10.3 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2657773631229876		[learning rate: 0.0056182]
	Learning Rate: 0.00561824
	LOSS [training: 1.2657773631229876 | validation: 1.015327279751369]
	TIME [epoch: 10.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.896012601840187		[learning rate: 0.005601]
	Learning Rate: 0.00560101
	LOSS [training: 0.896012601840187 | validation: 0.8586657567322128]
	TIME [epoch: 10.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0061324853594165		[learning rate: 0.0055838]
	Learning Rate: 0.00558384
	LOSS [training: 1.0061324853594165 | validation: 1.0720560686408274]
	TIME [epoch: 10.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.231658811260351		[learning rate: 0.0055667]
	Learning Rate: 0.00556673
	LOSS [training: 1.231658811260351 | validation: 1.2527691443018762]
	TIME [epoch: 10.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0480869476739394		[learning rate: 0.0055497]
	Learning Rate: 0.00554966
	LOSS [training: 1.0480869476739394 | validation: 1.1819367281644169]
	TIME [epoch: 10.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0670388268081663		[learning rate: 0.0055327]
	Learning Rate: 0.00553265
	LOSS [training: 1.0670388268081663 | validation: 0.981684776869995]
	TIME [epoch: 10.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1643490637449658		[learning rate: 0.0055157]
	Learning Rate: 0.00551569
	LOSS [training: 1.1643490637449658 | validation: 1.2781123035188275]
	TIME [epoch: 10.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0790240530549728		[learning rate: 0.0054988]
	Learning Rate: 0.00549878
	LOSS [training: 1.0790240530549728 | validation: 1.4085778859268725]
	TIME [epoch: 10.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2956849502449994		[learning rate: 0.0054819]
	Learning Rate: 0.00548193
	LOSS [training: 1.2956849502449994 | validation: 1.2270096046740455]
	TIME [epoch: 10.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0702252476101854		[learning rate: 0.0054651]
	Learning Rate: 0.00546512
	LOSS [training: 1.0702252476101854 | validation: 1.2287763849873843]
	TIME [epoch: 10.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1241227890873502		[learning rate: 0.0054484]
	Learning Rate: 0.00544837
	LOSS [training: 1.1241227890873502 | validation: 1.1346390724643387]
	TIME [epoch: 10.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9779421804606798		[learning rate: 0.0054317]
	Learning Rate: 0.00543167
	LOSS [training: 0.9779421804606798 | validation: 0.9760569648374469]
	TIME [epoch: 10.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1268782006223277		[learning rate: 0.005415]
	Learning Rate: 0.00541502
	LOSS [training: 1.1268782006223277 | validation: 0.9495964007825985]
	TIME [epoch: 10.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.238829954108351		[learning rate: 0.0053984]
	Learning Rate: 0.00539842
	LOSS [training: 1.238829954108351 | validation: 1.173005282676389]
	TIME [epoch: 10.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0735576265925222		[learning rate: 0.0053819]
	Learning Rate: 0.00538187
	LOSS [training: 1.0735576265925222 | validation: 0.9330815711554574]
	TIME [epoch: 10.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1430850940151085		[learning rate: 0.0053654]
	Learning Rate: 0.00536537
	LOSS [training: 1.1430850940151085 | validation: 1.1482711631650415]
	TIME [epoch: 10.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1354033238884251		[learning rate: 0.0053489]
	Learning Rate: 0.00534893
	LOSS [training: 1.1354033238884251 | validation: 1.1441924241484078]
	TIME [epoch: 10.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1532836125002306		[learning rate: 0.0053325]
	Learning Rate: 0.00533253
	LOSS [training: 1.1532836125002306 | validation: 1.2976179454865184]
	TIME [epoch: 10.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0249986063601866		[learning rate: 0.0053162]
	Learning Rate: 0.00531618
	LOSS [training: 1.0249986063601866 | validation: 1.2247737648848576]
	TIME [epoch: 10.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.361159066021202		[learning rate: 0.0052999]
	Learning Rate: 0.00529989
	LOSS [training: 1.361159066021202 | validation: 1.5097848560877019]
	TIME [epoch: 10.3 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3113175652826894		[learning rate: 0.0052836]
	Learning Rate: 0.00528364
	LOSS [training: 1.3113175652826894 | validation: 1.5229540699057318]
	TIME [epoch: 10.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5528765309235397		[learning rate: 0.0052674]
	Learning Rate: 0.00526744
	LOSS [training: 1.5528765309235397 | validation: 1.7117501989621704]
	TIME [epoch: 10.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.375249227302302		[learning rate: 0.0052513]
	Learning Rate: 0.0052513
	LOSS [training: 1.375249227302302 | validation: 1.2098336180487381]
	TIME [epoch: 10.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1283544314029466		[learning rate: 0.0052352]
	Learning Rate: 0.0052352
	LOSS [training: 1.1283544314029466 | validation: 1.286867346166801]
	TIME [epoch: 10.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1675871594823246		[learning rate: 0.0052192]
	Learning Rate: 0.00521915
	LOSS [training: 1.1675871594823246 | validation: 1.0652378792577608]
	TIME [epoch: 10.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9520370073266088		[learning rate: 0.0052032]
	Learning Rate: 0.00520315
	LOSS [training: 0.9520370073266088 | validation: 1.3007419687964856]
	TIME [epoch: 10.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0020189681171312		[learning rate: 0.0051872]
	Learning Rate: 0.0051872
	LOSS [training: 1.0020189681171312 | validation: 0.8226910206901431]
	TIME [epoch: 10.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0970816104640542		[learning rate: 0.0051713]
	Learning Rate: 0.0051713
	LOSS [training: 1.0970816104640542 | validation: 1.1821275618030354]
	TIME [epoch: 10.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1017126165644848		[learning rate: 0.0051555]
	Learning Rate: 0.00515545
	LOSS [training: 1.1017126165644848 | validation: 1.0162550863597535]
	TIME [epoch: 10.3 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2617528882761273		[learning rate: 0.0051396]
	Learning Rate: 0.00513965
	LOSS [training: 1.2617528882761273 | validation: 1.1624484956664056]
	TIME [epoch: 10.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9531042889711439		[learning rate: 0.0051239]
	Learning Rate: 0.00512389
	LOSS [training: 0.9531042889711439 | validation: 1.058859721099079]
	TIME [epoch: 10.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0379716333633195		[learning rate: 0.0051082]
	Learning Rate: 0.00510819
	LOSS [training: 1.0379716333633195 | validation: 0.9547683030974627]
	TIME [epoch: 10.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9660413128034968		[learning rate: 0.0050925]
	Learning Rate: 0.00509253
	LOSS [training: 0.9660413128034968 | validation: 1.1087159896428196]
	TIME [epoch: 10.3 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9402001022613774		[learning rate: 0.0050769]
	Learning Rate: 0.00507692
	LOSS [training: 0.9402001022613774 | validation: 1.148774386647873]
	TIME [epoch: 10.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0453645327983776		[learning rate: 0.0050614]
	Learning Rate: 0.00506135
	LOSS [training: 1.0453645327983776 | validation: 0.9255708158903311]
	TIME [epoch: 10.3 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8697683674881409		[learning rate: 0.0050458]
	Learning Rate: 0.00504584
	LOSS [training: 0.8697683674881409 | validation: 0.8685704902603081]
	TIME [epoch: 10.3 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8980533717980412		[learning rate: 0.0050304]
	Learning Rate: 0.00503037
	LOSS [training: 0.8980533717980412 | validation: 0.8497948234585933]
	TIME [epoch: 10.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0850163559772672		[learning rate: 0.005015]
	Learning Rate: 0.00501495
	LOSS [training: 1.0850163559772672 | validation: 1.0317141660007239]
	TIME [epoch: 10.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1695112106526655		[learning rate: 0.0049996]
	Learning Rate: 0.00499958
	LOSS [training: 1.1695112106526655 | validation: 0.9669066735157984]
	TIME [epoch: 10.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0995018656845852		[learning rate: 0.0049843]
	Learning Rate: 0.00498425
	LOSS [training: 1.0995018656845852 | validation: 0.8695657797134082]
	TIME [epoch: 10.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8904618208714734		[learning rate: 0.004969]
	Learning Rate: 0.00496897
	LOSS [training: 0.8904618208714734 | validation: 0.8684102699704811]
	TIME [epoch: 10.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8174593865752344		[learning rate: 0.0049537]
	Learning Rate: 0.00495374
	LOSS [training: 0.8174593865752344 | validation: 0.8348090232865607]
	TIME [epoch: 10.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1226164377394983		[learning rate: 0.0049386]
	Learning Rate: 0.00493856
	LOSS [training: 1.1226164377394983 | validation: 1.0774504713746438]
	TIME [epoch: 10.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9036215359207554		[learning rate: 0.0049234]
	Learning Rate: 0.00492342
	LOSS [training: 0.9036215359207554 | validation: 0.904114080874381]
	TIME [epoch: 10.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0775927546289528		[learning rate: 0.0049083]
	Learning Rate: 0.00490832
	LOSS [training: 1.0775927546289528 | validation: 0.7776758093310026]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8326772906457436		[learning rate: 0.0048933]
	Learning Rate: 0.00489328
	LOSS [training: 0.8326772906457436 | validation: 0.9335328563361199]
	TIME [epoch: 10.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9624632130021187		[learning rate: 0.0048783]
	Learning Rate: 0.00487828
	LOSS [training: 0.9624632130021187 | validation: 0.9741519087113364]
	TIME [epoch: 10.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0821321658588101		[learning rate: 0.0048633]
	Learning Rate: 0.00486333
	LOSS [training: 1.0821321658588101 | validation: 1.1440135781375111]
	TIME [epoch: 10.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9294379811085307		[learning rate: 0.0048484]
	Learning Rate: 0.00484842
	LOSS [training: 0.9294379811085307 | validation: 0.947106147522639]
	TIME [epoch: 10.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.051907174349419		[learning rate: 0.0048336]
	Learning Rate: 0.00483355
	LOSS [training: 1.051907174349419 | validation: 1.4607920064616013]
	TIME [epoch: 10.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0304873975434128		[learning rate: 0.0048187]
	Learning Rate: 0.00481874
	LOSS [training: 1.0304873975434128 | validation: 1.3014668810815289]
	TIME [epoch: 10.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1119798715715514		[learning rate: 0.004804]
	Learning Rate: 0.00480397
	LOSS [training: 1.1119798715715514 | validation: 1.200544681384699]
	TIME [epoch: 10.3 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9675687319696162		[learning rate: 0.0047892]
	Learning Rate: 0.00478924
	LOSS [training: 0.9675687319696162 | validation: 1.019829302815229]
	TIME [epoch: 10.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8585195227975065		[learning rate: 0.0047746]
	Learning Rate: 0.00477456
	LOSS [training: 0.8585195227975065 | validation: 0.9440175549421863]
	TIME [epoch: 10.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9420337351352519		[learning rate: 0.0047599]
	Learning Rate: 0.00475992
	LOSS [training: 0.9420337351352519 | validation: 0.9430915343994388]
	TIME [epoch: 10.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1372742860628384		[learning rate: 0.0047453]
	Learning Rate: 0.00474533
	LOSS [training: 1.1372742860628384 | validation: 1.3935357076877455]
	TIME [epoch: 10.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3442641257944017		[learning rate: 0.0047308]
	Learning Rate: 0.00473079
	LOSS [training: 1.3442641257944017 | validation: 1.3531670413548955]
	TIME [epoch: 10.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9927456474142353		[learning rate: 0.0047163]
	Learning Rate: 0.00471628
	LOSS [training: 0.9927456474142353 | validation: 1.0869817423645691]
	TIME [epoch: 10.3 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9019455349929697		[learning rate: 0.0047018]
	Learning Rate: 0.00470183
	LOSS [training: 0.9019455349929697 | validation: 0.7245556307881253]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.808047371175759		[learning rate: 0.0046874]
	Learning Rate: 0.00468741
	LOSS [training: 0.808047371175759 | validation: 0.9147456933944157]
	TIME [epoch: 10.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8819991228312736		[learning rate: 0.004673]
	Learning Rate: 0.00467305
	LOSS [training: 0.8819991228312736 | validation: 0.9759340080320509]
	TIME [epoch: 10.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.87962015796719		[learning rate: 0.0046587]
	Learning Rate: 0.00465872
	LOSS [training: 0.87962015796719 | validation: 0.7999538871044163]
	TIME [epoch: 10.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9322185094621933		[learning rate: 0.0046444]
	Learning Rate: 0.00464444
	LOSS [training: 0.9322185094621933 | validation: 0.8538560685690004]
	TIME [epoch: 10.3 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9349359097382528		[learning rate: 0.0046302]
	Learning Rate: 0.0046302
	LOSS [training: 0.9349359097382528 | validation: 0.7222460585006075]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_751.pth
	Model improved!!!
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6890457457041091		[learning rate: 0.004616]
	Learning Rate: 0.00461601
	LOSS [training: 0.6890457457041091 | validation: 0.7193369920729354]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6966272027758409		[learning rate: 0.0046019]
	Learning Rate: 0.00460186
	LOSS [training: 0.6966272027758409 | validation: 0.9871681775060204]
	TIME [epoch: 10.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.882209477015004		[learning rate: 0.0045878]
	Learning Rate: 0.00458775
	LOSS [training: 0.882209477015004 | validation: 0.9776944448867757]
	TIME [epoch: 10.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9395736305939412		[learning rate: 0.0045737]
	Learning Rate: 0.00457369
	LOSS [training: 0.9395736305939412 | validation: 0.722748093561582]
	TIME [epoch: 10.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9449011546323683		[learning rate: 0.0045597]
	Learning Rate: 0.00455967
	LOSS [training: 0.9449011546323683 | validation: 0.9990789888244459]
	TIME [epoch: 10.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8762913186172966		[learning rate: 0.0045457]
	Learning Rate: 0.00454569
	LOSS [training: 0.8762913186172966 | validation: 0.8107748176308184]
	TIME [epoch: 10.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9152069271158364		[learning rate: 0.0045318]
	Learning Rate: 0.00453176
	LOSS [training: 0.9152069271158364 | validation: 1.0882609702359454]
	TIME [epoch: 10.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8978814444175681		[learning rate: 0.0045179]
	Learning Rate: 0.00451787
	LOSS [training: 0.8978814444175681 | validation: 0.9725937993295501]
	TIME [epoch: 10.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9514710832571053		[learning rate: 0.004504]
	Learning Rate: 0.00450402
	LOSS [training: 0.9514710832571053 | validation: 0.7450400362188038]
	TIME [epoch: 10.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7168704911132988		[learning rate: 0.0044902]
	Learning Rate: 0.00449021
	LOSS [training: 0.7168704911132988 | validation: 0.8255032005046319]
	TIME [epoch: 10.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9734940708786771		[learning rate: 0.0044764]
	Learning Rate: 0.00447645
	LOSS [training: 0.9734940708786771 | validation: 0.958831939634238]
	TIME [epoch: 10.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.905958804095588		[learning rate: 0.0044627]
	Learning Rate: 0.00446272
	LOSS [training: 0.905958804095588 | validation: 0.8505066909209537]
	TIME [epoch: 10.3 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.885434175748426		[learning rate: 0.004449]
	Learning Rate: 0.00444904
	LOSS [training: 0.885434175748426 | validation: 0.8264650030173235]
	TIME [epoch: 10.3 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.788499620919447		[learning rate: 0.0044354]
	Learning Rate: 0.0044354
	LOSS [training: 0.788499620919447 | validation: 0.6521547826966879]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_765.pth
	Model improved!!!
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6507299127176306		[learning rate: 0.0044218]
	Learning Rate: 0.00442181
	LOSS [training: 0.6507299127176306 | validation: 0.7001088766825244]
	TIME [epoch: 10.3 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7647801474302092		[learning rate: 0.0044083]
	Learning Rate: 0.00440825
	LOSS [training: 0.7647801474302092 | validation: 0.7872468084349069]
	TIME [epoch: 10.3 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8309337643243975		[learning rate: 0.0043947]
	Learning Rate: 0.00439474
	LOSS [training: 0.8309337643243975 | validation: 0.8647627987254661]
	TIME [epoch: 10.3 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7683385343738636		[learning rate: 0.0043813]
	Learning Rate: 0.00438127
	LOSS [training: 0.7683385343738636 | validation: 0.800955449982244]
	TIME [epoch: 10.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7274836667285912		[learning rate: 0.0043678]
	Learning Rate: 0.00436784
	LOSS [training: 0.7274836667285912 | validation: 0.6759812120593052]
	TIME [epoch: 10.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6884694319548981		[learning rate: 0.0043544]
	Learning Rate: 0.00435445
	LOSS [training: 0.6884694319548981 | validation: 0.7422925790624748]
	TIME [epoch: 10.3 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8458872171036589		[learning rate: 0.0043411]
	Learning Rate: 0.0043411
	LOSS [training: 0.8458872171036589 | validation: 1.1555127821483155]
	TIME [epoch: 10.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.912861405020885		[learning rate: 0.0043278]
	Learning Rate: 0.0043278
	LOSS [training: 0.912861405020885 | validation: 0.6332024900543769]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8143196252067908		[learning rate: 0.0043145]
	Learning Rate: 0.00431453
	LOSS [training: 0.8143196252067908 | validation: 1.1922081288691353]
	TIME [epoch: 10.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9785285211926216		[learning rate: 0.0043013]
	Learning Rate: 0.0043013
	LOSS [training: 0.9785285211926216 | validation: 0.77612759557213]
	TIME [epoch: 10.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7917328541548514		[learning rate: 0.0042881]
	Learning Rate: 0.00428812
	LOSS [training: 0.7917328541548514 | validation: 0.7491194116495697]
	TIME [epoch: 10.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8260331814464706		[learning rate: 0.004275]
	Learning Rate: 0.00427497
	LOSS [training: 0.8260331814464706 | validation: 0.7408460004635883]
	TIME [epoch: 10.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8100822927001724		[learning rate: 0.0042619]
	Learning Rate: 0.00426187
	LOSS [training: 0.8100822927001724 | validation: 0.9374717488773205]
	TIME [epoch: 10.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9650950461474672		[learning rate: 0.0042488]
	Learning Rate: 0.0042488
	LOSS [training: 0.9650950461474672 | validation: 0.7492814603536195]
	TIME [epoch: 10.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6653518063231194		[learning rate: 0.0042358]
	Learning Rate: 0.00423578
	LOSS [training: 0.6653518063231194 | validation: 0.7836597711232405]
	TIME [epoch: 10.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8480746650352902		[learning rate: 0.0042228]
	Learning Rate: 0.00422279
	LOSS [training: 0.8480746650352902 | validation: 0.7575322096081348]
	TIME [epoch: 10.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8266488466449932		[learning rate: 0.0042098]
	Learning Rate: 0.00420985
	LOSS [training: 0.8266488466449932 | validation: 0.7594034273547494]
	TIME [epoch: 10.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8627318890443736		[learning rate: 0.0041969]
	Learning Rate: 0.00419695
	LOSS [training: 0.8627318890443736 | validation: 1.0700029003886868]
	TIME [epoch: 10.3 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1102623892407801		[learning rate: 0.0041841]
	Learning Rate: 0.00418408
	LOSS [training: 1.1102623892407801 | validation: 0.8193472529010751]
	TIME [epoch: 10.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7916028764945527		[learning rate: 0.0041713]
	Learning Rate: 0.00417125
	LOSS [training: 0.7916028764945527 | validation: 0.7343606180131264]
	TIME [epoch: 10.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8405728970842719		[learning rate: 0.0041585]
	Learning Rate: 0.00415847
	LOSS [training: 0.8405728970842719 | validation: 0.9198282609311818]
	TIME [epoch: 10.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7367024350884993		[learning rate: 0.0041457]
	Learning Rate: 0.00414572
	LOSS [training: 0.7367024350884993 | validation: 0.7475392555175009]
	TIME [epoch: 10.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6521238205060451		[learning rate: 0.004133]
	Learning Rate: 0.00413301
	LOSS [training: 0.6521238205060451 | validation: 0.7514651901552963]
	TIME [epoch: 10.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7511512003157065		[learning rate: 0.0041203]
	Learning Rate: 0.00412034
	LOSS [training: 0.7511512003157065 | validation: 0.6100682804453735]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6743186440807205		[learning rate: 0.0041077]
	Learning Rate: 0.00410771
	LOSS [training: 0.6743186440807205 | validation: 0.6780345154672593]
	TIME [epoch: 10.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699772200898712		[learning rate: 0.0040951]
	Learning Rate: 0.00409512
	LOSS [training: 0.699772200898712 | validation: 1.0194066108486024]
	TIME [epoch: 10.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8244963738261102		[learning rate: 0.0040826]
	Learning Rate: 0.00408257
	LOSS [training: 0.8244963738261102 | validation: 0.625922876270709]
	TIME [epoch: 10.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7460968083454296		[learning rate: 0.0040701]
	Learning Rate: 0.00407005
	LOSS [training: 0.7460968083454296 | validation: 0.9183234604318204]
	TIME [epoch: 10.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8589104536751542		[learning rate: 0.0040576]
	Learning Rate: 0.00405758
	LOSS [training: 0.8589104536751542 | validation: 0.9405741574788978]
	TIME [epoch: 10.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6540683987780427		[learning rate: 0.0040451]
	Learning Rate: 0.00404514
	LOSS [training: 0.6540683987780427 | validation: 0.6696714623122182]
	TIME [epoch: 10.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.708187104057426		[learning rate: 0.0040327]
	Learning Rate: 0.00403274
	LOSS [training: 0.708187104057426 | validation: 0.6608143832942502]
	TIME [epoch: 10.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.977069287340728		[learning rate: 0.0040204]
	Learning Rate: 0.00402038
	LOSS [training: 0.977069287340728 | validation: 0.8669894020387338]
	TIME [epoch: 10.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8466004220832494		[learning rate: 0.0040081]
	Learning Rate: 0.00400805
	LOSS [training: 0.8466004220832494 | validation: 0.9215292496559429]
	TIME [epoch: 10.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.777058843676321		[learning rate: 0.0039958]
	Learning Rate: 0.00399577
	LOSS [training: 0.777058843676321 | validation: 0.8524968858911313]
	TIME [epoch: 10.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6997051124772857		[learning rate: 0.0039835]
	Learning Rate: 0.00398352
	LOSS [training: 0.6997051124772857 | validation: 0.6960108569574665]
	TIME [epoch: 10.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7910226693097139		[learning rate: 0.0039713]
	Learning Rate: 0.00397131
	LOSS [training: 0.7910226693097139 | validation: 1.317252485787046]
	TIME [epoch: 10.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8445818389338786		[learning rate: 0.0039591]
	Learning Rate: 0.00395913
	LOSS [training: 0.8445818389338786 | validation: 0.8283247167177703]
	TIME [epoch: 10.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9366571386189516		[learning rate: 0.003947]
	Learning Rate: 0.003947
	LOSS [training: 0.9366571386189516 | validation: 1.2923758772314842]
	TIME [epoch: 10.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0017255347233163		[learning rate: 0.0039349]
	Learning Rate: 0.0039349
	LOSS [training: 1.0017255347233163 | validation: 0.8365147540453919]
	TIME [epoch: 10.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7014666597039639		[learning rate: 0.0039228]
	Learning Rate: 0.00392283
	LOSS [training: 0.7014666597039639 | validation: 0.9626976027712333]
	TIME [epoch: 10.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0240515525509124		[learning rate: 0.0039108]
	Learning Rate: 0.00391081
	LOSS [training: 1.0240515525509124 | validation: 1.1603218071862673]
	TIME [epoch: 10.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9438461590552649		[learning rate: 0.0038988]
	Learning Rate: 0.00389882
	LOSS [training: 0.9438461590552649 | validation: 1.5003734924660626]
	TIME [epoch: 10.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3859406863860955		[learning rate: 0.0038869]
	Learning Rate: 0.00388687
	LOSS [training: 1.3859406863860955 | validation: 0.9854883097513241]
	TIME [epoch: 10.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8486572544308046		[learning rate: 0.003875]
	Learning Rate: 0.00387495
	LOSS [training: 0.8486572544308046 | validation: 0.9143995607638838]
	TIME [epoch: 10.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9515384525646334		[learning rate: 0.0038631]
	Learning Rate: 0.00386308
	LOSS [training: 0.9515384525646334 | validation: 0.7017629097450692]
	TIME [epoch: 10.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6187763430510462		[learning rate: 0.0038512]
	Learning Rate: 0.00385123
	LOSS [training: 0.6187763430510462 | validation: 0.6782428857168943]
	TIME [epoch: 10.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6359880880376416		[learning rate: 0.0038394]
	Learning Rate: 0.00383943
	LOSS [training: 0.6359880880376416 | validation: 0.627248113212669]
	TIME [epoch: 10.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6150055734934458		[learning rate: 0.0038277]
	Learning Rate: 0.00382766
	LOSS [training: 0.6150055734934458 | validation: 0.6170017649402928]
	TIME [epoch: 10.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6224021145416933		[learning rate: 0.0038159]
	Learning Rate: 0.00381593
	LOSS [training: 0.6224021145416933 | validation: 0.7132062632943706]
	TIME [epoch: 10.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5982723992748857		[learning rate: 0.0038042]
	Learning Rate: 0.00380423
	LOSS [training: 0.5982723992748857 | validation: 0.8000336632163166]
	TIME [epoch: 10.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7102532533210527		[learning rate: 0.0037926]
	Learning Rate: 0.00379257
	LOSS [training: 0.7102532533210527 | validation: 0.7973112304171346]
	TIME [epoch: 10.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8235033131939724		[learning rate: 0.0037809]
	Learning Rate: 0.00378094
	LOSS [training: 0.8235033131939724 | validation: 1.12691251200527]
	TIME [epoch: 10.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0186446993241862		[learning rate: 0.0037694]
	Learning Rate: 0.00376935
	LOSS [training: 1.0186446993241862 | validation: 0.7033077327713025]
	TIME [epoch: 10.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8687761275475964		[learning rate: 0.0037578]
	Learning Rate: 0.0037578
	LOSS [training: 0.8687761275475964 | validation: 0.963542146752281]
	TIME [epoch: 10.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8194839575766293		[learning rate: 0.0037463]
	Learning Rate: 0.00374628
	LOSS [training: 0.8194839575766293 | validation: 1.0838972893825207]
	TIME [epoch: 10.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9990909585349627		[learning rate: 0.0037348]
	Learning Rate: 0.00373479
	LOSS [training: 0.9990909585349627 | validation: 1.3377717330090053]
	TIME [epoch: 10.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.255330599338198		[learning rate: 0.0037233]
	Learning Rate: 0.00372335
	LOSS [training: 1.255330599338198 | validation: 0.8952528693817028]
	TIME [epoch: 10.3 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.839168580030725		[learning rate: 0.0037119]
	Learning Rate: 0.00371193
	LOSS [training: 0.839168580030725 | validation: 1.2490465806772164]
	TIME [epoch: 10.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9511829483957828		[learning rate: 0.0037006]
	Learning Rate: 0.00370055
	LOSS [training: 0.9511829483957828 | validation: 0.9066226971746759]
	TIME [epoch: 10.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7739200771549974		[learning rate: 0.0036892]
	Learning Rate: 0.00368921
	LOSS [training: 0.7739200771549974 | validation: 0.7118907525210958]
	TIME [epoch: 10.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5656864236217767		[learning rate: 0.0036779]
	Learning Rate: 0.0036779
	LOSS [training: 0.5656864236217767 | validation: 0.9289772908151782]
	TIME [epoch: 10.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7129513028987269		[learning rate: 0.0036666]
	Learning Rate: 0.00366663
	LOSS [training: 0.7129513028987269 | validation: 0.5285535732503115]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5795460818758218		[learning rate: 0.0036554]
	Learning Rate: 0.00365539
	LOSS [training: 0.5795460818758218 | validation: 0.7785545440774823]
	TIME [epoch: 10.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6824024978280503		[learning rate: 0.0036442]
	Learning Rate: 0.00364418
	LOSS [training: 0.6824024978280503 | validation: 0.7776755771201371]
	TIME [epoch: 10.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8639147279224659		[learning rate: 0.003633]
	Learning Rate: 0.00363301
	LOSS [training: 0.8639147279224659 | validation: 0.851292757533679]
	TIME [epoch: 10.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7236364463667865		[learning rate: 0.0036219]
	Learning Rate: 0.00362187
	LOSS [training: 0.7236364463667865 | validation: 0.5709243763539218]
	TIME [epoch: 10.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6053666891397856		[learning rate: 0.0036108]
	Learning Rate: 0.00361077
	LOSS [training: 0.6053666891397856 | validation: 0.8097767167386576]
	TIME [epoch: 10.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7917128344020237		[learning rate: 0.0035997]
	Learning Rate: 0.0035997
	LOSS [training: 0.7917128344020237 | validation: 0.8742814722328447]
	TIME [epoch: 10.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.676397462055121		[learning rate: 0.0035887]
	Learning Rate: 0.00358867
	LOSS [training: 0.676397462055121 | validation: 0.6691642761061075]
	TIME [epoch: 10.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6344037750023802		[learning rate: 0.0035777]
	Learning Rate: 0.00357767
	LOSS [training: 0.6344037750023802 | validation: 0.5839547730801679]
	TIME [epoch: 10.3 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6444784799561184		[learning rate: 0.0035667]
	Learning Rate: 0.0035667
	LOSS [training: 0.6444784799561184 | validation: 0.792447073167264]
	TIME [epoch: 10.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6600815791651831		[learning rate: 0.0035558]
	Learning Rate: 0.00355577
	LOSS [training: 0.6600815791651831 | validation: 0.9102672302063625]
	TIME [epoch: 10.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7052147700655491		[learning rate: 0.0035449]
	Learning Rate: 0.00354487
	LOSS [training: 0.7052147700655491 | validation: 0.6778429893872285]
	TIME [epoch: 10.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6178103939393053		[learning rate: 0.003534]
	Learning Rate: 0.003534
	LOSS [training: 0.6178103939393053 | validation: 0.5993422755395915]
	TIME [epoch: 10.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5171266218446753		[learning rate: 0.0035232]
	Learning Rate: 0.00352317
	LOSS [training: 0.5171266218446753 | validation: 0.9246334457810343]
	TIME [epoch: 10.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6370725569759476		[learning rate: 0.0035124]
	Learning Rate: 0.00351237
	LOSS [training: 0.6370725569759476 | validation: 0.5907416701371094]
	TIME [epoch: 10.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6478986151617707		[learning rate: 0.0035016]
	Learning Rate: 0.0035016
	LOSS [training: 0.6478986151617707 | validation: 1.1007959741916566]
	TIME [epoch: 10.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.842372121998004		[learning rate: 0.0034909]
	Learning Rate: 0.00349087
	LOSS [training: 0.842372121998004 | validation: 0.7263478542159209]
	TIME [epoch: 10.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6675228977242036		[learning rate: 0.0034802]
	Learning Rate: 0.00348017
	LOSS [training: 0.6675228977242036 | validation: 0.5433871433152365]
	TIME [epoch: 10.3 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.714578108734343		[learning rate: 0.0034695]
	Learning Rate: 0.0034695
	LOSS [training: 0.714578108734343 | validation: 0.7416596340775548]
	TIME [epoch: 10.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6234613028453804		[learning rate: 0.0034589]
	Learning Rate: 0.00345886
	LOSS [training: 0.6234613028453804 | validation: 0.6083846932391448]
	TIME [epoch: 10.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5256751997604192		[learning rate: 0.0034483]
	Learning Rate: 0.00344826
	LOSS [training: 0.5256751997604192 | validation: 0.6570192838145001]
	TIME [epoch: 10.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5773203201135113		[learning rate: 0.0034377]
	Learning Rate: 0.00343769
	LOSS [training: 0.5773203201135113 | validation: 0.5883740103289583]
	TIME [epoch: 10.3 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49288794736991937		[learning rate: 0.0034272]
	Learning Rate: 0.00342715
	LOSS [training: 0.49288794736991937 | validation: 0.5258768219479061]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_849.pth
	Model improved!!!
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6052604990046879		[learning rate: 0.0034166]
	Learning Rate: 0.00341665
	LOSS [training: 0.6052604990046879 | validation: 0.6238095050672643]
	TIME [epoch: 10.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6563734699233844		[learning rate: 0.0034062]
	Learning Rate: 0.00340617
	LOSS [training: 0.6563734699233844 | validation: 0.8774956696476193]
	TIME [epoch: 10.3 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7011115007590822		[learning rate: 0.0033957]
	Learning Rate: 0.00339573
	LOSS [training: 0.7011115007590822 | validation: 0.6165824319775084]
	TIME [epoch: 10.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5195491365974256		[learning rate: 0.0033853]
	Learning Rate: 0.00338532
	LOSS [training: 0.5195491365974256 | validation: 0.5332522427542237]
	TIME [epoch: 10.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5059923042273338		[learning rate: 0.0033749]
	Learning Rate: 0.00337494
	LOSS [training: 0.5059923042273338 | validation: 0.4204899572229087]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.629891851997056		[learning rate: 0.0033646]
	Learning Rate: 0.0033646
	LOSS [training: 0.629891851997056 | validation: 0.7614227830453917]
	TIME [epoch: 10.3 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6881630479080942		[learning rate: 0.0033543]
	Learning Rate: 0.00335428
	LOSS [training: 0.6881630479080942 | validation: 0.6977209267329545]
	TIME [epoch: 10.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5230954999462576		[learning rate: 0.003344]
	Learning Rate: 0.003344
	LOSS [training: 0.5230954999462576 | validation: 0.5381940744136439]
	TIME [epoch: 10.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6017303635405573		[learning rate: 0.0033338]
	Learning Rate: 0.00333375
	LOSS [training: 0.6017303635405573 | validation: 0.5621031898227092]
	TIME [epoch: 10.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6013163212046596		[learning rate: 0.0033235]
	Learning Rate: 0.00332353
	LOSS [training: 0.6013163212046596 | validation: 0.6192982980612525]
	TIME [epoch: 10.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7848297036466295		[learning rate: 0.0033133]
	Learning Rate: 0.00331334
	LOSS [training: 0.7848297036466295 | validation: 0.7471300409803797]
	TIME [epoch: 10.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6296130893664568		[learning rate: 0.0033032]
	Learning Rate: 0.00330319
	LOSS [training: 0.6296130893664568 | validation: 0.5374020533871496]
	TIME [epoch: 10.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.809610828224457		[learning rate: 0.0032931]
	Learning Rate: 0.00329306
	LOSS [training: 0.809610828224457 | validation: 0.6256956429519762]
	TIME [epoch: 10.3 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6096429247910196		[learning rate: 0.003283]
	Learning Rate: 0.00328297
	LOSS [training: 0.6096429247910196 | validation: 0.6302336124074913]
	TIME [epoch: 10.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5247443503981197		[learning rate: 0.0032729]
	Learning Rate: 0.0032729
	LOSS [training: 0.5247443503981197 | validation: 0.5006504819485097]
	TIME [epoch: 10.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6055089614985343		[learning rate: 0.0032629]
	Learning Rate: 0.00326287
	LOSS [training: 0.6055089614985343 | validation: 0.4542933878152192]
	TIME [epoch: 10.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4234801706643242		[learning rate: 0.0032529]
	Learning Rate: 0.00325287
	LOSS [training: 0.4234801706643242 | validation: 0.6136857770815911]
	TIME [epoch: 10.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.590852746649015		[learning rate: 0.0032429]
	Learning Rate: 0.0032429
	LOSS [training: 0.590852746649015 | validation: 0.4972547296224253]
	TIME [epoch: 10.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5124326327890943		[learning rate: 0.003233]
	Learning Rate: 0.00323296
	LOSS [training: 0.5124326327890943 | validation: 0.902702422852746]
	TIME [epoch: 10.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5625421191931675		[learning rate: 0.003223]
	Learning Rate: 0.00322305
	LOSS [training: 0.5625421191931675 | validation: 0.5709626960678551]
	TIME [epoch: 10.3 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5175499354265294		[learning rate: 0.0032132]
	Learning Rate: 0.00321317
	LOSS [training: 0.5175499354265294 | validation: 0.7632486896830832]
	TIME [epoch: 10.3 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5229567622093974		[learning rate: 0.0032033]
	Learning Rate: 0.00320332
	LOSS [training: 0.5229567622093974 | validation: 0.7488954611383918]
	TIME [epoch: 10.3 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6411536259900377		[learning rate: 0.0031935]
	Learning Rate: 0.0031935
	LOSS [training: 0.6411536259900377 | validation: 0.990955360674541]
	TIME [epoch: 10.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7212124733816403		[learning rate: 0.0031837]
	Learning Rate: 0.00318371
	LOSS [training: 0.7212124733816403 | validation: 1.0144580358775332]
	TIME [epoch: 10.3 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.741539859008106		[learning rate: 0.0031739]
	Learning Rate: 0.00317395
	LOSS [training: 0.741539859008106 | validation: 0.847340479646557]
	TIME [epoch: 10.3 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.61711031955446		[learning rate: 0.0031642]
	Learning Rate: 0.00316422
	LOSS [training: 0.61711031955446 | validation: 0.5811022338097728]
	TIME [epoch: 10.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5957215134952762		[learning rate: 0.0031545]
	Learning Rate: 0.00315452
	LOSS [training: 0.5957215134952762 | validation: 0.8321161950560191]
	TIME [epoch: 10.3 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6056436644132035		[learning rate: 0.0031449]
	Learning Rate: 0.00314485
	LOSS [training: 0.6056436644132035 | validation: 0.9260749831607905]
	TIME [epoch: 10.3 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6964691830634981		[learning rate: 0.0031352]
	Learning Rate: 0.00313521
	LOSS [training: 0.6964691830634981 | validation: 0.6468655261395106]
	TIME [epoch: 10.3 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8410062376547648		[learning rate: 0.0031256]
	Learning Rate: 0.0031256
	LOSS [training: 0.8410062376547648 | validation: 0.6727807591885008]
	TIME [epoch: 10.3 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6258921258322361		[learning rate: 0.003116]
	Learning Rate: 0.00311602
	LOSS [training: 0.6258921258322361 | validation: 0.5469222019229527]
	TIME [epoch: 10.3 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4146464786432992		[learning rate: 0.0031065]
	Learning Rate: 0.00310647
	LOSS [training: 0.4146464786432992 | validation: 0.38410483091389114]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4415944951031431		[learning rate: 0.0030969]
	Learning Rate: 0.00309694
	LOSS [training: 0.4415944951031431 | validation: 0.5530288940165862]
	TIME [epoch: 10.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7082676904630731		[learning rate: 0.0030875]
	Learning Rate: 0.00308745
	LOSS [training: 0.7082676904630731 | validation: 0.8277199154766386]
	TIME [epoch: 10.3 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7223340209135921		[learning rate: 0.003078]
	Learning Rate: 0.00307799
	LOSS [training: 0.7223340209135921 | validation: 0.8639394162109634]
	TIME [epoch: 10.3 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7569177819655802		[learning rate: 0.0030686]
	Learning Rate: 0.00306855
	LOSS [training: 0.7569177819655802 | validation: 0.7468769689786359]
	TIME [epoch: 10.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6139487217144162		[learning rate: 0.0030591]
	Learning Rate: 0.00305914
	LOSS [training: 1.6139487217144162 | validation: 2.9012323013668144]
	TIME [epoch: 10.3 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4794968844290812		[learning rate: 0.0030498]
	Learning Rate: 0.00304977
	LOSS [training: 3.4794968844290812 | validation: 1.5879200994062774]
	TIME [epoch: 10.3 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.38831978246048		[learning rate: 0.0030404]
	Learning Rate: 0.00304042
	LOSS [training: 1.38831978246048 | validation: 0.6485016076383516]
	TIME [epoch: 10.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5076547699543579		[learning rate: 0.0030311]
	Learning Rate: 0.0030311
	LOSS [training: 0.5076547699543579 | validation: 0.5103505178135403]
	TIME [epoch: 10.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6580397152326778		[learning rate: 0.0030218]
	Learning Rate: 0.00302181
	LOSS [training: 0.6580397152326778 | validation: 0.5410550296671675]
	TIME [epoch: 10.3 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6191247872797415		[learning rate: 0.0030125]
	Learning Rate: 0.00301254
	LOSS [training: 0.6191247872797415 | validation: 0.4708988737825536]
	TIME [epoch: 10.3 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6628867880245012		[learning rate: 0.0030033]
	Learning Rate: 0.00300331
	LOSS [training: 0.6628867880245012 | validation: 0.6818983701902945]
	TIME [epoch: 10.3 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7300480800445095		[learning rate: 0.0029941]
	Learning Rate: 0.0029941
	LOSS [training: 0.7300480800445095 | validation: 0.8539423521199316]
	TIME [epoch: 10.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7910699440788587		[learning rate: 0.0029849]
	Learning Rate: 0.00298492
	LOSS [training: 0.7910699440788587 | validation: 0.6988978818222122]
	TIME [epoch: 10.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6136165440553973		[learning rate: 0.0029758]
	Learning Rate: 0.00297577
	LOSS [training: 0.6136165440553973 | validation: 0.6594360614466599]
	TIME [epoch: 10.3 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5350917299555277		[learning rate: 0.0029667]
	Learning Rate: 0.00296665
	LOSS [training: 0.5350917299555277 | validation: 0.5294100204363034]
	TIME [epoch: 10.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4921409232047581		[learning rate: 0.0029576]
	Learning Rate: 0.00295756
	LOSS [training: 0.4921409232047581 | validation: 0.5792216850257659]
	TIME [epoch: 10.3 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.511629247553355		[learning rate: 0.0029485]
	Learning Rate: 0.00294849
	LOSS [training: 0.511629247553355 | validation: 0.5128193978259685]
	TIME [epoch: 10.3 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49128507656399456		[learning rate: 0.0029395]
	Learning Rate: 0.00293945
	LOSS [training: 0.49128507656399456 | validation: 0.44897583944836383]
	TIME [epoch: 10.3 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47299074382682366		[learning rate: 0.0029304]
	Learning Rate: 0.00293044
	LOSS [training: 0.47299074382682366 | validation: 1.026365411615103]
	TIME [epoch: 10.3 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7201880472986584		[learning rate: 0.0029215]
	Learning Rate: 0.00292146
	LOSS [training: 0.7201880472986584 | validation: 0.5629338364322228]
	TIME [epoch: 10.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47727528581529555		[learning rate: 0.0029125]
	Learning Rate: 0.0029125
	LOSS [training: 0.47727528581529555 | validation: 0.43063180085905817]
	TIME [epoch: 10.3 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5809537204646725		[learning rate: 0.0029036]
	Learning Rate: 0.00290358
	LOSS [training: 0.5809537204646725 | validation: 0.518577198429202]
	TIME [epoch: 10.3 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5121546105263574		[learning rate: 0.0028947]
	Learning Rate: 0.00289468
	LOSS [training: 0.5121546105263574 | validation: 0.6177741390904928]
	TIME [epoch: 10.3 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5561990327215561		[learning rate: 0.0028858]
	Learning Rate: 0.0028858
	LOSS [training: 0.5561990327215561 | validation: 0.4886972689109706]
	TIME [epoch: 10.3 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47174257290687815		[learning rate: 0.002877]
	Learning Rate: 0.00287696
	LOSS [training: 0.47174257290687815 | validation: 0.5575137200426545]
	TIME [epoch: 10.3 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4675595030960749		[learning rate: 0.0028681]
	Learning Rate: 0.00286814
	LOSS [training: 0.4675595030960749 | validation: 0.6743933854464482]
	TIME [epoch: 10.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6439019113063811		[learning rate: 0.0028593]
	Learning Rate: 0.00285935
	LOSS [training: 0.6439019113063811 | validation: 0.7509447202020527]
	TIME [epoch: 10.3 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.147772692539197		[learning rate: 0.0028506]
	Learning Rate: 0.00285058
	LOSS [training: 1.147772692539197 | validation: 0.7707062310031256]
	TIME [epoch: 10.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7493088940630221		[learning rate: 0.0028418]
	Learning Rate: 0.00284184
	LOSS [training: 0.7493088940630221 | validation: 0.8197087779984298]
	TIME [epoch: 10.3 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9440023046055315		[learning rate: 0.0028331]
	Learning Rate: 0.00283313
	LOSS [training: 0.9440023046055315 | validation: 0.7345639519965995]
	TIME [epoch: 10.3 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.561304713747831		[learning rate: 0.0028244]
	Learning Rate: 0.00282445
	LOSS [training: 0.561304713747831 | validation: 0.5093348219976226]
	TIME [epoch: 10.3 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49621895393818977		[learning rate: 0.0028158]
	Learning Rate: 0.00281579
	LOSS [training: 0.49621895393818977 | validation: 0.5098930277115029]
	TIME [epoch: 10.3 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4864281940976253		[learning rate: 0.0028072]
	Learning Rate: 0.00280716
	LOSS [training: 0.4864281940976253 | validation: 0.5180307782491553]
	TIME [epoch: 10.3 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6026317628180797		[learning rate: 0.0027986]
	Learning Rate: 0.00279855
	LOSS [training: 0.6026317628180797 | validation: 0.38998083848312454]
	TIME [epoch: 10.3 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5760071200108119		[learning rate: 0.00279]
	Learning Rate: 0.00278997
	LOSS [training: 0.5760071200108119 | validation: 0.677462360577679]
	TIME [epoch: 10.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6965374277282811		[learning rate: 0.0027814]
	Learning Rate: 0.00278142
	LOSS [training: 0.6965374277282811 | validation: 0.5100024361243091]
	TIME [epoch: 10.3 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.514353513531094		[learning rate: 0.0027729]
	Learning Rate: 0.00277289
	LOSS [training: 0.514353513531094 | validation: 0.43054098632154325]
	TIME [epoch: 10.3 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5247966505903126		[learning rate: 0.0027644]
	Learning Rate: 0.00276439
	LOSS [training: 0.5247966505903126 | validation: 0.692813094913237]
	TIME [epoch: 10.3 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5732884466403458		[learning rate: 0.0027559]
	Learning Rate: 0.00275592
	LOSS [training: 0.5732884466403458 | validation: 0.5476203779664005]
	TIME [epoch: 10.3 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6035753438857163		[learning rate: 0.0027475]
	Learning Rate: 0.00274747
	LOSS [training: 0.6035753438857163 | validation: 0.5664908208511787]
	TIME [epoch: 10.3 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46768190625532347		[learning rate: 0.002739]
	Learning Rate: 0.00273905
	LOSS [training: 0.46768190625532347 | validation: 0.4551515795246644]
	TIME [epoch: 10.3 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38640703892581685		[learning rate: 0.0027307]
	Learning Rate: 0.00273065
	LOSS [training: 0.38640703892581685 | validation: 0.40423610822714995]
	TIME [epoch: 10.3 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5250285689801453		[learning rate: 0.0027223]
	Learning Rate: 0.00272228
	LOSS [training: 0.5250285689801453 | validation: 0.6743182867286279]
	TIME [epoch: 10.3 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49433108771262724		[learning rate: 0.0027139]
	Learning Rate: 0.00271394
	LOSS [training: 0.49433108771262724 | validation: 0.4037325885882474]
	TIME [epoch: 10.3 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3985028374723787		[learning rate: 0.0027056]
	Learning Rate: 0.00270562
	LOSS [training: 0.3985028374723787 | validation: 0.7346343617344596]
	TIME [epoch: 10.3 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6837880533923865		[learning rate: 0.0026973]
	Learning Rate: 0.00269733
	LOSS [training: 0.6837880533923865 | validation: 0.7278324140003675]
	TIME [epoch: 10.3 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6100598266655289		[learning rate: 0.0026891]
	Learning Rate: 0.00268906
	LOSS [training: 0.6100598266655289 | validation: 0.5727032606376242]
	TIME [epoch: 10.3 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4509328073248282		[learning rate: 0.0026808]
	Learning Rate: 0.00268081
	LOSS [training: 0.4509328073248282 | validation: 0.5031561733856682]
	TIME [epoch: 10.3 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.450086046954241		[learning rate: 0.0026726]
	Learning Rate: 0.0026726
	LOSS [training: 0.450086046954241 | validation: 0.47247403827408446]
	TIME [epoch: 10.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4391794244882038		[learning rate: 0.0026644]
	Learning Rate: 0.0026644
	LOSS [training: 0.4391794244882038 | validation: 0.9588723692629421]
	TIME [epoch: 10.3 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7441599494228541		[learning rate: 0.0026562]
	Learning Rate: 0.00265624
	LOSS [training: 0.7441599494228541 | validation: 0.4503113450867048]
	TIME [epoch: 10.3 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44960548116810195		[learning rate: 0.0026481]
	Learning Rate: 0.00264809
	LOSS [training: 0.44960548116810195 | validation: 0.7517158914843173]
	TIME [epoch: 10.3 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6327427795829822		[learning rate: 0.00264]
	Learning Rate: 0.00263998
	LOSS [training: 0.6327427795829822 | validation: 0.4480994194855674]
	TIME [epoch: 10.3 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4108859924883417		[learning rate: 0.0026319]
	Learning Rate: 0.00263188
	LOSS [training: 0.4108859924883417 | validation: 0.42777706927853737]
	TIME [epoch: 10.3 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5363850136764916		[learning rate: 0.0026238]
	Learning Rate: 0.00262382
	LOSS [training: 0.5363850136764916 | validation: 0.39937089827011035]
	TIME [epoch: 10.3 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3954051998599749		[learning rate: 0.0026158]
	Learning Rate: 0.00261577
	LOSS [training: 0.3954051998599749 | validation: 0.4218519986828768]
	TIME [epoch: 10.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4663305947806185		[learning rate: 0.0026078]
	Learning Rate: 0.00260775
	LOSS [training: 0.4663305947806185 | validation: 0.516863291779492]
	TIME [epoch: 10.3 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4541937288960587		[learning rate: 0.0025998]
	Learning Rate: 0.00259976
	LOSS [training: 0.4541937288960587 | validation: 0.5666573261531259]
	TIME [epoch: 10.3 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5731394777944601		[learning rate: 0.0025918]
	Learning Rate: 0.00259179
	LOSS [training: 0.5731394777944601 | validation: 0.7809803507393142]
	TIME [epoch: 10.3 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6063997356170497		[learning rate: 0.0025838]
	Learning Rate: 0.00258385
	LOSS [training: 0.6063997356170497 | validation: 0.3243659355899701]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4441785427098437		[learning rate: 0.0025759]
	Learning Rate: 0.00257593
	LOSS [training: 0.4441785427098437 | validation: 0.5022056461648219]
	TIME [epoch: 10.3 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4665151608503619		[learning rate: 0.002568]
	Learning Rate: 0.00256803
	LOSS [training: 0.4665151608503619 | validation: 0.48305124160625085]
	TIME [epoch: 10.3 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49379681669072123		[learning rate: 0.0025602]
	Learning Rate: 0.00256016
	LOSS [training: 0.49379681669072123 | validation: 0.5389363597999313]
	TIME [epoch: 10.3 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6284104222156043		[learning rate: 0.0025523]
	Learning Rate: 0.00255231
	LOSS [training: 0.6284104222156043 | validation: 0.6133749438424048]
	TIME [epoch: 10.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7629636290925755		[learning rate: 0.0025445]
	Learning Rate: 0.00254449
	LOSS [training: 0.7629636290925755 | validation: 0.5461105118457756]
	TIME [epoch: 10.3 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44239914800590985		[learning rate: 0.0025367]
	Learning Rate: 0.00253669
	LOSS [training: 0.44239914800590985 | validation: 0.5012336494741322]
	TIME [epoch: 10.3 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933027092685584		[learning rate: 0.0025289]
	Learning Rate: 0.00252891
	LOSS [training: 0.6933027092685584 | validation: 0.4215413285051038]
	TIME [epoch: 10.3 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49292268490977714		[learning rate: 0.0025212]
	Learning Rate: 0.00252116
	LOSS [training: 0.49292268490977714 | validation: 0.4600119294014952]
	TIME [epoch: 10.3 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38381057493814963		[learning rate: 0.0025134]
	Learning Rate: 0.00251343
	LOSS [training: 0.38381057493814963 | validation: 0.3933141689851273]
	TIME [epoch: 10.3 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4095448571049161		[learning rate: 0.0025057]
	Learning Rate: 0.00250572
	LOSS [training: 0.4095448571049161 | validation: 0.37195861523682866]
	TIME [epoch: 10.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43261407730933		[learning rate: 0.002498]
	Learning Rate: 0.00249804
	LOSS [training: 0.43261407730933 | validation: 0.5944023652669079]
	TIME [epoch: 10.3 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5773437821866054		[learning rate: 0.0024904]
	Learning Rate: 0.00249039
	LOSS [training: 0.5773437821866054 | validation: 0.6507187256133118]
	TIME [epoch: 10.3 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40780882996921203		[learning rate: 0.0024828]
	Learning Rate: 0.00248275
	LOSS [training: 0.40780882996921203 | validation: 0.37048435859298634]
	TIME [epoch: 10.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37762657568197405		[learning rate: 0.0024751]
	Learning Rate: 0.00247514
	LOSS [training: 0.37762657568197405 | validation: 0.45259814577495394]
	TIME [epoch: 10.3 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4278310349435811		[learning rate: 0.0024676]
	Learning Rate: 0.00246755
	LOSS [training: 0.4278310349435811 | validation: 0.49593029086009405]
	TIME [epoch: 10.3 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9124197289247962		[learning rate: 0.00246]
	Learning Rate: 0.00245999
	LOSS [training: 0.9124197289247962 | validation: 0.6025063928247372]
	TIME [epoch: 10.3 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.511740889467094		[learning rate: 0.0024524]
	Learning Rate: 0.00245245
	LOSS [training: 0.511740889467094 | validation: 0.4461464674189753]
	TIME [epoch: 10.3 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5832125107745474		[learning rate: 0.0024449]
	Learning Rate: 0.00244493
	LOSS [training: 0.5832125107745474 | validation: 0.7855655584791231]
	TIME [epoch: 10.3 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6066322069563003		[learning rate: 0.0024374]
	Learning Rate: 0.00243744
	LOSS [training: 0.6066322069563003 | validation: 0.6308475858959639]
	TIME [epoch: 10.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4409631434605335		[learning rate: 0.00243]
	Learning Rate: 0.00242996
	LOSS [training: 0.4409631434605335 | validation: 0.4499339460064729]
	TIME [epoch: 10.3 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4864274673544028		[learning rate: 0.0024225]
	Learning Rate: 0.00242252
	LOSS [training: 0.4864274673544028 | validation: 0.3586958042641551]
	TIME [epoch: 10.3 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3704829723491735		[learning rate: 0.0024151]
	Learning Rate: 0.00241509
	LOSS [training: 0.3704829723491735 | validation: 0.5328905819968867]
	TIME [epoch: 10.3 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4806864585265303		[learning rate: 0.0024077]
	Learning Rate: 0.00240769
	LOSS [training: 0.4806864585265303 | validation: 0.6558466222637592]
	TIME [epoch: 10.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49102607130374976		[learning rate: 0.0024003]
	Learning Rate: 0.00240031
	LOSS [training: 0.49102607130374976 | validation: 0.4151950802013595]
	TIME [epoch: 10.3 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49862505052634776		[learning rate: 0.0023929]
	Learning Rate: 0.00239295
	LOSS [training: 0.49862505052634776 | validation: 0.5464968063853524]
	TIME [epoch: 10.3 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44849985841444007		[learning rate: 0.0023856]
	Learning Rate: 0.00238561
	LOSS [training: 0.44849985841444007 | validation: 0.40927961859373524]
	TIME [epoch: 10.3 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41521491272220584		[learning rate: 0.0023783]
	Learning Rate: 0.0023783
	LOSS [training: 0.41521491272220584 | validation: 0.47027811349300813]
	TIME [epoch: 10.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4291361423064326		[learning rate: 0.002371]
	Learning Rate: 0.00237101
	LOSS [training: 0.4291361423064326 | validation: 0.5387684400520063]
	TIME [epoch: 10.3 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5939936403780929		[learning rate: 0.0023637]
	Learning Rate: 0.00236374
	LOSS [training: 0.5939936403780929 | validation: 0.3923891130456115]
	TIME [epoch: 10.3 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5551475984540006		[learning rate: 0.0023565]
	Learning Rate: 0.0023565
	LOSS [training: 0.5551475984540006 | validation: 0.4918063987630022]
	TIME [epoch: 10.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45380216316388544		[learning rate: 0.0023493]
	Learning Rate: 0.00234927
	LOSS [training: 0.45380216316388544 | validation: 0.4878066644990895]
	TIME [epoch: 10.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45242321508009464		[learning rate: 0.0023421]
	Learning Rate: 0.00234207
	LOSS [training: 0.45242321508009464 | validation: 0.38034480965339273]
	TIME [epoch: 10.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3620967805390361		[learning rate: 0.0023349]
	Learning Rate: 0.00233489
	LOSS [training: 0.3620967805390361 | validation: 0.4764911931596305]
	TIME [epoch: 10.3 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3165306322377187		[learning rate: 0.0023277]
	Learning Rate: 0.00232773
	LOSS [training: 0.3165306322377187 | validation: 0.35941315280555075]
	TIME [epoch: 10.3 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3532383563445433		[learning rate: 0.0023206]
	Learning Rate: 0.0023206
	LOSS [training: 0.3532383563445433 | validation: 0.5578350916775415]
	TIME [epoch: 10.3 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5639398069268868		[learning rate: 0.0023135]
	Learning Rate: 0.00231348
	LOSS [training: 0.5639398069268868 | validation: 0.3900234760517705]
	TIME [epoch: 10.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4668772574196162		[learning rate: 0.0023064]
	Learning Rate: 0.00230639
	LOSS [training: 0.4668772574196162 | validation: 0.5026150162290156]
	TIME [epoch: 10.3 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.518790666979936		[learning rate: 0.0022993]
	Learning Rate: 0.00229932
	LOSS [training: 0.518790666979936 | validation: 0.43556907226070424]
	TIME [epoch: 10.3 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44203074565016764		[learning rate: 0.0022923]
	Learning Rate: 0.00229227
	LOSS [training: 0.44203074565016764 | validation: 0.5728080513764735]
	TIME [epoch: 10.3 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4630488336829754		[learning rate: 0.0022852]
	Learning Rate: 0.00228525
	LOSS [training: 0.4630488336829754 | validation: 0.3478239786642136]
	TIME [epoch: 10.3 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34917330823235954		[learning rate: 0.0022782]
	Learning Rate: 0.00227824
	LOSS [training: 0.34917330823235954 | validation: 0.3665354225919359]
	TIME [epoch: 10.3 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3867102044971527		[learning rate: 0.0022713]
	Learning Rate: 0.00227126
	LOSS [training: 0.3867102044971527 | validation: 0.47769781072893686]
	TIME [epoch: 10.3 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48610910667210516		[learning rate: 0.0022643]
	Learning Rate: 0.0022643
	LOSS [training: 0.48610910667210516 | validation: 0.3570677993308117]
	TIME [epoch: 10.3 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3280142453653128		[learning rate: 0.0022574]
	Learning Rate: 0.00225736
	LOSS [training: 0.3280142453653128 | validation: 0.32503389009177325]
	TIME [epoch: 10.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3287509305830406		[learning rate: 0.0022504]
	Learning Rate: 0.00225044
	LOSS [training: 0.3287509305830406 | validation: 0.2760169558697296]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_986.pth
	Model improved!!!
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37218628362637757		[learning rate: 0.0022435]
	Learning Rate: 0.00224354
	LOSS [training: 0.37218628362637757 | validation: 0.6044334773171941]
	TIME [epoch: 10.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8434996152573613		[learning rate: 0.0022367]
	Learning Rate: 0.00223666
	LOSS [training: 0.8434996152573613 | validation: 0.41627985996630446]
	TIME [epoch: 10.3 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43142868841301285		[learning rate: 0.0022298]
	Learning Rate: 0.0022298
	LOSS [training: 0.43142868841301285 | validation: 0.4859178548790831]
	TIME [epoch: 10.3 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3957206910779694		[learning rate: 0.002223]
	Learning Rate: 0.00222297
	LOSS [training: 0.3957206910779694 | validation: 0.46161533566525637]
	TIME [epoch: 10.3 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5090546946016958		[learning rate: 0.0022162]
	Learning Rate: 0.00221615
	LOSS [training: 0.5090546946016958 | validation: 0.45888264689692415]
	TIME [epoch: 10.3 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47214883391332324		[learning rate: 0.0022094]
	Learning Rate: 0.00220936
	LOSS [training: 0.47214883391332324 | validation: 0.36145995956975974]
	TIME [epoch: 10.3 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4174474603266887		[learning rate: 0.0022026]
	Learning Rate: 0.00220259
	LOSS [training: 0.4174474603266887 | validation: 0.35128654797558057]
	TIME [epoch: 10.3 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40265925640969663		[learning rate: 0.0021958]
	Learning Rate: 0.00219584
	LOSS [training: 0.40265925640969663 | validation: 0.34573664746214117]
	TIME [epoch: 10.3 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4124808372571911		[learning rate: 0.0021891]
	Learning Rate: 0.00218911
	LOSS [training: 0.4124808372571911 | validation: 0.3633046804866143]
	TIME [epoch: 10.3 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4392097796864952		[learning rate: 0.0021824]
	Learning Rate: 0.00218239
	LOSS [training: 0.4392097796864952 | validation: 0.4148170356383926]
	TIME [epoch: 10.3 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37156259858337015		[learning rate: 0.0021757]
	Learning Rate: 0.00217571
	LOSS [training: 0.37156259858337015 | validation: 0.34429081221654884]
	TIME [epoch: 10.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3485300573909924		[learning rate: 0.002169]
	Learning Rate: 0.00216904
	LOSS [training: 0.3485300573909924 | validation: 0.35366030466913806]
	TIME [epoch: 10.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3837171409719141		[learning rate: 0.0021624]
	Learning Rate: 0.00216239
	LOSS [training: 0.3837171409719141 | validation: 0.4560452211130061]
	TIME [epoch: 10.3 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6750946446410161		[learning rate: 0.0021558]
	Learning Rate: 0.00215576
	LOSS [training: 0.6750946446410161 | validation: 0.5834863191295883]
	TIME [epoch: 10.3 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38362271653983393		[learning rate: 0.0021491]
	Learning Rate: 0.00214915
	LOSS [training: 0.38362271653983393 | validation: 0.24826202718260512]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1001.pth
	Model improved!!!
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2588769762452599		[learning rate: 0.0021426]
	Learning Rate: 0.00214256
	LOSS [training: 0.2588769762452599 | validation: 0.2758075109505946]
	TIME [epoch: 10.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37109701578841114		[learning rate: 0.002136]
	Learning Rate: 0.00213599
	LOSS [training: 0.37109701578841114 | validation: 0.3226564763442293]
	TIME [epoch: 10.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3633356273372312		[learning rate: 0.0021294]
	Learning Rate: 0.00212945
	LOSS [training: 0.3633356273372312 | validation: 0.42995472963512116]
	TIME [epoch: 10.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4080186133693441		[learning rate: 0.0021229]
	Learning Rate: 0.00212292
	LOSS [training: 0.4080186133693441 | validation: 0.48648618576229163]
	TIME [epoch: 10.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4196720213531588		[learning rate: 0.0021164]
	Learning Rate: 0.00211641
	LOSS [training: 0.4196720213531588 | validation: 0.3057437190858316]
	TIME [epoch: 10.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36900795031489847		[learning rate: 0.0021099]
	Learning Rate: 0.00210992
	LOSS [training: 0.36900795031489847 | validation: 0.4129733314501594]
	TIME [epoch: 10.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3505371831734577		[learning rate: 0.0021035]
	Learning Rate: 0.00210346
	LOSS [training: 0.3505371831734577 | validation: 0.31969923953560825]
	TIME [epoch: 10.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3063348464230146		[learning rate: 0.002097]
	Learning Rate: 0.00209701
	LOSS [training: 0.3063348464230146 | validation: 0.3076686477722874]
	TIME [epoch: 10.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4375422439647184		[learning rate: 0.0020906]
	Learning Rate: 0.00209058
	LOSS [training: 0.4375422439647184 | validation: 0.509834402066936]
	TIME [epoch: 10.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.518536855367963		[learning rate: 0.0020842]
	Learning Rate: 0.00208417
	LOSS [training: 0.518536855367963 | validation: 0.5246124485995343]
	TIME [epoch: 10.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6095959755758728		[learning rate: 0.0020778]
	Learning Rate: 0.00207778
	LOSS [training: 0.6095959755758728 | validation: 0.4402520628019055]
	TIME [epoch: 10.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44442855114990626		[learning rate: 0.0020714]
	Learning Rate: 0.00207141
	LOSS [training: 0.44442855114990626 | validation: 0.5651083514437777]
	TIME [epoch: 10.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3836672457387945		[learning rate: 0.0020651]
	Learning Rate: 0.00206506
	LOSS [training: 0.3836672457387945 | validation: 0.45835040615285594]
	TIME [epoch: 10.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3335158976402354		[learning rate: 0.0020587]
	Learning Rate: 0.00205873
	LOSS [training: 0.3335158976402354 | validation: 0.5456696958364596]
	TIME [epoch: 10.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4077854072225221		[learning rate: 0.0020524]
	Learning Rate: 0.00205242
	LOSS [training: 0.4077854072225221 | validation: 0.3745285737910615]
	TIME [epoch: 10.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5077729872596453		[learning rate: 0.0020461]
	Learning Rate: 0.00204613
	LOSS [training: 0.5077729872596453 | validation: 0.34795793173382505]
	TIME [epoch: 10.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32850548681728525		[learning rate: 0.0020399]
	Learning Rate: 0.00203986
	LOSS [training: 0.32850548681728525 | validation: 0.36332483281107336]
	TIME [epoch: 10.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4211468624142011		[learning rate: 0.0020336]
	Learning Rate: 0.00203361
	LOSS [training: 0.4211468624142011 | validation: 0.4640068080953175]
	TIME [epoch: 10.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.465635246922887		[learning rate: 0.0020274]
	Learning Rate: 0.00202737
	LOSS [training: 0.465635246922887 | validation: 0.35229077623985616]
	TIME [epoch: 10.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36199767751989487		[learning rate: 0.0020212]
	Learning Rate: 0.00202116
	LOSS [training: 0.36199767751989487 | validation: 0.3632628061756948]
	TIME [epoch: 10.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48138527749477406		[learning rate: 0.002015]
	Learning Rate: 0.00201496
	LOSS [training: 0.48138527749477406 | validation: 0.5939901509724302]
	TIME [epoch: 10.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5936863894027354		[learning rate: 0.0020088]
	Learning Rate: 0.00200878
	LOSS [training: 0.5936863894027354 | validation: 0.5724939628892802]
	TIME [epoch: 10.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43538560868336323		[learning rate: 0.0020026]
	Learning Rate: 0.00200263
	LOSS [training: 0.43538560868336323 | validation: 0.4781379659767967]
	TIME [epoch: 10.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.408900247363143		[learning rate: 0.0019965]
	Learning Rate: 0.00199649
	LOSS [training: 0.408900247363143 | validation: 0.4632438857995041]
	TIME [epoch: 10.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5143288497892782		[learning rate: 0.0019904]
	Learning Rate: 0.00199037
	LOSS [training: 0.5143288497892782 | validation: 0.5828292257627284]
	TIME [epoch: 10.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4567470775007304		[learning rate: 0.0019843]
	Learning Rate: 0.00198427
	LOSS [training: 0.4567470775007304 | validation: 0.32852995619782377]
	TIME [epoch: 10.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4837097131759759		[learning rate: 0.0019782]
	Learning Rate: 0.00197818
	LOSS [training: 0.4837097131759759 | validation: 0.42768744988892876]
	TIME [epoch: 10.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4861569025521195		[learning rate: 0.0019721]
	Learning Rate: 0.00197212
	LOSS [training: 0.4861569025521195 | validation: 0.37765523318060645]
	TIME [epoch: 10.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3657856407081721		[learning rate: 0.0019661]
	Learning Rate: 0.00196607
	LOSS [training: 0.3657856407081721 | validation: 0.40609795421420614]
	TIME [epoch: 10.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34246865333665		[learning rate: 0.00196]
	Learning Rate: 0.00196005
	LOSS [training: 0.34246865333665 | validation: 0.324447527485067]
	TIME [epoch: 10.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3701947082435929		[learning rate: 0.001954]
	Learning Rate: 0.00195404
	LOSS [training: 0.3701947082435929 | validation: 0.46259647300193196]
	TIME [epoch: 10.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35553601817862573		[learning rate: 0.001948]
	Learning Rate: 0.00194805
	LOSS [training: 0.35553601817862573 | validation: 0.37987999580605547]
	TIME [epoch: 10.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37382619852203003		[learning rate: 0.0019421]
	Learning Rate: 0.00194208
	LOSS [training: 0.37382619852203003 | validation: 0.3133251406740094]
	TIME [epoch: 10.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3527233060752395		[learning rate: 0.0019361]
	Learning Rate: 0.00193612
	LOSS [training: 0.3527233060752395 | validation: 0.9116803458647837]
	TIME [epoch: 10.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5869146444418576		[learning rate: 0.0019302]
	Learning Rate: 0.00193019
	LOSS [training: 0.5869146444418576 | validation: 0.2708987523067555]
	TIME [epoch: 10.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24137970290218705		[learning rate: 0.0019243]
	Learning Rate: 0.00192427
	LOSS [training: 0.24137970290218705 | validation: 0.3561558819687917]
	TIME [epoch: 10.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2641426490900237		[learning rate: 0.0019184]
	Learning Rate: 0.00191837
	LOSS [training: 0.2641426490900237 | validation: 0.33133891553974176]
	TIME [epoch: 10.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4086775112322549		[learning rate: 0.0019125]
	Learning Rate: 0.00191249
	LOSS [training: 0.4086775112322549 | validation: 0.5784751260490326]
	TIME [epoch: 10.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6115822315909		[learning rate: 0.0019066]
	Learning Rate: 0.00190663
	LOSS [training: 0.6115822315909 | validation: 0.47646855003374017]
	TIME [epoch: 10.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2721757564037376		[learning rate: 0.0019008]
	Learning Rate: 0.00190079
	LOSS [training: 0.2721757564037376 | validation: 0.4282950574687773]
	TIME [epoch: 10.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27199388122134793		[learning rate: 0.001895]
	Learning Rate: 0.00189496
	LOSS [training: 0.27199388122134793 | validation: 0.3860091810436477]
	TIME [epoch: 10.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3402334632941902		[learning rate: 0.0018892]
	Learning Rate: 0.00188915
	LOSS [training: 0.3402334632941902 | validation: 0.31157026072747457]
	TIME [epoch: 10.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33228337401001895		[learning rate: 0.0018834]
	Learning Rate: 0.00188336
	LOSS [training: 0.33228337401001895 | validation: 0.3164108809034661]
	TIME [epoch: 10.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2934049410282982		[learning rate: 0.0018776]
	Learning Rate: 0.00187759
	LOSS [training: 0.2934049410282982 | validation: 0.26647736172206254]
	TIME [epoch: 10.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3414632840640926		[learning rate: 0.0018718]
	Learning Rate: 0.00187183
	LOSS [training: 0.3414632840640926 | validation: 0.43152262803755465]
	TIME [epoch: 10.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3577345451941851		[learning rate: 0.0018661]
	Learning Rate: 0.00186609
	LOSS [training: 0.3577345451941851 | validation: 0.332342723081823]
	TIME [epoch: 10.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3306807706020466		[learning rate: 0.0018604]
	Learning Rate: 0.00186037
	LOSS [training: 0.3306807706020466 | validation: 0.3174597176799772]
	TIME [epoch: 10.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35677326636206164		[learning rate: 0.0018547]
	Learning Rate: 0.00185467
	LOSS [training: 0.35677326636206164 | validation: 0.32771738041623477]
	TIME [epoch: 10.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4905298996414372		[learning rate: 0.001849]
	Learning Rate: 0.00184898
	LOSS [training: 0.4905298996414372 | validation: 0.6217159232756783]
	TIME [epoch: 10.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45858686221838996		[learning rate: 0.0018433]
	Learning Rate: 0.00184332
	LOSS [training: 0.45858686221838996 | validation: 0.28738177774247625]
	TIME [epoch: 10.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2944091187133211		[learning rate: 0.0018377]
	Learning Rate: 0.00183767
	LOSS [training: 0.2944091187133211 | validation: 0.3633283443394153]
	TIME [epoch: 10.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35654134329312404		[learning rate: 0.001832]
	Learning Rate: 0.00183203
	LOSS [training: 0.35654134329312404 | validation: 0.3620831522028962]
	TIME [epoch: 10.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3426070353495705		[learning rate: 0.0018264]
	Learning Rate: 0.00182642
	LOSS [training: 0.3426070353495705 | validation: 0.4389446977282641]
	TIME [epoch: 10.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3351155656397286		[learning rate: 0.0018208]
	Learning Rate: 0.00182082
	LOSS [training: 0.3351155656397286 | validation: 0.30160778575579283]
	TIME [epoch: 10.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3247153059835945		[learning rate: 0.0018152]
	Learning Rate: 0.00181524
	LOSS [training: 0.3247153059835945 | validation: 0.331793785140482]
	TIME [epoch: 10.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36865550335610575		[learning rate: 0.0018097]
	Learning Rate: 0.00180967
	LOSS [training: 0.36865550335610575 | validation: 0.4559350442479426]
	TIME [epoch: 10.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3986808885104604		[learning rate: 0.0018041]
	Learning Rate: 0.00180412
	LOSS [training: 0.3986808885104604 | validation: 0.35956233179288233]
	TIME [epoch: 10.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38792983590107677		[learning rate: 0.0017986]
	Learning Rate: 0.00179859
	LOSS [training: 0.38792983590107677 | validation: 0.39186742117319096]
	TIME [epoch: 10.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35221499507440185		[learning rate: 0.0017931]
	Learning Rate: 0.00179308
	LOSS [training: 0.35221499507440185 | validation: 0.45822713470733273]
	TIME [epoch: 10.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694829831390894		[learning rate: 0.0017876]
	Learning Rate: 0.00178758
	LOSS [training: 0.694829831390894 | validation: 0.39963189618245337]
	TIME [epoch: 10.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3617609519688228		[learning rate: 0.0017821]
	Learning Rate: 0.00178211
	LOSS [training: 0.3617609519688228 | validation: 0.38180945342729117]
	TIME [epoch: 10.3 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35834773262264835		[learning rate: 0.0017766]
	Learning Rate: 0.00177664
	LOSS [training: 0.35834773262264835 | validation: 0.48729876656523136]
	TIME [epoch: 10.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4283186374297383		[learning rate: 0.0017712]
	Learning Rate: 0.0017712
	LOSS [training: 0.4283186374297383 | validation: 0.3417866296744772]
	TIME [epoch: 10.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3836118322396025		[learning rate: 0.0017658]
	Learning Rate: 0.00176577
	LOSS [training: 0.3836118322396025 | validation: 0.4444761982256493]
	TIME [epoch: 10.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.350381068719771		[learning rate: 0.0017604]
	Learning Rate: 0.00176035
	LOSS [training: 0.350381068719771 | validation: 0.40399429169609574]
	TIME [epoch: 10.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3934482339964783		[learning rate: 0.001755]
	Learning Rate: 0.00175496
	LOSS [training: 0.3934482339964783 | validation: 0.3450587692716022]
	TIME [epoch: 10.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5100065761304048		[learning rate: 0.0017496]
	Learning Rate: 0.00174958
	LOSS [training: 0.5100065761304048 | validation: 0.48066454943546133]
	TIME [epoch: 10.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5316168375712309		[learning rate: 0.0017442]
	Learning Rate: 0.00174421
	LOSS [training: 0.5316168375712309 | validation: 0.5123061140700635]
	TIME [epoch: 10.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39620442291934027		[learning rate: 0.0017389]
	Learning Rate: 0.00173887
	LOSS [training: 0.39620442291934027 | validation: 0.37088874043013376]
	TIME [epoch: 10.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38336961693352833		[learning rate: 0.0017335]
	Learning Rate: 0.00173354
	LOSS [training: 0.38336961693352833 | validation: 0.32235424844230826]
	TIME [epoch: 10.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30011688998693437		[learning rate: 0.0017282]
	Learning Rate: 0.00172822
	LOSS [training: 0.30011688998693437 | validation: 0.3947511477006059]
	TIME [epoch: 10.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3516134124594125		[learning rate: 0.0017229]
	Learning Rate: 0.00172293
	LOSS [training: 0.3516134124594125 | validation: 0.398424773417065]
	TIME [epoch: 10.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3092129568653748		[learning rate: 0.0017176]
	Learning Rate: 0.00171764
	LOSS [training: 0.3092129568653748 | validation: 0.27470624156515167]
	TIME [epoch: 10.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2963063198007294		[learning rate: 0.0017124]
	Learning Rate: 0.00171238
	LOSS [training: 0.2963063198007294 | validation: 0.4380770774697404]
	TIME [epoch: 10.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37998925608581746		[learning rate: 0.0017071]
	Learning Rate: 0.00170713
	LOSS [training: 0.37998925608581746 | validation: 0.28070674711757393]
	TIME [epoch: 10.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3845234526098145		[learning rate: 0.0017019]
	Learning Rate: 0.0017019
	LOSS [training: 0.3845234526098145 | validation: 0.44203224633153754]
	TIME [epoch: 10.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3566920219508103		[learning rate: 0.0016967]
	Learning Rate: 0.00169668
	LOSS [training: 0.3566920219508103 | validation: 0.34813478360312905]
	TIME [epoch: 10.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3187179698785691		[learning rate: 0.0016915]
	Learning Rate: 0.00169148
	LOSS [training: 0.3187179698785691 | validation: 0.3053056103309909]
	TIME [epoch: 10.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2787572457753581		[learning rate: 0.0016863]
	Learning Rate: 0.00168629
	LOSS [training: 0.2787572457753581 | validation: 0.2864155537656889]
	TIME [epoch: 10.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2963678932017686		[learning rate: 0.0016811]
	Learning Rate: 0.00168113
	LOSS [training: 0.2963678932017686 | validation: 0.371980085165019]
	TIME [epoch: 10.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3173609002382583		[learning rate: 0.001676]
	Learning Rate: 0.00167597
	LOSS [training: 0.3173609002382583 | validation: 0.37624983785222965]
	TIME [epoch: 10.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3309860560099165		[learning rate: 0.0016708]
	Learning Rate: 0.00167083
	LOSS [training: 0.3309860560099165 | validation: 0.30998071206559447]
	TIME [epoch: 10.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28941788650236083		[learning rate: 0.0016657]
	Learning Rate: 0.00166571
	LOSS [training: 0.28941788650236083 | validation: 0.31131819835089]
	TIME [epoch: 10.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3032204938689892		[learning rate: 0.0016606]
	Learning Rate: 0.00166061
	LOSS [training: 0.3032204938689892 | validation: 0.28576201124233735]
	TIME [epoch: 10.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2567729089438309		[learning rate: 0.0016555]
	Learning Rate: 0.00165552
	LOSS [training: 0.2567729089438309 | validation: 0.3178079847800199]
	TIME [epoch: 10.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31227076491959		[learning rate: 0.0016504]
	Learning Rate: 0.00165044
	LOSS [training: 0.31227076491959 | validation: 0.34740885144989536]
	TIME [epoch: 10.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26457654516994245		[learning rate: 0.0016454]
	Learning Rate: 0.00164538
	LOSS [training: 0.26457654516994245 | validation: 0.2771052418154488]
	TIME [epoch: 10.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31660582995991476		[learning rate: 0.0016403]
	Learning Rate: 0.00164034
	LOSS [training: 0.31660582995991476 | validation: 0.35847782052985694]
	TIME [epoch: 10.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3287402380535284		[learning rate: 0.0016353]
	Learning Rate: 0.00163531
	LOSS [training: 0.3287402380535284 | validation: 0.282305592272457]
	TIME [epoch: 10.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2862232028825385		[learning rate: 0.0016303]
	Learning Rate: 0.0016303
	LOSS [training: 0.2862232028825385 | validation: 0.3261946148161743]
	TIME [epoch: 10.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4066895577736628		[learning rate: 0.0016253]
	Learning Rate: 0.0016253
	LOSS [training: 0.4066895577736628 | validation: 0.30856750954800427]
	TIME [epoch: 10.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36010593464673313		[learning rate: 0.0016203]
	Learning Rate: 0.00162032
	LOSS [training: 0.36010593464673313 | validation: 0.33493782437274655]
	TIME [epoch: 10.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4458817881654781		[learning rate: 0.0016154]
	Learning Rate: 0.00161535
	LOSS [training: 0.4458817881654781 | validation: 0.681030193924762]
	TIME [epoch: 10.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5621758504928869		[learning rate: 0.0016104]
	Learning Rate: 0.0016104
	LOSS [training: 0.5621758504928869 | validation: 0.41498153064135684]
	TIME [epoch: 10.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40005383195098787		[learning rate: 0.0016055]
	Learning Rate: 0.00160546
	LOSS [training: 0.40005383195098787 | validation: 0.2762883156215799]
	TIME [epoch: 10.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3613869506752748		[learning rate: 0.0016005]
	Learning Rate: 0.00160054
	LOSS [training: 0.3613869506752748 | validation: 0.39618887869431335]
	TIME [epoch: 10.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36938129596238223		[learning rate: 0.0015956]
	Learning Rate: 0.00159563
	LOSS [training: 0.36938129596238223 | validation: 0.3836926469141674]
	TIME [epoch: 10.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27691810365302694		[learning rate: 0.0015907]
	Learning Rate: 0.00159074
	LOSS [training: 0.27691810365302694 | validation: 0.32948772403658433]
	TIME [epoch: 10.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3153282489190377		[learning rate: 0.0015859]
	Learning Rate: 0.00158587
	LOSS [training: 0.3153282489190377 | validation: 0.30990683830312493]
	TIME [epoch: 10.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3450503729884378		[learning rate: 0.001581]
	Learning Rate: 0.00158101
	LOSS [training: 0.3450503729884378 | validation: 0.4264273891270445]
	TIME [epoch: 10.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35435256575999874		[learning rate: 0.0015762]
	Learning Rate: 0.00157616
	LOSS [training: 0.35435256575999874 | validation: 0.44678713939191383]
	TIME [epoch: 10.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4683414342340658		[learning rate: 0.0015713]
	Learning Rate: 0.00157133
	LOSS [training: 0.4683414342340658 | validation: 0.5127465342179106]
	TIME [epoch: 10.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43223970403955203		[learning rate: 0.0015665]
	Learning Rate: 0.00156651
	LOSS [training: 0.43223970403955203 | validation: 0.351845688552521]
	TIME [epoch: 10.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2870336606831362		[learning rate: 0.0015617]
	Learning Rate: 0.00156171
	LOSS [training: 0.2870336606831362 | validation: 0.31715827106993355]
	TIME [epoch: 10.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37983186188681806		[learning rate: 0.0015569]
	Learning Rate: 0.00155692
	LOSS [training: 0.37983186188681806 | validation: 0.4191130275816032]
	TIME [epoch: 10.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3890587542287053		[learning rate: 0.0015521]
	Learning Rate: 0.00155215
	LOSS [training: 0.3890587542287053 | validation: 0.5746853343921626]
	TIME [epoch: 10.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3691620811681342		[learning rate: 0.0015474]
	Learning Rate: 0.00154739
	LOSS [training: 0.3691620811681342 | validation: 0.29674020104098725]
	TIME [epoch: 10.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33451304750760086		[learning rate: 0.0015426]
	Learning Rate: 0.00154265
	LOSS [training: 0.33451304750760086 | validation: 0.3381077664175108]
	TIME [epoch: 10.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33628337968733496		[learning rate: 0.0015379]
	Learning Rate: 0.00153792
	LOSS [training: 0.33628337968733496 | validation: 0.26679207808998373]
	TIME [epoch: 10.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22388677717571387		[learning rate: 0.0015332]
	Learning Rate: 0.0015332
	LOSS [training: 0.22388677717571387 | validation: 0.24135052002285345]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1111.pth
	Model improved!!!
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28721729143711067		[learning rate: 0.0015285]
	Learning Rate: 0.0015285
	LOSS [training: 0.28721729143711067 | validation: 0.28709888824471586]
	TIME [epoch: 10.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24194188151691684		[learning rate: 0.0015238]
	Learning Rate: 0.00152382
	LOSS [training: 0.24194188151691684 | validation: 0.31610411818654605]
	TIME [epoch: 10.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2571347082611828		[learning rate: 0.0015191]
	Learning Rate: 0.00151915
	LOSS [training: 0.2571347082611828 | validation: 0.2705105518165081]
	TIME [epoch: 10.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24045676945038724		[learning rate: 0.0015145]
	Learning Rate: 0.00151449
	LOSS [training: 0.24045676945038724 | validation: 0.3148825917615475]
	TIME [epoch: 10.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31760465465377374		[learning rate: 0.0015098]
	Learning Rate: 0.00150985
	LOSS [training: 0.31760465465377374 | validation: 0.286703903466118]
	TIME [epoch: 10.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2630625141995574		[learning rate: 0.0015052]
	Learning Rate: 0.00150522
	LOSS [training: 0.2630625141995574 | validation: 0.2692085313170774]
	TIME [epoch: 10.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25960813402252675		[learning rate: 0.0015006]
	Learning Rate: 0.00150061
	LOSS [training: 0.25960813402252675 | validation: 0.3945849669727632]
	TIME [epoch: 10.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35615969057307034		[learning rate: 0.001496]
	Learning Rate: 0.00149601
	LOSS [training: 0.35615969057307034 | validation: 0.4324053554369431]
	TIME [epoch: 10.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3734140513386009		[learning rate: 0.0014914]
	Learning Rate: 0.00149142
	LOSS [training: 0.3734140513386009 | validation: 0.27700388327445935]
	TIME [epoch: 10.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3365091470321864		[learning rate: 0.0014868]
	Learning Rate: 0.00148685
	LOSS [training: 0.3365091470321864 | validation: 0.39775625432773837]
	TIME [epoch: 10.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3622605214290304		[learning rate: 0.0014823]
	Learning Rate: 0.00148229
	LOSS [training: 0.3622605214290304 | validation: 0.2957112211373224]
	TIME [epoch: 10.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.296479093417057		[learning rate: 0.0014777]
	Learning Rate: 0.00147775
	LOSS [training: 0.296479093417057 | validation: 0.2443259753298283]
	TIME [epoch: 10.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27189224075162777		[learning rate: 0.0014732]
	Learning Rate: 0.00147322
	LOSS [training: 0.27189224075162777 | validation: 0.2788229431701393]
	TIME [epoch: 10.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29129053526272297		[learning rate: 0.0014687]
	Learning Rate: 0.0014687
	LOSS [training: 0.29129053526272297 | validation: 0.2796575298511698]
	TIME [epoch: 10.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2969154124107356		[learning rate: 0.0014642]
	Learning Rate: 0.0014642
	LOSS [training: 0.2969154124107356 | validation: 0.4654742401609518]
	TIME [epoch: 10.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39563835595983826		[learning rate: 0.0014597]
	Learning Rate: 0.00145971
	LOSS [training: 0.39563835595983826 | validation: 0.30517567236060644]
	TIME [epoch: 10.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36803817573215253		[learning rate: 0.0014552]
	Learning Rate: 0.00145524
	LOSS [training: 0.36803817573215253 | validation: 0.3409161949585597]
	TIME [epoch: 10.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45562033664210144		[learning rate: 0.0014508]
	Learning Rate: 0.00145077
	LOSS [training: 0.45562033664210144 | validation: 0.3540922605050281]
	TIME [epoch: 10.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38847924956278473		[learning rate: 0.0014463]
	Learning Rate: 0.00144633
	LOSS [training: 0.38847924956278473 | validation: 0.26698335364625575]
	TIME [epoch: 10.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27378204591998834		[learning rate: 0.0014419]
	Learning Rate: 0.00144189
	LOSS [training: 0.27378204591998834 | validation: 0.25806696018828346]
	TIME [epoch: 10.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32656923190161535		[learning rate: 0.0014375]
	Learning Rate: 0.00143747
	LOSS [training: 0.32656923190161535 | validation: 0.2371616673551459]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1132.pth
	Model improved!!!
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.257432833190034		[learning rate: 0.0014331]
	Learning Rate: 0.00143307
	LOSS [training: 0.257432833190034 | validation: 0.28328191168992417]
	TIME [epoch: 10.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28097393180283736		[learning rate: 0.0014287]
	Learning Rate: 0.00142867
	LOSS [training: 0.28097393180283736 | validation: 0.22222811284279165]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1134.pth
	Model improved!!!
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2912224484883302		[learning rate: 0.0014243]
	Learning Rate: 0.0014243
	LOSS [training: 0.2912224484883302 | validation: 0.3235469995316941]
	TIME [epoch: 10.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3878397878768537		[learning rate: 0.0014199]
	Learning Rate: 0.00141993
	LOSS [training: 0.3878397878768537 | validation: 0.3254906625100093]
	TIME [epoch: 10.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2393236356149102		[learning rate: 0.0014156]
	Learning Rate: 0.00141558
	LOSS [training: 0.2393236356149102 | validation: 0.35664711405447974]
	TIME [epoch: 10.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32427926118369477		[learning rate: 0.0014112]
	Learning Rate: 0.00141124
	LOSS [training: 0.32427926118369477 | validation: 0.2686144478276175]
	TIME [epoch: 10.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23408190889469985		[learning rate: 0.0014069]
	Learning Rate: 0.00140691
	LOSS [training: 0.23408190889469985 | validation: 0.2380878427220513]
	TIME [epoch: 10.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22865355577908666		[learning rate: 0.0014026]
	Learning Rate: 0.0014026
	LOSS [training: 0.22865355577908666 | validation: 0.2455732772345062]
	TIME [epoch: 10.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26244289135060356		[learning rate: 0.0013983]
	Learning Rate: 0.0013983
	LOSS [training: 0.26244289135060356 | validation: 0.3217317049528331]
	TIME [epoch: 10.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24878037302620193		[learning rate: 0.001394]
	Learning Rate: 0.00139401
	LOSS [training: 0.24878037302620193 | validation: 0.4257565003750347]
	TIME [epoch: 10.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2950075874357764		[learning rate: 0.0013897]
	Learning Rate: 0.00138974
	LOSS [training: 0.2950075874357764 | validation: 0.25242083115114994]
	TIME [epoch: 10.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23308763170273505		[learning rate: 0.0013855]
	Learning Rate: 0.00138548
	LOSS [training: 0.23308763170273505 | validation: 0.261086597342472]
	TIME [epoch: 10.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2588835410938997		[learning rate: 0.0013812]
	Learning Rate: 0.00138123
	LOSS [training: 0.2588835410938997 | validation: 0.2575494796465086]
	TIME [epoch: 10.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20449937758618764		[learning rate: 0.001377]
	Learning Rate: 0.001377
	LOSS [training: 0.20449937758618764 | validation: 0.44420428477244]
	TIME [epoch: 10.3 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3678604111567574		[learning rate: 0.0013728]
	Learning Rate: 0.00137278
	LOSS [training: 0.3678604111567574 | validation: 0.3689172113007684]
	TIME [epoch: 10.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2697127358803008		[learning rate: 0.0013686]
	Learning Rate: 0.00136857
	LOSS [training: 0.2697127358803008 | validation: 0.35385258627218563]
	TIME [epoch: 10.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3252250260518613		[learning rate: 0.0013644]
	Learning Rate: 0.00136437
	LOSS [training: 0.3252250260518613 | validation: 0.4441169508559176]
	TIME [epoch: 10.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39633509761431684		[learning rate: 0.0013602]
	Learning Rate: 0.00136019
	LOSS [training: 0.39633509761431684 | validation: 0.25176606854893424]
	TIME [epoch: 10.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25030144248269226		[learning rate: 0.001356]
	Learning Rate: 0.00135602
	LOSS [training: 0.25030144248269226 | validation: 0.24280076513756085]
	TIME [epoch: 10.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25100940336319505		[learning rate: 0.0013519]
	Learning Rate: 0.00135187
	LOSS [training: 0.25100940336319505 | validation: 0.3056237557044234]
	TIME [epoch: 10.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33355192319269167		[learning rate: 0.0013477]
	Learning Rate: 0.00134772
	LOSS [training: 0.33355192319269167 | validation: 0.2508600183012157]
	TIME [epoch: 10.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25581441194996557		[learning rate: 0.0013436]
	Learning Rate: 0.00134359
	LOSS [training: 0.25581441194996557 | validation: 0.23171159705290975]
	TIME [epoch: 10.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23459752852438717		[learning rate: 0.0013395]
	Learning Rate: 0.00133947
	LOSS [training: 0.23459752852438717 | validation: 0.2327614404672064]
	TIME [epoch: 10.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19034308120649618		[learning rate: 0.0013354]
	Learning Rate: 0.00133536
	LOSS [training: 0.19034308120649618 | validation: 0.22047231728295685]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1156.pth
	Model improved!!!
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20592925270518903		[learning rate: 0.0013313]
	Learning Rate: 0.00133127
	LOSS [training: 0.20592925270518903 | validation: 0.23327572462294494]
	TIME [epoch: 10.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2924661739087354		[learning rate: 0.0013272]
	Learning Rate: 0.00132719
	LOSS [training: 0.2924661739087354 | validation: 0.4153263840323084]
	TIME [epoch: 10.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3078386969245148		[learning rate: 0.0013231]
	Learning Rate: 0.00132312
	LOSS [training: 0.3078386969245148 | validation: 0.2132947941684398]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1159.pth
	Model improved!!!
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.185336701028546		[learning rate: 0.0013191]
	Learning Rate: 0.00131907
	LOSS [training: 0.185336701028546 | validation: 0.29866983313227535]
	TIME [epoch: 10.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26630904989714554		[learning rate: 0.001315]
	Learning Rate: 0.00131502
	LOSS [training: 0.26630904989714554 | validation: 0.30216635904221784]
	TIME [epoch: 10.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24009840612322134		[learning rate: 0.001311]
	Learning Rate: 0.00131099
	LOSS [training: 0.24009840612322134 | validation: 0.25925769375916075]
	TIME [epoch: 10.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20956402941007618		[learning rate: 0.001307]
	Learning Rate: 0.00130697
	LOSS [training: 0.20956402941007618 | validation: 0.21372485295749222]
	TIME [epoch: 10.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1978005225559306		[learning rate: 0.001303]
	Learning Rate: 0.00130297
	LOSS [training: 0.1978005225559306 | validation: 0.3753530809191507]
	TIME [epoch: 10.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33417466441199145		[learning rate: 0.001299]
	Learning Rate: 0.00129897
	LOSS [training: 0.33417466441199145 | validation: 0.29066783444639405]
	TIME [epoch: 10.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3052584246603985		[learning rate: 0.001295]
	Learning Rate: 0.00129499
	LOSS [training: 0.3052584246603985 | validation: 0.17463279384088728]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1166.pth
	Model improved!!!
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24578668316574087		[learning rate: 0.001291]
	Learning Rate: 0.00129102
	LOSS [training: 0.24578668316574087 | validation: 0.23921521188704553]
	TIME [epoch: 10.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21672724670393678		[learning rate: 0.0012871]
	Learning Rate: 0.00128706
	LOSS [training: 0.21672724670393678 | validation: 0.2715175120247675]
	TIME [epoch: 10.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23782679666127327		[learning rate: 0.0012831]
	Learning Rate: 0.00128312
	LOSS [training: 0.23782679666127327 | validation: 0.27201322220804086]
	TIME [epoch: 10.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2498527570118994		[learning rate: 0.0012792]
	Learning Rate: 0.00127918
	LOSS [training: 0.2498527570118994 | validation: 0.3151472164294992]
	TIME [epoch: 10.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2613931174844163		[learning rate: 0.0012753]
	Learning Rate: 0.00127526
	LOSS [training: 0.2613931174844163 | validation: 0.21932992983879715]
	TIME [epoch: 10.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19023168717392674		[learning rate: 0.0012714]
	Learning Rate: 0.00127135
	LOSS [training: 0.19023168717392674 | validation: 0.20394962400803368]
	TIME [epoch: 10.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23219576035186948		[learning rate: 0.0012675]
	Learning Rate: 0.00126746
	LOSS [training: 0.23219576035186948 | validation: 0.31476745861033467]
	TIME [epoch: 10.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27036797826588377		[learning rate: 0.0012636]
	Learning Rate: 0.00126357
	LOSS [training: 0.27036797826588377 | validation: 0.2073942252743032]
	TIME [epoch: 10.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19867411896511006		[learning rate: 0.0012597]
	Learning Rate: 0.0012597
	LOSS [training: 0.19867411896511006 | validation: 0.22749572707356858]
	TIME [epoch: 10.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2796312515030997		[learning rate: 0.0012558]
	Learning Rate: 0.00125584
	LOSS [training: 0.2796312515030997 | validation: 0.4003078039960299]
	TIME [epoch: 10.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2737434706629493		[learning rate: 0.001252]
	Learning Rate: 0.00125199
	LOSS [training: 0.2737434706629493 | validation: 0.23130663047628472]
	TIME [epoch: 10.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23731346874935685		[learning rate: 0.0012481]
	Learning Rate: 0.00124815
	LOSS [training: 0.23731346874935685 | validation: 0.2548992669947211]
	TIME [epoch: 10.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21991591697658358		[learning rate: 0.0012443]
	Learning Rate: 0.00124432
	LOSS [training: 0.21991591697658358 | validation: 0.23375688008820253]
	TIME [epoch: 10.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2413979477644844		[learning rate: 0.0012405]
	Learning Rate: 0.00124051
	LOSS [training: 0.2413979477644844 | validation: 0.3088322876966892]
	TIME [epoch: 10.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23524791265452194		[learning rate: 0.0012367]
	Learning Rate: 0.00123671
	LOSS [training: 0.23524791265452194 | validation: 0.24557342508135335]
	TIME [epoch: 10.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20894025253744059		[learning rate: 0.0012329]
	Learning Rate: 0.00123292
	LOSS [training: 0.20894025253744059 | validation: 0.19830910408348204]
	TIME [epoch: 10.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1565072159182507		[learning rate: 0.0012291]
	Learning Rate: 0.00122914
	LOSS [training: 0.1565072159182507 | validation: 0.2452953377644404]
	TIME [epoch: 10.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2482055419955847		[learning rate: 0.0012254]
	Learning Rate: 0.00122537
	LOSS [training: 0.2482055419955847 | validation: 0.2010085085325683]
	TIME [epoch: 10.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2769013896208796		[learning rate: 0.0012216]
	Learning Rate: 0.00122161
	LOSS [training: 0.2769013896208796 | validation: 0.3376497099133388]
	TIME [epoch: 10.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24353539990961762		[learning rate: 0.0012179]
	Learning Rate: 0.00121787
	LOSS [training: 0.24353539990961762 | validation: 0.19450096512247952]
	TIME [epoch: 10.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19472476952946657		[learning rate: 0.0012141]
	Learning Rate: 0.00121413
	LOSS [training: 0.19472476952946657 | validation: 0.20759431506128473]
	TIME [epoch: 10.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20791775584181468		[learning rate: 0.0012104]
	Learning Rate: 0.00121041
	LOSS [training: 0.20791775584181468 | validation: 0.22762587172964815]
	TIME [epoch: 10.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22623241784870318		[learning rate: 0.0012067]
	Learning Rate: 0.0012067
	LOSS [training: 0.22623241784870318 | validation: 0.5086027219999136]
	TIME [epoch: 10.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.448698531408545		[learning rate: 0.001203]
	Learning Rate: 0.001203
	LOSS [training: 0.448698531408545 | validation: 0.29831493557077876]
	TIME [epoch: 10.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2641908164147118		[learning rate: 0.0011993]
	Learning Rate: 0.00119932
	LOSS [training: 0.2641908164147118 | validation: 0.2431593908353895]
	TIME [epoch: 10.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2326368539608774		[learning rate: 0.0011956]
	Learning Rate: 0.00119564
	LOSS [training: 0.2326368539608774 | validation: 0.33368936889613127]
	TIME [epoch: 10.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27927325980392903		[learning rate: 0.001192]
	Learning Rate: 0.00119197
	LOSS [training: 0.27927325980392903 | validation: 0.2860432388374057]
	TIME [epoch: 10.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30937009015299244		[learning rate: 0.0011883]
	Learning Rate: 0.00118832
	LOSS [training: 0.30937009015299244 | validation: 0.24692264409238107]
	TIME [epoch: 10.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19661753330542653		[learning rate: 0.0011847]
	Learning Rate: 0.00118468
	LOSS [training: 0.19661753330542653 | validation: 0.29088886052385277]
	TIME [epoch: 10.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26077512464698893		[learning rate: 0.001181]
	Learning Rate: 0.00118105
	LOSS [training: 0.26077512464698893 | validation: 0.3176305758340068]
	TIME [epoch: 10.3 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25717715169833544		[learning rate: 0.0011774]
	Learning Rate: 0.00117743
	LOSS [training: 0.25717715169833544 | validation: 0.22164328262639593]
	TIME [epoch: 10.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19463475075603903		[learning rate: 0.0011738]
	Learning Rate: 0.00117382
	LOSS [training: 0.19463475075603903 | validation: 0.2987555699435322]
	TIME [epoch: 10.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1975854245561774		[learning rate: 0.0011702]
	Learning Rate: 0.00117022
	LOSS [training: 0.1975854245561774 | validation: 0.2108015265445904]
	TIME [epoch: 10.3 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17928423465435425		[learning rate: 0.0011666]
	Learning Rate: 0.00116663
	LOSS [training: 0.17928423465435425 | validation: 0.22925467448957468]
	TIME [epoch: 10.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2420075015336319		[learning rate: 0.0011631]
	Learning Rate: 0.00116305
	LOSS [training: 0.2420075015336319 | validation: 0.27111807443463704]
	TIME [epoch: 10.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2185783191050379		[learning rate: 0.0011595]
	Learning Rate: 0.00115949
	LOSS [training: 0.2185783191050379 | validation: 0.3095551541462105]
	TIME [epoch: 10.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29389794275737413		[learning rate: 0.0011559]
	Learning Rate: 0.00115593
	LOSS [training: 0.29389794275737413 | validation: 0.30216555462228356]
	TIME [epoch: 10.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23325608867269843		[learning rate: 0.0011524]
	Learning Rate: 0.00115239
	LOSS [training: 0.23325608867269843 | validation: 0.18661441624819425]
	TIME [epoch: 10.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23379190819791792		[learning rate: 0.0011489]
	Learning Rate: 0.00114886
	LOSS [training: 0.23379190819791792 | validation: 0.3137668263394668]
	TIME [epoch: 10.3 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2827002944115617		[learning rate: 0.0011453]
	Learning Rate: 0.00114534
	LOSS [training: 0.2827002944115617 | validation: 0.200343737657374]
	TIME [epoch: 10.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2709220703330785		[learning rate: 0.0011418]
	Learning Rate: 0.00114183
	LOSS [training: 0.2709220703330785 | validation: 0.30045070469455687]
	TIME [epoch: 10.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42066034688554443		[learning rate: 0.0011383]
	Learning Rate: 0.00113833
	LOSS [training: 0.42066034688554443 | validation: 0.3258570369164705]
	TIME [epoch: 10.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46695006024603425		[learning rate: 0.0011348]
	Learning Rate: 0.00113484
	LOSS [training: 0.46695006024603425 | validation: 0.333975377212428]
	TIME [epoch: 10.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.317355055329712		[learning rate: 0.0011314]
	Learning Rate: 0.00113136
	LOSS [training: 0.317355055329712 | validation: 0.2764819001247554]
	TIME [epoch: 10.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24914769292345404		[learning rate: 0.0011279]
	Learning Rate: 0.00112789
	LOSS [training: 0.24914769292345404 | validation: 0.21976688776512374]
	TIME [epoch: 10.3 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20438660633489839		[learning rate: 0.0011244]
	Learning Rate: 0.00112443
	LOSS [training: 0.20438660633489839 | validation: 0.19513861828643758]
	TIME [epoch: 10.3 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1684822855124825		[learning rate: 0.001121]
	Learning Rate: 0.00112099
	LOSS [training: 0.1684822855124825 | validation: 0.2035168460259223]
	TIME [epoch: 10.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20815380801108238		[learning rate: 0.0011175]
	Learning Rate: 0.00111755
	LOSS [training: 0.20815380801108238 | validation: 0.2538315602798432]
	TIME [epoch: 10.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32698669577757283		[learning rate: 0.0011141]
	Learning Rate: 0.00111412
	LOSS [training: 0.32698669577757283 | validation: 0.2762613097270352]
	TIME [epoch: 10.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26778900417450613		[learning rate: 0.0011107]
	Learning Rate: 0.00111071
	LOSS [training: 0.26778900417450613 | validation: 0.22649309996902994]
	TIME [epoch: 10.3 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23802219029518054		[learning rate: 0.0011073]
	Learning Rate: 0.0011073
	LOSS [training: 0.23802219029518054 | validation: 0.22662201471199203]
	TIME [epoch: 10.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19495195121751724		[learning rate: 0.0011039]
	Learning Rate: 0.00110391
	LOSS [training: 0.19495195121751724 | validation: 0.2102560783245739]
	TIME [epoch: 10.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29982818212031875		[learning rate: 0.0011005]
	Learning Rate: 0.00110053
	LOSS [training: 0.29982818212031875 | validation: 0.2668774797189483]
	TIME [epoch: 10.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22608742092253914		[learning rate: 0.0010972]
	Learning Rate: 0.00109715
	LOSS [training: 0.22608742092253914 | validation: 0.1988487924277275]
	TIME [epoch: 10.3 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1908125933215324		[learning rate: 0.0010938]
	Learning Rate: 0.00109379
	LOSS [training: 0.1908125933215324 | validation: 0.2647717304223038]
	TIME [epoch: 10.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18068879675196833		[learning rate: 0.0010904]
	Learning Rate: 0.00109044
	LOSS [training: 0.18068879675196833 | validation: 0.17350710774025255]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1222.pth
	Model improved!!!
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17503718990632064		[learning rate: 0.0010871]
	Learning Rate: 0.00108709
	LOSS [training: 0.17503718990632064 | validation: 0.2372541899088948]
	TIME [epoch: 10.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24413185080576377		[learning rate: 0.0010838]
	Learning Rate: 0.00108376
	LOSS [training: 0.24413185080576377 | validation: 0.21835745182734548]
	TIME [epoch: 10.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20517673977597556		[learning rate: 0.0010804]
	Learning Rate: 0.00108044
	LOSS [training: 0.20517673977597556 | validation: 0.21319442997378915]
	TIME [epoch: 10.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2570974429547327		[learning rate: 0.0010771]
	Learning Rate: 0.00107713
	LOSS [training: 0.2570974429547327 | validation: 0.23922191906434165]
	TIME [epoch: 10.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3053761647542905		[learning rate: 0.0010738]
	Learning Rate: 0.00107382
	LOSS [training: 0.3053761647542905 | validation: 0.25149499381626655]
	TIME [epoch: 10.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2500372611989379		[learning rate: 0.0010705]
	Learning Rate: 0.00107053
	LOSS [training: 0.2500372611989379 | validation: 0.2417193139313576]
	TIME [epoch: 10.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2234474193536319		[learning rate: 0.0010673]
	Learning Rate: 0.00106725
	LOSS [training: 0.2234474193536319 | validation: 0.20346223257905352]
	TIME [epoch: 10.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21583251882713217		[learning rate: 0.001064]
	Learning Rate: 0.00106398
	LOSS [training: 0.21583251882713217 | validation: 0.24712381698125782]
	TIME [epoch: 10.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2487403706139463		[learning rate: 0.0010607]
	Learning Rate: 0.00106072
	LOSS [training: 0.2487403706139463 | validation: 0.25396766948229293]
	TIME [epoch: 10.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21095891046568002		[learning rate: 0.0010575]
	Learning Rate: 0.00105747
	LOSS [training: 0.21095891046568002 | validation: 0.20115078164236042]
	TIME [epoch: 10.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20805040273186476		[learning rate: 0.0010542]
	Learning Rate: 0.00105422
	LOSS [training: 0.20805040273186476 | validation: 0.22793105676876857]
	TIME [epoch: 10.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19966364623004404		[learning rate: 0.001051]
	Learning Rate: 0.00105099
	LOSS [training: 0.19966364623004404 | validation: 0.23743408013776748]
	TIME [epoch: 10.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18768586692292816		[learning rate: 0.0010478]
	Learning Rate: 0.00104777
	LOSS [training: 0.18768586692292816 | validation: 0.26821061155690806]
	TIME [epoch: 10.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22856575673737867		[learning rate: 0.0010446]
	Learning Rate: 0.00104456
	LOSS [training: 0.22856575673737867 | validation: 0.23220647714967535]
	TIME [epoch: 10.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19317674481926697		[learning rate: 0.0010414]
	Learning Rate: 0.00104136
	LOSS [training: 0.19317674481926697 | validation: 0.23850053554134568]
	TIME [epoch: 10.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2504013888613855		[learning rate: 0.0010382]
	Learning Rate: 0.00103817
	LOSS [training: 0.2504013888613855 | validation: 0.3445130765154902]
	TIME [epoch: 10.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3361667248839698		[learning rate: 0.001035]
	Learning Rate: 0.00103498
	LOSS [training: 0.3361667248839698 | validation: 0.325975113595607]
	TIME [epoch: 10.3 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34347306170891667		[learning rate: 0.0010318]
	Learning Rate: 0.00103181
	LOSS [training: 0.34347306170891667 | validation: 0.3398776160185304]
	TIME [epoch: 10.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29647338554220837		[learning rate: 0.0010286]
	Learning Rate: 0.00102865
	LOSS [training: 0.29647338554220837 | validation: 0.3365186215710819]
	TIME [epoch: 10.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2621082859699176		[learning rate: 0.0010255]
	Learning Rate: 0.00102549
	LOSS [training: 0.2621082859699176 | validation: 0.32748319266660075]
	TIME [epoch: 10.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2816585961933996		[learning rate: 0.0010224]
	Learning Rate: 0.00102235
	LOSS [training: 0.2816585961933996 | validation: 0.2713531711259013]
	TIME [epoch: 10.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24979727819253422		[learning rate: 0.0010192]
	Learning Rate: 0.00101922
	LOSS [training: 0.24979727819253422 | validation: 0.22716242448388171]
	TIME [epoch: 10.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23402203682530925		[learning rate: 0.0010161]
	Learning Rate: 0.00101609
	LOSS [training: 0.23402203682530925 | validation: 0.20151905371790776]
	TIME [epoch: 10.3 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21220721721040178		[learning rate: 0.001013]
	Learning Rate: 0.00101298
	LOSS [training: 0.21220721721040178 | validation: 0.2605778652401316]
	TIME [epoch: 10.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2695820457368362		[learning rate: 0.0010099]
	Learning Rate: 0.00100987
	LOSS [training: 0.2695820457368362 | validation: 0.31365695758089623]
	TIME [epoch: 10.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2724807610197057		[learning rate: 0.0010068]
	Learning Rate: 0.00100678
	LOSS [training: 0.2724807610197057 | validation: 0.32341423302952294]
	TIME [epoch: 10.3 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3352938454765993		[learning rate: 0.0010037]
	Learning Rate: 0.00100369
	LOSS [training: 0.3352938454765993 | validation: 0.3335310773427183]
	TIME [epoch: 10.3 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2730691507827586		[learning rate: 0.0010006]
	Learning Rate: 0.00100061
	LOSS [training: 0.2730691507827586 | validation: 0.28617124286925305]
	TIME [epoch: 10.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2228971083977585		[learning rate: 0.00099755]
	Learning Rate: 0.000997547
	LOSS [training: 0.2228971083977585 | validation: 0.19018944125361464]
	TIME [epoch: 10.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20517787851795508		[learning rate: 0.00099449]
	Learning Rate: 0.000994489
	LOSS [training: 0.20517787851795508 | validation: 0.22507500621532595]
	TIME [epoch: 10.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20253782008926438		[learning rate: 0.00099144]
	Learning Rate: 0.00099144
	LOSS [training: 0.20253782008926438 | validation: 0.24442406484048732]
	TIME [epoch: 10.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2302889185490864		[learning rate: 0.0009884]
	Learning Rate: 0.000988401
	LOSS [training: 0.2302889185490864 | validation: 0.2081712346003895]
	TIME [epoch: 10.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1906622573194921		[learning rate: 0.00098537]
	Learning Rate: 0.000985371
	LOSS [training: 0.1906622573194921 | validation: 0.2301193062339042]
	TIME [epoch: 10.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18866976016068376		[learning rate: 0.00098235]
	Learning Rate: 0.000982351
	LOSS [training: 0.18866976016068376 | validation: 0.21802660316446976]
	TIME [epoch: 10.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19985956816037515		[learning rate: 0.00097934]
	Learning Rate: 0.000979339
	LOSS [training: 0.19985956816037515 | validation: 0.17392792310718466]
	TIME [epoch: 10.3 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.240413654345492		[learning rate: 0.00097634]
	Learning Rate: 0.000976337
	LOSS [training: 0.240413654345492 | validation: 0.36911777049382793]
	TIME [epoch: 10.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27232495994418765		[learning rate: 0.00097334]
	Learning Rate: 0.000973345
	LOSS [training: 0.27232495994418765 | validation: 0.23302078775886323]
	TIME [epoch: 10.3 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2188048906376343		[learning rate: 0.00097036]
	Learning Rate: 0.000970361
	LOSS [training: 0.2188048906376343 | validation: 0.1945523831205459]
	TIME [epoch: 10.3 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16178536873291025		[learning rate: 0.00096739]
	Learning Rate: 0.000967386
	LOSS [training: 0.16178536873291025 | validation: 0.21942088145848335]
	TIME [epoch: 10.3 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2134365959013062		[learning rate: 0.00096442]
	Learning Rate: 0.000964421
	LOSS [training: 0.2134365959013062 | validation: 0.26719234948214615]
	TIME [epoch: 10.3 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3280556797193051		[learning rate: 0.00096146]
	Learning Rate: 0.000961464
	LOSS [training: 0.3280556797193051 | validation: 0.3348614770215573]
	TIME [epoch: 10.3 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2259412719296295		[learning rate: 0.00095852]
	Learning Rate: 0.000958517
	LOSS [training: 0.2259412719296295 | validation: 0.2548907874640118]
	TIME [epoch: 10.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2325324722042758		[learning rate: 0.00095558]
	Learning Rate: 0.000955579
	LOSS [training: 0.2325324722042758 | validation: 0.23456998873336424]
	TIME [epoch: 10.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24558445466192674		[learning rate: 0.00095265]
	Learning Rate: 0.00095265
	LOSS [training: 0.24558445466192674 | validation: 0.24412969291702807]
	TIME [epoch: 10.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22084992077501645		[learning rate: 0.00094973]
	Learning Rate: 0.00094973
	LOSS [training: 0.22084992077501645 | validation: 0.2618124661492703]
	TIME [epoch: 10.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2218359876661154		[learning rate: 0.00094682]
	Learning Rate: 0.000946818
	LOSS [training: 0.2218359876661154 | validation: 0.2465989213867683]
	TIME [epoch: 10.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17520767620169495		[learning rate: 0.00094392]
	Learning Rate: 0.000943916
	LOSS [training: 0.17520767620169495 | validation: 0.17048329547657246]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1269.pth
	Model improved!!!
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2392098795280176		[learning rate: 0.00094102]
	Learning Rate: 0.000941023
	LOSS [training: 0.2392098795280176 | validation: 0.21209703141856182]
	TIME [epoch: 10.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15677875456420576		[learning rate: 0.00093814]
	Learning Rate: 0.000938138
	LOSS [training: 0.15677875456420576 | validation: 0.24619113499486922]
	TIME [epoch: 10.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30480180062141754		[learning rate: 0.00093526]
	Learning Rate: 0.000935262
	LOSS [training: 0.30480180062141754 | validation: 0.19463285291042945]
	TIME [epoch: 10.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1959892613151715		[learning rate: 0.0009324]
	Learning Rate: 0.000932395
	LOSS [training: 0.1959892613151715 | validation: 0.2412601714825859]
	TIME [epoch: 10.3 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2027642096581252		[learning rate: 0.00092954]
	Learning Rate: 0.000929537
	LOSS [training: 0.2027642096581252 | validation: 0.2034405969008442]
	TIME [epoch: 10.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19917326058987767		[learning rate: 0.00092669]
	Learning Rate: 0.000926688
	LOSS [training: 0.19917326058987767 | validation: 0.26205636719400266]
	TIME [epoch: 10.3 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26991744202041507		[learning rate: 0.00092385]
	Learning Rate: 0.000923847
	LOSS [training: 0.26991744202041507 | validation: 0.21902433695856402]
	TIME [epoch: 10.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2228489389829303		[learning rate: 0.00092101]
	Learning Rate: 0.000921015
	LOSS [training: 0.2228489389829303 | validation: 0.21596082450617535]
	TIME [epoch: 10.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1789503537106597		[learning rate: 0.00091819]
	Learning Rate: 0.000918192
	LOSS [training: 0.1789503537106597 | validation: 0.2668351594245974]
	TIME [epoch: 10.3 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25260899072273163		[learning rate: 0.00091538]
	Learning Rate: 0.000915377
	LOSS [training: 0.25260899072273163 | validation: 0.26025827073360247]
	TIME [epoch: 10.3 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20779626511187127		[learning rate: 0.00091257]
	Learning Rate: 0.000912571
	LOSS [training: 0.20779626511187127 | validation: 0.1596058369757976]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1280.pth
	Model improved!!!
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15552087599706452		[learning rate: 0.00090977]
	Learning Rate: 0.000909774
	LOSS [training: 0.15552087599706452 | validation: 0.18622362774286969]
	TIME [epoch: 10.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16988532876589651		[learning rate: 0.00090698]
	Learning Rate: 0.000906985
	LOSS [training: 0.16988532876589651 | validation: 0.1881727200713883]
	TIME [epoch: 10.3 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16916334904588007		[learning rate: 0.0009042]
	Learning Rate: 0.000904204
	LOSS [training: 0.16916334904588007 | validation: 0.20032828222159876]
	TIME [epoch: 10.3 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21606180185977597		[learning rate: 0.00090143]
	Learning Rate: 0.000901433
	LOSS [training: 0.21606180185977597 | validation: 0.19205314315861668]
	TIME [epoch: 10.3 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21663289038375927		[learning rate: 0.00089867]
	Learning Rate: 0.000898669
	LOSS [training: 0.21663289038375927 | validation: 0.20577727542224364]
	TIME [epoch: 10.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2061255167453265		[learning rate: 0.00089591]
	Learning Rate: 0.000895915
	LOSS [training: 0.2061255167453265 | validation: 0.25245855670789935]
	TIME [epoch: 10.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19822266937586613		[learning rate: 0.00089317]
	Learning Rate: 0.000893168
	LOSS [training: 0.19822266937586613 | validation: 0.17233314760224913]
	TIME [epoch: 10.3 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23143705020165584		[learning rate: 0.00089043]
	Learning Rate: 0.00089043
	LOSS [training: 0.23143705020165584 | validation: 0.21916748300892624]
	TIME [epoch: 10.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18448664656552774		[learning rate: 0.0008877]
	Learning Rate: 0.000887701
	LOSS [training: 0.18448664656552774 | validation: 0.20687894491176245]
	TIME [epoch: 10.3 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17818066100953853		[learning rate: 0.00088498]
	Learning Rate: 0.00088498
	LOSS [training: 0.17818066100953853 | validation: 0.18011929498686674]
	TIME [epoch: 10.3 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1989473191617212		[learning rate: 0.00088227]
	Learning Rate: 0.000882267
	LOSS [training: 0.1989473191617212 | validation: 0.19133008983408664]
	TIME [epoch: 10.3 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17758789391219315		[learning rate: 0.00087956]
	Learning Rate: 0.000879562
	LOSS [training: 0.17758789391219315 | validation: 0.25772161289841616]
	TIME [epoch: 10.3 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22740136488327875		[learning rate: 0.00087687]
	Learning Rate: 0.000876866
	LOSS [training: 0.22740136488327875 | validation: 0.20660407347988838]
	TIME [epoch: 10.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1975388894823958		[learning rate: 0.00087418]
	Learning Rate: 0.000874178
	LOSS [training: 0.1975388894823958 | validation: 0.23683409367666103]
	TIME [epoch: 10.3 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38224787693977547		[learning rate: 0.0008715]
	Learning Rate: 0.000871498
	LOSS [training: 0.38224787693977547 | validation: 0.28551204302229466]
	TIME [epoch: 10.3 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21559596789725716		[learning rate: 0.00086883]
	Learning Rate: 0.000868827
	LOSS [training: 0.21559596789725716 | validation: 0.19500951838175046]
	TIME [epoch: 10.3 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19240653178417183		[learning rate: 0.00086616]
	Learning Rate: 0.000866164
	LOSS [training: 0.19240653178417183 | validation: 0.32893864879787116]
	TIME [epoch: 10.3 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23618679662258266		[learning rate: 0.00086351]
	Learning Rate: 0.000863509
	LOSS [training: 0.23618679662258266 | validation: 0.24370170098965485]
	TIME [epoch: 10.3 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2422566446338547		[learning rate: 0.00086086]
	Learning Rate: 0.000860861
	LOSS [training: 0.2422566446338547 | validation: 0.26624808721647153]
	TIME [epoch: 10.3 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19508958258913717		[learning rate: 0.00085822]
	Learning Rate: 0.000858223
	LOSS [training: 0.19508958258913717 | validation: 0.2406994838427985]
	TIME [epoch: 10.3 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21254331392732057		[learning rate: 0.00085559]
	Learning Rate: 0.000855592
	LOSS [training: 0.21254331392732057 | validation: 0.2699600213195165]
	TIME [epoch: 10.3 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18521546274458403		[learning rate: 0.00085297]
	Learning Rate: 0.000852969
	LOSS [training: 0.18521546274458403 | validation: 0.24203093495400044]
	TIME [epoch: 10.3 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22015332636101953		[learning rate: 0.00085035]
	Learning Rate: 0.000850354
	LOSS [training: 0.22015332636101953 | validation: 0.22632599219181182]
	TIME [epoch: 10.3 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17496635939832114		[learning rate: 0.00084775]
	Learning Rate: 0.000847748
	LOSS [training: 0.17496635939832114 | validation: 0.2063753462378974]
	TIME [epoch: 10.3 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20152208330516902		[learning rate: 0.00084515]
	Learning Rate: 0.000845149
	LOSS [training: 0.20152208330516902 | validation: 0.22875886231188042]
	TIME [epoch: 10.3 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20469212723940727		[learning rate: 0.00084256]
	Learning Rate: 0.000842558
	LOSS [training: 0.20469212723940727 | validation: 0.22451074619546127]
	TIME [epoch: 10.3 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20974521870258572		[learning rate: 0.00083998]
	Learning Rate: 0.000839976
	LOSS [training: 0.20974521870258572 | validation: 0.17982551163395805]
	TIME [epoch: 10.3 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25365390527820175		[learning rate: 0.0008374]
	Learning Rate: 0.000837401
	LOSS [training: 0.25365390527820175 | validation: 0.272265478857421]
	TIME [epoch: 10.3 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21410661360804747		[learning rate: 0.00083483]
	Learning Rate: 0.000834834
	LOSS [training: 0.21410661360804747 | validation: 0.20748826966323947]
	TIME [epoch: 10.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.182713038589045		[learning rate: 0.00083227]
	Learning Rate: 0.000832274
	LOSS [training: 0.182713038589045 | validation: 0.21322787872624893]
	TIME [epoch: 10.3 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2039150900905565		[learning rate: 0.00082972]
	Learning Rate: 0.000829723
	LOSS [training: 0.2039150900905565 | validation: 0.1931223917111999]
	TIME [epoch: 10.3 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1493305539708767		[learning rate: 0.00082718]
	Learning Rate: 0.00082718
	LOSS [training: 0.1493305539708767 | validation: 0.1964548159128443]
	TIME [epoch: 10.3 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1529147623846515		[learning rate: 0.00082464]
	Learning Rate: 0.000824644
	LOSS [training: 0.1529147623846515 | validation: 0.18964554892893204]
	TIME [epoch: 10.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2079112212603671		[learning rate: 0.00082212]
	Learning Rate: 0.000822116
	LOSS [training: 0.2079112212603671 | validation: 0.27762473291181733]
	TIME [epoch: 10.3 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2530637830937827		[learning rate: 0.0008196]
	Learning Rate: 0.000819596
	LOSS [training: 0.2530637830937827 | validation: 0.18894683761398703]
	TIME [epoch: 10.3 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17323950017003592		[learning rate: 0.00081708]
	Learning Rate: 0.000817084
	LOSS [training: 0.17323950017003592 | validation: 0.20005609310404077]
	TIME [epoch: 10.3 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18671786723652334		[learning rate: 0.00081458]
	Learning Rate: 0.000814579
	LOSS [training: 0.18671786723652334 | validation: 0.185576035523731]
	TIME [epoch: 10.3 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19382066825778327		[learning rate: 0.00081208]
	Learning Rate: 0.000812082
	LOSS [training: 0.19382066825778327 | validation: 0.23901056697022827]
	TIME [epoch: 10.3 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20933404308987474		[learning rate: 0.00080959]
	Learning Rate: 0.000809593
	LOSS [training: 0.20933404308987474 | validation: 0.19634252576248468]
	TIME [epoch: 10.3 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18763257336421543		[learning rate: 0.00080711]
	Learning Rate: 0.000807111
	LOSS [training: 0.18763257336421543 | validation: 0.2733090328179721]
	TIME [epoch: 10.3 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2194746603670291		[learning rate: 0.00080464]
	Learning Rate: 0.000804637
	LOSS [training: 0.2194746603670291 | validation: 0.2255624764826021]
	TIME [epoch: 10.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20203917353428752		[learning rate: 0.00080217]
	Learning Rate: 0.00080217
	LOSS [training: 0.20203917353428752 | validation: 0.2025675100119958]
	TIME [epoch: 10.3 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1914263600258646		[learning rate: 0.00079971]
	Learning Rate: 0.000799712
	LOSS [training: 0.1914263600258646 | validation: 0.18651875385287014]
	TIME [epoch: 10.3 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19267482510758774		[learning rate: 0.00079726]
	Learning Rate: 0.00079726
	LOSS [training: 0.19267482510758774 | validation: 0.21877374396303295]
	TIME [epoch: 10.3 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14881873351775204		[learning rate: 0.00079482]
	Learning Rate: 0.000794816
	LOSS [training: 0.14881873351775204 | validation: 0.18886765562542593]
	TIME [epoch: 10.3 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16917844463239692		[learning rate: 0.00079238]
	Learning Rate: 0.00079238
	LOSS [training: 0.16917844463239692 | validation: 0.19438748692762187]
	TIME [epoch: 10.3 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16054347549424647		[learning rate: 0.00078995]
	Learning Rate: 0.000789951
	LOSS [training: 0.16054347549424647 | validation: 0.19365734668668794]
	TIME [epoch: 10.3 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15683249295339702		[learning rate: 0.00078753]
	Learning Rate: 0.000787529
	LOSS [training: 0.15683249295339702 | validation: 0.19793599316985278]
	TIME [epoch: 10.3 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15546500959229853		[learning rate: 0.00078512]
	Learning Rate: 0.000785115
	LOSS [training: 0.15546500959229853 | validation: 0.18009906979438317]
	TIME [epoch: 10.3 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15901760461683287		[learning rate: 0.00078271]
	Learning Rate: 0.000782708
	LOSS [training: 0.15901760461683287 | validation: 0.18601291653183658]
	TIME [epoch: 10.3 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18093079304081666		[learning rate: 0.00078031]
	Learning Rate: 0.000780309
	LOSS [training: 0.18093079304081666 | validation: 0.24114650526819284]
	TIME [epoch: 10.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17225372489296048		[learning rate: 0.00077792]
	Learning Rate: 0.000777917
	LOSS [training: 0.17225372489296048 | validation: 0.1720042803322145]
	TIME [epoch: 10.3 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1433897510127092		[learning rate: 0.00077553]
	Learning Rate: 0.000775533
	LOSS [training: 0.1433897510127092 | validation: 0.22120361452129841]
	TIME [epoch: 10.3 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23074855620704726		[learning rate: 0.00077316]
	Learning Rate: 0.000773155
	LOSS [training: 0.23074855620704726 | validation: 0.19661316373807808]
	TIME [epoch: 10.3 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16955376324018206		[learning rate: 0.00077079]
	Learning Rate: 0.000770785
	LOSS [training: 0.16955376324018206 | validation: 0.23259491318108858]
	TIME [epoch: 10.3 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.181397309245402		[learning rate: 0.00076842]
	Learning Rate: 0.000768422
	LOSS [training: 0.181397309245402 | validation: 0.22131392985757609]
	TIME [epoch: 10.3 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15520628365267164		[learning rate: 0.00076607]
	Learning Rate: 0.000766067
	LOSS [training: 0.15520628365267164 | validation: 0.19886620856953058]
	TIME [epoch: 10.3 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20228866644015814		[learning rate: 0.00076372]
	Learning Rate: 0.000763719
	LOSS [training: 0.20228866644015814 | validation: 0.22526805553424056]
	TIME [epoch: 10.3 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17606733872882682		[learning rate: 0.00076138]
	Learning Rate: 0.000761377
	LOSS [training: 0.17606733872882682 | validation: 0.19573144181727714]
	TIME [epoch: 10.3 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1522076639548054		[learning rate: 0.00075904]
	Learning Rate: 0.000759043
	LOSS [training: 0.1522076639548054 | validation: 0.17717293561501976]
	TIME [epoch: 10.3 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15123690153780017		[learning rate: 0.00075672]
	Learning Rate: 0.000756717
	LOSS [training: 0.15123690153780017 | validation: 0.15488073782224646]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1341.pth
	Model improved!!!
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16693916512249918		[learning rate: 0.0007544]
	Learning Rate: 0.000754397
	LOSS [training: 0.16693916512249918 | validation: 0.21372377035545803]
	TIME [epoch: 10.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21834445862478039		[learning rate: 0.00075208]
	Learning Rate: 0.000752084
	LOSS [training: 0.21834445862478039 | validation: 0.19648414500394892]
	TIME [epoch: 10.3 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15694646746489943		[learning rate: 0.00074978]
	Learning Rate: 0.000749779
	LOSS [training: 0.15694646746489943 | validation: 0.17694399186246576]
	TIME [epoch: 10.3 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22741401542558454		[learning rate: 0.00074748]
	Learning Rate: 0.000747481
	LOSS [training: 0.22741401542558454 | validation: 0.33181072954337426]
	TIME [epoch: 10.3 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24914884242340704		[learning rate: 0.00074519]
	Learning Rate: 0.000745189
	LOSS [training: 0.24914884242340704 | validation: 0.22717969478358757]
	TIME [epoch: 10.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16906842408322756		[learning rate: 0.0007429]
	Learning Rate: 0.000742905
	LOSS [training: 0.16906842408322756 | validation: 0.1811423912350528]
	TIME [epoch: 10.3 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1415603670606511		[learning rate: 0.00074063]
	Learning Rate: 0.000740628
	LOSS [training: 0.1415603670606511 | validation: 0.1566923758494224]
	TIME [epoch: 10.3 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.196322918236734		[learning rate: 0.00073836]
	Learning Rate: 0.000738357
	LOSS [training: 0.196322918236734 | validation: 0.17992769738616446]
	TIME [epoch: 10.3 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21903292554072742		[learning rate: 0.00073609]
	Learning Rate: 0.000736094
	LOSS [training: 0.21903292554072742 | validation: 0.16271049717179484]
	TIME [epoch: 10.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20171286778445108		[learning rate: 0.00073384]
	Learning Rate: 0.000733838
	LOSS [training: 0.20171286778445108 | validation: 0.23120139724785196]
	TIME [epoch: 10.3 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2125754237770013		[learning rate: 0.00073159]
	Learning Rate: 0.000731588
	LOSS [training: 0.2125754237770013 | validation: 0.20409622649436357]
	TIME [epoch: 10.3 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19193930282870061		[learning rate: 0.00072935]
	Learning Rate: 0.000729345
	LOSS [training: 0.19193930282870061 | validation: 0.18739530617625944]
	TIME [epoch: 10.3 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18597938526109417		[learning rate: 0.00072711]
	Learning Rate: 0.00072711
	LOSS [training: 0.18597938526109417 | validation: 0.1895120441989512]
	TIME [epoch: 10.3 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17670851872844667		[learning rate: 0.00072488]
	Learning Rate: 0.000724881
	LOSS [training: 0.17670851872844667 | validation: 0.17824874282640515]
	TIME [epoch: 10.3 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19359809868676944		[learning rate: 0.00072266]
	Learning Rate: 0.000722659
	LOSS [training: 0.19359809868676944 | validation: 0.18433585877381206]
	TIME [epoch: 10.3 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1641526968967449		[learning rate: 0.00072044]
	Learning Rate: 0.000720444
	LOSS [training: 0.1641526968967449 | validation: 0.19702728727904423]
	TIME [epoch: 10.3 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1963617418481665		[learning rate: 0.00071824]
	Learning Rate: 0.000718235
	LOSS [training: 0.1963617418481665 | validation: 0.22510917379850873]
	TIME [epoch: 10.3 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31121311043483835		[learning rate: 0.00071603]
	Learning Rate: 0.000716033
	LOSS [training: 0.31121311043483835 | validation: 0.20798446441308255]
	TIME [epoch: 10.3 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1693427855217971		[learning rate: 0.00071384]
	Learning Rate: 0.000713839
	LOSS [training: 0.1693427855217971 | validation: 0.20722772602468262]
	TIME [epoch: 10.3 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18402327785232672		[learning rate: 0.00071165]
	Learning Rate: 0.00071165
	LOSS [training: 0.18402327785232672 | validation: 0.19337368433561572]
	TIME [epoch: 10.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16810434114587572		[learning rate: 0.00070947]
	Learning Rate: 0.000709469
	LOSS [training: 0.16810434114587572 | validation: 0.177165659205557]
	TIME [epoch: 10.3 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.250339712763339		[learning rate: 0.00070729]
	Learning Rate: 0.000707294
	LOSS [training: 0.250339712763339 | validation: 0.21372783927740643]
	TIME [epoch: 10.3 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25328009915854033		[learning rate: 0.00070513]
	Learning Rate: 0.000705126
	LOSS [training: 0.25328009915854033 | validation: 0.26637166704463344]
	TIME [epoch: 10.3 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27467585681094886		[learning rate: 0.00070296]
	Learning Rate: 0.000702964
	LOSS [training: 0.27467585681094886 | validation: 0.26941157980659436]
	TIME [epoch: 10.3 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30735081914691		[learning rate: 0.00070081]
	Learning Rate: 0.00070081
	LOSS [training: 0.30735081914691 | validation: 0.21519942801965342]
	TIME [epoch: 10.3 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22699651902012893		[learning rate: 0.00069866]
	Learning Rate: 0.000698661
	LOSS [training: 0.22699651902012893 | validation: 0.2619656436239656]
	TIME [epoch: 10.3 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23759965303620562		[learning rate: 0.00069652]
	Learning Rate: 0.000696519
	LOSS [training: 0.23759965303620562 | validation: 0.22323874176223882]
	TIME [epoch: 10.3 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20578403347252813		[learning rate: 0.00069438]
	Learning Rate: 0.000694384
	LOSS [training: 0.20578403347252813 | validation: 0.21658786709107083]
	TIME [epoch: 10.3 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22549401470374333		[learning rate: 0.00069226]
	Learning Rate: 0.000692256
	LOSS [training: 0.22549401470374333 | validation: 0.2086421293779323]
	TIME [epoch: 10.3 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.184927615602611		[learning rate: 0.00069013]
	Learning Rate: 0.000690134
	LOSS [training: 0.184927615602611 | validation: 0.21260182909038614]
	TIME [epoch: 10.3 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22715206116181325		[learning rate: 0.00068802]
	Learning Rate: 0.000688018
	LOSS [training: 0.22715206116181325 | validation: 0.20105470542187445]
	TIME [epoch: 10.3 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16508523815913434		[learning rate: 0.00068591]
	Learning Rate: 0.000685909
	LOSS [training: 0.16508523815913434 | validation: 0.15756533790946314]
	TIME [epoch: 10.3 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1671157041089795		[learning rate: 0.00068381]
	Learning Rate: 0.000683807
	LOSS [training: 0.1671157041089795 | validation: 0.1610821397640294]
	TIME [epoch: 10.3 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16768820155169753		[learning rate: 0.00068171]
	Learning Rate: 0.000681711
	LOSS [training: 0.16768820155169753 | validation: 0.16437638150996856]
	TIME [epoch: 10.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16631577485921434		[learning rate: 0.00067962]
	Learning Rate: 0.000679621
	LOSS [training: 0.16631577485921434 | validation: 0.16205439846571565]
	TIME [epoch: 10.3 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.168088293375024		[learning rate: 0.00067754]
	Learning Rate: 0.000677538
	LOSS [training: 0.168088293375024 | validation: 0.1845352776614241]
	TIME [epoch: 10.3 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17720474238998868		[learning rate: 0.00067546]
	Learning Rate: 0.000675461
	LOSS [training: 0.17720474238998868 | validation: 0.21150014345033558]
	TIME [epoch: 10.3 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19091609194434184		[learning rate: 0.00067339]
	Learning Rate: 0.00067339
	LOSS [training: 0.19091609194434184 | validation: 0.2019794392437915]
	TIME [epoch: 10.3 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2009691102872851		[learning rate: 0.00067133]
	Learning Rate: 0.000671326
	LOSS [training: 0.2009691102872851 | validation: 0.24453323193639268]
	TIME [epoch: 10.3 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22553909290211474		[learning rate: 0.00066927]
	Learning Rate: 0.000669268
	LOSS [training: 0.22553909290211474 | validation: 0.22957487751268438]
	TIME [epoch: 10.3 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1899363047030757		[learning rate: 0.00066722]
	Learning Rate: 0.000667216
	LOSS [training: 0.1899363047030757 | validation: 0.21222451486652294]
	TIME [epoch: 10.3 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2140764618565231		[learning rate: 0.00066517]
	Learning Rate: 0.000665171
	LOSS [training: 0.2140764618565231 | validation: 0.28163933578575756]
	TIME [epoch: 10.3 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18004257904666593		[learning rate: 0.00066313]
	Learning Rate: 0.000663132
	LOSS [training: 0.18004257904666593 | validation: 0.2000640712738304]
	TIME [epoch: 10.3 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16542399139504965		[learning rate: 0.0006611]
	Learning Rate: 0.000661099
	LOSS [training: 0.16542399139504965 | validation: 0.2028802606292406]
	TIME [epoch: 10.3 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1657313533272125		[learning rate: 0.00065907]
	Learning Rate: 0.000659073
	LOSS [training: 0.1657313533272125 | validation: 0.2549079605512024]
	TIME [epoch: 10.3 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2644518144560071		[learning rate: 0.00065705]
	Learning Rate: 0.000657052
	LOSS [training: 0.2644518144560071 | validation: 0.24575750651350423]
	TIME [epoch: 10.3 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18181440786965672		[learning rate: 0.00065504]
	Learning Rate: 0.000655038
	LOSS [training: 0.18181440786965672 | validation: 0.1938483425459949]
	TIME [epoch: 10.3 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15255754347233724		[learning rate: 0.00065303]
	Learning Rate: 0.00065303
	LOSS [training: 0.15255754347233724 | validation: 0.2179771339387566]
	TIME [epoch: 10.3 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1433640106855981		[learning rate: 0.00065103]
	Learning Rate: 0.000651028
	LOSS [training: 0.1433640106855981 | validation: 0.17625624398054726]
	TIME [epoch: 10.3 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16233900035758778		[learning rate: 0.00064903]
	Learning Rate: 0.000649033
	LOSS [training: 0.16233900035758778 | validation: 0.1756106439492117]
	TIME [epoch: 10.3 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.166214401953221		[learning rate: 0.00064704]
	Learning Rate: 0.000647043
	LOSS [training: 0.166214401953221 | validation: 0.19793635171992358]
	TIME [epoch: 10.3 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1822271635710156		[learning rate: 0.00064506]
	Learning Rate: 0.00064506
	LOSS [training: 0.1822271635710156 | validation: 0.2168790363377645]
	TIME [epoch: 10.3 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16897639129498152		[learning rate: 0.00064308]
	Learning Rate: 0.000643082
	LOSS [training: 0.16897639129498152 | validation: 0.1770346450320885]
	TIME [epoch: 10.3 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15057569002652		[learning rate: 0.00064111]
	Learning Rate: 0.000641111
	LOSS [training: 0.15057569002652 | validation: 0.206717172989538]
	TIME [epoch: 10.3 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17225465862353087		[learning rate: 0.00063915]
	Learning Rate: 0.000639146
	LOSS [training: 0.17225465862353087 | validation: 0.2119594030315487]
	TIME [epoch: 10.3 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14504333975787698		[learning rate: 0.00063719]
	Learning Rate: 0.000637187
	LOSS [training: 0.14504333975787698 | validation: 0.15070820913754088]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1397.pth
	Model improved!!!
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13412240825081828		[learning rate: 0.00063523]
	Learning Rate: 0.000635233
	LOSS [training: 0.13412240825081828 | validation: 0.1645156287151145]
	TIME [epoch: 10.3 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15007956684691748		[learning rate: 0.00063329]
	Learning Rate: 0.000633286
	LOSS [training: 0.15007956684691748 | validation: 0.22795867306457196]
	TIME [epoch: 10.3 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.200476571081683		[learning rate: 0.00063134]
	Learning Rate: 0.000631345
	LOSS [training: 0.200476571081683 | validation: 0.18709127729384775]
	TIME [epoch: 10.3 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15052952590342358		[learning rate: 0.00062941]
	Learning Rate: 0.000629409
	LOSS [training: 0.15052952590342358 | validation: 0.2406610386333173]
	TIME [epoch: 10.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16677052726848124		[learning rate: 0.00062748]
	Learning Rate: 0.00062748
	LOSS [training: 0.16677052726848124 | validation: 0.20944417491172723]
	TIME [epoch: 10.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21133480639530533		[learning rate: 0.00062556]
	Learning Rate: 0.000625557
	LOSS [training: 0.21133480639530533 | validation: 0.2066942935164086]
	TIME [epoch: 10.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14985180772594225		[learning rate: 0.00062364]
	Learning Rate: 0.000623639
	LOSS [training: 0.14985180772594225 | validation: 0.19470618521317523]
	TIME [epoch: 10.3 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17165491301958288		[learning rate: 0.00062173]
	Learning Rate: 0.000621727
	LOSS [training: 0.17165491301958288 | validation: 0.2588535141409182]
	TIME [epoch: 10.3 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18143443594178618		[learning rate: 0.00061982]
	Learning Rate: 0.000619821
	LOSS [training: 0.18143443594178618 | validation: 0.23802790465363516]
	TIME [epoch: 10.3 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18033885147898313		[learning rate: 0.00061792]
	Learning Rate: 0.000617922
	LOSS [training: 0.18033885147898313 | validation: 0.18352176858447153]
	TIME [epoch: 10.3 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18642654716419704		[learning rate: 0.00061603]
	Learning Rate: 0.000616027
	LOSS [training: 0.18642654716419704 | validation: 0.19202645712176142]
	TIME [epoch: 10.3 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16805088175134036		[learning rate: 0.00061414]
	Learning Rate: 0.000614139
	LOSS [training: 0.16805088175134036 | validation: 0.2216287354747035]
	TIME [epoch: 10.3 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17808997643787056		[learning rate: 0.00061226]
	Learning Rate: 0.000612256
	LOSS [training: 0.17808997643787056 | validation: 0.18393771850282348]
	TIME [epoch: 10.3 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15131259873946384		[learning rate: 0.00061038]
	Learning Rate: 0.00061038
	LOSS [training: 0.15131259873946384 | validation: 0.17312821341704102]
	TIME [epoch: 10.3 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1327785874930591		[learning rate: 0.00060851]
	Learning Rate: 0.000608509
	LOSS [training: 0.1327785874930591 | validation: 0.15696964049385304]
	TIME [epoch: 10.3 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12216439972897197		[learning rate: 0.00060664]
	Learning Rate: 0.000606643
	LOSS [training: 0.12216439972897197 | validation: 0.17629904762308038]
	TIME [epoch: 10.3 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1413547672936243		[learning rate: 0.00060478]
	Learning Rate: 0.000604784
	LOSS [training: 0.1413547672936243 | validation: 0.2046032796990094]
	TIME [epoch: 10.3 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18103313250646055		[learning rate: 0.00060293]
	Learning Rate: 0.00060293
	LOSS [training: 0.18103313250646055 | validation: 0.19616777312371794]
	TIME [epoch: 10.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16005280577491898		[learning rate: 0.00060108]
	Learning Rate: 0.000601081
	LOSS [training: 0.16005280577491898 | validation: 0.22248455115005056]
	TIME [epoch: 10.3 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.165390879405443		[learning rate: 0.00059924]
	Learning Rate: 0.000599239
	LOSS [training: 0.165390879405443 | validation: 0.19535845621628656]
	TIME [epoch: 10.3 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16277522374237216		[learning rate: 0.0005974]
	Learning Rate: 0.000597402
	LOSS [training: 0.16277522374237216 | validation: 0.17808217227862563]
	TIME [epoch: 10.3 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15400209807426316		[learning rate: 0.00059557]
	Learning Rate: 0.000595571
	LOSS [training: 0.15400209807426316 | validation: 0.33407797035316705]
	TIME [epoch: 10.3 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23461690089525017		[learning rate: 0.00059375]
	Learning Rate: 0.000593745
	LOSS [training: 0.23461690089525017 | validation: 0.16861002937749253]
	TIME [epoch: 10.3 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1478418834452866		[learning rate: 0.00059192]
	Learning Rate: 0.000591925
	LOSS [training: 0.1478418834452866 | validation: 0.18451299274450306]
	TIME [epoch: 10.3 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1452726612833421		[learning rate: 0.00059011]
	Learning Rate: 0.00059011
	LOSS [training: 0.1452726612833421 | validation: 0.17438287697807758]
	TIME [epoch: 10.3 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15766344362110346		[learning rate: 0.0005883]
	Learning Rate: 0.000588302
	LOSS [training: 0.15766344362110346 | validation: 0.21840746542785447]
	TIME [epoch: 10.3 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18751193455135978		[learning rate: 0.0005865]
	Learning Rate: 0.000586498
	LOSS [training: 0.18751193455135978 | validation: 0.18479646453539958]
	TIME [epoch: 10.3 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15278372730618756		[learning rate: 0.0005847]
	Learning Rate: 0.0005847
	LOSS [training: 0.15278372730618756 | validation: 0.21606485806877337]
	TIME [epoch: 10.3 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15257398096402272		[learning rate: 0.00058291]
	Learning Rate: 0.000582908
	LOSS [training: 0.15257398096402272 | validation: 0.28719279851181556]
	TIME [epoch: 10.3 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19144487343186883		[learning rate: 0.00058112]
	Learning Rate: 0.000581121
	LOSS [training: 0.19144487343186883 | validation: 0.19080143975299863]
	TIME [epoch: 10.3 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14579547090218034		[learning rate: 0.00057934]
	Learning Rate: 0.00057934
	LOSS [training: 0.14579547090218034 | validation: 0.22447924646291115]
	TIME [epoch: 10.3 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17661927421760457		[learning rate: 0.00057756]
	Learning Rate: 0.000577564
	LOSS [training: 0.17661927421760457 | validation: 0.19757308769249343]
	TIME [epoch: 10.3 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14733965475236363		[learning rate: 0.00057579]
	Learning Rate: 0.000575793
	LOSS [training: 0.14733965475236363 | validation: 0.1892769031482434]
	TIME [epoch: 10.3 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13466063023843358		[learning rate: 0.00057403]
	Learning Rate: 0.000574028
	LOSS [training: 0.13466063023843358 | validation: 0.1514210744095098]
	TIME [epoch: 10.3 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1373530191087764		[learning rate: 0.00057227]
	Learning Rate: 0.000572269
	LOSS [training: 0.1373530191087764 | validation: 0.18013799068687533]
	TIME [epoch: 10.3 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14189837873272232		[learning rate: 0.00057051]
	Learning Rate: 0.000570514
	LOSS [training: 0.14189837873272232 | validation: 0.1692092036944781]
	TIME [epoch: 10.3 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13953539525003184		[learning rate: 0.00056877]
	Learning Rate: 0.000568766
	LOSS [training: 0.13953539525003184 | validation: 0.26068092757215544]
	TIME [epoch: 10.3 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16431741048316406		[learning rate: 0.00056702]
	Learning Rate: 0.000567022
	LOSS [training: 0.16431741048316406 | validation: 0.1856287723025575]
	TIME [epoch: 10.3 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14294550298084535		[learning rate: 0.00056528]
	Learning Rate: 0.000565284
	LOSS [training: 0.14294550298084535 | validation: 0.16735148833614097]
	TIME [epoch: 10.3 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17195484429826885		[learning rate: 0.00056355]
	Learning Rate: 0.000563551
	LOSS [training: 0.17195484429826885 | validation: 0.18175363921569038]
	TIME [epoch: 10.3 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15036839449591805		[learning rate: 0.00056182]
	Learning Rate: 0.000561824
	LOSS [training: 0.15036839449591805 | validation: 0.1516201162404157]
	TIME [epoch: 10.4 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1367870668338816		[learning rate: 0.0005601]
	Learning Rate: 0.000560101
	LOSS [training: 0.1367870668338816 | validation: 0.1847248544281653]
	TIME [epoch: 10.3 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1905360562016608		[learning rate: 0.00055838]
	Learning Rate: 0.000558385
	LOSS [training: 0.1905360562016608 | validation: 0.2152848350949802]
	TIME [epoch: 10.3 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19642537284530237		[learning rate: 0.00055667]
	Learning Rate: 0.000556673
	LOSS [training: 0.19642537284530237 | validation: 0.2172620168920966]
	TIME [epoch: 10.3 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1254926488672416		[learning rate: 0.00055497]
	Learning Rate: 0.000554966
	LOSS [training: 0.1254926488672416 | validation: 0.17157601662860728]
	TIME [epoch: 10.3 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14594182076389886		[learning rate: 0.00055327]
	Learning Rate: 0.000553265
	LOSS [training: 0.14594182076389886 | validation: 0.16498797956515637]
	TIME [epoch: 10.3 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12921214905556827		[learning rate: 0.00055157]
	Learning Rate: 0.000551569
	LOSS [training: 0.12921214905556827 | validation: 0.18319186706291996]
	TIME [epoch: 10.3 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14803307662515244		[learning rate: 0.00054988]
	Learning Rate: 0.000549878
	LOSS [training: 0.14803307662515244 | validation: 0.1584474497306531]
	TIME [epoch: 10.3 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1302283981127313		[learning rate: 0.00054819]
	Learning Rate: 0.000548193
	LOSS [training: 0.1302283981127313 | validation: 0.16448455002826315]
	TIME [epoch: 10.3 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1388302983057617		[learning rate: 0.00054651]
	Learning Rate: 0.000546512
	LOSS [training: 0.1388302983057617 | validation: 0.16418797290852816]
	TIME [epoch: 10.3 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13951656454392009		[learning rate: 0.00054484]
	Learning Rate: 0.000544837
	LOSS [training: 0.13951656454392009 | validation: 0.16887051742874679]
	TIME [epoch: 10.3 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12826046250034556		[learning rate: 0.00054317]
	Learning Rate: 0.000543167
	LOSS [training: 0.12826046250034556 | validation: 0.16805352745860383]
	TIME [epoch: 10.3 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1334150399374998		[learning rate: 0.0005415]
	Learning Rate: 0.000541502
	LOSS [training: 0.1334150399374998 | validation: 0.18001355052554172]
	TIME [epoch: 10.3 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1563412571540196		[learning rate: 0.00053984]
	Learning Rate: 0.000539842
	LOSS [training: 0.1563412571540196 | validation: 0.16057059164310183]
	TIME [epoch: 10.3 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12568839897649292		[learning rate: 0.00053819]
	Learning Rate: 0.000538187
	LOSS [training: 0.12568839897649292 | validation: 0.18309500206016524]
	TIME [epoch: 10.3 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14570273824343186		[learning rate: 0.00053654]
	Learning Rate: 0.000536537
	LOSS [training: 0.14570273824343186 | validation: 0.17916282176398177]
	TIME [epoch: 10.3 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13431867908322256		[learning rate: 0.00053489]
	Learning Rate: 0.000534893
	LOSS [training: 0.13431867908322256 | validation: 0.18289192968402213]
	TIME [epoch: 10.3 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1278337778376503		[learning rate: 0.00053325]
	Learning Rate: 0.000533253
	LOSS [training: 0.1278337778376503 | validation: 0.14407805767299464]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1455.pth
	Model improved!!!
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1184278476422252		[learning rate: 0.00053162]
	Learning Rate: 0.000531618
	LOSS [training: 0.1184278476422252 | validation: 0.16658032811526005]
	TIME [epoch: 10.3 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15444547109696863		[learning rate: 0.00052999]
	Learning Rate: 0.000529989
	LOSS [training: 0.15444547109696863 | validation: 0.2685333744493512]
	TIME [epoch: 10.3 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17718231147496838		[learning rate: 0.00052836]
	Learning Rate: 0.000528364
	LOSS [training: 0.17718231147496838 | validation: 0.216247299961842]
	TIME [epoch: 10.3 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17061003422534568		[learning rate: 0.00052674]
	Learning Rate: 0.000526744
	LOSS [training: 0.17061003422534568 | validation: 0.19074609337563325]
	TIME [epoch: 10.3 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16528845234449585		[learning rate: 0.00052513]
	Learning Rate: 0.00052513
	LOSS [training: 0.16528845234449585 | validation: 0.1925942757458993]
	TIME [epoch: 10.3 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1557918729355941		[learning rate: 0.00052352]
	Learning Rate: 0.00052352
	LOSS [training: 0.1557918729355941 | validation: 0.21252035814779682]
	TIME [epoch: 10.3 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1593574149369598		[learning rate: 0.00052192]
	Learning Rate: 0.000521915
	LOSS [training: 0.1593574149369598 | validation: 0.16362715300121664]
	TIME [epoch: 10.3 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1327490047090116		[learning rate: 0.00052032]
	Learning Rate: 0.000520315
	LOSS [training: 0.1327490047090116 | validation: 0.1721411141274553]
	TIME [epoch: 10.3 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1480000479622229		[learning rate: 0.00051872]
	Learning Rate: 0.00051872
	LOSS [training: 0.1480000479622229 | validation: 0.196350772711935]
	TIME [epoch: 10.3 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1911502192822045		[learning rate: 0.00051713]
	Learning Rate: 0.00051713
	LOSS [training: 0.1911502192822045 | validation: 0.17085445379252392]
	TIME [epoch: 10.3 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18298159435475853		[learning rate: 0.00051555]
	Learning Rate: 0.000515545
	LOSS [training: 0.18298159435475853 | validation: 0.1942995362923845]
	TIME [epoch: 10.3 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14771320485034106		[learning rate: 0.00051396]
	Learning Rate: 0.000513965
	LOSS [training: 0.14771320485034106 | validation: 0.1476821065148997]
	TIME [epoch: 10.3 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11648709761543674		[learning rate: 0.00051239]
	Learning Rate: 0.000512389
	LOSS [training: 0.11648709761543674 | validation: 0.15437784580512381]
	TIME [epoch: 10.3 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13575820861814236		[learning rate: 0.00051082]
	Learning Rate: 0.000510818
	LOSS [training: 0.13575820861814236 | validation: 0.16319918015579382]
	TIME [epoch: 10.3 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14364393776085593		[learning rate: 0.00050925]
	Learning Rate: 0.000509253
	LOSS [training: 0.14364393776085593 | validation: 0.17400066410063242]
	TIME [epoch: 10.3 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1600968129620137		[learning rate: 0.00050769]
	Learning Rate: 0.000507692
	LOSS [training: 0.1600968129620137 | validation: 0.15927127042039843]
	TIME [epoch: 10.3 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17721341144327613		[learning rate: 0.00050614]
	Learning Rate: 0.000506135
	LOSS [training: 0.17721341144327613 | validation: 0.30731963562870535]
	TIME [epoch: 10.3 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24287510165515047		[learning rate: 0.00050458]
	Learning Rate: 0.000504584
	LOSS [training: 0.24287510165515047 | validation: 0.2266550573164285]
	TIME [epoch: 10.3 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.185965577033619		[learning rate: 0.00050304]
	Learning Rate: 0.000503037
	LOSS [training: 0.185965577033619 | validation: 0.19164854683271024]
	TIME [epoch: 10.3 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15170328830083274		[learning rate: 0.00050149]
	Learning Rate: 0.000501495
	LOSS [training: 0.15170328830083274 | validation: 0.16408172094096024]
	TIME [epoch: 10.3 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13351552523072663		[learning rate: 0.00049996]
	Learning Rate: 0.000499958
	LOSS [training: 0.13351552523072663 | validation: 0.15555149334199211]
	TIME [epoch: 10.3 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11259088297341154		[learning rate: 0.00049843]
	Learning Rate: 0.000498425
	LOSS [training: 0.11259088297341154 | validation: 0.16068134295004494]
	TIME [epoch: 10.3 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1351271880403286		[learning rate: 0.0004969]
	Learning Rate: 0.000496897
	LOSS [training: 0.1351271880403286 | validation: 0.1709649859691001]
	TIME [epoch: 10.3 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18000277017970814		[learning rate: 0.00049537]
	Learning Rate: 0.000495374
	LOSS [training: 0.18000277017970814 | validation: 0.22728329989829146]
	TIME [epoch: 10.3 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24462469578317414		[learning rate: 0.00049386]
	Learning Rate: 0.000493856
	LOSS [training: 0.24462469578317414 | validation: 0.23899651418008566]
	TIME [epoch: 10.3 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21637022334816963		[learning rate: 0.00049234]
	Learning Rate: 0.000492342
	LOSS [training: 0.21637022334816963 | validation: 0.20821308145870848]
	TIME [epoch: 10.3 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23009956218707767		[learning rate: 0.00049083]
	Learning Rate: 0.000490832
	LOSS [training: 0.23009956218707767 | validation: 0.23161430096313113]
	TIME [epoch: 10.3 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19302245871260118		[learning rate: 0.00048933]
	Learning Rate: 0.000489328
	LOSS [training: 0.19302245871260118 | validation: 0.1865415107807289]
	TIME [epoch: 10.3 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1538432627196541		[learning rate: 0.00048783]
	Learning Rate: 0.000487828
	LOSS [training: 0.1538432627196541 | validation: 0.23057120629462433]
	TIME [epoch: 10.3 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17727992532960102		[learning rate: 0.00048633]
	Learning Rate: 0.000486333
	LOSS [training: 0.17727992532960102 | validation: 0.25174949190498086]
	TIME [epoch: 10.3 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1812939064181879		[learning rate: 0.00048484]
	Learning Rate: 0.000484842
	LOSS [training: 0.1812939064181879 | validation: 0.21533533583612818]
	TIME [epoch: 10.3 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16733510482537603		[learning rate: 0.00048336]
	Learning Rate: 0.000483356
	LOSS [training: 0.16733510482537603 | validation: 0.2275288731432277]
	TIME [epoch: 10.3 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18261067056758923		[learning rate: 0.00048187]
	Learning Rate: 0.000481874
	LOSS [training: 0.18261067056758923 | validation: 0.22040727789851006]
	TIME [epoch: 10.3 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21863401107506567		[learning rate: 0.0004804]
	Learning Rate: 0.000480397
	LOSS [training: 0.21863401107506567 | validation: 0.19692013720617035]
	TIME [epoch: 10.3 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17771362628522974		[learning rate: 0.00047892]
	Learning Rate: 0.000478924
	LOSS [training: 0.17771362628522974 | validation: 0.1983990394016645]
	TIME [epoch: 10.3 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15500607292621899		[learning rate: 0.00047746]
	Learning Rate: 0.000477456
	LOSS [training: 0.15500607292621899 | validation: 0.18606720226984813]
	TIME [epoch: 10.3 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1434759532242044		[learning rate: 0.00047599]
	Learning Rate: 0.000475992
	LOSS [training: 0.1434759532242044 | validation: 0.1516188186874632]
	TIME [epoch: 10.3 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14479759915280027		[learning rate: 0.00047453]
	Learning Rate: 0.000474533
	LOSS [training: 0.14479759915280027 | validation: 0.20078002457105423]
	TIME [epoch: 10.3 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14106355172056		[learning rate: 0.00047308]
	Learning Rate: 0.000473079
	LOSS [training: 0.14106355172056 | validation: 0.20793767930419815]
	TIME [epoch: 10.3 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16451290926058126		[learning rate: 0.00047163]
	Learning Rate: 0.000471628
	LOSS [training: 0.16451290926058126 | validation: 0.16659826331435376]
	TIME [epoch: 10.3 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13534184910208907		[learning rate: 0.00047018]
	Learning Rate: 0.000470183
	LOSS [training: 0.13534184910208907 | validation: 0.16790320794960623]
	TIME [epoch: 10.3 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12330703717501837		[learning rate: 0.00046874]
	Learning Rate: 0.000468741
	LOSS [training: 0.12330703717501837 | validation: 0.17436156660038066]
	TIME [epoch: 10.3 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12081041692149061		[learning rate: 0.0004673]
	Learning Rate: 0.000467304
	LOSS [training: 0.12081041692149061 | validation: 0.1669085774015047]
	TIME [epoch: 10.3 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14879085102503642		[learning rate: 0.00046587]
	Learning Rate: 0.000465872
	LOSS [training: 0.14879085102503642 | validation: 0.19783079950980387]
	TIME [epoch: 10.3 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14097406839126436		[learning rate: 0.00046444]
	Learning Rate: 0.000464444
	LOSS [training: 0.14097406839126436 | validation: 0.19261086759410326]
	TIME [epoch: 10.3 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14618404975432892		[learning rate: 0.00046302]
	Learning Rate: 0.00046302
	LOSS [training: 0.14618404975432892 | validation: 0.16754536327239541]
	TIME [epoch: 10.3 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1210470306266099		[learning rate: 0.0004616]
	Learning Rate: 0.000461601
	LOSS [training: 0.1210470306266099 | validation: 0.15989305918464586]
	TIME [epoch: 10.3 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11897160083883081		[learning rate: 0.00046019]
	Learning Rate: 0.000460186
	LOSS [training: 0.11897160083883081 | validation: 0.14977067100655928]
	TIME [epoch: 10.3 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12489089423426214		[learning rate: 0.00045878]
	Learning Rate: 0.000458775
	LOSS [training: 0.12489089423426214 | validation: 0.18819183407743878]
	TIME [epoch: 10.3 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14738604361492375		[learning rate: 0.00045737]
	Learning Rate: 0.000457369
	LOSS [training: 0.14738604361492375 | validation: 0.16924507548979606]
	TIME [epoch: 10.3 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1327306345600265		[learning rate: 0.00045597]
	Learning Rate: 0.000455967
	LOSS [training: 0.1327306345600265 | validation: 0.1680594144307012]
	TIME [epoch: 10.3 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1169203338355157		[learning rate: 0.00045457]
	Learning Rate: 0.000454569
	LOSS [training: 0.1169203338355157 | validation: 0.16657554086454052]
	TIME [epoch: 10.3 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14608078096560997		[learning rate: 0.00045318]
	Learning Rate: 0.000453176
	LOSS [training: 0.14608078096560997 | validation: 0.1828254907536551]
	TIME [epoch: 10.3 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15435783754711738		[learning rate: 0.00045179]
	Learning Rate: 0.000451787
	LOSS [training: 0.15435783754711738 | validation: 0.17931543392952357]
	TIME [epoch: 10.3 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13559741230310024		[learning rate: 0.0004504]
	Learning Rate: 0.000450402
	LOSS [training: 0.13559741230310024 | validation: 0.17427711628610007]
	TIME [epoch: 10.3 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1242222424407234		[learning rate: 0.00044902]
	Learning Rate: 0.000449021
	LOSS [training: 0.1242222424407234 | validation: 0.14561080482986402]
	TIME [epoch: 10.3 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1275427655435915		[learning rate: 0.00044764]
	Learning Rate: 0.000447645
	LOSS [training: 0.1275427655435915 | validation: 0.18412804979667394]
	TIME [epoch: 10.3 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17127106495604616		[learning rate: 0.00044627]
	Learning Rate: 0.000446272
	LOSS [training: 0.17127106495604616 | validation: 0.18450906140403128]
	TIME [epoch: 10.3 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16388952103875573		[learning rate: 0.0004449]
	Learning Rate: 0.000444904
	LOSS [training: 0.16388952103875573 | validation: 0.16220318501044084]
	TIME [epoch: 10.3 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12381022449925652		[learning rate: 0.00044354]
	Learning Rate: 0.000443541
	LOSS [training: 0.12381022449925652 | validation: 0.16205868610119914]
	TIME [epoch: 10.3 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16873288298090403		[learning rate: 0.00044218]
	Learning Rate: 0.000442181
	LOSS [training: 0.16873288298090403 | validation: 0.24899863569132366]
	TIME [epoch: 10.3 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17262597878327943		[learning rate: 0.00044083]
	Learning Rate: 0.000440825
	LOSS [training: 0.17262597878327943 | validation: 0.21452918695673817]
	TIME [epoch: 10.3 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16803339851417112		[learning rate: 0.00043947]
	Learning Rate: 0.000439474
	LOSS [training: 0.16803339851417112 | validation: 0.19814915164841027]
	TIME [epoch: 10.3 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14900750029843673		[learning rate: 0.00043813]
	Learning Rate: 0.000438127
	LOSS [training: 0.14900750029843673 | validation: 0.18083522270078603]
	TIME [epoch: 10.3 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15754616718728356		[learning rate: 0.00043678]
	Learning Rate: 0.000436784
	LOSS [training: 0.15754616718728356 | validation: 0.189988653679729]
	TIME [epoch: 10.3 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15322481949506916		[learning rate: 0.00043544]
	Learning Rate: 0.000435445
	LOSS [training: 0.15322481949506916 | validation: 0.1907248470405102]
	TIME [epoch: 10.3 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1835582752965053		[learning rate: 0.00043411]
	Learning Rate: 0.00043411
	LOSS [training: 0.1835582752965053 | validation: 0.2205894661405452]
	TIME [epoch: 10.3 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15298200371242687		[learning rate: 0.00043278]
	Learning Rate: 0.00043278
	LOSS [training: 0.15298200371242687 | validation: 0.20181280004023244]
	TIME [epoch: 10.3 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15572578579943092		[learning rate: 0.00043145]
	Learning Rate: 0.000431453
	LOSS [training: 0.15572578579943092 | validation: 0.2180416651623743]
	TIME [epoch: 10.3 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1432968683518602		[learning rate: 0.00043013]
	Learning Rate: 0.00043013
	LOSS [training: 0.1432968683518602 | validation: 0.22015415562900173]
	TIME [epoch: 10.3 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15651886976258927		[learning rate: 0.00042881]
	Learning Rate: 0.000428812
	LOSS [training: 0.15651886976258927 | validation: 0.1669775577620939]
	TIME [epoch: 10.3 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13395655860223177		[learning rate: 0.0004275]
	Learning Rate: 0.000427497
	LOSS [training: 0.13395655860223177 | validation: 0.16268964828029123]
	TIME [epoch: 10.3 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12057303105711636		[learning rate: 0.00042619]
	Learning Rate: 0.000426187
	LOSS [training: 0.12057303105711636 | validation: 0.14130920918350223]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1528.pth
	Model improved!!!
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12584666224849378		[learning rate: 0.00042488]
	Learning Rate: 0.00042488
	LOSS [training: 0.12584666224849378 | validation: 0.15087243578030235]
	TIME [epoch: 10.3 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13562327138810923		[learning rate: 0.00042358]
	Learning Rate: 0.000423578
	LOSS [training: 0.13562327138810923 | validation: 0.16258209502904142]
	TIME [epoch: 10.3 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14304441191288836		[learning rate: 0.00042228]
	Learning Rate: 0.000422279
	LOSS [training: 0.14304441191288836 | validation: 0.14978444959250542]
	TIME [epoch: 10.3 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1584819680472394		[learning rate: 0.00042098]
	Learning Rate: 0.000420985
	LOSS [training: 0.1584819680472394 | validation: 0.20085197747230354]
	TIME [epoch: 10.3 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16456090920119446		[learning rate: 0.00041969]
	Learning Rate: 0.000419694
	LOSS [training: 0.16456090920119446 | validation: 0.16174833201701475]
	TIME [epoch: 10.3 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1450930786993994		[learning rate: 0.00041841]
	Learning Rate: 0.000418408
	LOSS [training: 0.1450930786993994 | validation: 0.16281836965313645]
	TIME [epoch: 10.3 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12722979067743023		[learning rate: 0.00041713]
	Learning Rate: 0.000417125
	LOSS [training: 0.12722979067743023 | validation: 0.15512861416569318]
	TIME [epoch: 10.3 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12683100351486334		[learning rate: 0.00041585]
	Learning Rate: 0.000415847
	LOSS [training: 0.12683100351486334 | validation: 0.15556387722533058]
	TIME [epoch: 10.3 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13320230283788087		[learning rate: 0.00041457]
	Learning Rate: 0.000414572
	LOSS [training: 0.13320230283788087 | validation: 0.14724636556416992]
	TIME [epoch: 10.3 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12496668978312757		[learning rate: 0.0004133]
	Learning Rate: 0.000413301
	LOSS [training: 0.12496668978312757 | validation: 0.17034171435348924]
	TIME [epoch: 10.3 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1230624489842321		[learning rate: 0.00041203]
	Learning Rate: 0.000412034
	LOSS [training: 0.1230624489842321 | validation: 0.17963651360205046]
	TIME [epoch: 10.3 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13790790359291333		[learning rate: 0.00041077]
	Learning Rate: 0.000410771
	LOSS [training: 0.13790790359291333 | validation: 0.17085622214373142]
	TIME [epoch: 10.3 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1441448973732667		[learning rate: 0.00040951]
	Learning Rate: 0.000409512
	LOSS [training: 0.1441448973732667 | validation: 0.15709278513620908]
	TIME [epoch: 10.3 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1221429805471483		[learning rate: 0.00040826]
	Learning Rate: 0.000408257
	LOSS [training: 0.1221429805471483 | validation: 0.1902240086516811]
	TIME [epoch: 10.3 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14207054652281997		[learning rate: 0.00040701]
	Learning Rate: 0.000407005
	LOSS [training: 0.14207054652281997 | validation: 0.16428125952398012]
	TIME [epoch: 10.3 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12649668831023358		[learning rate: 0.00040576]
	Learning Rate: 0.000405758
	LOSS [training: 0.12649668831023358 | validation: 0.19962638918439435]
	TIME [epoch: 10.3 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15309680318001687		[learning rate: 0.00040451]
	Learning Rate: 0.000404514
	LOSS [training: 0.15309680318001687 | validation: 0.22082808930634923]
	TIME [epoch: 10.3 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.149215289069664		[learning rate: 0.00040327]
	Learning Rate: 0.000403274
	LOSS [training: 0.149215289069664 | validation: 0.1639295850983531]
	TIME [epoch: 10.3 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1206394695143386		[learning rate: 0.00040204]
	Learning Rate: 0.000402038
	LOSS [training: 0.1206394695143386 | validation: 0.18638830881913326]
	TIME [epoch: 10.3 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1342526358434841		[learning rate: 0.00040081]
	Learning Rate: 0.000400805
	LOSS [training: 0.1342526358434841 | validation: 0.17375040758626917]
	TIME [epoch: 10.3 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11804283298872267		[learning rate: 0.00039958]
	Learning Rate: 0.000399577
	LOSS [training: 0.11804283298872267 | validation: 0.16577254510762152]
	TIME [epoch: 10.3 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1536579518944185		[learning rate: 0.00039835]
	Learning Rate: 0.000398352
	LOSS [training: 0.1536579518944185 | validation: 0.22180850862260562]
	TIME [epoch: 10.3 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17168079346695983		[learning rate: 0.00039713]
	Learning Rate: 0.000397131
	LOSS [training: 0.17168079346695983 | validation: 0.16626944822402756]
	TIME [epoch: 10.3 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11483294460051678		[learning rate: 0.00039591]
	Learning Rate: 0.000395913
	LOSS [training: 0.11483294460051678 | validation: 0.16490004901574723]
	TIME [epoch: 10.3 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13361799534811064		[learning rate: 0.0003947]
	Learning Rate: 0.0003947
	LOSS [training: 0.13361799534811064 | validation: 0.17823394210984095]
	TIME [epoch: 10.3 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13720998825911343		[learning rate: 0.00039349]
	Learning Rate: 0.00039349
	LOSS [training: 0.13720998825911343 | validation: 0.16652689675942053]
	TIME [epoch: 10.3 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1250036728126221		[learning rate: 0.00039228]
	Learning Rate: 0.000392283
	LOSS [training: 0.1250036728126221 | validation: 0.18504571703027392]
	TIME [epoch: 10.3 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13406479496819956		[learning rate: 0.00039108]
	Learning Rate: 0.000391081
	LOSS [training: 0.13406479496819956 | validation: 0.14102936820313938]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1556.pth
	Model improved!!!
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12272197834518099		[learning rate: 0.00038988]
	Learning Rate: 0.000389882
	LOSS [training: 0.12272197834518099 | validation: 0.16311468826637018]
	TIME [epoch: 10.3 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12875415014591637		[learning rate: 0.00038869]
	Learning Rate: 0.000388687
	LOSS [training: 0.12875415014591637 | validation: 0.1417706851601123]
	TIME [epoch: 10.3 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12375504470132341		[learning rate: 0.0003875]
	Learning Rate: 0.000387495
	LOSS [training: 0.12375504470132341 | validation: 0.16116193667262624]
	TIME [epoch: 10.3 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10772693945624032		[learning rate: 0.00038631]
	Learning Rate: 0.000386308
	LOSS [training: 0.10772693945624032 | validation: 0.15222918297578136]
	TIME [epoch: 10.3 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12772156248533567		[learning rate: 0.00038512]
	Learning Rate: 0.000385123
	LOSS [training: 0.12772156248533567 | validation: 0.17589694903867176]
	TIME [epoch: 10.3 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11953680617261475		[learning rate: 0.00038394]
	Learning Rate: 0.000383943
	LOSS [training: 0.11953680617261475 | validation: 0.16806227723986908]
	TIME [epoch: 10.3 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1323869385829725		[learning rate: 0.00038277]
	Learning Rate: 0.000382766
	LOSS [training: 0.1323869385829725 | validation: 0.16356562613778497]
	TIME [epoch: 10.3 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.165658794650389		[learning rate: 0.00038159]
	Learning Rate: 0.000381593
	LOSS [training: 0.165658794650389 | validation: 0.17153690808621552]
	TIME [epoch: 10.3 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11846785947792368		[learning rate: 0.00038042]
	Learning Rate: 0.000380423
	LOSS [training: 0.11846785947792368 | validation: 0.16627966572658487]
	TIME [epoch: 10.3 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1270088112192132		[learning rate: 0.00037926]
	Learning Rate: 0.000379257
	LOSS [training: 0.1270088112192132 | validation: 0.15537184149293412]
	TIME [epoch: 10.3 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1302504224760313		[learning rate: 0.00037809]
	Learning Rate: 0.000378094
	LOSS [training: 0.1302504224760313 | validation: 0.1588947862269896]
	TIME [epoch: 10.3 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12347605673599875		[learning rate: 0.00037694]
	Learning Rate: 0.000376935
	LOSS [training: 0.12347605673599875 | validation: 0.1497319635786027]
	TIME [epoch: 10.3 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13095981092393993		[learning rate: 0.00037578]
	Learning Rate: 0.00037578
	LOSS [training: 0.13095981092393993 | validation: 0.1875517456573982]
	TIME [epoch: 10.3 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1688629951722086		[learning rate: 0.00037463]
	Learning Rate: 0.000374628
	LOSS [training: 0.1688629951722086 | validation: 0.17791968894409924]
	TIME [epoch: 10.3 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13278969503576582		[learning rate: 0.00037348]
	Learning Rate: 0.000373479
	LOSS [training: 0.13278969503576582 | validation: 0.1828764276213822]
	TIME [epoch: 10.3 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16427170793800056		[learning rate: 0.00037233]
	Learning Rate: 0.000372335
	LOSS [training: 0.16427170793800056 | validation: 0.19790915830151024]
	TIME [epoch: 10.3 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15789205195882708		[learning rate: 0.00037119]
	Learning Rate: 0.000371193
	LOSS [training: 0.15789205195882708 | validation: 0.16931717622596287]
	TIME [epoch: 10.3 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13016160959227718		[learning rate: 0.00037006]
	Learning Rate: 0.000370055
	LOSS [training: 0.13016160959227718 | validation: 0.16215000523402862]
	TIME [epoch: 10.3 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13309938784981473		[learning rate: 0.00036892]
	Learning Rate: 0.000368921
	LOSS [training: 0.13309938784981473 | validation: 0.15775857209043756]
	TIME [epoch: 10.3 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1204387078818577		[learning rate: 0.00036779]
	Learning Rate: 0.00036779
	LOSS [training: 0.1204387078818577 | validation: 0.1676327606908696]
	TIME [epoch: 10.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12427822563029882		[learning rate: 0.00036666]
	Learning Rate: 0.000366663
	LOSS [training: 0.12427822563029882 | validation: 0.16694850572352896]
	TIME [epoch: 10.3 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17279869750481505		[learning rate: 0.00036554]
	Learning Rate: 0.000365539
	LOSS [training: 0.17279869750481505 | validation: 0.24780598389444808]
	TIME [epoch: 10.3 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19282084810807726		[learning rate: 0.00036442]
	Learning Rate: 0.000364418
	LOSS [training: 0.19282084810807726 | validation: 0.1514233703565046]
	TIME [epoch: 10.3 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12757365129897053		[learning rate: 0.0003633]
	Learning Rate: 0.000363301
	LOSS [training: 0.12757365129897053 | validation: 0.1438636858889635]
	TIME [epoch: 10.3 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1313656933607243		[learning rate: 0.00036219]
	Learning Rate: 0.000362187
	LOSS [training: 0.1313656933607243 | validation: 0.15254485103600576]
	TIME [epoch: 10.3 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14718634779254886		[learning rate: 0.00036108]
	Learning Rate: 0.000361077
	LOSS [training: 0.14718634779254886 | validation: 0.20487628067521266]
	TIME [epoch: 10.3 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18149053474828108		[learning rate: 0.00035997]
	Learning Rate: 0.00035997
	LOSS [training: 0.18149053474828108 | validation: 0.2688444165065443]
	TIME [epoch: 10.3 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1655391037025346		[learning rate: 0.00035887]
	Learning Rate: 0.000358867
	LOSS [training: 0.1655391037025346 | validation: 0.188739930020366]
	TIME [epoch: 10.3 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.145861304810563		[learning rate: 0.00035777]
	Learning Rate: 0.000357767
	LOSS [training: 0.145861304810563 | validation: 0.2234539077540412]
	TIME [epoch: 10.3 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1543606065065381		[learning rate: 0.00035667]
	Learning Rate: 0.00035667
	LOSS [training: 0.1543606065065381 | validation: 0.1828224705812289]
	TIME [epoch: 10.3 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.156813966517217		[learning rate: 0.00035558]
	Learning Rate: 0.000355577
	LOSS [training: 0.156813966517217 | validation: 0.14306688470704046]
	TIME [epoch: 10.3 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12862561343574397		[learning rate: 0.00035449]
	Learning Rate: 0.000354487
	LOSS [training: 0.12862561343574397 | validation: 0.19485812645945103]
	TIME [epoch: 10.3 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13605394656073014		[learning rate: 0.0003534]
	Learning Rate: 0.0003534
	LOSS [training: 0.13605394656073014 | validation: 0.1639821206631216]
	TIME [epoch: 10.3 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11731071259876243		[learning rate: 0.00035232]
	Learning Rate: 0.000352317
	LOSS [training: 0.11731071259876243 | validation: 0.15558556984956173]
	TIME [epoch: 10.3 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1115213701838194		[learning rate: 0.00035124]
	Learning Rate: 0.000351237
	LOSS [training: 0.1115213701838194 | validation: 0.1836146399554673]
	TIME [epoch: 10.3 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13141325814459673		[learning rate: 0.00035016]
	Learning Rate: 0.00035016
	LOSS [training: 0.13141325814459673 | validation: 0.17561222900692208]
	TIME [epoch: 10.3 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12664371350361106		[learning rate: 0.00034909]
	Learning Rate: 0.000349087
	LOSS [training: 0.12664371350361106 | validation: 0.193640948303553]
	TIME [epoch: 10.3 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1353719847277647		[learning rate: 0.00034802]
	Learning Rate: 0.000348017
	LOSS [training: 0.1353719847277647 | validation: 0.17771091192363378]
	TIME [epoch: 10.3 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.159518385722152		[learning rate: 0.00034695]
	Learning Rate: 0.00034695
	LOSS [training: 0.159518385722152 | validation: 0.1871951637046166]
	TIME [epoch: 10.3 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14270725693362268		[learning rate: 0.00034589]
	Learning Rate: 0.000345886
	LOSS [training: 0.14270725693362268 | validation: 0.16968189415956286]
	TIME [epoch: 10.3 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16768428453917322		[learning rate: 0.00034483]
	Learning Rate: 0.000344826
	LOSS [training: 0.16768428453917322 | validation: 0.15512209226467485]
	TIME [epoch: 10.3 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15681442542136484		[learning rate: 0.00034377]
	Learning Rate: 0.000343769
	LOSS [training: 0.15681442542136484 | validation: 0.19121772222476857]
	TIME [epoch: 10.3 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14680575992768424		[learning rate: 0.00034272]
	Learning Rate: 0.000342715
	LOSS [training: 0.14680575992768424 | validation: 0.13723200855685602]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1599.pth
	Model improved!!!
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11748945553297363		[learning rate: 0.00034166]
	Learning Rate: 0.000341665
	LOSS [training: 0.11748945553297363 | validation: 0.1383870747485614]
	TIME [epoch: 10.3 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13166543623086663		[learning rate: 0.00034062]
	Learning Rate: 0.000340617
	LOSS [training: 0.13166543623086663 | validation: 0.1654910948127196]
	TIME [epoch: 10.3 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15599818254767084		[learning rate: 0.00033957]
	Learning Rate: 0.000339573
	LOSS [training: 0.15599818254767084 | validation: 0.17593106492141591]
	TIME [epoch: 10.3 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13545834427283443		[learning rate: 0.00033853]
	Learning Rate: 0.000338532
	LOSS [training: 0.13545834427283443 | validation: 0.1378758528518505]
	TIME [epoch: 10.3 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.127274567633694		[learning rate: 0.00033749]
	Learning Rate: 0.000337494
	LOSS [training: 0.127274567633694 | validation: 0.1466967382377807]
	TIME [epoch: 10.3 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12098841954262915		[learning rate: 0.00033646]
	Learning Rate: 0.00033646
	LOSS [training: 0.12098841954262915 | validation: 0.14353381961150558]
	TIME [epoch: 10.3 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14212901468537537		[learning rate: 0.00033543]
	Learning Rate: 0.000335428
	LOSS [training: 0.14212901468537537 | validation: 0.15537707096174005]
	TIME [epoch: 10.3 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1500664716063402		[learning rate: 0.0003344]
	Learning Rate: 0.0003344
	LOSS [training: 0.1500664716063402 | validation: 0.1783200596020807]
	TIME [epoch: 10.3 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14351815564300785		[learning rate: 0.00033338]
	Learning Rate: 0.000333375
	LOSS [training: 0.14351815564300785 | validation: 0.14512666901578944]
	TIME [epoch: 10.3 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13810972701176266		[learning rate: 0.00033235]
	Learning Rate: 0.000332353
	LOSS [training: 0.13810972701176266 | validation: 0.15412454167473497]
	TIME [epoch: 10.3 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12718954711482217		[learning rate: 0.00033133]
	Learning Rate: 0.000331334
	LOSS [training: 0.12718954711482217 | validation: 0.1753248632215984]
	TIME [epoch: 10.3 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12363417163728903		[learning rate: 0.00033032]
	Learning Rate: 0.000330319
	LOSS [training: 0.12363417163728903 | validation: 0.13771706290227215]
	TIME [epoch: 10.3 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1224297123502317		[learning rate: 0.00032931]
	Learning Rate: 0.000329306
	LOSS [training: 0.1224297123502317 | validation: 0.14295335343637133]
	TIME [epoch: 10.3 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11486411891999644		[learning rate: 0.0003283]
	Learning Rate: 0.000328297
	LOSS [training: 0.11486411891999644 | validation: 0.17672951088929065]
	TIME [epoch: 10.3 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1367853679778323		[learning rate: 0.00032729]
	Learning Rate: 0.00032729
	LOSS [training: 0.1367853679778323 | validation: 0.15768811369419722]
	TIME [epoch: 10.3 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11334907976465589		[learning rate: 0.00032629]
	Learning Rate: 0.000326287
	LOSS [training: 0.11334907976465589 | validation: 0.18040398184778098]
	TIME [epoch: 10.3 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1348833078577897		[learning rate: 0.00032529]
	Learning Rate: 0.000325287
	LOSS [training: 0.1348833078577897 | validation: 0.14341948799949342]
	TIME [epoch: 10.3 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12117370086075982		[learning rate: 0.00032429]
	Learning Rate: 0.00032429
	LOSS [training: 0.12117370086075982 | validation: 0.13967724375864196]
	TIME [epoch: 10.3 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10774458785537763		[learning rate: 0.0003233]
	Learning Rate: 0.000323296
	LOSS [training: 0.10774458785537763 | validation: 0.12123064170553705]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1618.pth
	Model improved!!!
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11624190047768754		[learning rate: 0.0003223]
	Learning Rate: 0.000322305
	LOSS [training: 0.11624190047768754 | validation: 0.13656719930379302]
	TIME [epoch: 10.3 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13025950476550907		[learning rate: 0.00032132]
	Learning Rate: 0.000321317
	LOSS [training: 0.13025950476550907 | validation: 0.15770890189484427]
	TIME [epoch: 10.3 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1263538541609856		[learning rate: 0.00032033]
	Learning Rate: 0.000320332
	LOSS [training: 0.1263538541609856 | validation: 0.17311023312074356]
	TIME [epoch: 10.3 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13104996439166694		[learning rate: 0.00031935]
	Learning Rate: 0.00031935
	LOSS [training: 0.13104996439166694 | validation: 0.17252679901933354]
	TIME [epoch: 10.3 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12612393235183755		[learning rate: 0.00031837]
	Learning Rate: 0.000318371
	LOSS [training: 0.12612393235183755 | validation: 0.1571926270496821]
	TIME [epoch: 10.3 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11803782587773334		[learning rate: 0.00031739]
	Learning Rate: 0.000317395
	LOSS [training: 0.11803782587773334 | validation: 0.19316053658690038]
	TIME [epoch: 10.3 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14471709816982783		[learning rate: 0.00031642]
	Learning Rate: 0.000316422
	LOSS [training: 0.14471709816982783 | validation: 0.16987124958176267]
	TIME [epoch: 10.3 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13656212721012811		[learning rate: 0.00031545]
	Learning Rate: 0.000315452
	LOSS [training: 0.13656212721012811 | validation: 0.167228948942676]
	TIME [epoch: 10.3 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14268085464020958		[learning rate: 0.00031449]
	Learning Rate: 0.000314485
	LOSS [training: 0.14268085464020958 | validation: 0.19793509058378306]
	TIME [epoch: 10.3 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12896341762589644		[learning rate: 0.00031352]
	Learning Rate: 0.000313521
	LOSS [training: 0.12896341762589644 | validation: 0.18927455143095032]
	TIME [epoch: 10.3 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12066693504522472		[learning rate: 0.00031256]
	Learning Rate: 0.00031256
	LOSS [training: 0.12066693504522472 | validation: 0.16779831604956388]
	TIME [epoch: 10.3 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13285650973868768		[learning rate: 0.0003116]
	Learning Rate: 0.000311602
	LOSS [training: 0.13285650973868768 | validation: 0.1618003739268869]
	TIME [epoch: 10.3 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12966465827132928		[learning rate: 0.00031065]
	Learning Rate: 0.000310647
	LOSS [training: 0.12966465827132928 | validation: 0.17436369952550507]
	TIME [epoch: 10.3 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1315972323682519		[learning rate: 0.00030969]
	Learning Rate: 0.000309694
	LOSS [training: 0.1315972323682519 | validation: 0.1558182440741668]
	TIME [epoch: 10.3 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12227390676965695		[learning rate: 0.00030874]
	Learning Rate: 0.000308745
	LOSS [training: 0.12227390676965695 | validation: 0.170635882617074]
	TIME [epoch: 10.3 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12419574888557401		[learning rate: 0.0003078]
	Learning Rate: 0.000307799
	LOSS [training: 0.12419574888557401 | validation: 0.18051477423207046]
	TIME [epoch: 10.3 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12142631981434922		[learning rate: 0.00030686]
	Learning Rate: 0.000306855
	LOSS [training: 0.12142631981434922 | validation: 0.16191218386637374]
	TIME [epoch: 10.3 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12048044024027035		[learning rate: 0.00030591]
	Learning Rate: 0.000305914
	LOSS [training: 0.12048044024027035 | validation: 0.16856657401725986]
	TIME [epoch: 10.3 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12908709292897613		[learning rate: 0.00030498]
	Learning Rate: 0.000304977
	LOSS [training: 0.12908709292897613 | validation: 0.1612993422095226]
	TIME [epoch: 10.3 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11961132710354094		[learning rate: 0.00030404]
	Learning Rate: 0.000304042
	LOSS [training: 0.11961132710354094 | validation: 0.14486315058609056]
	TIME [epoch: 10.3 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11006054256492415		[learning rate: 0.00030311]
	Learning Rate: 0.00030311
	LOSS [training: 0.11006054256492415 | validation: 0.1439541788387766]
	TIME [epoch: 10.3 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10635468318941466		[learning rate: 0.00030218]
	Learning Rate: 0.000302181
	LOSS [training: 0.10635468318941466 | validation: 0.13863074675377457]
	TIME [epoch: 10.3 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10726458782068653		[learning rate: 0.00030125]
	Learning Rate: 0.000301254
	LOSS [training: 0.10726458782068653 | validation: 0.1314569034715383]
	TIME [epoch: 10.3 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11803319166070718		[learning rate: 0.00030033]
	Learning Rate: 0.000300331
	LOSS [training: 0.11803319166070718 | validation: 0.1551298625464941]
	TIME [epoch: 10.3 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1289103622308554		[learning rate: 0.00029941]
	Learning Rate: 0.00029941
	LOSS [training: 0.1289103622308554 | validation: 0.14811404075373752]
	TIME [epoch: 10.3 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10561144617808879		[learning rate: 0.00029849]
	Learning Rate: 0.000298492
	LOSS [training: 0.10561144617808879 | validation: 0.14777477395853705]
	TIME [epoch: 10.3 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1332761845191372		[learning rate: 0.00029758]
	Learning Rate: 0.000297577
	LOSS [training: 0.1332761845191372 | validation: 0.1635955898371757]
	TIME [epoch: 10.3 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1205459109235694		[learning rate: 0.00029667]
	Learning Rate: 0.000296665
	LOSS [training: 0.1205459109235694 | validation: 0.156256875887267]
	TIME [epoch: 10.3 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10749267327407057		[learning rate: 0.00029576]
	Learning Rate: 0.000295756
	LOSS [training: 0.10749267327407057 | validation: 0.161136968367538]
	TIME [epoch: 10.3 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10893896183341667		[learning rate: 0.00029485]
	Learning Rate: 0.000294849
	LOSS [training: 0.10893896183341667 | validation: 0.160317464416105]
	TIME [epoch: 10.3 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11771420938231132		[learning rate: 0.00029395]
	Learning Rate: 0.000293945
	LOSS [training: 0.11771420938231132 | validation: 0.17602303254065704]
	TIME [epoch: 10.3 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11624331363914062		[learning rate: 0.00029304]
	Learning Rate: 0.000293044
	LOSS [training: 0.11624331363914062 | validation: 0.14322391735417944]
	TIME [epoch: 10.3 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10672233869881143		[learning rate: 0.00029215]
	Learning Rate: 0.000292146
	LOSS [training: 0.10672233869881143 | validation: 0.1499117464836309]
	TIME [epoch: 10.3 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10157029750151832		[learning rate: 0.00029125]
	Learning Rate: 0.00029125
	LOSS [training: 0.10157029750151832 | validation: 0.13304975021152754]
	TIME [epoch: 10.3 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11698227647022799		[learning rate: 0.00029036]
	Learning Rate: 0.000290358
	LOSS [training: 0.11698227647022799 | validation: 0.16945040871086392]
	TIME [epoch: 10.3 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12290577078809473		[learning rate: 0.00028947]
	Learning Rate: 0.000289468
	LOSS [training: 0.12290577078809473 | validation: 0.14661582220741765]
	TIME [epoch: 10.3 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11531009083494939		[learning rate: 0.00028858]
	Learning Rate: 0.00028858
	LOSS [training: 0.11531009083494939 | validation: 0.14752578005023062]
	TIME [epoch: 10.3 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12659479877656984		[learning rate: 0.0002877]
	Learning Rate: 0.000287696
	LOSS [training: 0.12659479877656984 | validation: 0.15910123327095563]
	TIME [epoch: 10.3 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13656029034623213		[learning rate: 0.00028681]
	Learning Rate: 0.000286814
	LOSS [training: 0.13656029034623213 | validation: 0.17474232510816287]
	TIME [epoch: 10.3 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12056165109947008		[learning rate: 0.00028593]
	Learning Rate: 0.000285935
	LOSS [training: 0.12056165109947008 | validation: 0.14057163248944668]
	TIME [epoch: 10.3 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11355561120666761		[learning rate: 0.00028506]
	Learning Rate: 0.000285058
	LOSS [training: 0.11355561120666761 | validation: 0.15012711591072678]
	TIME [epoch: 10.3 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11506217121721152		[learning rate: 0.00028418]
	Learning Rate: 0.000284184
	LOSS [training: 0.11506217121721152 | validation: 0.16198720483541382]
	TIME [epoch: 10.3 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1110801319584406		[learning rate: 0.00028331]
	Learning Rate: 0.000283313
	LOSS [training: 0.1110801319584406 | validation: 0.1921361947873291]
	TIME [epoch: 10.3 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12233988157048122		[learning rate: 0.00028244]
	Learning Rate: 0.000282445
	LOSS [training: 0.12233988157048122 | validation: 0.13964209348505968]
	TIME [epoch: 10.3 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11774106385000009		[learning rate: 0.00028158]
	Learning Rate: 0.000281579
	LOSS [training: 0.11774106385000009 | validation: 0.1444425732212292]
	TIME [epoch: 10.3 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10866709013483986		[learning rate: 0.00028072]
	Learning Rate: 0.000280716
	LOSS [training: 0.10866709013483986 | validation: 0.1650526264377821]
	TIME [epoch: 10.3 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1493926499756723		[learning rate: 0.00027986]
	Learning Rate: 0.000279855
	LOSS [training: 0.1493926499756723 | validation: 0.1998672912349811]
	TIME [epoch: 10.3 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15293082800015115		[learning rate: 0.000279]
	Learning Rate: 0.000278997
	LOSS [training: 0.15293082800015115 | validation: 0.1659419727035924]
	TIME [epoch: 10.3 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13090520163686106		[learning rate: 0.00027814]
	Learning Rate: 0.000278142
	LOSS [training: 0.13090520163686106 | validation: 0.1499790958752338]
	TIME [epoch: 10.3 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10885922569333617		[learning rate: 0.00027729]
	Learning Rate: 0.000277289
	LOSS [training: 0.10885922569333617 | validation: 0.14778346790541524]
	TIME [epoch: 10.3 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10751758467434006		[learning rate: 0.00027644]
	Learning Rate: 0.000276439
	LOSS [training: 0.10751758467434006 | validation: 0.14052666776776074]
	TIME [epoch: 10.3 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11247795978505756		[learning rate: 0.00027559]
	Learning Rate: 0.000275592
	LOSS [training: 0.11247795978505756 | validation: 0.18471139821648483]
	TIME [epoch: 10.3 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13912972141211769		[learning rate: 0.00027475]
	Learning Rate: 0.000274747
	LOSS [training: 0.13912972141211769 | validation: 0.15132715052243956]
	TIME [epoch: 10.3 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11325392031268841		[learning rate: 0.00027391]
	Learning Rate: 0.000273905
	LOSS [training: 0.11325392031268841 | validation: 0.12140941536422326]
	TIME [epoch: 10.3 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10949480502475362		[learning rate: 0.00027307]
	Learning Rate: 0.000273065
	LOSS [training: 0.10949480502475362 | validation: 0.1246500504263677]
	TIME [epoch: 10.3 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10473338460785726		[learning rate: 0.00027223]
	Learning Rate: 0.000272228
	LOSS [training: 0.10473338460785726 | validation: 0.1383194290253675]
	TIME [epoch: 10.3 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11264515338462595		[learning rate: 0.00027139]
	Learning Rate: 0.000271394
	LOSS [training: 0.11264515338462595 | validation: 0.1479669111215538]
	TIME [epoch: 10.3 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1035071077108061		[learning rate: 0.00027056]
	Learning Rate: 0.000270562
	LOSS [training: 0.1035071077108061 | validation: 0.14076117305661423]
	TIME [epoch: 10.3 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10481249731857326		[learning rate: 0.00026973]
	Learning Rate: 0.000269733
	LOSS [training: 0.10481249731857326 | validation: 0.13126884433344313]
	TIME [epoch: 10.3 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09890831470702995		[learning rate: 0.00026891]
	Learning Rate: 0.000268906
	LOSS [training: 0.09890831470702995 | validation: 0.13116924359016008]
	TIME [epoch: 10.3 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09419541316289451		[learning rate: 0.00026808]
	Learning Rate: 0.000268081
	LOSS [training: 0.09419541316289451 | validation: 0.1442034888943095]
	TIME [epoch: 10.3 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11288615944954242		[learning rate: 0.00026726]
	Learning Rate: 0.00026726
	LOSS [training: 0.11288615944954242 | validation: 0.15523464532482073]
	TIME [epoch: 10.3 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13365194206351177		[learning rate: 0.00026644]
	Learning Rate: 0.00026644
	LOSS [training: 0.13365194206351177 | validation: 0.13377187389392692]
	TIME [epoch: 10.3 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1156977650643258		[learning rate: 0.00026562]
	Learning Rate: 0.000265624
	LOSS [training: 0.1156977650643258 | validation: 0.12631091063913308]
	TIME [epoch: 10.3 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10421557376560975		[learning rate: 0.00026481]
	Learning Rate: 0.000264809
	LOSS [training: 0.10421557376560975 | validation: 0.14282651425491133]
	TIME [epoch: 10.3 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09778725490845688		[learning rate: 0.000264]
	Learning Rate: 0.000263998
	LOSS [training: 0.09778725490845688 | validation: 0.13998579569693728]
	TIME [epoch: 10.3 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10516736455456357		[learning rate: 0.00026319]
	Learning Rate: 0.000263188
	LOSS [training: 0.10516736455456357 | validation: 0.13380455080151454]
	TIME [epoch: 10.3 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11958352999376032		[learning rate: 0.00026238]
	Learning Rate: 0.000262382
	LOSS [training: 0.11958352999376032 | validation: 0.12927369993742324]
	TIME [epoch: 10.3 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11104576211441648		[learning rate: 0.00026158]
	Learning Rate: 0.000261577
	LOSS [training: 0.11104576211441648 | validation: 0.14257303286099546]
	TIME [epoch: 10.3 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10884942433056839		[learning rate: 0.00026078]
	Learning Rate: 0.000260775
	LOSS [training: 0.10884942433056839 | validation: 0.14423731466067635]
	TIME [epoch: 10.3 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12654328330487435		[learning rate: 0.00025998]
	Learning Rate: 0.000259976
	LOSS [training: 0.12654328330487435 | validation: 0.1745996241232438]
	TIME [epoch: 10.3 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.136265806697212		[learning rate: 0.00025918]
	Learning Rate: 0.000259179
	LOSS [training: 0.136265806697212 | validation: 0.14892734824349374]
	TIME [epoch: 10.3 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1350525112515021		[learning rate: 0.00025838]
	Learning Rate: 0.000258385
	LOSS [training: 0.1350525112515021 | validation: 0.1768120974538698]
	TIME [epoch: 10.3 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12789015636823137		[learning rate: 0.00025759]
	Learning Rate: 0.000257593
	LOSS [training: 0.12789015636823137 | validation: 0.18614801814553023]
	TIME [epoch: 10.3 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13432685265784844		[learning rate: 0.0002568]
	Learning Rate: 0.000256803
	LOSS [training: 0.13432685265784844 | validation: 0.16083071548653904]
	TIME [epoch: 10.3 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11060808100060271		[learning rate: 0.00025602]
	Learning Rate: 0.000256016
	LOSS [training: 0.11060808100060271 | validation: 0.16283762316261716]
	TIME [epoch: 10.3 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12591943884204718		[learning rate: 0.00025523]
	Learning Rate: 0.000255231
	LOSS [training: 0.12591943884204718 | validation: 0.1328045002403365]
	TIME [epoch: 10.3 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10796869011553478		[learning rate: 0.00025445]
	Learning Rate: 0.000254449
	LOSS [training: 0.10796869011553478 | validation: 0.15334249971487957]
	TIME [epoch: 10.3 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1139544655692736		[learning rate: 0.00025367]
	Learning Rate: 0.000253669
	LOSS [training: 0.1139544655692736 | validation: 0.15734632437049895]
	TIME [epoch: 10.3 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12995865489333552		[learning rate: 0.00025289]
	Learning Rate: 0.000252891
	LOSS [training: 0.12995865489333552 | validation: 0.1365039949773907]
	TIME [epoch: 10.3 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1122590876067215		[learning rate: 0.00025212]
	Learning Rate: 0.000252116
	LOSS [training: 0.1122590876067215 | validation: 0.14292111287020315]
	TIME [epoch: 10.3 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11268211972213374		[learning rate: 0.00025134]
	Learning Rate: 0.000251343
	LOSS [training: 0.11268211972213374 | validation: 0.14556342905739567]
	TIME [epoch: 10.3 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1294691710698343		[learning rate: 0.00025057]
	Learning Rate: 0.000250572
	LOSS [training: 0.1294691710698343 | validation: 0.1412345372819935]
	TIME [epoch: 10.3 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12962067233815913		[learning rate: 0.0002498]
	Learning Rate: 0.000249804
	LOSS [training: 0.12962067233815913 | validation: 0.14851784025542425]
	TIME [epoch: 10.3 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12169045129613001		[learning rate: 0.00024904]
	Learning Rate: 0.000249039
	LOSS [training: 0.12169045129613001 | validation: 0.13966040724546117]
	TIME [epoch: 10.3 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10397117195261865		[learning rate: 0.00024828]
	Learning Rate: 0.000248275
	LOSS [training: 0.10397117195261865 | validation: 0.13925777874472328]
	TIME [epoch: 10.3 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10415315382539259		[learning rate: 0.00024751]
	Learning Rate: 0.000247514
	LOSS [training: 0.10415315382539259 | validation: 0.1536173607323298]
	TIME [epoch: 10.3 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10834570251041162		[learning rate: 0.00024676]
	Learning Rate: 0.000246755
	LOSS [training: 0.10834570251041162 | validation: 0.14873690891067182]
	TIME [epoch: 10.3 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10636311299477492		[learning rate: 0.000246]
	Learning Rate: 0.000245999
	LOSS [training: 0.10636311299477492 | validation: 0.13077509532969775]
	TIME [epoch: 10.3 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11905939737351516		[learning rate: 0.00024524]
	Learning Rate: 0.000245245
	LOSS [training: 0.11905939737351516 | validation: 0.1379194449061372]
	TIME [epoch: 10.3 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10308569739838987		[learning rate: 0.00024449]
	Learning Rate: 0.000244493
	LOSS [training: 0.10308569739838987 | validation: 0.14882433811381168]
	TIME [epoch: 10.3 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11005297411753898		[learning rate: 0.00024374]
	Learning Rate: 0.000243744
	LOSS [training: 0.11005297411753898 | validation: 0.14429526614839291]
	TIME [epoch: 10.3 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11530115530048117		[learning rate: 0.000243]
	Learning Rate: 0.000242996
	LOSS [training: 0.11530115530048117 | validation: 0.1502343105421132]
	TIME [epoch: 10.3 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10749903688447697		[learning rate: 0.00024225]
	Learning Rate: 0.000242252
	LOSS [training: 0.10749903688447697 | validation: 0.14972948482210738]
	TIME [epoch: 10.3 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1059710031082963		[learning rate: 0.00024151]
	Learning Rate: 0.000241509
	LOSS [training: 0.1059710031082963 | validation: 0.16234310541101352]
	TIME [epoch: 10.3 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12852070524766473		[learning rate: 0.00024077]
	Learning Rate: 0.000240769
	LOSS [training: 0.12852070524766473 | validation: 0.17678591137669009]
	TIME [epoch: 10.3 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12160604808302569		[learning rate: 0.00024003]
	Learning Rate: 0.000240031
	LOSS [training: 0.12160604808302569 | validation: 0.17220212568923984]
	TIME [epoch: 10.3 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11981684417404274		[learning rate: 0.00023929]
	Learning Rate: 0.000239295
	LOSS [training: 0.11981684417404274 | validation: 0.14370869394218028]
	TIME [epoch: 10.3 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11947247138992764		[learning rate: 0.00023856]
	Learning Rate: 0.000238561
	LOSS [training: 0.11947247138992764 | validation: 0.1683440152610549]
	TIME [epoch: 10.3 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12325910158799772		[learning rate: 0.00023783]
	Learning Rate: 0.00023783
	LOSS [training: 0.12325910158799772 | validation: 0.1540627572042114]
	TIME [epoch: 10.3 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10238924483032404		[learning rate: 0.0002371]
	Learning Rate: 0.000237101
	LOSS [training: 0.10238924483032404 | validation: 0.15728294305011672]
	TIME [epoch: 10.3 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10556954881940306		[learning rate: 0.00023637]
	Learning Rate: 0.000236374
	LOSS [training: 0.10556954881940306 | validation: 0.14486093533488245]
	TIME [epoch: 10.3 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10997809179962448		[learning rate: 0.00023565]
	Learning Rate: 0.00023565
	LOSS [training: 0.10997809179962448 | validation: 0.15682221543656572]
	TIME [epoch: 10.3 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12055262140105971		[learning rate: 0.00023493]
	Learning Rate: 0.000234927
	LOSS [training: 0.12055262140105971 | validation: 0.1697291499564848]
	TIME [epoch: 10.3 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12879264995330425		[learning rate: 0.00023421]
	Learning Rate: 0.000234207
	LOSS [training: 0.12879264995330425 | validation: 0.1671041681179933]
	TIME [epoch: 10.3 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13463455911125724		[learning rate: 0.00023349]
	Learning Rate: 0.000233489
	LOSS [training: 0.13463455911125724 | validation: 0.16381915183972928]
	TIME [epoch: 10.3 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12252982420188507		[learning rate: 0.00023277]
	Learning Rate: 0.000232773
	LOSS [training: 0.12252982420188507 | validation: 0.16860796742241735]
	TIME [epoch: 10.3 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11002956723462982		[learning rate: 0.00023206]
	Learning Rate: 0.00023206
	LOSS [training: 0.11002956723462982 | validation: 0.15785683029982403]
	TIME [epoch: 10.3 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1167359218453398		[learning rate: 0.00023135]
	Learning Rate: 0.000231348
	LOSS [training: 0.1167359218453398 | validation: 0.154380628765799]
	TIME [epoch: 10.3 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12459924138627895		[learning rate: 0.00023064]
	Learning Rate: 0.000230639
	LOSS [training: 0.12459924138627895 | validation: 0.15609298333115937]
	TIME [epoch: 10.3 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12677339612058794		[learning rate: 0.00022993]
	Learning Rate: 0.000229932
	LOSS [training: 0.12677339612058794 | validation: 0.1581393760340539]
	TIME [epoch: 10.3 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11817811739157646		[learning rate: 0.00022923]
	Learning Rate: 0.000229227
	LOSS [training: 0.11817811739157646 | validation: 0.1591598423684403]
	TIME [epoch: 10.3 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12232194661118914		[learning rate: 0.00022852]
	Learning Rate: 0.000228525
	LOSS [training: 0.12232194661118914 | validation: 0.15623559240404383]
	TIME [epoch: 10.3 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12213482526823463		[learning rate: 0.00022782]
	Learning Rate: 0.000227824
	LOSS [training: 0.12213482526823463 | validation: 0.15222100745190845]
	TIME [epoch: 10.3 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12073795707827198		[learning rate: 0.00022713]
	Learning Rate: 0.000227126
	LOSS [training: 0.12073795707827198 | validation: 0.16147763284458724]
	TIME [epoch: 10.3 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1292464956639023		[learning rate: 0.00022643]
	Learning Rate: 0.00022643
	LOSS [training: 0.1292464956639023 | validation: 0.17163875690702043]
	TIME [epoch: 10.3 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14107320137570042		[learning rate: 0.00022574]
	Learning Rate: 0.000225736
	LOSS [training: 0.14107320137570042 | validation: 0.16788538580227957]
	TIME [epoch: 10.3 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13017902385767713		[learning rate: 0.00022504]
	Learning Rate: 0.000225044
	LOSS [training: 0.13017902385767713 | validation: 0.13429603724547998]
	TIME [epoch: 10.3 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1156639012386919		[learning rate: 0.00022435]
	Learning Rate: 0.000224354
	LOSS [training: 0.1156639012386919 | validation: 0.1535464322803171]
	TIME [epoch: 10.3 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11983364923738178		[learning rate: 0.00022367]
	Learning Rate: 0.000223666
	LOSS [training: 0.11983364923738178 | validation: 0.13998608382759073]
	TIME [epoch: 10.3 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09917151363352157		[learning rate: 0.00022298]
	Learning Rate: 0.00022298
	LOSS [training: 0.09917151363352157 | validation: 0.160026821472093]
	TIME [epoch: 10.3 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10615015858836215		[learning rate: 0.0002223]
	Learning Rate: 0.000222297
	LOSS [training: 0.10615015858836215 | validation: 0.1423173306783833]
	TIME [epoch: 10.3 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11993751383961937		[learning rate: 0.00022162]
	Learning Rate: 0.000221615
	LOSS [training: 0.11993751383961937 | validation: 0.1394560989208672]
	TIME [epoch: 10.3 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1076050572273696		[learning rate: 0.00022094]
	Learning Rate: 0.000220936
	LOSS [training: 0.1076050572273696 | validation: 0.14274820687107664]
	TIME [epoch: 10.3 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10544192197918882		[learning rate: 0.00022026]
	Learning Rate: 0.000220259
	LOSS [training: 0.10544192197918882 | validation: 0.18024112644953946]
	TIME [epoch: 10.3 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1277647090108335		[learning rate: 0.00021958]
	Learning Rate: 0.000219584
	LOSS [training: 0.1277647090108335 | validation: 0.1552680479055173]
	TIME [epoch: 10.3 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1202200122915621		[learning rate: 0.00021891]
	Learning Rate: 0.000218911
	LOSS [training: 0.1202200122915621 | validation: 0.15690416469543472]
	TIME [epoch: 10.3 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12690772549939777		[learning rate: 0.00021824]
	Learning Rate: 0.000218239
	LOSS [training: 0.12690772549939777 | validation: 0.15750876632216243]
	TIME [epoch: 10.3 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11354903368414		[learning rate: 0.00021757]
	Learning Rate: 0.00021757
	LOSS [training: 0.11354903368414 | validation: 0.1488249082087496]
	TIME [epoch: 10.3 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11796512089438962		[learning rate: 0.0002169]
	Learning Rate: 0.000216904
	LOSS [training: 0.11796512089438962 | validation: 0.13994609392222698]
	TIME [epoch: 10.3 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09461905947142832		[learning rate: 0.00021624]
	Learning Rate: 0.000216239
	LOSS [training: 0.09461905947142832 | validation: 0.14091509121813495]
	TIME [epoch: 10.3 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11056217669399493		[learning rate: 0.00021558]
	Learning Rate: 0.000215576
	LOSS [training: 0.11056217669399493 | validation: 0.1619378525217898]
	TIME [epoch: 10.3 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11056824069302158		[learning rate: 0.00021491]
	Learning Rate: 0.000214915
	LOSS [training: 0.11056824069302158 | validation: 0.1596001930431835]
	TIME [epoch: 10.3 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11002414833935754		[learning rate: 0.00021426]
	Learning Rate: 0.000214256
	LOSS [training: 0.11002414833935754 | validation: 0.16471288949670104]
	TIME [epoch: 10.3 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1179808179003801		[learning rate: 0.0002136]
	Learning Rate: 0.000213599
	LOSS [training: 0.1179808179003801 | validation: 0.1676378162938814]
	TIME [epoch: 10.3 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11583981408084632		[learning rate: 0.00021294]
	Learning Rate: 0.000212945
	LOSS [training: 0.11583981408084632 | validation: 0.17085345631677604]
	TIME [epoch: 10.3 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11894544458473581		[learning rate: 0.00021229]
	Learning Rate: 0.000212292
	LOSS [training: 0.11894544458473581 | validation: 0.14791712893491255]
	TIME [epoch: 10.3 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11049980295347113		[learning rate: 0.00021164]
	Learning Rate: 0.000211641
	LOSS [training: 0.11049980295347113 | validation: 0.155054584960308]
	TIME [epoch: 10.3 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10738919186020408		[learning rate: 0.00021099]
	Learning Rate: 0.000210992
	LOSS [training: 0.10738919186020408 | validation: 0.15406518740132796]
	TIME [epoch: 10.3 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10402082923593343		[learning rate: 0.00021035]
	Learning Rate: 0.000210346
	LOSS [training: 0.10402082923593343 | validation: 0.1526955632934957]
	TIME [epoch: 10.3 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0987131158956325		[learning rate: 0.0002097]
	Learning Rate: 0.000209701
	LOSS [training: 0.0987131158956325 | validation: 0.15834798173581618]
	TIME [epoch: 10.3 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11544418093945641		[learning rate: 0.00020906]
	Learning Rate: 0.000209058
	LOSS [training: 0.11544418093945641 | validation: 0.13671523889816833]
	TIME [epoch: 10.3 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09726663091031593		[learning rate: 0.00020842]
	Learning Rate: 0.000208417
	LOSS [training: 0.09726663091031593 | validation: 0.1610139963311083]
	TIME [epoch: 10.3 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10143719656021559		[learning rate: 0.00020778]
	Learning Rate: 0.000207778
	LOSS [training: 0.10143719656021559 | validation: 0.14058314288716134]
	TIME [epoch: 10.3 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11212776064444145		[learning rate: 0.00020714]
	Learning Rate: 0.000207141
	LOSS [training: 0.11212776064444145 | validation: 0.15860802178820105]
	TIME [epoch: 10.3 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1356771902248098		[learning rate: 0.00020651]
	Learning Rate: 0.000206506
	LOSS [training: 0.1356771902248098 | validation: 0.1569359064179144]
	TIME [epoch: 10.3 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1138007045743737		[learning rate: 0.00020587]
	Learning Rate: 0.000205873
	LOSS [training: 0.1138007045743737 | validation: 0.14149240664353746]
	TIME [epoch: 10.3 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10810113587091505		[learning rate: 0.00020524]
	Learning Rate: 0.000205242
	LOSS [training: 0.10810113587091505 | validation: 0.1446344768222801]
	TIME [epoch: 10.3 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11795227442767935		[learning rate: 0.00020461]
	Learning Rate: 0.000204613
	LOSS [training: 0.11795227442767935 | validation: 0.17028834235427398]
	TIME [epoch: 10.3 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1345595018264171		[learning rate: 0.00020399]
	Learning Rate: 0.000203986
	LOSS [training: 0.1345595018264171 | validation: 0.19795740914021864]
	TIME [epoch: 10.3 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15092149328923682		[learning rate: 0.00020336]
	Learning Rate: 0.00020336
	LOSS [training: 0.15092149328923682 | validation: 0.21492535320692505]
	TIME [epoch: 10.3 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1478959080492369		[learning rate: 0.00020274]
	Learning Rate: 0.000202737
	LOSS [training: 0.1478959080492369 | validation: 0.19937452315201115]
	TIME [epoch: 10.3 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12636619811352207		[learning rate: 0.00020212]
	Learning Rate: 0.000202116
	LOSS [training: 0.12636619811352207 | validation: 0.1729157112244743]
	TIME [epoch: 10.3 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1262885484039369		[learning rate: 0.0002015]
	Learning Rate: 0.000201496
	LOSS [training: 0.1262885484039369 | validation: 0.1591348467741439]
	TIME [epoch: 10.3 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11779704627796492		[learning rate: 0.00020088]
	Learning Rate: 0.000200878
	LOSS [training: 0.11779704627796492 | validation: 0.15184283283006317]
	TIME [epoch: 10.3 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12803961509466621		[learning rate: 0.00020026]
	Learning Rate: 0.000200263
	LOSS [training: 0.12803961509466621 | validation: 0.1654214546509946]
	TIME [epoch: 10.3 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11777354169343876		[learning rate: 0.00019965]
	Learning Rate: 0.000199649
	LOSS [training: 0.11777354169343876 | validation: 0.15304446514013448]
	TIME [epoch: 10.3 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1252712847584388		[learning rate: 0.00019904]
	Learning Rate: 0.000199037
	LOSS [training: 0.1252712847584388 | validation: 0.15422294974926165]
	TIME [epoch: 10.3 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11344062992418924		[learning rate: 0.00019843]
	Learning Rate: 0.000198427
	LOSS [training: 0.11344062992418924 | validation: 0.17422998720225524]
	TIME [epoch: 10.3 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1350946653582727		[learning rate: 0.00019782]
	Learning Rate: 0.000197818
	LOSS [training: 0.1350946653582727 | validation: 0.13916434971269395]
	TIME [epoch: 10.3 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10543689184118146		[learning rate: 0.00019721]
	Learning Rate: 0.000197212
	LOSS [training: 0.10543689184118146 | validation: 0.1500926886079504]
	TIME [epoch: 10.3 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11226616765896713		[learning rate: 0.00019661]
	Learning Rate: 0.000196607
	LOSS [training: 0.11226616765896713 | validation: 0.15415206686698243]
	TIME [epoch: 10.3 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10926498374831017		[learning rate: 0.000196]
	Learning Rate: 0.000196005
	LOSS [training: 0.10926498374831017 | validation: 0.1379536589160373]
	TIME [epoch: 10.3 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11135277395266477		[learning rate: 0.0001954]
	Learning Rate: 0.000195404
	LOSS [training: 0.11135277395266477 | validation: 0.15375621761870692]
	TIME [epoch: 10.3 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1128389729254704		[learning rate: 0.0001948]
	Learning Rate: 0.000194805
	LOSS [training: 0.1128389729254704 | validation: 0.14819900747924228]
	TIME [epoch: 10.3 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11133159647768447		[learning rate: 0.00019421]
	Learning Rate: 0.000194208
	LOSS [training: 0.11133159647768447 | validation: 0.13984141400662592]
	TIME [epoch: 10.3 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1063909659653732		[learning rate: 0.00019361]
	Learning Rate: 0.000193612
	LOSS [training: 0.1063909659653732 | validation: 0.1439908671806244]
	TIME [epoch: 10.3 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10424472497486596		[learning rate: 0.00019302]
	Learning Rate: 0.000193019
	LOSS [training: 0.10424472497486596 | validation: 0.13934148935194152]
	TIME [epoch: 10.3 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10910274339272874		[learning rate: 0.00019243]
	Learning Rate: 0.000192427
	LOSS [training: 0.10910274339272874 | validation: 0.1405285349336683]
	TIME [epoch: 10.3 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10040959492059791		[learning rate: 0.00019184]
	Learning Rate: 0.000191837
	LOSS [training: 0.10040959492059791 | validation: 0.12824898589387643]
	TIME [epoch: 10.3 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1052952747099942		[learning rate: 0.00019125]
	Learning Rate: 0.000191249
	LOSS [training: 0.1052952747099942 | validation: 0.13370265385629843]
	TIME [epoch: 10.3 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11050190223005218		[learning rate: 0.00019066]
	Learning Rate: 0.000190663
	LOSS [training: 0.11050190223005218 | validation: 0.12050652970204119]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1790.pth
	Model improved!!!
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10297383614098421		[learning rate: 0.00019008]
	Learning Rate: 0.000190079
	LOSS [training: 0.10297383614098421 | validation: 0.1438936251930755]
	TIME [epoch: 10.3 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10201794841644292		[learning rate: 0.0001895]
	Learning Rate: 0.000189496
	LOSS [training: 0.10201794841644292 | validation: 0.1441100683569829]
	TIME [epoch: 10.3 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10728787275498412		[learning rate: 0.00018892]
	Learning Rate: 0.000188915
	LOSS [training: 0.10728787275498412 | validation: 0.12655731312343874]
	TIME [epoch: 10.3 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10510745145148896		[learning rate: 0.00018834]
	Learning Rate: 0.000188336
	LOSS [training: 0.10510745145148896 | validation: 0.14415517863318988]
	TIME [epoch: 10.3 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1076716331463791		[learning rate: 0.00018776]
	Learning Rate: 0.000187759
	LOSS [training: 0.1076716331463791 | validation: 0.14244325576557423]
	TIME [epoch: 10.3 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1034695278265785		[learning rate: 0.00018718]
	Learning Rate: 0.000187183
	LOSS [training: 0.1034695278265785 | validation: 0.13393968501318157]
	TIME [epoch: 10.3 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10765924741027524		[learning rate: 0.00018661]
	Learning Rate: 0.000186609
	LOSS [training: 0.10765924741027524 | validation: 0.1538285090005125]
	TIME [epoch: 10.3 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11496268569249062		[learning rate: 0.00018604]
	Learning Rate: 0.000186037
	LOSS [training: 0.11496268569249062 | validation: 0.15095349020249021]
	TIME [epoch: 10.3 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11410780432986192		[learning rate: 0.00018547]
	Learning Rate: 0.000185467
	LOSS [training: 0.11410780432986192 | validation: 0.14191610055723897]
	TIME [epoch: 10.3 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10751550584550025		[learning rate: 0.0001849]
	Learning Rate: 0.000184898
	LOSS [training: 0.10751550584550025 | validation: 0.15262192760461735]
	TIME [epoch: 10.3 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10296063235844992		[learning rate: 0.00018433]
	Learning Rate: 0.000184332
	LOSS [training: 0.10296063235844992 | validation: 0.14551047218657384]
	TIME [epoch: 10.3 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10551815850062544		[learning rate: 0.00018377]
	Learning Rate: 0.000183767
	LOSS [training: 0.10551815850062544 | validation: 0.12655330673139376]
	TIME [epoch: 10.3 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0994750209495438		[learning rate: 0.0001832]
	Learning Rate: 0.000183203
	LOSS [training: 0.0994750209495438 | validation: 0.12653797094944585]
	TIME [epoch: 10.3 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10085346388578462		[learning rate: 0.00018264]
	Learning Rate: 0.000182642
	LOSS [training: 0.10085346388578462 | validation: 0.14590143978978148]
	TIME [epoch: 10.3 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09900046550847592		[learning rate: 0.00018208]
	Learning Rate: 0.000182082
	LOSS [training: 0.09900046550847592 | validation: 0.15405514691713654]
	TIME [epoch: 10.3 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10519194207874383		[learning rate: 0.00018152]
	Learning Rate: 0.000181524
	LOSS [training: 0.10519194207874383 | validation: 0.12058246095996382]
	TIME [epoch: 10.3 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10200887023747843		[learning rate: 0.00018097]
	Learning Rate: 0.000180967
	LOSS [training: 0.10200887023747843 | validation: 0.13974558978779597]
	TIME [epoch: 10.3 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10032671733752803		[learning rate: 0.00018041]
	Learning Rate: 0.000180412
	LOSS [training: 0.10032671733752803 | validation: 0.12958790843655074]
	TIME [epoch: 10.3 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10025628013376683		[learning rate: 0.00017986]
	Learning Rate: 0.000179859
	LOSS [training: 0.10025628013376683 | validation: 0.11465616920596414]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1809.pth
	Model improved!!!
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09959220714163401		[learning rate: 0.00017931]
	Learning Rate: 0.000179308
	LOSS [training: 0.09959220714163401 | validation: 0.11867447621599915]
	TIME [epoch: 10.3 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10859035997920699		[learning rate: 0.00017876]
	Learning Rate: 0.000178758
	LOSS [training: 0.10859035997920699 | validation: 0.1347969597226318]
	TIME [epoch: 10.3 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10238736092007908		[learning rate: 0.00017821]
	Learning Rate: 0.00017821
	LOSS [training: 0.10238736092007908 | validation: 0.12168137100096887]
	TIME [epoch: 10.3 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10149010826976959		[learning rate: 0.00017766]
	Learning Rate: 0.000177664
	LOSS [training: 0.10149010826976959 | validation: 0.12286701871138686]
	TIME [epoch: 10.3 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10367937317528053		[learning rate: 0.00017712]
	Learning Rate: 0.00017712
	LOSS [training: 0.10367937317528053 | validation: 0.13809589781401008]
	TIME [epoch: 10.3 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10356360434959429		[learning rate: 0.00017658]
	Learning Rate: 0.000176577
	LOSS [training: 0.10356360434959429 | validation: 0.11024352296259142]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1/tr_study5/model_tr_study5_r2_20240217_083135/states/model_tr_study5_1815.pth
	Model improved!!!
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08602776924591729		[learning rate: 0.00017604]
	Learning Rate: 0.000176035
	LOSS [training: 0.08602776924591729 | validation: 0.13794484657413372]
	TIME [epoch: 10.3 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09356416008814043		[learning rate: 0.0001755]
	Learning Rate: 0.000175496
	LOSS [training: 0.09356416008814043 | validation: 0.11473640443019019]
	TIME [epoch: 10.3 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10981549509123453		[learning rate: 0.00017496]
	Learning Rate: 0.000174958
	LOSS [training: 0.10981549509123453 | validation: 0.14430372623756646]
	TIME [epoch: 10.3 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10714114831916415		[learning rate: 0.00017442]
	Learning Rate: 0.000174421
	LOSS [training: 0.10714114831916415 | validation: 0.13881150334773015]
	TIME [epoch: 10.3 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1246494369033004		[learning rate: 0.00017389]
	Learning Rate: 0.000173887
	LOSS [training: 0.1246494369033004 | validation: 0.13485938397103034]
	TIME [epoch: 10.3 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09493759277566957		[learning rate: 0.00017335]
	Learning Rate: 0.000173354
	LOSS [training: 0.09493759277566957 | validation: 0.1374657735672405]
	TIME [epoch: 10.3 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10074863956386522		[learning rate: 0.00017282]
	Learning Rate: 0.000172822
	LOSS [training: 0.10074863956386522 | validation: 0.13984431262702116]
	TIME [epoch: 10.3 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10520893825720168		[learning rate: 0.00017229]
	Learning Rate: 0.000172293
	LOSS [training: 0.10520893825720168 | validation: 0.13349951819710906]
	TIME [epoch: 10.3 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10614676504936553		[learning rate: 0.00017176]
	Learning Rate: 0.000171764
	LOSS [training: 0.10614676504936553 | validation: 0.13913357452728586]
	TIME [epoch: 10.3 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10754821956127245		[learning rate: 0.00017124]
	Learning Rate: 0.000171238
	LOSS [training: 0.10754821956127245 | validation: 0.14585352669251753]
	TIME [epoch: 10.3 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11014653981287437		[learning rate: 0.00017071]
	Learning Rate: 0.000170713
	LOSS [training: 0.11014653981287437 | validation: 0.13882722709223594]
	TIME [epoch: 10.3 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10071781106543834		[learning rate: 0.00017019]
	Learning Rate: 0.00017019
	LOSS [training: 0.10071781106543834 | validation: 0.15365894929121346]
	TIME [epoch: 10.3 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11760088868601273		[learning rate: 0.00016967]
	Learning Rate: 0.000169668
	LOSS [training: 0.11760088868601273 | validation: 0.15113064229985057]
	TIME [epoch: 10.3 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10504928839504576		[learning rate: 0.00016915]
	Learning Rate: 0.000169148
	LOSS [training: 0.10504928839504576 | validation: 0.1503041597896243]
	TIME [epoch: 10.3 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1067476610571562		[learning rate: 0.00016863]
	Learning Rate: 0.000168629
	LOSS [training: 0.1067476610571562 | validation: 0.13169947797936415]
	TIME [epoch: 10.3 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11018551369952168		[learning rate: 0.00016811]
	Learning Rate: 0.000168112
	LOSS [training: 0.11018551369952168 | validation: 0.1447452974684173]
	TIME [epoch: 10.3 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09906758403696327		[learning rate: 0.0001676]
	Learning Rate: 0.000167597
	LOSS [training: 0.09906758403696327 | validation: 0.13960946362790605]
	TIME [epoch: 10.3 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13883538640929904		[learning rate: 0.00016708]
	Learning Rate: 0.000167083
	LOSS [training: 0.13883538640929904 | validation: 0.17015741119335637]
	TIME [epoch: 10.3 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10987951532473046		[learning rate: 0.00016657]
	Learning Rate: 0.000166571
	LOSS [training: 0.10987951532473046 | validation: 0.15015022614732168]
	TIME [epoch: 10.3 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10803962128733469		[learning rate: 0.00016606]
	Learning Rate: 0.000166061
	LOSS [training: 0.10803962128733469 | validation: 0.14771744545379217]
	TIME [epoch: 10.3 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1204004636446621		[learning rate: 0.00016555]
	Learning Rate: 0.000165552
	LOSS [training: 0.1204004636446621 | validation: 0.14381685291925284]
	TIME [epoch: 10.3 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12103965929714144		[learning rate: 0.00016504]
	Learning Rate: 0.000165044
	LOSS [training: 0.12103965929714144 | validation: 0.16502799665258613]
	TIME [epoch: 10.3 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12988320533839517		[learning rate: 0.00016454]
	Learning Rate: 0.000164538
	LOSS [training: 0.12988320533839517 | validation: 0.15363776968603246]
	TIME [epoch: 10.3 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11227847970778133		[learning rate: 0.00016403]
	Learning Rate: 0.000164034
	LOSS [training: 0.11227847970778133 | validation: 0.12871902880825192]
	TIME [epoch: 10.3 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10020757168304448		[learning rate: 0.00016353]
	Learning Rate: 0.000163531
	LOSS [training: 0.10020757168304448 | validation: 0.12519228795138948]
	TIME [epoch: 10.3 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09732917036824502		[learning rate: 0.00016303]
	Learning Rate: 0.00016303
	LOSS [training: 0.09732917036824502 | validation: 0.13036349121749058]
	TIME [epoch: 10.3 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10519613390294144		[learning rate: 0.00016253]
	Learning Rate: 0.00016253
	LOSS [training: 0.10519613390294144 | validation: 0.132636098361598]
	TIME [epoch: 10.3 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0947327136623111		[learning rate: 0.00016203]
	Learning Rate: 0.000162032
	LOSS [training: 0.0947327136623111 | validation: 0.14344490259138729]
	TIME [epoch: 10.3 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09468646271949234		[learning rate: 0.00016153]
	Learning Rate: 0.000161535
	LOSS [training: 0.09468646271949234 | validation: 0.13460553555708477]
	TIME [epoch: 10.3 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09474903731145914		[learning rate: 0.00016104]
	Learning Rate: 0.00016104
	LOSS [training: 0.09474903731145914 | validation: 0.11396161798787077]
	TIME [epoch: 10.3 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09344925972544191		[learning rate: 0.00016055]
	Learning Rate: 0.000160546
	LOSS [training: 0.09344925972544191 | validation: 0.1377562227160035]
	TIME [epoch: 10.3 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10357530136135865		[learning rate: 0.00016005]
	Learning Rate: 0.000160054
	LOSS [training: 0.10357530136135865 | validation: 0.1353823155456105]
	TIME [epoch: 10.3 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.103694911168749		[learning rate: 0.00015956]
	Learning Rate: 0.000159563
	LOSS [training: 0.103694911168749 | validation: 0.16983120622611034]
	TIME [epoch: 10.3 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10406489279834699		[learning rate: 0.00015907]
	Learning Rate: 0.000159074
	LOSS [training: 0.10406489279834699 | validation: 0.14826961871034114]
	TIME [epoch: 10.3 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09692685375297674		[learning rate: 0.00015859]
	Learning Rate: 0.000158587
	LOSS [training: 0.09692685375297674 | validation: 0.13175829144071727]
	TIME [epoch: 10.3 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10952325899764626		[learning rate: 0.0001581]
	Learning Rate: 0.000158101
	LOSS [training: 0.10952325899764626 | validation: 0.13222964205391324]
	TIME [epoch: 10.3 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.110328821464546		[learning rate: 0.00015762]
	Learning Rate: 0.000157616
	LOSS [training: 0.110328821464546 | validation: 0.12425943373182279]
	TIME [epoch: 10.3 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10334179617581174		[learning rate: 0.00015713]
	Learning Rate: 0.000157133
	LOSS [training: 0.10334179617581174 | validation: 0.13844432228334883]
	TIME [epoch: 10.3 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10510607673340377		[learning rate: 0.00015665]
	Learning Rate: 0.000156651
	LOSS [training: 0.10510607673340377 | validation: 0.14042617966879486]
	TIME [epoch: 10.3 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11665460353093642		[learning rate: 0.00015617]
	Learning Rate: 0.000156171
	LOSS [training: 0.11665460353093642 | validation: 0.14163437376636265]
	TIME [epoch: 10.3 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1061807645818155		[learning rate: 0.00015569]
	Learning Rate: 0.000155692
	LOSS [training: 0.1061807645818155 | validation: 0.1341977401326656]
	TIME [epoch: 10.3 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11939790967485679		[learning rate: 0.00015521]
	Learning Rate: 0.000155215
	LOSS [training: 0.11939790967485679 | validation: 0.15104038688538649]
	TIME [epoch: 10.3 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13192087068405697		[learning rate: 0.00015474]
	Learning Rate: 0.000154739
	LOSS [training: 0.13192087068405697 | validation: 0.14462511990867136]
	TIME [epoch: 10.3 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11751273518530556		[learning rate: 0.00015426]
	Learning Rate: 0.000154265
	LOSS [training: 0.11751273518530556 | validation: 0.147751321719486]
	TIME [epoch: 10.3 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12300811519007518		[learning rate: 0.00015379]
	Learning Rate: 0.000153792
	LOSS [training: 0.12300811519007518 | validation: 0.14859039638711238]
	TIME [epoch: 10.3 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1148380017235298		[learning rate: 0.00015332]
	Learning Rate: 0.00015332
	LOSS [training: 0.1148380017235298 | validation: 0.13951900486488641]
	TIME [epoch: 10.3 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10869245166874295		[learning rate: 0.00015285]
	Learning Rate: 0.00015285
	LOSS [training: 0.10869245166874295 | validation: 0.14623722532597075]
	TIME [epoch: 10.3 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09807430128692134		[learning rate: 0.00015238]
	Learning Rate: 0.000152382
	LOSS [training: 0.09807430128692134 | validation: 0.1598313184704536]
	TIME [epoch: 10.3 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1118544011521635		[learning rate: 0.00015191]
	Learning Rate: 0.000151915
	LOSS [training: 0.1118544011521635 | validation: 0.12723058547386337]
	TIME [epoch: 10.3 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10785390468917563		[learning rate: 0.00015145]
	Learning Rate: 0.000151449
	LOSS [training: 0.10785390468917563 | validation: 0.12750901251355898]
	TIME [epoch: 10.3 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10175543379119104		[learning rate: 0.00015098]
	Learning Rate: 0.000150985
	LOSS [training: 0.10175543379119104 | validation: 0.13324393851066463]
	TIME [epoch: 10.3 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10055736482232232		[learning rate: 0.00015052]
	Learning Rate: 0.000150522
	LOSS [training: 0.10055736482232232 | validation: 0.1417567166582838]
	TIME [epoch: 10.3 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10897637423380077		[learning rate: 0.00015006]
	Learning Rate: 0.000150061
	LOSS [training: 0.10897637423380077 | validation: 0.15528310171606996]
	TIME [epoch: 10.3 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12822709299210672		[learning rate: 0.0001496]
	Learning Rate: 0.000149601
	LOSS [training: 0.12822709299210672 | validation: 0.14255276719710983]
	TIME [epoch: 10.3 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11883686008933769		[learning rate: 0.00014914]
	Learning Rate: 0.000149142
	LOSS [training: 0.11883686008933769 | validation: 0.16712968034819425]
	TIME [epoch: 10.3 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11878697788245376		[learning rate: 0.00014868]
	Learning Rate: 0.000148685
	LOSS [training: 0.11878697788245376 | validation: 0.14641449882652519]
	TIME [epoch: 10.3 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10951705955042848		[learning rate: 0.00014823]
	Learning Rate: 0.000148229
	LOSS [training: 0.10951705955042848 | validation: 0.13510740626599332]
	TIME [epoch: 10.3 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10937280729279841		[learning rate: 0.00014777]
	Learning Rate: 0.000147775
	LOSS [training: 0.10937280729279841 | validation: 0.15154748281187444]
	TIME [epoch: 10.3 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10213106525012214		[learning rate: 0.00014732]
	Learning Rate: 0.000147322
	LOSS [training: 0.10213106525012214 | validation: 0.12339050020875761]
	TIME [epoch: 10.3 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09376484882416289		[learning rate: 0.00014687]
	Learning Rate: 0.00014687
	LOSS [training: 0.09376484882416289 | validation: 0.11904483079156056]
	TIME [epoch: 10.3 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10019400424327932		[learning rate: 0.00014642]
	Learning Rate: 0.00014642
	LOSS [training: 0.10019400424327932 | validation: 0.13285969228189454]
	TIME [epoch: 10.3 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09369205406638985		[learning rate: 0.00014597]
	Learning Rate: 0.000145971
	LOSS [training: 0.09369205406638985 | validation: 0.12696653454054233]
	TIME [epoch: 10.3 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09177815404917233		[learning rate: 0.00014552]
	Learning Rate: 0.000145524
	LOSS [training: 0.09177815404917233 | validation: 0.13474933946404832]
	TIME [epoch: 10.3 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09782304103724807		[learning rate: 0.00014508]
	Learning Rate: 0.000145077
	LOSS [training: 0.09782304103724807 | validation: 0.13259752446731807]
	TIME [epoch: 10.3 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10685077268156466		[learning rate: 0.00014463]
	Learning Rate: 0.000144633
	LOSS [training: 0.10685077268156466 | validation: 0.13224701658578034]
	TIME [epoch: 10.3 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09424191791258352		[learning rate: 0.00014419]
	Learning Rate: 0.000144189
	LOSS [training: 0.09424191791258352 | validation: 0.13302581704114308]
	TIME [epoch: 10.3 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1004565779013237		[learning rate: 0.00014375]
	Learning Rate: 0.000143747
	LOSS [training: 0.1004565779013237 | validation: 0.1431022776815919]
	TIME [epoch: 10.3 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1036234943233306		[learning rate: 0.00014331]
	Learning Rate: 0.000143307
	LOSS [training: 0.1036234943233306 | validation: 0.1320055001404949]
	TIME [epoch: 10.3 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09232988108386533		[learning rate: 0.00014287]
	Learning Rate: 0.000142867
	LOSS [training: 0.09232988108386533 | validation: 0.13325309446596467]
	TIME [epoch: 10.3 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10817680439667843		[learning rate: 0.00014243]
	Learning Rate: 0.00014243
	LOSS [training: 0.10817680439667843 | validation: 0.13620782857492647]
	TIME [epoch: 10.3 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10591644443751372		[learning rate: 0.00014199]
	Learning Rate: 0.000141993
	LOSS [training: 0.10591644443751372 | validation: 0.13449991099079248]
	TIME [epoch: 10.3 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10444441059577343		[learning rate: 0.00014156]
	Learning Rate: 0.000141558
	LOSS [training: 0.10444441059577343 | validation: 0.15217614459461093]
	TIME [epoch: 10.3 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09639008860815748		[learning rate: 0.00014112]
	Learning Rate: 0.000141124
	LOSS [training: 0.09639008860815748 | validation: 0.1505044403728171]
	TIME [epoch: 10.3 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09779520477256307		[learning rate: 0.00014069]
	Learning Rate: 0.000140691
	LOSS [training: 0.09779520477256307 | validation: 0.13465079307799405]
	TIME [epoch: 10.3 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10275205946897073		[learning rate: 0.00014026]
	Learning Rate: 0.00014026
	LOSS [training: 0.10275205946897073 | validation: 0.14368296345549098]
	TIME [epoch: 10.3 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09658472473488151		[learning rate: 0.00013983]
	Learning Rate: 0.00013983
	LOSS [training: 0.09658472473488151 | validation: 0.13872877208292064]
	TIME [epoch: 10.3 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09188201239222718		[learning rate: 0.0001394]
	Learning Rate: 0.000139401
	LOSS [training: 0.09188201239222718 | validation: 0.12861242614225796]
	TIME [epoch: 10.3 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1056457060135146		[learning rate: 0.00013897]
	Learning Rate: 0.000138974
	LOSS [training: 0.1056457060135146 | validation: 0.14674801223233572]
	TIME [epoch: 10.3 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1038288070079925		[learning rate: 0.00013855]
	Learning Rate: 0.000138548
	LOSS [training: 0.1038288070079925 | validation: 0.13409315429610977]
	TIME [epoch: 10.3 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10004872229708402		[learning rate: 0.00013812]
	Learning Rate: 0.000138123
	LOSS [training: 0.10004872229708402 | validation: 0.1392723802329961]
	TIME [epoch: 10.3 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10053978619822615		[learning rate: 0.0001377]
	Learning Rate: 0.0001377
	LOSS [training: 0.10053978619822615 | validation: 0.13746088050271416]
	TIME [epoch: 10.3 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10378304508739422		[learning rate: 0.00013728]
	Learning Rate: 0.000137278
	LOSS [training: 0.10378304508739422 | validation: 0.15627665404186245]
	TIME [epoch: 10.3 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12038301674088356		[learning rate: 0.00013686]
	Learning Rate: 0.000136857
	LOSS [training: 0.12038301674088356 | validation: 0.13600506870505255]
	TIME [epoch: 10.3 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10752514536544192		[learning rate: 0.00013644]
	Learning Rate: 0.000136437
	LOSS [training: 0.10752514536544192 | validation: 0.1542129766789836]
	TIME [epoch: 10.3 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10580309195810522		[learning rate: 0.00013602]
	Learning Rate: 0.000136019
	LOSS [training: 0.10580309195810522 | validation: 0.14249475523393104]
	TIME [epoch: 10.3 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1016602281199304		[learning rate: 0.0001356]
	Learning Rate: 0.000135602
	LOSS [training: 0.1016602281199304 | validation: 0.14089309749693057]
	TIME [epoch: 10.3 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09323258717417152		[learning rate: 0.00013519]
	Learning Rate: 0.000135186
	LOSS [training: 0.09323258717417152 | validation: 0.12375695316093877]
	TIME [epoch: 10.3 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08614176995895081		[learning rate: 0.00013477]
	Learning Rate: 0.000134772
	LOSS [training: 0.08614176995895081 | validation: 0.12887510116772508]
	TIME [epoch: 10.3 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09493128707134625		[learning rate: 0.00013436]
	Learning Rate: 0.000134359
	LOSS [training: 0.09493128707134625 | validation: 0.15466135458332084]
	TIME [epoch: 10.3 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09777232747217819		[learning rate: 0.00013395]
	Learning Rate: 0.000133947
	LOSS [training: 0.09777232747217819 | validation: 0.1390718296452041]
	TIME [epoch: 10.3 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10215954132838061		[learning rate: 0.00013354]
	Learning Rate: 0.000133536
	LOSS [training: 0.10215954132838061 | validation: 0.13831283067284605]
	TIME [epoch: 10.3 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09123819324526093		[learning rate: 0.00013313]
	Learning Rate: 0.000133127
	LOSS [training: 0.09123819324526093 | validation: 0.128292298770807]
	TIME [epoch: 10.3 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09622979507068292		[learning rate: 0.00013272]
	Learning Rate: 0.000132719
	LOSS [training: 0.09622979507068292 | validation: 0.13314716765679388]
	TIME [epoch: 10.3 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10415733107639699		[learning rate: 0.00013231]
	Learning Rate: 0.000132312
	LOSS [training: 0.10415733107639699 | validation: 0.1382952528848365]
	TIME [epoch: 10.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09929441172337208		[learning rate: 0.00013191]
	Learning Rate: 0.000131907
	LOSS [training: 0.09929441172337208 | validation: 0.15555534375167643]
	TIME [epoch: 10.3 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09897793067304902		[learning rate: 0.0001315]
	Learning Rate: 0.000131502
	LOSS [training: 0.09897793067304902 | validation: 0.14018701611458548]
	TIME [epoch: 10.3 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09617485009252187		[learning rate: 0.0001311]
	Learning Rate: 0.000131099
	LOSS [training: 0.09617485009252187 | validation: 0.14931106207845166]
	TIME [epoch: 10.3 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10199172722713246		[learning rate: 0.0001307]
	Learning Rate: 0.000130697
	LOSS [training: 0.10199172722713246 | validation: 0.16473592402010936]
	TIME [epoch: 10.3 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11113969387886373		[learning rate: 0.0001303]
	Learning Rate: 0.000130297
	LOSS [training: 0.11113969387886373 | validation: 0.15580152941010672]
	TIME [epoch: 10.3 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11108135857373964		[learning rate: 0.0001299]
	Learning Rate: 0.000129897
	LOSS [training: 0.11108135857373964 | validation: 0.15641345493469336]
	TIME [epoch: 10.3 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12083410093087081		[learning rate: 0.0001295]
	Learning Rate: 0.000129499
	LOSS [training: 0.12083410093087081 | validation: 0.1858681774812817]
	TIME [epoch: 10.3 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12004887500462205		[learning rate: 0.0001291]
	Learning Rate: 0.000129102
	LOSS [training: 0.12004887500462205 | validation: 0.17274049805809774]
	TIME [epoch: 10.3 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1212874129749999		[learning rate: 0.00012871]
	Learning Rate: 0.000128706
	LOSS [training: 0.1212874129749999 | validation: 0.1713125069823888]
	TIME [epoch: 10.3 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1285515487028299		[learning rate: 0.00012831]
	Learning Rate: 0.000128312
	LOSS [training: 0.1285515487028299 | validation: 0.18838731187958835]
	TIME [epoch: 10.3 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12051320774138705		[learning rate: 0.00012792]
	Learning Rate: 0.000127918
	LOSS [training: 0.12051320774138705 | validation: 0.1717777081805548]
	TIME [epoch: 10.3 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11886711615618503		[learning rate: 0.00012753]
	Learning Rate: 0.000127526
	LOSS [training: 0.11886711615618503 | validation: 0.17831419770728268]
	TIME [epoch: 10.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.12294795540285572		[learning rate: 0.00012714]
	Learning Rate: 0.000127135
	LOSS [training: 0.12294795540285572 | validation: 0.14485607625212119]
	TIME [epoch: 10.3 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10578521289125083		[learning rate: 0.00012675]
	Learning Rate: 0.000126746
	LOSS [training: 0.10578521289125083 | validation: 0.1479710943638207]
	TIME [epoch: 10.3 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10988641259560752		[learning rate: 0.00012636]
	Learning Rate: 0.000126357
	LOSS [training: 0.10988641259560752 | validation: 0.14870945424732396]
	TIME [epoch: 10.3 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10199722155163823		[learning rate: 0.00012597]
	Learning Rate: 0.00012597
	LOSS [training: 0.10199722155163823 | validation: 0.13288529559087386]
	TIME [epoch: 10.3 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1171750588115894		[learning rate: 0.00012558]
	Learning Rate: 0.000125584
	LOSS [training: 0.1171750588115894 | validation: 0.1680731079809484]
	TIME [epoch: 10.3 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10788444861289488		[learning rate: 0.0001252]
	Learning Rate: 0.000125199
	LOSS [training: 0.10788444861289488 | validation: 0.17014154423384154]
	TIME [epoch: 10.3 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11747839441305123		[learning rate: 0.00012481]
	Learning Rate: 0.000124815
	LOSS [training: 0.11747839441305123 | validation: 0.14318014627665362]
	TIME [epoch: 10.3 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11056571347656605		[learning rate: 0.00012443]
	Learning Rate: 0.000124432
	LOSS [training: 0.11056571347656605 | validation: 0.1320088425174158]
	TIME [epoch: 10.3 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10283134174744094		[learning rate: 0.00012405]
	Learning Rate: 0.000124051
	LOSS [training: 0.10283134174744094 | validation: 0.14472752303593553]
	TIME [epoch: 10.3 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10859277199823932		[learning rate: 0.00012367]
	Learning Rate: 0.000123671
	LOSS [training: 0.10859277199823932 | validation: 0.14984542075944984]
	TIME [epoch: 10.3 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1046190527152453		[learning rate: 0.00012329]
	Learning Rate: 0.000123292
	LOSS [training: 0.1046190527152453 | validation: 0.1451140197538917]
	TIME [epoch: 10.3 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10117503207439307		[learning rate: 0.00012291]
	Learning Rate: 0.000122914
	LOSS [training: 0.10117503207439307 | validation: 0.14621129761175056]
	TIME [epoch: 10.3 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09483516895233952		[learning rate: 0.00012254]
	Learning Rate: 0.000122537
	LOSS [training: 0.09483516895233952 | validation: 0.14959486479003592]
	TIME [epoch: 10.3 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10406376454363335		[learning rate: 0.00012216]
	Learning Rate: 0.000122161
	LOSS [training: 0.10406376454363335 | validation: 0.144044128949261]
	TIME [epoch: 10.3 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10401648041973902		[learning rate: 0.00012179]
	Learning Rate: 0.000121787
	LOSS [training: 0.10401648041973902 | validation: 0.1493310527259924]
	TIME [epoch: 10.3 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1057337741043766		[learning rate: 0.00012141]
	Learning Rate: 0.000121413
	LOSS [training: 0.1057337741043766 | validation: 0.15006889123961756]
	TIME [epoch: 10.3 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10482100632471851		[learning rate: 0.00012104]
	Learning Rate: 0.000121041
	LOSS [training: 0.10482100632471851 | validation: 0.12771673002210776]
	TIME [epoch: 10.3 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09875468636625015		[learning rate: 0.00012067]
	Learning Rate: 0.00012067
	LOSS [training: 0.09875468636625015 | validation: 0.15722617072425973]
	TIME [epoch: 10.3 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1079219758066116		[learning rate: 0.0001203]
	Learning Rate: 0.0001203
	LOSS [training: 0.1079219758066116 | validation: 0.15245655832857619]
	TIME [epoch: 10.3 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10592992095211977		[learning rate: 0.00011993]
	Learning Rate: 0.000119932
	LOSS [training: 0.10592992095211977 | validation: 0.14795700577188686]
	TIME [epoch: 10.3 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10293320750936427		[learning rate: 0.00011956]
	Learning Rate: 0.000119564
	LOSS [training: 0.10293320750936427 | validation: 0.1460331797629252]
	TIME [epoch: 10.3 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10893666492971607		[learning rate: 0.0001192]
	Learning Rate: 0.000119197
	LOSS [training: 0.10893666492971607 | validation: 0.15863294512547518]
	TIME [epoch: 10.3 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1024112364534208		[learning rate: 0.00011883]
	Learning Rate: 0.000118832
	LOSS [training: 0.1024112364534208 | validation: 0.16740005931431165]
	TIME [epoch: 10.3 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11460312959084669		[learning rate: 0.00011847]
	Learning Rate: 0.000118468
	LOSS [training: 0.11460312959084669 | validation: 0.16298386297620496]
	TIME [epoch: 10.3 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11826680837876502		[learning rate: 0.0001181]
	Learning Rate: 0.000118105
	LOSS [training: 0.11826680837876502 | validation: 0.16496785428515362]
	TIME [epoch: 10.3 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1111963921232407		[learning rate: 0.00011774]
	Learning Rate: 0.000117743
	LOSS [training: 0.1111963921232407 | validation: 0.1424616760696946]
	TIME [epoch: 10.3 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11160805067532893		[learning rate: 0.00011738]
	Learning Rate: 0.000117382
	LOSS [training: 0.11160805067532893 | validation: 0.14929263003717905]
	TIME [epoch: 10.3 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10466998390477551		[learning rate: 0.00011702]
	Learning Rate: 0.000117022
	LOSS [training: 0.10466998390477551 | validation: 0.1450736753492564]
	TIME [epoch: 10.3 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10153702808005091		[learning rate: 0.00011666]
	Learning Rate: 0.000116663
	LOSS [training: 0.10153702808005091 | validation: 0.15020796151404806]
	TIME [epoch: 10.3 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1072930251843784		[learning rate: 0.00011631]
	Learning Rate: 0.000116305
	LOSS [training: 0.1072930251843784 | validation: 0.14609564783324344]
	TIME [epoch: 10.3 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10421390837948367		[learning rate: 0.00011595]
	Learning Rate: 0.000115949
	LOSS [training: 0.10421390837948367 | validation: 0.11515098957876038]
	TIME [epoch: 10.3 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1023562543298242		[learning rate: 0.00011559]
	Learning Rate: 0.000115593
	LOSS [training: 0.1023562543298242 | validation: 0.12165104824368086]
	TIME [epoch: 10.3 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09890612715531016		[learning rate: 0.00011524]
	Learning Rate: 0.000115239
	LOSS [training: 0.09890612715531016 | validation: 0.1319582560443561]
	TIME [epoch: 10.3 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10146830429821987		[learning rate: 0.00011489]
	Learning Rate: 0.000114886
	LOSS [training: 0.10146830429821987 | validation: 0.13309674962589582]
	TIME [epoch: 10.3 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10361776200825083		[learning rate: 0.00011453]
	Learning Rate: 0.000114534
	LOSS [training: 0.10361776200825083 | validation: 0.13755231858817918]
	TIME [epoch: 10.3 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10132485796271355		[learning rate: 0.00011418]
	Learning Rate: 0.000114183
	LOSS [training: 0.10132485796271355 | validation: 0.14423547976702206]
	TIME [epoch: 10.3 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1029331396239797		[learning rate: 0.00011383]
	Learning Rate: 0.000113833
	LOSS [training: 0.1029331396239797 | validation: 0.13950262794717389]
	TIME [epoch: 10.3 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1043022782389359		[learning rate: 0.00011348]
	Learning Rate: 0.000113484
	LOSS [training: 0.1043022782389359 | validation: 0.13322671856879476]
	TIME [epoch: 10.3 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10687021650277298		[learning rate: 0.00011314]
	Learning Rate: 0.000113136
	LOSS [training: 0.10687021650277298 | validation: 0.14469943845849184]
	TIME [epoch: 10.3 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11864084806580148		[learning rate: 0.00011279]
	Learning Rate: 0.000112789
	LOSS [training: 0.11864084806580148 | validation: 0.13412112558594438]
	TIME [epoch: 10.3 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11431126854933467		[learning rate: 0.00011244]
	Learning Rate: 0.000112443
	LOSS [training: 0.11431126854933467 | validation: 0.13465738908965064]
	TIME [epoch: 10.3 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11117437284364598		[learning rate: 0.0001121]
	Learning Rate: 0.000112099
	LOSS [training: 0.11117437284364598 | validation: 0.1321508010228477]
	TIME [epoch: 10.3 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10447436596834958		[learning rate: 0.00011175]
	Learning Rate: 0.000111755
	LOSS [training: 0.10447436596834958 | validation: 0.1532941829973237]
	TIME [epoch: 10.3 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10519412351316604		[learning rate: 0.00011141]
	Learning Rate: 0.000111412
	LOSS [training: 0.10519412351316604 | validation: 0.13620621259498894]
	TIME [epoch: 10.3 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09918965195363491		[learning rate: 0.00011107]
	Learning Rate: 0.000111071
	LOSS [training: 0.09918965195363491 | validation: 0.1439745721380193]
	TIME [epoch: 10.3 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09981468536694298		[learning rate: 0.00011073]
	Learning Rate: 0.00011073
	LOSS [training: 0.09981468536694298 | validation: 0.1352124728117061]
	TIME [epoch: 10.3 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10733937244864489		[learning rate: 0.00011039]
	Learning Rate: 0.000110391
	LOSS [training: 0.10733937244864489 | validation: 0.14546636513148245]
	TIME [epoch: 10.3 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11376908029676562		[learning rate: 0.00011005]
	Learning Rate: 0.000110053
	LOSS [training: 0.11376908029676562 | validation: 0.14635230053167425]
	TIME [epoch: 10.3 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10551369477887063		[learning rate: 0.00010972]
	Learning Rate: 0.000109715
	LOSS [training: 0.10551369477887063 | validation: 0.13434362992377075]
	TIME [epoch: 10.3 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10408583784998085		[learning rate: 0.00010938]
	Learning Rate: 0.000109379
	LOSS [training: 0.10408583784998085 | validation: 0.13358188438387386]
	TIME [epoch: 10.3 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1077768053192341		[learning rate: 0.00010904]
	Learning Rate: 0.000109044
	LOSS [training: 0.1077768053192341 | validation: 0.1366351085718619]
	TIME [epoch: 10.3 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10179758542434807		[learning rate: 0.00010871]
	Learning Rate: 0.000108709
	LOSS [training: 0.10179758542434807 | validation: 0.13610520976258347]
	TIME [epoch: 10.3 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09915262485708434		[learning rate: 0.00010838]
	Learning Rate: 0.000108376
	LOSS [training: 0.09915262485708434 | validation: 0.14007111383978468]
	TIME [epoch: 10.3 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10438627484692217		[learning rate: 0.00010804]
	Learning Rate: 0.000108044
	LOSS [training: 0.10438627484692217 | validation: 0.13951956283403577]
	TIME [epoch: 10.3 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.11138081157562767		[learning rate: 0.00010771]
	Learning Rate: 0.000107713
	LOSS [training: 0.11138081157562767 | validation: 0.14589788952173854]
	TIME [epoch: 10.3 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09733833879747453		[learning rate: 0.00010738]
	Learning Rate: 0.000107382
	LOSS [training: 0.09733833879747453 | validation: 0.14775239632510437]
	TIME [epoch: 10.3 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10108675992031191		[learning rate: 0.00010705]
	Learning Rate: 0.000107053
	LOSS [training: 0.10108675992031191 | validation: 0.12798094864767895]
	TIME [epoch: 10.3 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.0967731340007798		[learning rate: 0.00010673]
	Learning Rate: 0.000106725
	LOSS [training: 0.0967731340007798 | validation: 0.11128204308321632]
	TIME [epoch: 10.3 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09335708980584434		[learning rate: 0.0001064]
	Learning Rate: 0.000106398
	LOSS [training: 0.09335708980584434 | validation: 0.13416561610321162]
	TIME [epoch: 10.3 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10077438804645747		[learning rate: 0.00010607]
	Learning Rate: 0.000106072
	LOSS [training: 0.10077438804645747 | validation: 0.13284131579628874]
	TIME [epoch: 10.3 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09690709214516449		[learning rate: 0.00010575]
	Learning Rate: 0.000105747
	LOSS [training: 0.09690709214516449 | validation: 0.12832130135231082]
	TIME [epoch: 10.3 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1029234947078411		[learning rate: 0.00010542]
	Learning Rate: 0.000105423
	LOSS [training: 0.1029234947078411 | validation: 0.13112884187378782]
	TIME [epoch: 10.3 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09871382024333561		[learning rate: 0.0001051]
	Learning Rate: 0.000105099
	LOSS [training: 0.09871382024333561 | validation: 0.12553767484274775]
	TIME [epoch: 10.3 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10180920377391552		[learning rate: 0.00010478]
	Learning Rate: 0.000104777
	LOSS [training: 0.10180920377391552 | validation: 0.14373934262151633]
	TIME [epoch: 10.3 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.10186261778367053		[learning rate: 0.00010446]
	Learning Rate: 0.000104456
	LOSS [training: 0.10186261778367053 | validation: 0.11943154063958225]
	TIME [epoch: 10.3 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09761439061863376		[learning rate: 0.00010414]
	Learning Rate: 0.000104136
	LOSS [training: 0.09761439061863376 | validation: 0.12245286841048929]
	TIME [epoch: 10.3 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09222988083439473		[learning rate: 0.00010382]
	Learning Rate: 0.000103817
	LOSS [training: 0.09222988083439473 | validation: 0.12313462220781904]
	TIME [epoch: 10.3 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09204432975358329		[learning rate: 0.0001035]
	Learning Rate: 0.000103498
	LOSS [training: 0.09204432975358329 | validation: 0.12695990804443344]
	TIME [epoch: 10.3 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09569280191461163		[learning rate: 0.00010318]
	Learning Rate: 0.000103181
	LOSS [training: 0.09569280191461163 | validation: 0.12975457207516752]
	TIME [epoch: 10.3 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09890876994651769		[learning rate: 0.00010286]
	Learning Rate: 0.000102865
	LOSS [training: 0.09890876994651769 | validation: 0.14155485369346876]
	TIME [epoch: 10.3 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09166342522796445		[learning rate: 0.00010255]
	Learning Rate: 0.000102549
	LOSS [training: 0.09166342522796445 | validation: 0.12742990295324608]
	TIME [epoch: 10.3 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09424632907183303		[learning rate: 0.00010224]
	Learning Rate: 0.000102235
	LOSS [training: 0.09424632907183303 | validation: 0.12445447320052289]
	TIME [epoch: 10.3 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09445686155482427		[learning rate: 0.00010192]
	Learning Rate: 0.000101922
	LOSS [training: 0.09445686155482427 | validation: 0.13590600948566103]
	TIME [epoch: 10.3 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09499805733309055		[learning rate: 0.00010161]
	Learning Rate: 0.000101609
	LOSS [training: 0.09499805733309055 | validation: 0.12882655392577783]
	TIME [epoch: 10.3 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09613978359149722		[learning rate: 0.0001013]
	Learning Rate: 0.000101298
	LOSS [training: 0.09613978359149722 | validation: 0.11158293379577724]
	TIME [epoch: 10.3 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.08914004978004839		[learning rate: 0.00010099]
	Learning Rate: 0.000100987
	LOSS [training: 0.08914004978004839 | validation: 0.12697145638962976]
	TIME [epoch: 10.3 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09681286583018246		[learning rate: 0.00010068]
	Learning Rate: 0.000100678
	LOSS [training: 0.09681286583018246 | validation: 0.12847656081223563]
	TIME [epoch: 10.3 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09179934449851784		[learning rate: 0.00010037]
	Learning Rate: 0.000100369
	LOSS [training: 0.09179934449851784 | validation: 0.12624025608274866]
	TIME [epoch: 10.3 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.09979645025563386		[learning rate: 0.00010006]
	Learning Rate: 0.000100061
	LOSS [training: 0.09979645025563386 | validation: 0.12727999464000586]
	TIME [epoch: 10.3 sec]
Finished training in 20721.439 seconds.
